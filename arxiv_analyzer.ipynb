{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa14517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.8/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: feedparser in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.8/site-packages (from arxiv) (6.0.10)\r\n",
      "Requirement already satisfied: sgmllib3k in /home/alatus/anaconda3/envs/10_days_AI/lib/python3.8/site-packages (from feedparser->arxiv) (1.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe9f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead6ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = ['deepfake', 'deepfake?', '\"deep-fake\"', '\"deep fake\"', '\"voice cloning\"', '\"voice conversion\"', 'fake AND voice', '\"image synthesis\"', '\"image generation\" AND \"face\"', '\"video forgery\"', '\"fake video\"']\n",
    "#keyphrases = []\n",
    "\n",
    "\n",
    "\n",
    "#?? \"video editing\"\n",
    "#?? \"image inpainting\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbf74b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e5c582af8847b0b1203b0b11b678ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7f2c11b075401384487cb6b30db27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482a5765fc744fc0b7c48934a909ca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09565494c0db497b9d9991865b869708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34664b591554f698c41f32a0afb3429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ba0d42610f47a2a1ffcbf7d269e4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca830bc2f4084918893eb789769e5d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dac2470de54d6fa4251f9485b1ed7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3f799463694bea9a935b5a8e5a3666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cedb840b8841ec86d1ef55c03945c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073f05bc96d4763965d68de7be564ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f597a195bc4c2799c52eb9e5f3a1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>published</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-10-03 14:20:01+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2210.00957v1</td>\n",
       "      <td>UnGANable: Defending Against GAN-based Face Ma...</td>\n",
       "      <td>cs.CR</td>\n",
       "      <td>[cs.CR, cs.CV, cs.LG]</td>\n",
       "      <td>Deepfakes pose severe threats of visual misinf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-01 20:37:24+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2210.00361v1</td>\n",
       "      <td>Evaluation of Pre-Trained CNN Models for Geogr...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Thanks to the remarkable advances in generativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 13:46:29+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2209.14098v1</td>\n",
       "      <td>Deepfake audio detection by speaker verification</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>[cs.SD, cs.CV, eess.AS]</td>\n",
       "      <td>Thanks to recent advances in deep learning, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28 02:46:04+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2209.13792v1</td>\n",
       "      <td>A Machine Learning Approach for DeepFake Detec...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, I.4.7; I.5.0]</td>\n",
       "      <td>With the spread of DeepFake techniques, this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27 10:15:46+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2209.13289v1</td>\n",
       "      <td>When Handcrafted Features and Deep Features Me...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>The accelerated growth in synthetic visual med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-09 14:37:17+00:00</th>\n",
       "      <td>http://arxiv.org/abs/2001.03024v2</td>\n",
       "      <td>DeeperForensics-1.0: A Large-Scale Dataset for...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>We present our on-going effort of constructing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21 18:29:57+00:00</th>\n",
       "      <td>http://arxiv.org/abs/1906.09288v1</td>\n",
       "      <td>Hiding Faces in Plain Sight: Disrupting AI Fac...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>Recent years have seen fast development in syn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25 07:09:14+00:00</th>\n",
       "      <td>http://arxiv.org/abs/1901.08759v1</td>\n",
       "      <td>Misleading Metadata Detection on YouTube</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL, cs.CV, eess.IV]</td>\n",
       "      <td>YouTube is the leading social media platform f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 19:25:02+00:00</th>\n",
       "      <td>http://arxiv.org/abs/1803.09803v2</td>\n",
       "      <td>Generating Talking Face Landmarks from Speech</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>The presence of a corresponding talking face h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24 23:12:44+00:00</th>\n",
       "      <td>http://arxiv.org/abs/1803.09179v1</td>\n",
       "      <td>FaceForensics: A Large-scale Video Dataset for...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>With recent advances in computer vision and gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1645 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         url  \\\n",
       "published                                                      \n",
       "2022-10-03 14:20:01+00:00  http://arxiv.org/abs/2210.00957v1   \n",
       "2022-10-01 20:37:24+00:00  http://arxiv.org/abs/2210.00361v1   \n",
       "2022-09-28 13:46:29+00:00  http://arxiv.org/abs/2209.14098v1   \n",
       "2022-09-28 02:46:04+00:00  http://arxiv.org/abs/2209.13792v1   \n",
       "2022-09-27 10:15:46+00:00  http://arxiv.org/abs/2209.13289v1   \n",
       "...                                                      ...   \n",
       "2020-01-09 14:37:17+00:00  http://arxiv.org/abs/2001.03024v2   \n",
       "2019-06-21 18:29:57+00:00  http://arxiv.org/abs/1906.09288v1   \n",
       "2019-01-25 07:09:14+00:00  http://arxiv.org/abs/1901.08759v1   \n",
       "2018-03-26 19:25:02+00:00  http://arxiv.org/abs/1803.09803v2   \n",
       "2018-03-24 23:12:44+00:00  http://arxiv.org/abs/1803.09179v1   \n",
       "\n",
       "                                                                       title  \\\n",
       "published                                                                      \n",
       "2022-10-03 14:20:01+00:00  UnGANable: Defending Against GAN-based Face Ma...   \n",
       "2022-10-01 20:37:24+00:00  Evaluation of Pre-Trained CNN Models for Geogr...   \n",
       "2022-09-28 13:46:29+00:00   Deepfake audio detection by speaker verification   \n",
       "2022-09-28 02:46:04+00:00  A Machine Learning Approach for DeepFake Detec...   \n",
       "2022-09-27 10:15:46+00:00  When Handcrafted Features and Deep Features Me...   \n",
       "...                                                                      ...   \n",
       "2020-01-09 14:37:17+00:00  DeeperForensics-1.0: A Large-Scale Dataset for...   \n",
       "2019-06-21 18:29:57+00:00  Hiding Faces in Plain Sight: Disrupting AI Fac...   \n",
       "2019-01-25 07:09:14+00:00           Misleading Metadata Detection on YouTube   \n",
       "2018-03-26 19:25:02+00:00      Generating Talking Face Landmarks from Speech   \n",
       "2018-03-24 23:12:44+00:00  FaceForensics: A Large-scale Video Dataset for...   \n",
       "\n",
       "                          primary_category               categories  \\\n",
       "published                                                             \n",
       "2022-10-03 14:20:01+00:00            cs.CR    [cs.CR, cs.CV, cs.LG]   \n",
       "2022-10-01 20:37:24+00:00            cs.CV                  [cs.CV]   \n",
       "2022-09-28 13:46:29+00:00            cs.SD  [cs.SD, cs.CV, eess.AS]   \n",
       "2022-09-28 02:46:04+00:00            cs.CV    [cs.CV, I.4.7; I.5.0]   \n",
       "2022-09-27 10:15:46+00:00            cs.CV           [cs.CV, cs.LG]   \n",
       "...                                    ...                      ...   \n",
       "2020-01-09 14:37:17+00:00            cs.CV           [cs.CV, cs.LG]   \n",
       "2019-06-21 18:29:57+00:00            cs.CV                  [cs.CV]   \n",
       "2019-01-25 07:09:14+00:00            cs.CL  [cs.CL, cs.CV, eess.IV]   \n",
       "2018-03-26 19:25:02+00:00            cs.CV                  [cs.CV]   \n",
       "2018-03-24 23:12:44+00:00            cs.CV                  [cs.CV]   \n",
       "\n",
       "                                                                    abstract  \n",
       "published                                                                     \n",
       "2022-10-03 14:20:01+00:00  Deepfakes pose severe threats of visual misinf...  \n",
       "2022-10-01 20:37:24+00:00  Thanks to the remarkable advances in generativ...  \n",
       "2022-09-28 13:46:29+00:00  Thanks to recent advances in deep learning, so...  \n",
       "2022-09-28 02:46:04+00:00  With the spread of DeepFake techniques, this t...  \n",
       "2022-09-27 10:15:46+00:00  The accelerated growth in synthetic visual med...  \n",
       "...                                                                      ...  \n",
       "2020-01-09 14:37:17+00:00  We present our on-going effort of constructing...  \n",
       "2019-06-21 18:29:57+00:00  Recent years have seen fast development in syn...  \n",
       "2019-01-25 07:09:14+00:00  YouTube is the leading social media platform f...  \n",
       "2018-03-26 19:25:02+00:00  The presence of a corresponding talking face h...  \n",
       "2018-03-24 23:12:44+00:00  With recent advances in computer vision and gr...  \n",
       "\n",
       "[1645 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def do_search(keyphrase):\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "      query = keyphrase,\n",
    "      #id_list: List[str] = [],\n",
    "      max_results = 10000,\n",
    "      sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "      #sort_by: SortCriterion = SortCriterion.Relevanvce,\n",
    "      sort_order = arxiv.SortOrder.Descending)\n",
    "    \n",
    "    return search\n",
    "\n",
    "def parse_search_to_df(search):\n",
    "    \n",
    "    results = []\n",
    "  \n",
    "    for raw_result in tqdm(search.results()):\n",
    "        parsed_result = {}\n",
    "        parsed_result[\"published\"] = raw_result.published\n",
    "        parsed_result[\"url\"] = raw_result.entry_id\n",
    "        parsed_result[\"title\"] = raw_result.title\n",
    "        parsed_result[\"primary_category\"] = raw_result.primary_category\n",
    "        parsed_result[\"categories\"] = raw_result.categories\n",
    "        parsed_result[\"abstract\"] = raw_result.summary\n",
    "        results.append(parsed_result)\n",
    "    \n",
    "    local_result_df = pd.DataFrame(results)\n",
    "    if results:\n",
    "        local_result_df[\"published\"] = pd.to_datetime(local_result_df[\"published\"])\n",
    "        local_result_df.set_index(\"published\", inplace=True)\n",
    "        local_result_df[\"categories\"] = local_result_df[\"categories\"].astype(str)\n",
    "    \n",
    "    return local_result_df\n",
    "\n",
    "def do_keyphrase_list(keyphrases):\n",
    "    \n",
    "    for keyphrase in tqdm(keyphrases):\n",
    "        if \"local_result_df\" in locals():\n",
    "            tmp_df = parse_search_to_df(do_search(keyphrase))\n",
    "            local_result_df = pd.concat([local_result_df,tmp_df])#, ignore_index=True)\n",
    "            local_result_df.drop_duplicates(inplace=True)\n",
    "        else:\n",
    "            local_result_df = parse_search_to_df(do_search(keyphrase))\n",
    "\n",
    "    local_result_df[\"categories\"] = local_result_df[\"categories\"].apply(lambda x: literal_eval(str(x)))\n",
    "    return local_result_df\n",
    "\n",
    "result_df = do_keyphrase_list(keyphrases)\n",
    "result_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d31386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Secondary Image Synthesis In Electronic Computer Photography (Vtorichnyj Sintez Izobrazhenij V Elektronnoj Komp'Yuternoj Fotografii)\"]\n"
     ]
    }
   ],
   "source": [
    "print(result_df.sort_index().head(1).title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4fb144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers 1645\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of papers\",len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1c969e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df.primary_category.value_counts().index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b5ab1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1994-09-08 10:01:48+0000', tz='UTC')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f59163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-10-03 14:56:05+0000', tz='UTC')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882b4177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = result_df[\"url\"].resample(\"MS\").count().plot()\n",
    "ax.set_title(\"Number of relevant monthly publications on ArXiv in time\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Number of publications\")\n",
    "plt.gcf().set_size_inches(10, 6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d16210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2018-04-01 00:00:00+0000', tz='UTC', freq='MS'), 17]\n",
      "[Timestamp('2019-04-01 00:00:00+0000', tz='UTC', freq='MS'), 29]\n",
      "[Timestamp('2020-04-01 00:00:00+0000', tz='UTC', freq='MS'), 42]\n",
      "[Timestamp('2021-10-01 00:00:00+0000', tz='UTC', freq='MS'), 47]\n",
      "[Timestamp('2022-03-01 00:00:00+0000', tz='UTC', freq='MS'), 63]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import peakdetect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resampled_df = result_df[result_df.index.to_series().gt(\"2014\") & result_df.index.to_series().lt(\"2022-10\")][\"url\"].resample(\"MS\").count()\n",
    "arxiv_peaks = peakdetect.peakdetect(resampled_df, resampled_df.index, lookahead=3, delta=7)\n",
    "\n",
    "ax = resampled_df.plot()\n",
    "\n",
    "locs, labels = plt.xticks()\n",
    "\n",
    "for peak in arxiv_peaks[0]:\n",
    "    print(peak)\n",
    "    ax.annotate(\"*\"+str(peak[0])[:7],xy=(peak[0],peak[1]+1), fontsize=10, color=\"red\")\n",
    "    \n",
    "ax.set_title(\"Number of relevant monthly publications on ArXiv in time \\n With peak detection\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Number of publications\")\n",
    "plt.gcf().set_size_inches(10, 6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ccf6a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alatus/anaconda3/envs/10_days_AI/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGDCAYAAAD6aR7qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzvklEQVR4nO3de5xdVXnw8d+TBCSBQICE6wBRB7WIVTGgVqsoYkURKIqXVhsVRdu3xr5YK1prQe3rFauDtkq1Gm8oRQRUUJGK1mqNEJC7ZqARhksuSCAh4RLyvH/sPXIyzGWfyex9Zs78vp/P+Zxz9mWtZ52zz5xn1l57nchMJEmSVL8ZnQ5AkiRpujDxkiRJaoiJlyRJUkNMvCRJkhpi4iVJktQQEy9JkqSGmHipFhHxmYj4hwkqa/+I2BARM8vnl0bEGyei7LK8iyJi8USV10a9H4iItRFxR9N1jyUiVkbEC0ZYd3hEDDQUx4S+11PJRH6G2qw3I6J3hHWvi4iftjzfEBGPqSGGayPi8IkuV5oMTLzUtvJLeVNErI+IdRHxs4h4S0T8/njKzLdk5vsrljXsF3xLWTdn5k6Z+dAExH5qRHxlSPlHZebSbS27zTj2A94OHJSZezVZ9zCxfDEiPtDJGCazqslfROxYJiIXVth2p/LY/7OWZXMj4uaIeDlU/wx1Uvm5vGlbyhju+MvMJ2bmpdsUnDRJmXhpvF6amXOBA4APAe8EPj/RlUTErIkuc5I4ALgzM1d3OpBO6cL39uXA/cALI2LvkTaKiFmZuQE4CfhkRCwoV30EuCwzz6k/VNWlk8d1F36mulNmevPW1g1YCbxgyLLDgC3AweXzLwIfKB/PB74DrAN+B/wXRdL/5XKfTcAG4O+AhUACJwI3Az9pWTarLO9S4IPAMuBu4Hxgt3Ld4cDAcPECLwIeAB4s6/tVS3lvLB/PAN4D/BZYDXwJ2KVcNxjH4jK2tcDfj/I67VLuv6Ys7z1l+S8o27yljOOLw+x7ODBQviargduB44AXA78pX8d3t2z/KOATwG3l7RPAo4aU9faWsl5frjupfD0eKGP5dstr9rfAVeVr/A1gh6GvMfAO4JtDYj8D+MQox847y3LvB2YBzwB+RnF8/Ao4vGX737835fM3ANcDdwHfBw4ol38G+NiQus4HTi4fnwLcCKwHrgP+tGW71wE/BT5Wlvu/wFHlun8CHgLuK1+fT43yfv9nuf1y4G/HanfL5+Ss8jW9E9i7ZZ8v8vBn6Hrg6JZ1syiOv0NGOXbeXW6zEvjzUV7T1wE/bXmewBLgpnL/jwIzRtm2t3w8Gzid4li/u3xNZ5fr/gO4o1z+E+CJFY6/F2zLsV2uf3H5fq8Hbh36vgx5Df6b4ti9G7gBOGLIZ/nzZfm3Ah8AZg7Z958pPpcfGFL2XsBGYPeWZU+j+Luw3WjHdbnuk8AtwD3A5cAft6w7FTgH+Eq5/o3Dtc/b5Lp1PABvU+/GMIlXufxm4C/Lx1/k4S+ND1J8MW5X3v4YiOHK4uHk5kvAjuUf88FlrYnXrcDB5TbfBL5SrjucERKv8vGpg9u2rL+UhxOvNwD9wGOAnYBzgS8Pie3fyrieTPEl+gcjvE5fovjyn1vu+xvgxJHiHLLv4cBm4L3la/am8g/118rynkiRDDym3P59wP8AewALKBKZ9w8p631lWS+m+CLYdeh7NeQ1WwbsA+xG8aXwlqGxA3sD9wLzyuezKL4AnzbKsXMlsF/5Gu5LkXC8mCIpPbJ8vmCY9+a48r35g7Ke9wA/K9c9h+LLafC42pUiud2nfH5C2ZYZwCvLmPcu172O4sv/TcBM4C8pvuBjaAyjvF/7UyTSB1EkAVeN1u6W5btSfJmvpSVhGOYz9F7gqy3rXgLcMMax83GKpOW5ZXsfP1x7GD6Z+lH5vu9Pcdy+cZRtBxOvT5dl71u+jn/EwwnSGyiO28Ek6srh2jnCZ3Zbju3bKROV8rV+RKLa0q7NwP8ty3klRQI2+A/decBnKf7e7EHx2XjzkH3fSnFczh6m/Asp/zaWz/8ZOGOs47pc/xpg93Ld2ykS2MF/gk6lOHaPozi2H1G3t8l363gA3qbejZETr/+h7AFi6y+N91EkIL1jlcXDyc1jhlnWmnh9qGX9QRT/Mc9k2xOvS4C/aln3+PIP26yWOHpa1i8DXjVMu2ZSJGUHtSx7M3Bp+fgRcQ7Z/3CKxGHwv+q5Zd1Pb9nmcuC48vGNwItb1v0JsHJIWbNa1q8GnjH0vRrymr2m5flHgM8MFztwEfCm8vHRwHVjHDtvaHn+TsrEtmXZ94HFw7w3F1EmruXzGRRfsgcAQZH4P6dc9ybgP0eJ40rg2PLx64D+lnVzytd6r6ExjFLeeyiTCYoE7yHgqSO1e8i+PyzbscuQ5b9/X4Beil6bOeXzrwLvHeXY2Qzs2LLsbOAfhmsPwydTL2p5/lfAJaNs21u+F5uAJ4/2OpX7zCv322WM42/wM7stx/bNFJ+7nceI6XW0JNstn+3XAntSfJZbE+ZXAz9q2ffmMcp/JfDf5eOZFMnTYWMd1yOUddfg60zx9+wnY73m3ibXzTFemkj7UnS1D/VRiv/ofhARN0XEKRXKuqWN9b+l+C91fqUoR7dPWV5r2bMo/vgOar0KcSNFz9hQ84Hthylr3zZiuTMfvqBgU3m/qmX9ppa6h4t7nyFlba4Qd6sq7QRYSvFfOeX9l8cot/W9OwA4obxIY11ErAOeTdGTNtQBFGOiBrf7HUXCtW8W30Jfp/hCBPgziuQEgIj4i4i4smXfg9n6ePl9WzNzY/lwrNen1V8M1peZtwE/pjgl3eoRx3REvIYiof8h8OGRCs/Mfopex5dGxBzgGIrez5HclZn3tjwfejyMZejna6x95wM7UCRJW4mImRHxoYi4MSLuoUiqBvepYluO7ZdR9IL9NiJ+HBHPHKWeW8vjaGg9B1D8fbm95fj5LEXP16Cx/l6dDxxUXgF6JHB3Zi4r1414XANExNsj4vqIuLtcvwtbv3Zj1a1JxsRLEyIiDqX4Q/HToesyc31mvj0zHwO8FDg5Io4YXD1CkSMtH7Rfy+P9KXql1lKcUpnTEtdMitMTVcu9jeIPYWvZm9k64alibRnT0LJubbOcqoaL+7aK+471mozlPOAPI+Jgih6vr46++Vb13ULR4zWv5bZjZn5omP1uoTi907rt7Mz8Wbn+LODlEXEA8HSKU9CUz/8N+GuKcTbzgGsovtyqGPX1iYg/Ag4E3hURd5TTgzwdePWQwc45ZL89KE45vYmiV+YVEfGcUao6iyKxPJaiV7F/lG13jYgdW563Hg9bfUYoxiANNfTzNdaxtJbi1Pdjh1n3Z2XML6BIGhaWywdf//F8Jisd25n5y8w8liJJOo+i528k+0ZE6zExWM8tFD1e81uOu50z84mtVY0Rx31l3X9O0YvW+s/JiMd1RPwxRa/wKyhOn86jOAXaGue2fn7VMBMvbZOI2DkijqbobfhKZl49zDZHR0Rv+UftHorTMIM9OasoxlO16zURcVD53//7gHPK3qHfADtExEsiYjuKU0CPatlvFbCwdeqLIc4C/m9EPDoidgL+H/CNIf9Rj6mM5Wzgn8ppAg4ATqYYBFuHs4D3RMSCiJhPMSaoal3jfQ+A33+pnEPRA7MsM29uY/evUPTi/EnZM7JDOU9YzzDbfoYiuXkiQETsEhEntMRxBcU4uM8B38/MdeWqHSm+nNaU+72eoserqrFen8XAxRSnvJ9S3g6mSG6OGmW/TwHnZeaPMvN2igsp/i0iHjXC9l8HXkgxBm203q5Bp0XE9uWX99EUA9yhOM16fETMKefrOnGYfd8REbuW0568jeLiihFl5hbg34GPR8Q+5Xv5zLItcykSlzspXpP/N2T3sV7fcR3bZdv/PCJ2ycwHefhvz0j2AJZExHblcfUHwIXle/MD4PTy792MiHhsRDx3rBiG+BLFacljhsQ/2nE9l+IfvzXArIh4L7Bzm/VqkjHx0nh9OyLWU/y39vcUA3lfP8K2B1KcStkA/Bz4l3x4jp4PUvxRXRcRf9tG/V+mGBtyB8UpjiUAmXk3xZiUz1H0Lt1LcdXToMEvnzsjYvkw5f57WfZPKK5uu49i0Ox4vLWs/yaKnsCvleXX4QPAZRRXzV1NcWVd1bm5Pk9xGmRdRJw3zvqXAk9i7NOMW8nMWyh6Q95N8eVyC8WVko/425SZ36I4Hff18pTVNTwysTmLomflay37XUdxtd3PKb7kn0RxFVpVn6ToSbsrIvpaV0TEDhS9EWdk5h0tt/+leC2Gnm4c3O84ilOq72iJ83MUx+p7h9unTAB+TjFofdREiOJzcRdFj81XKS6MuKFc988UYyJXUbxvw/VQnk8xhvBK4LtUmyrmbymOvV9SnC77MMX7+CWK03a3Ulxh+D9D9hvr+NuWY/u1wMryeHkLD58SH84vKP5WraW4OvXlmXlnue4vKIYOXEfxup7D8KfDR5SZ/01xAcbyzFzZsny04/r7FGPAfkPxGt6HpxanvMGrdiRp3CJif4pL8PfKzHs6Hc90FsWM71/JzOF6DTWMiHgdxQUHz665nv8EvlYm2ZqmnGxN0jYpT9ueDHzdpEsaXjkO9hCKHl5NYyZeksatHMC9iuI0yIs6HI40KUXEUoq5tt6Wmes7HI46zFONkiRJDXFwvSRJUkNMvCRJkhoyJcZ4zZ8/PxcuXNjpMCRJksZ0+eWXr83MBcOtmxKJ18KFC7nssss6HYYkSdKYIuK3I63zVKMkSVJDTLwkSZIaYuIlSZLUEBMvSZKkhph4SZIkNcTES5IkqSEmXpIkSQ0x8ZIkSWqIiZckSVJDTLwkSZIaYuIlSZLUkCnxW42SJGn66uvro7+/v/L2AwMDAPT09FTep7e3lyVLlrQdW7tMvCRJUlfZtGlTp0MYkYmXJEma1NrtiRrcvq+vr45wtoljvCRJkhpi4iVJktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJaoiJlyRJUkNMvCRJkhpSa+IVEfMi4pyIuCEiro+IZ0bEbhFxcUSsKO93rTMGSZKkyaLuHq9PAt/LzCcATwauB04BLsnMA4FLyueSJEldr7bEKyJ2Bp4DfB4gMx/IzHXAscDScrOlwHF1xSBJkjSZ1Nnj9RhgDfCFiLgiIj4XETsCe2bm7QDl/R41xiBJkjRp1Jl4zQIOAf41M58K3EsbpxUj4qSIuCwiLluzZk1dMUqSJDWmzsRrABjIzF+Uz8+hSMRWRcTeAOX96uF2zswzM3NRZi5asGBBjWFKkiQ1o7bEKzPvAG6JiMeXi44ArgMuABaXyxYD59cVgyRJ0mQyq+by3wp8NSK2B24CXk+R7J0dEScCNwMn1ByDJEnSpFBr4pWZVwKLhll1RJ31SpIkTUbOXC9JktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJaoiJlyRJUkNMvCRJkhpi4iVJktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJaoiJlyRJUkNMvCRJkhpi4iVJktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJaoiJlyRJUkNMvCRJkhpi4iVJktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJaoiJlyRJUkNMvCRJkhpi4iVJktQQEy9JkqSGmHhJkiQ1xMRLkiSpISZekiRJDTHxkiRJasisOguPiJXAeuAhYHNmLoqI3YBvAAuBlcArMvOuOuOQJEmaDJro8XpeZj4lMxeVz08BLsnMA4FLyueSJEldrxOnGo8FlpaPlwLHdSAGSZKkxtV6qhFI4AcRkcBnM/NMYM/MvB0gM2+PiD1qjkGSJE0SfX199Pf311rHihUrAFiyZEmt9fT29rZdR92J17My87Yyubo4Im6oumNEnAScBLD//vvXFZ8kSWpQf38/1113BfMXZI21BACr1yyvrYa1a2Jc+9WaeGXmbeX96oj4FnAYsCoi9i57u/YGVo+w75nAmQCLFi2q892RJEkNmr8gOf74BzodxjY599ztx7VfbWO8ImLHiJg7+Bh4IXANcAGwuNxsMXB+XTFIkiRNJmP2eEXEAuBNFNM//H77zHzDGLvuCXwrIgbr+Vpmfi8ifgmcHREnAjcDJ4wvdEmSpKmlyqnG84H/An5IMR9XJZl5E/DkYZbfCRxRtRxJkqRuUSXxmpOZ76w9EkmSpC5XZYzXdyLixbVHIkmS1OVG7PGKiPUU83AF8O6IuB94sHyemblzMyFKkiR1hxETr8yc22QgkiRJ3W7MU40RcUmVZZIkSRrdaKcadwB2BOZHxK4MTgMLOwP7NBCbJElSVxntqsY3A39DkWRdzsOJ1z3Ap+sNS5IkqfuMNsbrk8AnI+KtmXlGgzFJkiR1pTHn8crMMyLij3jkzPVfqjEuSZI0RF9fH/39/ZW3HxgYAKCnp6fyPr29vSxZsqTt2FRNlZ8M+jLwWOBKHp65PgETL0mSJrFNmzZ1OgQNUWXm+kXAQZmZdQcjSZJG1m5P1OD2fX19dYSjcagyc/01wF51ByJJktTtqvR4zQeui4hlwP2DCzPzmNqikiRJ6kJVEq9T6w5CkiRpOqhyVeOPI2JP4NBy0bLMXF1vWJIkSd2nyk8GvQJYBpwAvAL4RUS8vO7AJEmSuk2VU41/Dxw62MsVEQuAHwLn1BmYJElSt6lyVeOMIacW76y4nyRJklpU6fH6XkR8HzirfP5K4KL6QpIkSepOVQbXvyMiXgY8i+KHss/MzG/VHpkkSVKXqdLjRWZ+MyIuHtw+InbLzN/VGpkkSVKXqfJbjW8G3gdsArZQ9Hol8Jh6Q5MkSeouVXq8/hZ4YmaurTsYSZKkblbl6sQbgY11ByJJktTtqvR4vQv4WUT8gq1/q7G9n0iXJEma5qokXp8F/hO4mmKMlyRJksahSuK1OTNPrj0SSZKkLlcl8fpRRJwEfJutTzU6nYQkqSP6+vro7++vvP3AwAAAPT09lffp7e1lyRJH1WhiVUm8/qy8f1fLMqeTkCRNGZs2bep0CBJQbeb6RzcRiCRJVbXbEzW4fV9fXx3hSJX5Y9eSJEkNqfSTQZIkaWK1O05tPFasWAG030PYLsfDVTdq4hURAfRk5i0NxSNJ0rTQ39/PNddcw0477VRbHQ8++CAAK1eurK2ODRs21FZ2Nxo18crMjIjzgKc1E44kSdPHTjvtxCGHHNLpMLbJ8uXLOx3ClFJljNf/RMShtUciSZLU5aqM8Xoe8JaIWAncCwRFZ9gf1hmYJElSt6mSeB1VexSSJEnTwJinGjPzt8B+wPPLxxur7CdJkqStjZlARcQ/Au/k4ZnrtwO+UrWCiJgZEVdExHfK57tFxMURsaK833U8gUuSJE01VXqu/hQ4hmJ8F5l5GzC3jTreBlzf8vwU4JLMPBC4pHwuSZLU9aokXg9kZlL8PiMRsWPVwiOiB3gJ8LmWxccCS8vHS4HjqpYnSZI0lVVJvM6OiM8C8yLiTcAPgX+rWP4ngL8DtrQs2zMzbwco7/eoHq4kSdLUVeVHsj8WEUcC9wCPA96bmRePtV9EHA2szszLI+LwdgOLiJOAkwD233//dneXJEmadKr+VuPVwGyK041XV9znWcAxEfFiYAdg54j4CrAqIvbOzNsjYm9g9XA7Z+aZwJkAixYtyop1SpIkTVpVrmp8I7AMOB54OcVM9m8Ya7/MfFdm9mTmQuBVwH9m5muAC4DF5WaLgfPHGbskSdKUUqXH6x3AUzPzToCI2B34GfDv46zzQxTjxk4EbgZOGGc5kiRJU0qVxGsAWN/yfD1wSzuVZOalwKXl4zuBI9rZX5IkqRtUSbxuBX4REedTjPE6FlgWEScDZObHa4xPktTl+vr66O/vr7WOFStWALBkyZJa6+nt7a29Dk1tVRKvG8vboMExWe1MoipJ0rD6+/v59TXXs9/cvWqrY7vNxZDmjb+9q7Y6bll/R21lq3tUmU7itCYCkSRNX/vN3Yu3H/b6ToexTU5f9oVOh6ApoOp0EpIkSdtsYGCAe+4Jzj13+06Hsk3WrgkeuH+g7f2qzFwvSZKkCWCPlyRJakxPTw+r16zm+OMf6HQo2+Tcc7dnjwU9be9XZQLVj0TEzhGxXURcEhFrI+I144pSkiRpGqtyqvGFmXkPcDTFnF6Po5hUVZIkSW2ocqpxu/L+xcBZmfm7iKgxJEmSut/AwADr169n+fLlnQ5lm6xfv56BgfYHmU9XVRKvb0fEDcAm4K8iYgFwX71hSZIkdZ8q83idEhEfBu7JzIci4l6K2eslSdI49fT0sHnzZg455JBOh7JNli9fTk9P+4PMp6uqVzX+AbAwIlq3/1IN8UiSJHWtMROviPgy8FjgSuChcnFi4iVJktSWKj1ei4CDMjPrDkaSJKmbVZlO4hqgvl8ulSRJmiaq9HjNB66LiGXA/YMLM/OY2qKSJEnqQlUSr1PrDkKSJGk6qDKdxI8jYk/g0HLRssxcXW9YkiSpW61dE5x77va1lX/3umKi913m1Tc8fe2aYI8F7e9X5arGVwAfBS4FAjgjIt6Rmee0X50kSZrOent7a6/j7nUrANhjwYG11bHHgvG1pcqpxr8HDh3s5Spnrv8hYOIlSZLasmTJksbq6Ovrq72udlW5qnHGkFOLd1bcT5IkSS2q9Hh9LyK+D5xVPn8lcGF9IUmSJHWnKoPr3xERLwOeRTHG68zM/FbtkUmSJHWZSr/VmJnfBL5ZcyySJEldbcTEKyJ+mpnPjoj1FL/N+PtVQGbmzrVHJ0mS1EVGTLwy89nl/dzmwpEkSepeY16dGBFfrrJMkiRJo6syLcQTW59ExCzgafWEI0mS1L1GTLwi4l3l+K4/jIh7ytt6YBVwfmMRSpIkdYkRE6/M/GA5vuujmblzeZubmbtn5rsajFGSJKkrVJnH610RsStwILBDy/Kf1BmYJElSt6nyI9lvBN4G9ABXAs8Afg48v9bIJEnqchs2bGD58uW1lb9x40YA5syZU1sdGzZsqK3sblRlAtW3AYcC/5OZz4uIJwCn1RuWJEndrbe3t/Y6VqxYAcDChQtrraeJtnSLKonXfZl5X0QQEY/KzBsi4vG1RyZJUhdbsmRJY3X09fXVXpeqqZJ4DUTEPOA84OKIuAu4rc6gJEmSulGVwfV/Wj48NSJ+BOwCfK/WqCRJkrpQlZnrnxERcwEy88fAj4Cn1h2YJElSt6kyc/2/Aq2XLNxbLpMkSVIbqiRekZk5+CQzt1BtGoodImJZRPwqIq6NiNPK5btFxMURsaK833X84UuSJE0dVRKvmyJiSURsV97eBtxUYb/7gedn5pOBpwAviohnAKcAl2TmgcAl5XNJkqSuVyXxegvwR8CtwADwdOCksXbKwuApyu3KWwLHAkvL5UuB49oLWZIkaWqqclXjauBV4yk8ImYClwO9wKcz8xcRsWdm3l6WfXtE7DGesiVJkqaaEROviPi7zPxIRJxB0VO1lcwcc+a3zHwIeEo5D9i3IuLgqoFFxEmUPWv7779/1d0kSZImrdF6vK4v7y/b1koyc11EXAq8CFgVEXuXvV17A6tH2OdM4EyARYsWPSLxkyRJmmpGTLwy89vl/dKRthlNRCwAHiyTrtnAC4APAxcAi4EPlffnj6d8SZKkqWa0U43fZphTjIMy85gxyt4bWFqO85oBnJ2Z34mInwNnR8SJwM3ACe2HLUmC4jf4+vv7K28/MDAAQE9PT+V9ent7a/1dwYGBAe5dv57Tl32htjqacMv6O9hx4N5Oh6FJbrRTjR/bloIz8yqGmeE+M+8EjtiWsiVJ47Np06ZOhyBNa6Odavzx4OOI2B54AkUP2K8z84EGYpMkjaHdnqjB7fv6+uoIZ1x6enrY+NBdvP2w13c6lG1y+rIvMKfHOcE1uioz0L8E+AxwIxDAoyPizZl5Ud3BSZIkdZMxEy/gdOB5mdkPEBGPBb4LmHhJkiS1ocrM9asHk67STYwwBYQkSZJGNtpVjceXD6+NiAuBsynGeJ0A/LKB2CRJkrrKaKcaX9ryeBXw3PLxGsDRg5IkSW0a7arGqX15iSRJ0iRT5arGLzD8bzW+oZaIJEmSulSVqxq/0/J4B+BPgdvqCUeSJKl7jZl4ZeY3W59HxFnAD2uLSJIkqUtVmU5iqAOB/Sc6EEmSpG5XZYzXeooxXlHe3wG8s+a4JEmSuk6VU41zmwhEkiSp21UZXD84meqzKXq8/iszz6szKEmSpG405hiviPgX4C3A1cA1wFsi4tN1ByZJktRtqvR4PRc4ODMTICKWUiRhkiRJakOVqxp/zdZXMe4HXFVPOJIkSd2rSo/X7sD1EbGsfH4o8POIuAAgM4+pKzhJkqRuUiXxem/tUUiSJE0DVaaT+HETgUiSJHW78cxcL0mSpHEw8ZIkSWrIiKcaI+KSzDwiIj6cmf5EkCRJHdbX10d/f3/l7VesWAHAkiVLKu/T29vb1vZqz2hjvPaOiOcCx0TE1yl+q/H3MnN5rZFJkqRtMnv27E6HoCFGS7zeC5wC9AAfH7IugefXFZQkSXoke6KmvhETr8w8BzgnIv4hM9/fYEySJEldqcp0Eu+PiGOA55SLLs3M79QbliRJUvep8iPZHwTeBlxX3t5WLpMkSVIbqsxc/xLgKZm5BX7/I9lXAO+qMzBJ0vRxy/o7OH3ZF2orf/XG3wGwx5zdaqvjlvV38Hh2ra18dYcqiRfAPOB35eNd6glFkqa3dqcKGI/xTC8wHu1MSdDb21trLAAPrlgLwJwD6kuMHs+ujbRFU1uVxOuDwBUR8SOKKSWeg71dkjTh+vv7ueZXv2Lu9lX/J27f5s0PAfDb66+trY71D2xua/smrtQbrKOvr6/2uqTRVBlcf1ZEXAocSpF4vTMz76g7MEmajuZuP4vD9pzap6uWrbqr0yFIk1alf6sy83bggppjkSRJ6mr+VqMkSVJDTLwkSZIaMmriFREzIuKapoKRJEnqZqOO8crMLRHxq4jYPzNvbiooSZKkQe1OtTKeaVPamQJlW1QZXL83cG1ELAPuHVyYmcfUFpUkSdI4zZ49u9MhjKhK4nXaeAqOiP2ALwF7AVuAMzPzkxGxG/ANYCGwEnhFZnrtsSRJGlYTPVFNGXNwfWb+mCJB2q58/EtgeYWyNwNvz8w/AJ4B/J+IOAg4BbgkMw8ELimfS5Ikdb0qP5L9JuAc4LPlon2B88baLzNvz8zl5eP1wPXlvscCS8vNlgLHtRu0JEnSVFRlOon/AzwLuAcgM1cAe7RTSUQsBJ4K/ALYs5yQdXBi1rbKkiRJmqqqJF73Z+YDg08iYhaQVSuIiJ2AbwJ/k5n3tLHfSRFxWURctmbNmqq7SZIkTVpVEq8fR8S7gdkRcSTwH8C3qxQeEdtRJF1fzcxzy8WrImLvcv3ewOrh9s3MMzNzUWYuWrBgQZXqJEmSJrUqidcpwBrgauDNwIXAe8baKSIC+DxwfWZ+vGXVBcDi8vFi4Px2ApYkSZqqxpxOopxEdSnF+KwEfp2ZVU41Pgt4LXB1RFxZLns38CHg7Ig4EbgZOGE8gUuSJE01YyZeEfES4DPAjUAAj46IN2fmRaPtl5k/LbcfzhHtBipJkjTVVZlA9XTgeZnZDxARjwW+C4yaeEmSJGlrVRKv1YNJV+kmRhgQL0kav4GBAdY/sJllq6b2j3msf2AzAwMDnQ5DmpRGTLwi4vjy4bURcSFwNsUYrxMoZq+XJElSG0br8Xppy+NVwHPLx2uAXWuLSJKmqZ6eHh5afzeH7Tm1/8QuW3UXPT09nQ5DmpRGTLwy8/VNBiJJktTtqlzV+GjgrcDC1u0z85j6wpIkSeo+VQbXn0cxEeq3gS21RiNJktTFqiRe92VmX+2RSJIkdbkqidcnI+IfgR8A9w8uzMzltUUlSW3q6+ujv79/7A1Lg9MdtDMIvLe3lyVLlrQdmyQNqpJ4PYnip3+ez8OnGrN8LklT0qZNmzodgqRpqEri9afAYzLzgbqDkaTxarcnanD7vj5HUkhqzowK2/wKmFdzHJIkSV2vSo/XnsANEfFLth7j5XQSkiRJbaiSeP1j7VFIktSGdi+mWLFiBdDeKWkvplAdxky8MvPHTQQiSVJdZs+e3ekQJKDazPXrKa5iBNge2A64NzN3rjMwSZJGYk+UpqoqPV5zW59HxHHAYXUFJEmS1K2qjPHaSmaeFxGn1BGMJE136x/YzLJVd9VW/sbNDwEwZ9bM2upY/8Dm2sqWproqpxqPb3k6A1jEw6ceJUkTpLe3t/Y6BgeZH3DggbXW00RbpKmoSo/XS1sebwZWAsfWEo0kTWNNjFty4lips6qM8Xp9E4FIkiR1uxETr4h47yj7ZWa+v4Z4JEmSutZoPV73DrNsR+BEYHfAxEuSJKkNIyZemXn64OOImAu8DXg98HXg9JH2kyRJ0vBGHeMVEbsBJwN/DiwFDsnM+q5zliRJ6mKjjfH6KHA8cCbwpMzc0FhUkiRJXWjGKOveDuwDvAe4LSLuKW/rI+KeZsKTJEnqHqON8RotKZMkSVKbTK4kSZIa0vZvNUpS3fr6+ujv76+1jsGfzql7tvje3t5GZqSXNDWYeEmadPr7+7n26uuZN2eP2urY8kAAcOuNd9ZWx7qNq2srW9LUZOIlaVKaN2cPnveEV3U6jG3yoxu+3ukQJE0yjvGSJElqiImXJElSQ0y8JEmSGmLiJUmS1BAH10tdpt2pGAYGBgDo6empvE/dUyQMDAxw98b1U35w+rqNq8mBTZ0OQ9IkYuIlTXObNpkYSFJTaku8IuLfgaOB1Zl5cLlsN+AbwEJgJfCKzLyrrhik6ajdnqjB7fv6+uoIZ1x6enqI++/siukk9u3ZvdNhSJpE6hzj9UXgRUOWnQJckpkHApeUzyVJkqaF2hKvzPwJ8Lshi48FlpaPlwLH1VW/JEnSZNP0VY17ZubtAOV9fb8HIkmSNMlM2ukkIuKkiLgsIi5bs2ZNp8ORJEnaZk0nXqsiYm+A8n7EX5DNzDMzc1FmLlqwYEFjAUqSJNWl6cTrAmBx+XgxcH7D9UuSJHVMbYlXRJwF/Bx4fEQMRMSJwIeAIyNiBXBk+VySJGlaqG0er8x89QirjqirTkmSpMnMmeul0tq1aznttNM49dRT2X13J73stHUbV9f6k0Eb7ivmbt5ph11rq2PdxtXsi8eSpIeZeEmlpUuXctVVV7F06VJOPvnkToczrfX29tZex4oVxTSD+z62vsRoX3ZvpC2Spg4TL4mit+uiiy4iM7noootYvHixvV4dVOcPcA+tYzL9VJKk7mfiJVH0dmUmAFu2bLHXS1NGX18f/f39lbdfsWIF0F5y29vb20gyLE0Hk3YCValJF198MQ8++CAADz74ID/4wQ86HJFUj9mzZzN79uxOhyFNW/Z4ScCRRx7JhRdeyIMPPsh2223HC1/4wk6HJFViT5Q0tdjjJQGLFy8mIgCYMWMGixcvHmMPSZLaZ+IlAfPnz+eoo44iIjjqqKMcWC9JqoWnGqXS4sWLWbly5aTq7Wp34PR4jGew9XjUPUDbQeaSpgITL6k0f/58zjjjjE6HsZX+/n5uuPJK9qqxjsFu73VXXllbHXfUVvL4OcBcUieYeEmlyTpz/V7AiUSnw9gmnydrr8OeKElTgWO8pFLrzPWSJNXBxEvikTPX33nnnZ0OSZLUhUy8JIafuV6SpIlm4iXhzPWSpGY4uF5i8s5cPzAwwHqaGZxep9uBDQMDnQ5DkjrOHi8JZ66XJDXDHi91rXYn1BxMvHbaaSdOO+20SvvUPaFmT08P69au7YrpJOb19HQ6DEnqOHu8pNKMGTOYMWMGe+1V53SlkqTpzB4vPcJknUi0Xe32RA1u39fXV0c4kiTZ46VHciJRSZLqYeKlrTiRqCRJ9THx0lacSFSSpPo4xktbGW4i0ZNPPrnDUU1vd1DvPF6DfZp1jua7A5hXY/mSNFWYeGkrk3Ui0emqt7e39jrWrFgBwLwDD6ytjnk00xZJmuxMvLSVxYsXc9FFFwFOJDoZ1DlH2NA6vJpTkurnGC9tZf78+Rx11FFEBEcdddSUnk5CkqTJxh4vPcLixYtZuXKlvV2SJE0wEy89wvz58znjjDM6HcZW2v35n/FYUY51qvv0Xt0/MyRJmrxMvDQl9Pf3c8W1V9R7adyW4u6KW6+or4519RUtSZr8TLw0dcyDLYdv6XQU22TGpQ6rlKTpzG8BSZKkhtjjNQ20Oz5qYGAAgJ6ensr7OG5p8mj3/R7P2Dbfb0kaHxMvPcKmTZs6HYIaNHv27E6HIEnThonXNNBuz8RknFBzYGAA7u6CMVLrYCAHaq3CnihJmrxMvKYYp1WQJGnqmrKJ13gSkIGBgdpPo82ePbutsVHQXgJy6aWX8ru1a3jUzPp+NPnBLQHAr69eXlsd9z8UDAwMVG53T08Pa2JNV1zV2LNve8eHJKl7TNnEq7+/nyuuvo4tc3arvE/ct5HY8mCNUcH6B5JV999RefsZG3/Xdh2PmpkcMPehtvebTH67fmb7O62r+VTjhvJ+p/qqYB2wb43lS5ImtSmbeAFsmbMb9x10dKfD2CY7XPedtrbv6enhvs23855FG8beeBL7wGU7sUObV03WbfAU64H7HlhfJfs20xZJ0uTUkcQrIl4EfBKYCXwuMz/UiTg0dTQxFmwyXlQgSeoujV8iFhEzgU8DRwEHAa+OiIOajkOSJKlpnejxOgzoz8ybACLi68CxwHXtFDIwMMCM9Xcy57Kl1Xfa8hBkfYPSAYiAGW2MX3poMwMDm9uq4uYNM/nAZdUHIq3aOIP7Hoq26mjXDjOTPedUH/h+84aZPK7GeMCJRCVJk08nEq99gVtang8ATx+6UUScBJwEsP/++z+ikHnz5rV9heL999/Pli31XhU3Y8YMHvWo7dvYY3vmzZtXeevxjA+aOTDAjJqv5pw5e3ZbY7Yex+Qb6+REopKkukXW3QM0tMKIE4A/ycw3ls9fCxyWmW8daZ9FixblZZdd1lSIkiRJ4xYRl2fmouHWdWIa8AFgv5bnPcBtHYhDkiSpUZ1IvH4JHBgRj46I7YFXARd0IA5JkqRGNT7GKzM3R8RfA9+nmE7i3zPz2qbjkCRJalpH5vHKzAuBCztRtyRJUqd04lSjJEnStGTiJUmS1BATL0mSpIaYeEmSJDXExEuSJKkhJl6SJEkNMfGSJElqiImXJElSQ0y8JEmSGhKZ2ekYxhQRa4Dfdqj6+cDaDtXdSbZ7erHd04vtnl5sd/MOyMwFw62YEolXJ0XEZZm5qNNxNM12Ty+2e3qx3dOL7Z5cPNUoSZLUEBMvSZKkhph4je3MTgfQIbZ7erHd04vtnl5s9yTiGC9JkqSG2OMlSZLUkGmXeEXEfhHxo4i4PiKujYi3lct3i4iLI2JFeb9ruXz3cvsNEfGpEcq8ICKuabId7ZrIdkfEpRHx64i4srzt0Yk2VTHB7d4+Is6MiN9ExA0R8bJOtKmKiWp3RMxteZ+vjIi1EfGJDjVrTBP8fr86Iq6OiKsi4nsRMb8Tbapigtv9yrLN10bERzrRnqrG0e4jI+Ly8n29PCKe31LW08rl/RHRFxHRqXaNZYLb/U8RcUtEbOhUe6qaqHZHxJyI+G75d/zaiPhQow3JzGl1A/YGDikfzwV+AxwEfAQ4pVx+CvDh8vGOwLOBtwCfGqa844GvAdd0um1NtRu4FFjU6TZ1oN2nAR8oH88A5ne6fU20e0i5lwPP6XT76m43MAtYPfgel/uf2un2NdDu3YGbgQXl86XAEZ1u3wS2+6nAPuXjg4FbW8paBjwTCOAi4KhOt6+hdj+jLG9Dp9vVVLuBOcDzysfbA//V5Pvd8Rey0zfgfOBI4NfA3i1v7q+HbPc6HvlFvBPw0/KNn9SJ1wS3+1KmSOI1we2+Bdix021out0t6w4sX4PodHvqbjewHbAGOIDii/gzwEmdbk8D7T4U+GHL89cC/9Lp9kx0u8vlAdwJPKrc5oaWda8GPtvp9tTd7iHLJ33iVUe7y3WfBN7UVNzT7lRjq4hYSJER/wLYMzNvByjvq5w+ez9wOrCxrhjrMAHtBvhCeerpHyZzl3yrbWl3RMwrH74/IpZHxH9ExJ41hjthJuj9huLL6BtZ/qWa7Lal3Zn5IPCXwNXAbRT/XH2+zngnyja+3/3AEyJiYUTMAo4D9qsv2okzjna/DLgiM+8H9gUGWtYNlMsmvW1s95Q1Ue0u/7a/FLikznhbTdvEKyJ2Ar4J/E1m3jOO/Z8C9GbmtyY6tjpta7tLf56ZTwL+uLy9dqLiq8sEtHsW0AP8d2YeAvwc+NgEhliLCXq/B70KOGvbo6rfBHy+t6NIvJ4K7ANcBbxrQoOswba2OzPvomj3NyhOv6wENk9kjHVot90R8UTgw8CbBxcNs9mk/wdjAto9JU1Uu8t/Ls4C+jLzpjpiHc60TLzKP6rfBL6ameeWi1dFxN7l+r0pxneM5pnA0yJiJcXpxsdFxKX1RDwxJqjdZOat5f16ivFth9UT8cSYoHbfSdGzOZho/wdwSA3hTpiJer/LbZ8MzMrMy2sJdgJNULufApCZN5Y9fGcDf1RPxBNjAj/f387Mp2fmMylO4ayoK+aJ0G67I6KH4nP8F5l5Y7l4gOIfq0E9FD2dk9YEtXvKmeB2nwmsyMxP1B54i2mXeJWnxT4PXJ+ZH29ZdQGwuHy8mOLc8Ygy818zc5/MXEgxSPU3mXn4xEc8MSaq3RExK8qru8oPwNHApL2icwLf7wS+DRxeLjoCuG5Cg51AE9XuFq9mCvR2TWC7bwUOiojBH7k9Erh+ImOdSBP5fkd5lXJ5ZdhfAZ+b2GgnTrvtLk8rfRd4V2b+9+DG5emp9RHxjLLMv6D6Z6NxE9XuqWYi2x0RHwB2Af6m3qiH0dRgsslyo0iSkuLUwZXl7cUUV/NcQvHf3SXAbi37rAR+B2yg+M/ooCFlLmSSD66fqHZTXA11eVnOtRSDEmd2un1NvN8UA61/UpZ1CbB/p9vX1HEO3AQ8odPtavj9fgtFsnUVRdK9e6fb11C7z6L4p+I64FWdbttEtht4D3Bvy7ZXAnuU6xZR/BN5I/ApJvFFJBPc7o+U7/+W8v7UTrev7nZT9Ghm+fkeXP7GptrhzPWSJEkNmXanGiVJkjrFxEuSJKkhJl6SJEkNMfGSJElqiImXJElSQ0y8JHWFKPw0Io5qWfaKiPheJ+OSpFZOJyGpa0TEwRS/KvBUYCbF/DwvynHM1B0RMzPzoYmNUNJ0Z+IlqatExEcoJk3csbw/AHgSxe9tnpqZ55c/sPvlchuAv87Mn0XE4cA/ArdT/GzQoRQ/F9RDkci9PzO/0VRbJHUfEy9JXSUidgSWAw8A3wGuzcyvlD8fsoyiNyyBLZl5X0QcCJyVmYvKxOu7wMGZ+b8R8TKKHrM3lWXvkpl3N94oSV3DxEtS14mI91H8FM4rgB2AzeWq3YA/ofgB5E9R9Go9BDwuM+cM9nhl5vPKch4HfJ+i1+s7mflfzbVCUjea1ekAJKkGW8pbAC/LzF+3royIU4FVwJMpLjK6r2X1vYMPMvM3EfE0it+D+2BE/CAz31dz7JK6mFc1Supm3wfeGhEBEBFPLZfvAtyemVuA11KM33qEiNgH2JiZXwE+BhxSf8iSupk9XpK62fuBTwBXlcnXSuBo4F+Ab0bECcCPaOnlGuJJwEcjYgvwIPCXdQcsqbs5xkuSJKkhnmqUJElqiImXJElSQ0y8JEmSGmLiJUmS1BATL0mSpIaYeEmSJDXExEuSJKkhJl6SJEkN+f+yw8U4fgTTXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "sns.boxplot(resampled_df.index.year, resampled_df, ax=ax)\n",
    "plt.title(\"Distribution of monthly relevant ArXiv publications per year\")\n",
    "ax.set_xlabel(\"Years\")\n",
    "ax.set_ylabel(\"Number of publications per month\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb0cd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df[result_df.index.to_series().gt(\"2014\")][\"url\"].resample(\"MS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9af1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.resample(\"MS\").count().to_csv(\"arxiv_publications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab45560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a19e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cs.CV                934\n",
       "eess.AS              204\n",
       "cs.SD                160\n",
       "eess.IV               89\n",
       "cs.LG                 80\n",
       "cs.CR                 37\n",
       "cs.CL                 23\n",
       "cs.GR                 22\n",
       "stat.ML               16\n",
       "cs.CY                 11\n",
       "cs.HC                  9\n",
       "cs.MM                  8\n",
       "physics.med-ph         8\n",
       "astro-ph.IM            6\n",
       "cs.SI                  4\n",
       "cs.NI                  3\n",
       "cs.RO                  3\n",
       "cs.NE                  3\n",
       "astro-ph               2\n",
       "astro-ph.SR            2\n",
       "q-bio.NC               2\n",
       "astro-ph.GA            2\n",
       "astro-ph.CO            2\n",
       "astro-ph.EP            2\n",
       "cs.AI                  2\n",
       "adap-org               2\n",
       "cs.IR                  1\n",
       "cs.SE                  1\n",
       "stat.ME                1\n",
       "physics.soc-ph         1\n",
       "q-bio.BM               1\n",
       "math.ST                1\n",
       "physics.geo-ph         1\n",
       "cond-mat.mtrl-sci      1\n",
       "cs.ET                  1\n",
       "Name: primary_category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.primary_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99ce4984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepfakes pose severe threats of visual misinformation to our society. One\n",
      "representative deepfake application is face manipulation that modifies a\n",
      "victim's facial attributes in an image, e.g., changing her age or hair color.\n",
      "The state-of-the-art face manipulation techniques rely on Generative\n",
      "Adversarial Networks (GANs). In this paper, we propose the first defense\n",
      "system, namely UnGANable, against GAN-inversion-based face manipulation. In\n",
      "specific, UnGANable focuses on defending GAN inversion, an essential step for\n",
      "face manipulation. Its core technique is to search for alternative images\n",
      "(called cloaked images) around the original images (called target images) in\n",
      "image space. When posted online, these cloaked images can jeopardize the GAN\n",
      "inversion process. We consider two state-of-the-art inversion techniques\n",
      "including optimization-based inversion and hybrid inversion, and design five\n",
      "different defenses under five scenarios depending on the defender's background\n",
      "knowledge. Extensive experiments on four popular GAN models trained on two\n",
      "benchmark face datasets show that UnGANable achieves remarkable effectiveness\n",
      "and utility performance, and outperforms multiple baseline methods. We further\n",
      "investigate four adaptive adversaries to bypass UnGANable and show that some of\n",
      "them are slightly effective. \n",
      "\n",
      "\n",
      "Thanks to the remarkable advances in generative adversarial networks (GANs),\n",
      "it is becoming increasingly easy to generate/manipulate images. The existing\n",
      "works have mainly focused on deepfake in face images and videos. However, we\n",
      "are currently witnessing the emergence of fake satellite images, which can be\n",
      "misleading or even threatening to national security. Consequently, there is an\n",
      "urgent need to develop detection methods capable of distinguishing between real\n",
      "and fake satellite images. To advance the field, in this paper, we explore the\n",
      "suitability of several convolutional neural network (CNN) architectures for\n",
      "fake satellite image detection. Specifically, we benchmark four CNN models by\n",
      "conducting extensive experiments to evaluate their performance and robustness\n",
      "against various image distortions. This work allows the establishment of new\n",
      "baselines and may be useful for the development of CNN-based methods for fake\n",
      "satellite image detection. \n",
      "\n",
      "\n",
      "Thanks to recent advances in deep learning, sophisticated generation tools\n",
      "exist, nowadays, that produce extremely realistic synthetic speech. However,\n",
      "malicious uses of such tools are possible and likely, posing a serious threat\n",
      "to our society. Hence, synthetic voice detection has become a pressing research\n",
      "topic, and a large variety of detection methods have been recently proposed.\n",
      "Unfortunately, they hardly generalize to synthetic audios generated by tools\n",
      "never seen in the training phase, which makes them unfit to face real-world\n",
      "scenarios. In this work, we aim at overcoming this issue by proposing a new\n",
      "detection approach that leverages only the biometric characteristics of the\n",
      "speaker, with no reference to specific manipulations. Since the detector is\n",
      "trained only on real data, generalization is automatically ensured. The\n",
      "proposed approach can be implemented based on off-the-shelf speaker\n",
      "verification tools. We test several such solutions on three popular test sets,\n",
      "obtaining good performance, high generalization ability, and high robustness to\n",
      "audio impairment. \n",
      "\n",
      "\n",
      "With the spread of DeepFake techniques, this technology has become quite\n",
      "accessible and good enough that there is concern about its malicious use. Faced\n",
      "with this problem, detecting forged faces is of utmost importance to ensure\n",
      "security and avoid socio-political problems, both on a global and private\n",
      "scale. This paper presents a solution for the detection of DeepFakes using\n",
      "convolution neural networks and a dataset developed for this purpose -\n",
      "Celeb-DF. The results show that, with an overall accuracy of 95% in the\n",
      "classification of these images, the proposed model is close to what exists in\n",
      "the state of the art with the possibility of adjustment for better results in\n",
      "the manipulation techniques that arise in the future. \n",
      "\n",
      "\n",
      "The accelerated growth in synthetic visual media generation and manipulation\n",
      "has now reached the point of raising significant concerns and posing enormous\n",
      "intimidations towards society. There is an imperative need for automatic\n",
      "detection networks towards false digital content and avoid the spread of\n",
      "dangerous artificial information to contend with this threat. In this paper, we\n",
      "utilize and compare two kinds of handcrafted features(SIFT and HoG) and two\n",
      "kinds of deep features(Xception and CNN+RNN) for the deepfake detection task.\n",
      "We also check the performance of these features when there are mismatches\n",
      "between training sets and test sets. Evaluation is performed on the famous\n",
      "FaceForensics++ dataset, which contains four sub-datasets, Deepfakes,\n",
      "Face2Face, FaceSwap and NeuralTextures. The best results are from Xception,\n",
      "where the accuracy could surpass over 99\\% when the training and test set are\n",
      "both from the same sub-dataset. In comparison, the results drop dramatically\n",
      "when the training set mismatches the test set. This phenomenon reveals the\n",
      "challenge of creating a universal deepfake detection system. \n",
      "\n",
      "\n",
      "Deepfake refers to tailored and synthetically generated videos which are now\n",
      "prevalent and spreading on a large scale, threatening the trustworthiness of\n",
      "the information available online. While existing datasets contain different\n",
      "kinds of deepfakes which vary in their generation technique, they do not\n",
      "consider progression of deepfakes in a \"phylogenetic\" manner. It is possible\n",
      "that an existing deepfake face is swapped with another face. This process of\n",
      "face swapping can be performed multiple times and the resultant deepfake can be\n",
      "evolved to confuse the deepfake detection algorithms. Further, many databases\n",
      "do not provide the employed generative model as target labels. Model\n",
      "attribution helps in enhancing the explainability of the detection results by\n",
      "providing information on the generative model employed. In order to enable the\n",
      "research community to address these questions, this paper proposes DeePhy, a\n",
      "novel Deepfake Phylogeny dataset which consists of 5040 deepfake videos\n",
      "generated using three different generation techniques. There are 840 videos of\n",
      "one-time swapped deepfakes, 2520 videos of two-times swapped deepfakes and 1680\n",
      "videos of three-times swapped deepfakes. With over 30 GBs in size, the database\n",
      "is prepared in over 1100 hours using 18 GPUs of 1,352 GB cumulative memory. We\n",
      "also present the benchmark on DeePhy dataset using six deepfake detection\n",
      "algorithms. The results highlight the need to evolve the research of model\n",
      "attribution of deepfakes and generalize the process over a variety of deepfake\n",
      "generation techniques. The database is available at:\n",
      "http://iab-rubric.org/deephy-database \n",
      "\n",
      "\n",
      "With the rapid development of deep learning techniques, the generation and\n",
      "counterfeiting of multimedia material are becoming increasingly straightforward\n",
      "to perform. At the same time, sharing fake content on the web has become so\n",
      "simple that malicious users can create unpleasant situations with minimal\n",
      "effort. Also, forged media are getting more and more complex, with manipulated\n",
      "videos that are taking the scene over still images. The multimedia forensic\n",
      "community has addressed the possible threats that this situation could imply by\n",
      "developing detectors that verify the authenticity of multimedia objects.\n",
      "However, the vast majority of these tools only analyze one modality at a time.\n",
      "This was not a problem as long as still images were considered the most widely\n",
      "edited media, but now, since manipulated videos are becoming customary,\n",
      "performing monomodal analyses could be reductive. Nonetheless, there is a lack\n",
      "in the literature regarding multimodal detectors, mainly due to the scarsity of\n",
      "datasets containing forged multimodal data to train and test the designed\n",
      "algorithms. In this paper we focus on the generation of an audio-visual\n",
      "deepfake dataset. First, we present a general pipeline for synthesizing speech\n",
      "deepfake content from a given real or fake video, facilitating the creation of\n",
      "counterfeit multimodal material. The proposed method uses Text-to-Speech (TTS)\n",
      "and Dynamic Time Warping techniques to achieve realistic speech tracks. Then,\n",
      "we use the pipeline to generate and release TIMIT-TTS, a synthetic speech\n",
      "dataset containing the most cutting-edge methods in the TTS field. This can be\n",
      "used as a standalone audio dataset, or combined with other state-of-the-art\n",
      "sets to perform multimodal research. Finally, we present numerous experiments\n",
      "to benchmark the proposed dataset in both mono and multimodal conditions,\n",
      "showing the need for multimodal forensic detectors and more suitable data. \n",
      "\n",
      "\n",
      "With the rapid development of face forgery technology, deepfake videos have\n",
      "attracted widespread attention in digital media. Perpetrators heavily utilize\n",
      "these videos to spread disinformation and make misleading statements. Most\n",
      "existing methods for deepfake detection mainly focus on texture features, which\n",
      "are likely to be impacted by external fluctuations, such as illumination and\n",
      "noise. Besides, detection methods based on facial landmarks are more robust\n",
      "against external variables but lack sufficient detail. Thus, how to effectively\n",
      "mine distinctive features in the spatial, temporal, and frequency domains and\n",
      "fuse them with facial landmarks for forgery video detection is still an open\n",
      "question. To this end, we propose a Landmark Enhanced Multimodal Graph Neural\n",
      "Network (LEM-GNN) based on multiple modalities' information and geometric\n",
      "features of facial landmarks. Specifically, at the frame level, we have\n",
      "designed a fusion mechanism to mine a joint representation of the spatial and\n",
      "frequency domain elements while introducing geometric facial features to\n",
      "enhance the robustness of the model. At the video level, we first regard each\n",
      "frame in a video as a node in a graph and encode temporal information into the\n",
      "edges of the graph. Then, by applying the message passing mechanism of the\n",
      "graph neural network (GNN), the multimodal feature will be effectively combined\n",
      "to obtain a comprehensive representation of the video forgery. Extensive\n",
      "experiments show that our method consistently outperforms the state-of-the-art\n",
      "(SOTA) on widely-used benchmarks. \n",
      "\n",
      "\n",
      "Recently, Deepfake has drawn considerable public attention due to security\n",
      "and privacy concerns in social media digital forensics. As the wildly spreading\n",
      "Deepfake videos on the Internet become more realistic, traditional detection\n",
      "techniques have failed in distinguishing between the real and fake. Most\n",
      "existing deep learning methods mainly focus on local features and relations\n",
      "within the face image using convolutional neural networks as a backbone.\n",
      "However, local features and relations are insufficient for model training to\n",
      "learn enough general information for Deepfake detection. Therefore, the\n",
      "existing Deepfake detection methods have reached a bottleneck to further\n",
      "improving the detection performance. To address this issue, we propose a deep\n",
      "convolutional Transformer to incorporate the decisive image features both\n",
      "locally and globally. Specifically, we apply convolutional pooling and\n",
      "re-attention to enrich the extracted features and enhance the efficacy.\n",
      "Moreover, we employ the barely discussed image keyframes in model training for\n",
      "performance improvement and visualize the feature quantity gap between the key\n",
      "and normal image frames caused by video compression. We finally illustrate the\n",
      "transferability with extensive experiments on several Deepfake benchmark\n",
      "datasets. The proposed solution consistently outperforms several\n",
      "state-of-the-art baselines on both within- and cross-dataset experiments. \n",
      "\n",
      "\n",
      "Over a five-year period, computing methods for generating high-fidelity,\n",
      "fictional depictions of people and events moved from exotic demonstrations by\n",
      "computer science research teams into ongoing use as a tool of disinformation.\n",
      "The methods, referred to with the portmanteau of \"deepfakes,\" have been used to\n",
      "create compelling audiovisual content. Here, I share challenges ahead with\n",
      "malevolent uses of two classes of deepfakes that we can expect to come into\n",
      "practice with costly implications for society: interactive and compositional\n",
      "deepfakes. Interactive deepfakes have the capability to impersonate people with\n",
      "realistic interactive behaviors, taking advantage of advances in multimodal\n",
      "interaction. Compositional deepfakes leverage synthetic content in larger\n",
      "disinformation plans that integrate sets of deepfakes over time with observed,\n",
      "expected, and engineered world events to create persuasive synthetic histories.\n",
      "Synthetic histories can be constructed manually but may one day be guided by\n",
      "adversarial generative explanation (AGE) techniques. In the absence of\n",
      "mitigations, interactive and compositional deepfakes threaten to move us closer\n",
      "to a post-epistemic world, where fact cannot be distinguished from fiction. I\n",
      "shall describe interactive and compositional deepfakes and reflect about\n",
      "cautions and potential mitigations to defend against them. \n",
      "\n",
      "\n",
      "Over the last few decades, many aspects of human life have been enhanced with\n",
      "virtual domains, from the advent of digital assistants such as Amazon's Alexa\n",
      "and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These\n",
      "trends underscore the importance of generating photorealistic visual depictions\n",
      "of humans. This has led to the rapid growth of so-called deepfake and talking\n",
      "head generation methods in recent years. Despite their impressive results and\n",
      "popularity, they usually lack certain qualitative aspects such as texture\n",
      "quality, lips synchronization, or resolution, and practical aspects such as the\n",
      "ability to run in real-time. To allow for virtual human avatars to be used in\n",
      "practical scenarios, we propose an end-to-end framework for synthesizing\n",
      "high-quality virtual human faces capable of speech with a special emphasis on\n",
      "performance. We introduce a novel network utilizing visemes as an intermediate\n",
      "audio representation and a novel data augmentation strategy employing a\n",
      "hierarchical image synthesis approach that allows disentanglement of the\n",
      "different modalities used to control the global head motion. Our method runs in\n",
      "real-time, and is able to deliver superior results compared to the current\n",
      "state-of-the-art. \n",
      "\n",
      "\n",
      "Human motion transfer refers to synthesizing photo-realistic and temporally\n",
      "coherent videos that enable one person to imitate the motion of others.\n",
      "However, current synthetic videos suffer from the temporal inconsistency in\n",
      "sequential frames that significantly degrades the video quality, yet is far\n",
      "from solved by existing methods in the pixel domain. Recently, some works on\n",
      "DeepFake detection try to distinguish the natural and synthetic images in the\n",
      "frequency domain because of the frequency insufficiency of image synthesizing\n",
      "methods. Nonetheless, there is no work to study the temporal inconsistency of\n",
      "synthetic videos from the aspects of the frequency-domain gap between natural\n",
      "and synthetic videos. In this paper, we propose to delve into the frequency\n",
      "space for temporally consistent human motion transfer. First of all, we make\n",
      "the first comprehensive analysis of natural and synthetic videos in the\n",
      "frequency domain to reveal the frequency gap in both the spatial dimension of\n",
      "individual frames and the temporal dimension of the video. To close the\n",
      "frequency gap between the natural and synthetic videos, we propose a novel\n",
      "Frequency-based human MOtion TRansfer framework, named FreMOTR, which can\n",
      "effectively mitigate the spatial artifacts and the temporal inconsistency of\n",
      "the synthesized videos. FreMOTR explores two novel frequency-based\n",
      "regularization modules: 1) the Frequency-domain Appearance Regularization (FAR)\n",
      "to improve the appearance of the person in individual frames and 2) Temporal\n",
      "Frequency Regularization (TFR) to guarantee the temporal consistency between\n",
      "adjacent frames. Finally, comprehensive experiments demonstrate that the\n",
      "FreMOTR not only yields superior performance in temporal consistency metrics\n",
      "but also improves the frame-level visual quality of synthetic videos. In\n",
      "particular, the temporal consistency metrics are improved by nearly 30% than\n",
      "the state-of-the-art model. \n",
      "\n",
      "\n",
      "Fake content has grown at an incredible rate over the past few years. The\n",
      "spread of social media and online platforms makes their dissemination on a\n",
      "large scale increasingly accessible by malicious actors. In parallel, due to\n",
      "the growing diffusion of fake image generation methods, many Deep\n",
      "Learning-based detection techniques have been proposed. Most of those methods\n",
      "rely on extracting salient features from RGB images to detect through a binary\n",
      "classifier if the image is fake or real. In this paper, we proposed DepthFake,\n",
      "a study on how to improve classical RGB-based approaches with depth-maps. The\n",
      "depth information is extracted from RGB images with recent monocular depth\n",
      "estimation techniques. Here, we demonstrate the effective contribution of\n",
      "depth-maps to the deepfake detection task on robust pre-trained architectures.\n",
      "The proposed RGBD approach is in fact able to achieve an average improvement of\n",
      "3.20% and up to 11.7% for some deepfake attacks with respect to standard RGB\n",
      "architectures over the FaceForensic++ dataset. \n",
      "\n",
      "\n",
      "Recent advancements in AI, especially deep learning, have contributed to a\n",
      "significant increase in the creation of new realistic-looking synthetic media\n",
      "(video, image, and audio) and manipulation of existing media, which has led to\n",
      "the creation of the new term ``deepfake''. Based on both the research\n",
      "literature and resources in English and in Chinese, this paper gives a\n",
      "comprehensive overview of deepfake, covering multiple important aspects of this\n",
      "emerging concept, including 1) different definitions, 2) commonly used\n",
      "performance metrics and standards, and 3) deepfake-related datasets,\n",
      "challenges, competitions and benchmarks. In addition, the paper also reports a\n",
      "meta-review of 12 selected deepfake-related survey papers published in 2020 and\n",
      "2021, focusing not only on the mentioned aspects, but also on the analysis of\n",
      "key challenges and recommendations. We believe that this paper is the most\n",
      "comprehensive review of deepfake in terms of aspects covered, and the first one\n",
      "covering both the English and Chinese literature and sources. \n",
      "\n",
      "\n",
      "Many effective attempts have been made for deepfake audio detection. However,\n",
      "they can only distinguish between real and fake. For many practical application\n",
      "scenarios, what tool or algorithm generated the deepfake audio also is needed.\n",
      "This raises a question: Can we detect the system fingerprints of deepfake\n",
      "audio? Therefore, this paper conducts a preliminary investigation to detect\n",
      "system fingerprints of deepfake audio. Experiments are conducted on deepfake\n",
      "audio datasets from five latest deep-learning speech synthesis systems. The\n",
      "results show that LFCC features are relatively more suitable for system\n",
      "fingerprints detection. Moreover, the ResNet achieves the best detection\n",
      "results among LCNN and x-vector based models. The t-SNE visualization shows\n",
      "that different speech synthesis systems generate distinct system fingerprints. \n",
      "\n",
      "\n",
      "Many effective attempts have been made for fake audio detection. However,\n",
      "they can only provide detection results but no countermeasures to curb this\n",
      "harm. For many related practical applications, what model or algorithm\n",
      "generated the fake audio also is needed. Therefore, We propose a new problem\n",
      "for detecting vocoder fingerprints of fake audio. Experiments are conducted on\n",
      "the datasets synthesized by eight state-of-the-art vocoders. We have\n",
      "preliminarily explored the features and model architectures. The t-SNE\n",
      "visualization shows that different vocoders generate distinct vocoder\n",
      "fingerprints. \n",
      "\n",
      "\n",
      "Social engineering (SE) is a form of deception that aims to trick people into\n",
      "giving access to data, information, networks and even money. For decades SE has\n",
      "been a key method for attackers to gain access to an organization, virtually\n",
      "skipping all lines of defense. Attackers also regularly use SE to scam innocent\n",
      "people by making threatening phone calls which impersonate an authority or by\n",
      "sending infected emails which look like they have been sent from a loved one.\n",
      "SE attacks will likely remain a top attack vector for criminals because humans\n",
      "are the weakest link in cyber security.\n",
      "  Unfortunately, the threat will only get worse now that a new technology\n",
      "called deepfakes as arrived. A deepfake is believable media (e.g., videos)\n",
      "created by an AI. Although the technology has mostly been used to swap the\n",
      "faces of celebrities, it can also be used to `puppet' different personas.\n",
      "Recently, researchers have shown how this technology can be deployed in\n",
      "real-time to clone someone's voice in a phone call or reenact a face in a video\n",
      "call. Given that any novice user can download this technology to use it, it is\n",
      "no surprise that criminals have already begun to monetize it to perpetrate\n",
      "their SE attacks.\n",
      "  In this paper, we propose a lightweight application which can protect\n",
      "organizations and individuals from deepfake SE attacks. Through a challenge and\n",
      "response approach, we leverage the technical and theoretical limitations of\n",
      "deepfake technologies to expose the attacker. Existing defence solutions are\n",
      "too heavy as an end-point solution and can be evaded by a dynamic attacker. In\n",
      "contrast, our approach is lightweight and breaks the reactive arms race,\n",
      "putting the attacker at a disadvantage. \n",
      "\n",
      "\n",
      "This paper presents our results and findings on the use of temporal images\n",
      "for deepfake detection. We modelled temporal relations that exist in the\n",
      "movement of 468 facial landmarks across frames of a given video as spatial\n",
      "relations by constructing an image (referred to as temporal image) using the\n",
      "pixel values at these facial landmarks. CNNs are capable of recognizing spatial\n",
      "relationships that exist between the pixels of a given image. 10 different\n",
      "ImageNet models were considered for the study. \n",
      "\n",
      "\n",
      "In recent years, image and video manipulations with DeepFake have become a\n",
      "severe concern for security and society. Therefore, many detection models and\n",
      "databases have been proposed to detect DeepFake data reliably. However, there\n",
      "is an increased concern that these models and training databases might be\n",
      "biased and thus, cause DeepFake detectors to fail. In this work, we tackle\n",
      "these issues by (a) providing large-scale demographic and non-demographic\n",
      "attribute annotations of 41 different attributes for five popular DeepFake\n",
      "datasets and (b) comprehensively analysing AI-bias of multiple state-of-the-art\n",
      "DeepFake detection models on these databases. The investigation analyses the\n",
      "influence of a large variety of distinctive attributes (from over 65M labels)\n",
      "on the detection performance, including demographic (age, gender, ethnicity)\n",
      "and non-demographic (hair, skin, accessories, etc.) information. The results\n",
      "indicate that investigated databases lack diversity and, more importantly, show\n",
      "that the utilised DeepFake detection models are strongly biased towards many\n",
      "investigated attributes. Moreover, the results show that the models'\n",
      "decision-making might be based on several questionable (biased) assumptions,\n",
      "such if a person is smiling or wearing a hat. Depending on the application of\n",
      "such DeepFake detection methods, these biases can lead to generalizability,\n",
      "fairness, and security issues. We hope that the findings of this study and the\n",
      "annotation databases will help to evaluate and mitigate bias in future DeepFake\n",
      "detection techniques. Our annotation datasets are made publicly available. \n",
      "\n",
      "\n",
      "Deepfake media is becoming widespread nowadays because of the easily\n",
      "available tools and mobile apps which can generate realistic looking deepfake\n",
      "videos/images without requiring any technical knowledge. With further advances\n",
      "in this field of technology in the near future, the quantity and quality of\n",
      "deepfake media is also expected to flourish, while making deepfake media a\n",
      "likely new practical tool to spread mis/disinformation. Because of these\n",
      "concerns, the deepfake media detection tools are becoming a necessity. In this\n",
      "study, we propose a novel hybrid transformer network utilizing early feature\n",
      "fusion strategy for deepfake video detection. Our model employs two different\n",
      "CNN networks, i.e., (1) XceptionNet and (2) EfficientNet-B4 as feature\n",
      "extractors. We train both feature extractors along with the transformer in an\n",
      "end-to-end manner on FaceForensics++, DFDC benchmarks. Our model, while having\n",
      "relatively straightforward architecture, achieves comparable results to other\n",
      "more advanced state-of-the-art approaches when evaluated on FaceForensics++ and\n",
      "DFDC benchmarks. Besides this, we also propose novel face cut-out\n",
      "augmentations, as well as random cut-out augmentations. We show that the\n",
      "proposed augmentations improve the detection performance of our model and\n",
      "reduce overfitting. In addition to that, we show that our model is capable of\n",
      "learning from considerably small amount of data. \n",
      "\n",
      "\n",
      "Face anti-spoofing (FAS) and face forgery detection play vital roles in\n",
      "securing face biometric systems from presentation attacks (PAs) and vicious\n",
      "digital manipulation (e.g., deepfakes). Despite promising performance upon\n",
      "large-scale data and powerful deep models, the generalization problem of\n",
      "existing approaches is still an open issue. Most of recent approaches focus on\n",
      "1) unimodal visual appearance or physiological (i.e., remote\n",
      "photoplethysmography (rPPG)) cues; and 2) separated feature representation for\n",
      "FAS or face forgery detection. On one side, unimodal appearance and rPPG\n",
      "features are respectively vulnerable to high-fidelity face 3D mask and video\n",
      "replay attacks, inspiring us to design reliable multi-modal fusion mechanisms\n",
      "for generalized face attack detection. On the other side, there are rich common\n",
      "features across FAS and face forgery detection tasks (e.g., periodic rPPG\n",
      "rhythms and vanilla appearance for bonafides), providing solid evidence to\n",
      "design a joint FAS and face forgery detection system in a multi-task learning\n",
      "fashion. In this paper, we establish the first joint face spoofing and forgery\n",
      "detection benchmark using both visual appearance and physiological rPPG cues.\n",
      "To enhance the rPPG periodicity discrimination, we design a two-branch\n",
      "physiological network using both facial spatio-temporal rPPG signal map and its\n",
      "continuous wavelet transformed counterpart as inputs. To mitigate the modality\n",
      "bias and improve the fusion efficacy, we conduct a weighted batch and layer\n",
      "normalization for both appearance and rPPG features before multi-modal fusion.\n",
      "We find that the generalization capacities of both unimodal (appearance or\n",
      "rPPG) and multi-modal (appearance+rPPG) models can be obviously improved via\n",
      "joint training on these two tasks. We hope this new benchmark will facilitate\n",
      "the future research of both FAS and deepfake detection communities. \n",
      "\n",
      "\n",
      "There is strong interest in the generation of synthetic video imagery of\n",
      "people talking for various purposes, including entertainment, communication,\n",
      "training, and advertisement. With the development of deep fake generation\n",
      "models, synthetic video imagery will soon be visually indistinguishable to the\n",
      "naked eye from a naturally capture video. In addition, many methods are\n",
      "continuing to improve to avoid more careful, forensic visual analysis. Some\n",
      "deep fake videos are produced through the use of facial puppetry, which\n",
      "directly controls the head and face of the synthetic image through the\n",
      "movements of the actor, allow the actor to 'puppet' the image of another. In\n",
      "this paper, we address the question of whether one person's movements can be\n",
      "distinguished from the original speaker by controlling the visual appearance of\n",
      "the speaker but transferring the behavior signals from another source. We\n",
      "conduct a study by comparing synthetic imagery that: 1) originates from a\n",
      "different person speaking a different utterance, 2) originates from the same\n",
      "person speaking a different utterance, and 3) originates from a different\n",
      "person speaking the same utterance. Our study shows that synthetic videos in\n",
      "all three cases are seen as less real and less engaging than the original\n",
      "source video. Our results indicate that there could be a behavioral signature\n",
      "that is detectable from a person's movements that is separate from their visual\n",
      "appearance, and that this behavioral signature could be used to distinguish a\n",
      "deep fake from a properly captured video. \n",
      "\n",
      "\n",
      "Advances in AI based computer vision has led to a significant growth in\n",
      "synthetic image generation and artificial image tampering with serious\n",
      "implications for unethical exploitations that undermine person identification\n",
      "and could make render AI predictions less explainable.Morphing, Deepfake and\n",
      "other artificial generation of face photographs undermine the reliability of\n",
      "face biometrics authentication using different electronic ID documents.Morphed\n",
      "face photographs on e-passports can fool automated border control systems and\n",
      "human guards.This paper extends our previous work on using the persistent\n",
      "homology (PH) of texture landmarks to detect morphing attacks.We demonstrate\n",
      "that artificial image tampering distorts the spatial distribution of texture\n",
      "landmarks (i.e. their PH) as well as that of a set of image quality\n",
      "characteristics.We shall demonstrate that the tamper caused distortion of these\n",
      "two slim feature vectors provide significant potentials for building\n",
      "explainable (Handcrafted) tamper detectors with low error rates and suitable\n",
      "for implementation on constrained devices. \n",
      "\n",
      "\n",
      "Recently, pioneer research works have proposed a large number of acoustic\n",
      "features (log power spectrogram, linear frequency cepstral coefficients,\n",
      "constant Q cepstral coefficients, etc.) for audio deepfake detection, obtaining\n",
      "good performance, and showing that different subbands have different\n",
      "contributions to audio deepfake detection. However, this lacks an explanation\n",
      "of the specific information in the subband, and these features also lose\n",
      "information such as phase. Inspired by the mechanism of synthetic speech, the\n",
      "fundamental frequency (F0) information is used to improve the quality of\n",
      "synthetic speech, while the F0 of synthetic speech is still too average, which\n",
      "differs significantly from that of real speech. It is expected that F0 can be\n",
      "used as important information to discriminate between bonafide and fake speech,\n",
      "while this information cannot be used directly due to the irregular\n",
      "distribution of F0. Insteadly, the frequency band containing most of F0 is\n",
      "selected as the input feature. Meanwhile, to make full use of the phase and\n",
      "full-band information, we also propose to use real and imaginary spectrogram\n",
      "features as complementary input features and model the disjoint subbands\n",
      "separately. Finally, the results of F0, real and imaginary spectrogram features\n",
      "are fused. Experimental results on the ASVspoof 2019 LA dataset show that our\n",
      "proposed system is very effective for the audio deepfake detection task,\n",
      "achieving an equivalent error rate (EER) of 0.43%, which surpasses almost all\n",
      "systems. \n",
      "\n",
      "\n",
      "Cheapfake is a recently coined term that encompasses non-AI (``cheap'')\n",
      "manipulations of multimedia content. Cheapfakes are known to be more prevalent\n",
      "than deepfakes. Cheapfake media can be created using editing software for\n",
      "image/video manipulations, or even without using any software, by simply\n",
      "altering the context of an image/video by sharing the media alongside\n",
      "misleading claims. This alteration of context is referred to as out-of-context\n",
      "(OOC) misuse of media. OOC media is much harder to detect than fake media,\n",
      "since the images and videos are not tampered. In this challenge, we focus on\n",
      "detecting OOC images, and more specifically the misuse of real photographs with\n",
      "conflicting image captions in news items. The aim of this challenge is to\n",
      "develop and benchmark models that can be used to detect whether given samples\n",
      "(news image and associated captions) are OOC, based on the recently compiled\n",
      "COSMOS dataset. \n",
      "\n",
      "\n",
      "Deepfakes are the synthesized digital media in order to create\n",
      "ultra-realistic fake videos to trick the spectator. Deep generative algorithms,\n",
      "such as, Generative Adversarial Networks(GAN) are widely used to accomplish\n",
      "such tasks. This approach synthesizes pseudo-realistic contents that are very\n",
      "difficult to distinguish by traditional detection methods. In most cases,\n",
      "Convolutional Neural Network(CNN) based discriminators are being used for\n",
      "detecting such synthesized media. However, it emphasise primarily on the\n",
      "spatial attributes of individual video frames, thereby fail to learn the\n",
      "temporal information from their inter-frame relations. In this paper, we\n",
      "leveraged an optical flow based feature extraction approach to extract the\n",
      "temporal features, which are then fed to a hybrid model for classification.\n",
      "This hybrid model is based on the combination of CNN and recurrent neural\n",
      "network (RNN) architectures. The hybrid model provides effective performance on\n",
      "open source data-sets such as, DFDC, FF++ and Celeb-DF. This proposed method\n",
      "shows an accuracy of 66.26%, 91.21% and 79.49% in DFDC, FF++, and Celeb-DF\n",
      "respectively with a very reduced No of sample size of approx 100\n",
      "samples(frames). This promises early detection of fake contents compared to\n",
      "existing modalities. \n",
      "\n",
      "\n",
      "In the recent years, social media has grown to become a major source of\n",
      "information for many online users. This has given rise to the spread of\n",
      "misinformation through deepfakes. Deepfakes are videos or images that replace\n",
      "one persons face with another computer-generated face, often a more\n",
      "recognizable person in society. With the recent advances in technology, a\n",
      "person with little technological experience can generate these videos. This\n",
      "enables them to mimic a power figure in society, such as a president or\n",
      "celebrity, creating the potential danger of spreading misinformation and other\n",
      "nefarious uses of deepfakes. To combat this online threat, researchers have\n",
      "developed models that are designed to detect deepfakes. This study looks at\n",
      "various deepfake detection models that use deep learning algorithms to combat\n",
      "this looming threat. This survey focuses on providing a comprehensive overview\n",
      "of the current state of deepfake detection models and the unique approaches\n",
      "many researchers take to solving this problem. The benefits, limitations, and\n",
      "suggestions for future work will be thoroughly discussed throughout this paper. \n",
      "\n",
      "\n",
      "As tools for content editing mature, and artificial intelligence (AI) based\n",
      "algorithms for synthesizing media grow, the presence of manipulated content\n",
      "across online media is increasing. This phenomenon causes the spread of\n",
      "misinformation, creating a greater need to distinguish between \"real\" and\n",
      "\"manipulated\" content. To this end, we present VideoSham, a dataset consisting\n",
      "of 826 videos (413 real and 413 manipulated). Many of the existing deepfake\n",
      "datasets focus exclusively on two types of facial manipulations -- swapping\n",
      "with a different subject's face or altering the existing face. VideoSham, on\n",
      "the other hand, contains more diverse, context-rich, and human-centric,\n",
      "high-resolution videos manipulated using a combination of 6 different spatial\n",
      "and temporal attacks. Our analysis shows that state-of-the-art manipulation\n",
      "detection algorithms only work for a few specific attacks and do not scale well\n",
      "on VideoSham. We performed a user study on Amazon Mechanical Turk with 1200\n",
      "participants to understand if they can differentiate between the real and\n",
      "manipulated videos in VideoSham. Finally, we dig deeper into the strengths and\n",
      "weaknesses of performances by humans and SOTA-algorithms to identify gaps that\n",
      "need to be filled with better AI algorithms. \n",
      "\n",
      "\n",
      "We revisit the classic signal-to-symbol barrier in light of the remarkable\n",
      "ability of deep neural networks to generate realistic synthetic data. DeepFakes\n",
      "and spoofing highlight the feebleness of the link between physical reality and\n",
      "its abstract representation, whether learned by a digital computer or a\n",
      "biological agent. Starting from a widely applicable definition of abstract\n",
      "concept, we show that standard feed-forward architectures cannot capture but\n",
      "trivial concepts, regardless of the number of weights and the amount of\n",
      "training data, despite being extremely effective classifiers. On the other\n",
      "hand, architectures that incorporate recursion can represent a significantly\n",
      "larger class of concepts, but may still be unable to learn them from a finite\n",
      "dataset. We qualitatively describe the class of concepts that can be\n",
      "\"understood\" by modern architectures trained with variants of stochastic\n",
      "gradient descent, using a (free energy) Lagrangian to measure information\n",
      "complexity. Even if a concept has been understood, however, a network has no\n",
      "means of communicating its understanding to an external agent, except through\n",
      "continuous interaction and validation. We then characterize physical objects as\n",
      "abstract concepts and use the previous analysis to show that physical objects\n",
      "can be encoded by finite architectures. However, to understand physical\n",
      "concepts, sensors must provide persistently exciting observations, for which\n",
      "the ability to control the data acquisition process is essential (active\n",
      "perception). The importance of control depends on the modality, benefiting\n",
      "visual more than acoustic or chemical perception. Finally, we conclude that\n",
      "binding physical entities to digital identities is possible in finite time with\n",
      "finite resources, solving in principle the signal-to-symbol barrier problem,\n",
      "but we highlight the need for continuous validation. \n",
      "\n",
      "\n",
      "Advancements in generative models, like Deepfake allows users to imitate a\n",
      "targeted person and manipulate online interactions. It has been recognized that\n",
      "disinformation may cause disturbance in society and ruin the foundation of\n",
      "trust. This article presents DeFakePro, a decentralized consensus\n",
      "mechanism-based Deepfake detection technique in online video conferencing\n",
      "tools. Leveraging Electrical Network Frequency (ENF), an environmental\n",
      "fingerprint embedded in digital media recording, affords a consensus mechanism\n",
      "design called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal\n",
      "fluctuations is utilized in the PoENF algorithm to authenticate the media\n",
      "broadcasted in conferencing tools. By utilizing the video conferencing setup\n",
      "with malicious participants to broadcast deep fake video recordings to other\n",
      "participants, the DeFakePro system verifies the authenticity of the incoming\n",
      "media in both audio and video channels. \n",
      "\n",
      "\n",
      "Despite encouraging progress in deepfake detection, generalization to unseen\n",
      "forgery types remains a significant challenge due to the limited forgery clues\n",
      "explored during training. In contrast, we notice a common phenomenon in\n",
      "deepfake: fake video creation inevitably disrupts the statistical regularity in\n",
      "original videos. Inspired by this observation, we propose to boost the\n",
      "generalization of deepfake detection by distinguishing the \"regularity\n",
      "disruption\" that does not appear in real videos. Specifically, by carefully\n",
      "examining the spatial and temporal properties, we propose to disrupt a real\n",
      "video through a Pseudo-fake Generator and create a wide range of pseudo-fake\n",
      "videos for training. Such practice allows us to achieve deepfake detection\n",
      "without using fake videos and improves the generalization ability in a simple\n",
      "and efficient manner. To jointly capture the spatial and temporal disruptions,\n",
      "we propose a Spatio-Temporal Enhancement block to learn the regularity\n",
      "disruption across space and time on our self-created videos. Through\n",
      "comprehensive experiments, our method exhibits excellent performance on several\n",
      "datasets. \n",
      "\n",
      "\n",
      "Facial forgery by deepfakes has raised severe societal concerns. Several\n",
      "solutions have been proposed by the vision community to effectively combat the\n",
      "misinformation on the internet via automated deepfake detection systems. Recent\n",
      "studies have demonstrated that facial analysis-based deep learning models can\n",
      "discriminate based on protected attributes. For the commercial adoption and\n",
      "massive roll-out of the deepfake detection technology, it is vital to evaluate\n",
      "and understand the fairness (the absence of any prejudice or favoritism) of\n",
      "deepfake detectors across demographic variations such as gender and race. As\n",
      "the performance differential of deepfake detectors between demographic\n",
      "subgroups would impact millions of people of the deprived sub-group. This paper\n",
      "aims to evaluate the fairness of the deepfake detectors across males and\n",
      "females. However, existing deepfake datasets are not annotated with demographic\n",
      "labels to facilitate fairness analysis. To this aim, we manually annotated\n",
      "existing popular deepfake datasets with gender labels and evaluated the\n",
      "performance differential of current deepfake detectors across gender. Our\n",
      "analysis on the gender-labeled version of the datasets suggests (a) current\n",
      "deepfake datasets have skewed distribution across gender, and (b) commonly\n",
      "adopted deepfake detectors obtain unequal performance across gender with mostly\n",
      "males outperforming females. Finally, we contributed a gender-balanced and\n",
      "annotated deepfake dataset, GBDF, to mitigate the performance differential and\n",
      "to promote research and development towards fairness-aware deep fake detectors.\n",
      "The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF \n",
      "\n",
      "\n",
      "This paper aims to interpret how deepfake detection models learn artifact\n",
      "features of images when just supervised by binary labels. To this end, three\n",
      "hypotheses from the perspective of image matching are proposed as follows. 1.\n",
      "Deepfake detection models indicate real/fake images based on visual concepts\n",
      "that are neither source-relevant nor target-relevant, that is, considering such\n",
      "visual concepts as artifact-relevant. 2. Besides the supervision of binary\n",
      "labels, deepfake detection models implicitly learn artifact-relevant visual\n",
      "concepts through the FST-Matching (i.e. the matching fake, source, target\n",
      "images) in the training set. 3. Implicitly learned artifact visual concepts\n",
      "through the FST-Matching in the raw training set are vulnerable to video\n",
      "compression. In experiments, the above hypotheses are verified among various\n",
      "DNNs. Furthermore, based on this understanding, we propose the FST-Matching\n",
      "Deepfake Detection Model to boost the performance of forgery detection on\n",
      "compressed videos. Experiment results show that our method achieves great\n",
      "performance, especially on highly-compressed (e.g. c40) videos. \n",
      "\n",
      "\n",
      "Deepfakes can degrade the fabric of society by limiting our ability to trust\n",
      "video content from leaders, authorities, and even friends. Cryptographically\n",
      "secure digital signatures may be used by video streaming platforms to endorse\n",
      "content, but these signatures are applied by the content distributor rather\n",
      "than the participants in the video. We introduce WordSig, a simple protocol\n",
      "allowing video participants to digitally sign the words they speak using a\n",
      "stream of QR codes, and allowing viewers to verify the consistency of\n",
      "signatures across videos. This allows establishing a trusted connection between\n",
      "the viewer and the participant that is not mediated by the content distributor.\n",
      "Given the widespread adoption of QR codes for distributing hyperlinks and\n",
      "vaccination records, and the increasing prevalence of celebrity deepfakes, 2022\n",
      "or later may be a good time for public figures to begin using and promoting\n",
      "QR-based self-authentication tools. \n",
      "\n",
      "\n",
      "While the abuse of deepfake technology has caused serious concerns recently,\n",
      "how to detect deepfake videos is still a challenge due to the high\n",
      "photo-realistic synthesis of each frame. Existing image-level approaches often\n",
      "focus on single frame and ignore the spatiotemporal cues hidden in deepfake\n",
      "videos, resulting in poor generalization and robustness. The key of a\n",
      "video-level detector is to fully exploit the spatiotemporal inconsistency\n",
      "distributed in local facial regions across different frames in deepfake videos.\n",
      "Inspired by that, this paper proposes a simple yet effective patch-level\n",
      "approach to facilitate deepfake video detection via spatiotemporal dropout\n",
      "transformer. The approach reorganizes each input video into bag of patches that\n",
      "is then fed into a vision transformer to achieve robust representation.\n",
      "Specifically, a spatiotemporal dropout operation is proposed to fully explore\n",
      "patch-level spatiotemporal cues and serve as effective data augmentation to\n",
      "further enhance model's robustness and generalization ability. The operation is\n",
      "flexible and can be easily plugged into existing vision transformers. Extensive\n",
      "experiments demonstrate the effectiveness of our approach against 25\n",
      "state-of-the-arts with impressive robustness, generalizability, and\n",
      "representation ability. \n",
      "\n",
      "\n",
      "Deepfake face not only violates the privacy of personal identity, but also\n",
      "confuses the public and causes huge social harm. The current deepfake detection\n",
      "only stays at the level of distinguishing true and false, and cannot trace the\n",
      "original genuine face corresponding to the fake face, that is, it does not have\n",
      "the ability to trace the source of evidence. The deepfake countermeasure\n",
      "technology for judicial forensics urgently calls for deepfake traceability.\n",
      "This paper pioneers an interesting question about face deepfake, active\n",
      "forensics that \"know it and how it happened\". Given that deepfake faces do not\n",
      "completely discard the features of original faces, especially facial\n",
      "expressions and poses, we argue that original faces can be approximately\n",
      "speculated from their deepfake counterparts. Correspondingly, we design a\n",
      "disentangling reversing network that decouples latent space features of\n",
      "deepfake faces under the supervision of fake-original face pair samples to\n",
      "infer original faces in reverse. \n",
      "\n",
      "\n",
      "Recent advances in face forgery techniques produce nearly visually\n",
      "untraceable deepfake videos, which could be leveraged with malicious\n",
      "intentions. As a result, researchers have been devoted to deepfake detection.\n",
      "Previous studies have identified the importance of local low-level cues and\n",
      "temporal information in pursuit to generalize well across deepfake methods,\n",
      "however, they still suffer from robustness problem against post-processings. In\n",
      "this work, we propose the Local- & Temporal-aware Transformer-based Deepfake\n",
      "Detection (LTTD) framework, which adopts a local-to-global learning protocol\n",
      "with a particular focus on the valuable temporal information within local\n",
      "sequences. Specifically, we propose a Local Sequence Transformer (LST), which\n",
      "models the temporal consistency on sequences of restricted spatial regions,\n",
      "where low-level information is hierarchically enhanced with shallow layers of\n",
      "learned 3D filters. Based on the local temporal embeddings, we then achieve the\n",
      "final classification in a global contrastive way. Extensive experiments on\n",
      "popular datasets validate that our approach effectively spots local forgery\n",
      "cues and achieves state-of-the-art performance. \n",
      "\n",
      "\n",
      "Since photorealistic faces can be readily generated by facial manipulation\n",
      "technologies nowadays, potential malicious abuse of these technologies has\n",
      "drawn great concerns. Numerous deepfake detection methods are thus proposed.\n",
      "However, existing methods only focus on detecting one-step facial manipulation.\n",
      "As the emergence of easy-accessible facial editing applications, people can\n",
      "easily manipulate facial components using multi-step operations in a sequential\n",
      "manner. This new threat requires us to detect a sequence of facial\n",
      "manipulations, which is vital for both detecting deepfake media and recovering\n",
      "original faces afterwards. Motivated by this observation, we emphasize the need\n",
      "and propose a novel research problem called Detecting Sequential DeepFake\n",
      "Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only\n",
      "demanding a binary label prediction, detecting Seq-DeepFake manipulation\n",
      "requires correctly predicting a sequential vector of facial manipulation\n",
      "operations. To support a large-scale investigation, we construct the first\n",
      "Seq-DeepFake dataset, where face images are manipulated sequentially with\n",
      "corresponding annotations of sequential facial manipulation vectors. Based on\n",
      "this new dataset, we cast detecting Seq-DeepFake manipulation as a specific\n",
      "image-to-sequence (e.g. image captioning) task and propose a concise yet\n",
      "effective Seq-DeepFake Transformer (SeqFakeFormer). Moreover, we build a\n",
      "comprehensive benchmark and set up rigorous evaluation protocols and metrics\n",
      "for this new research problem. Extensive experiments demonstrate the\n",
      "effectiveness of SeqFakeFormer. Several valuable observations are also revealed\n",
      "to facilitate future research in broader deepfake detection problems. \n",
      "\n",
      "\n",
      "Machine learning models are often brittle on production data despite\n",
      "achieving high accuracy on benchmark datasets. Benchmark datasets have\n",
      "traditionally served dual purposes: first, benchmarks offer a standard on which\n",
      "machine learning researchers can compare different methods, and second,\n",
      "benchmarks provide a model, albeit imperfect, of the real world. The\n",
      "incompleteness of test benchmarks (and the data upon which models are trained)\n",
      "hinder robustness in machine learning, enable shortcut learning, and leave\n",
      "models systematically prone to err on out-of-distribution and adversarially\n",
      "perturbed data. The mismatch between a single static benchmark dataset and a\n",
      "production dataset has traditionally been described as a dataset shift. In an\n",
      "effort to clarify how to address the mismatch between test benchmarks and\n",
      "production data, we introduce context shift to describe semantically meaningful\n",
      "changes in the underlying data generation process. Moreover, we identify three\n",
      "methods for addressing context shift that would otherwise lead to model\n",
      "prediction errors: first, we describe how human intuition and expert knowledge\n",
      "can identify semantically meaningful features upon which models systematically\n",
      "fail, second, we detail how dynamic benchmarking - with its focus on capturing\n",
      "the data generation process - can promote generalizability through\n",
      "corroboration, and third, we highlight that clarifying a model's limitations\n",
      "can reduce unexpected errors. Robust machine learning is focused on model\n",
      "performance beyond benchmarks, and as such, we consider three model organism\n",
      "domains - facial expression recognition, deepfake detection, and medical\n",
      "diagnosis - to highlight how implicit assumptions in benchmark tasks lead to\n",
      "errors in practice. By paying close attention to the role of context,\n",
      "researchers can design more comprehensive benchmarks, reduce context shift\n",
      "errors, and increase generalizability. \n",
      "\n",
      "\n",
      "This paper presents the summary report on our DFGC 2022 competition. The\n",
      "DeepFake is rapidly evolving, and realistic face-swaps are becoming more\n",
      "deceptive and difficult to detect. On the contrary, methods for detecting\n",
      "DeepFakes are also improving. There is a two-party game between DeepFake\n",
      "creators and defenders. This competition provides a common platform for\n",
      "benchmarking the game between the current state-of-the-arts in DeepFake\n",
      "creation and detection methods. The main research question to be answered by\n",
      "this competition is the current state of the two adversaries when competed with\n",
      "each other. This is the second edition after the last year's DFGC 2021, with a\n",
      "new, more diverse video dataset, a more realistic game setting, and more\n",
      "reasonable evaluation metrics. With this competition, we aim to stimulate\n",
      "research ideas for building better defenses against the DeepFake threats. We\n",
      "also release our DFGC 2022 dataset contributed by both our participants and\n",
      "ourselves to enrich the DeepFake data resources for the research community\n",
      "(https://github.com/NiCE-X/DFGC-2022). \n",
      "\n",
      "\n",
      "Deepfake Generation Techniques are evolving at a rapid pace, making it\n",
      "possible to create realistic manipulated images and videos and endangering the\n",
      "serenity of modern society. The continual emergence of new and varied\n",
      "techniques brings with it a further problem to be faced, namely the ability of\n",
      "deepfake detection models to update themselves promptly in order to be able to\n",
      "identify manipulations carried out using even the most recent methods. This is\n",
      "an extremely complex problem to solve, as training a model requires large\n",
      "amounts of data, which are difficult to obtain if the deepfake generation\n",
      "method is too recent. Moreover, continuously retraining a network would be\n",
      "unfeasible. In this paper, we ask ourselves if, among the various deep learning\n",
      "techniques, there is one that is able to generalise the concept of deepfake to\n",
      "such an extent that it does not remain tied to one or more specific deepfake\n",
      "generation methods used in the training set. We compared a Vision Transformer\n",
      "with an EfficientNetV2 on a cross-forgery context based on the ForgeryNet\n",
      "dataset. From our experiments, It emerges that EfficientNetV2 has a greater\n",
      "tendency to specialize often obtaining better results on training methods while\n",
      "Vision Transformers exhibit a superior generalization ability that makes them\n",
      "more competent even on images generated with new methodologies. \n",
      "\n",
      "\n",
      "Audio DeepFakes allow the creation of high-quality, convincing utterances and\n",
      "therefore pose a threat due to its potential applications such as impersonation\n",
      "or fake news. Methods for detecting these manipulations should be characterized\n",
      "by good generalization and stability leading to robustness against attacks\n",
      "conducted with techniques that are not explicitly included in the training. In\n",
      "this work, we introduce Attack Agnostic Dataset - a combination of two audio\n",
      "DeepFakes and one anti-spoofing datasets that, thanks to the disjoint use of\n",
      "attacks, can lead to better generalization of detection methods. We present a\n",
      "thorough analysis of current DeepFake detection methods and consider different\n",
      "audio features (front-ends). In addition, we propose a model based on LCNN with\n",
      "LFCC and mel-spectrogram front-end, which not only is characterized by a good\n",
      "generalization and stability results but also shows improvement over LFCC-based\n",
      "mode - we decrease standard deviation on all folds and EER in two folds by up\n",
      "to 5%. \n",
      "\n",
      "\n",
      "Recent advances in deep learning have enabled realistic digital alterations\n",
      "to videos, known as deepfakes. This technology raises important societal\n",
      "concerns regarding disinformation and authenticity, galvanizing the development\n",
      "of numerous deepfake detection algorithms. At the same time, there are\n",
      "significant differences between training data and in-the-wild video data, which\n",
      "may undermine their practical efficacy. We simulate data corruption techniques\n",
      "and examine the performance of a state-of-the-art deepfake detection algorithm\n",
      "on corrupted variants of the FaceForensics++ dataset.\n",
      "  While deepfake detection models are robust against video corruptions that\n",
      "align with training-time augmentations, we find that they remain vulnerable to\n",
      "video corruptions that simulate decreases in video quality. Indeed, in the\n",
      "controversial case of the video of Gabonese President Bongo's new year address,\n",
      "the algorithm, which confidently authenticates the original video, judges\n",
      "highly corrupted variants of the video to be fake. Our work opens up both\n",
      "technical and ethical avenues of exploration into practical deepfake detection\n",
      "in global contexts. \n",
      "\n",
      "\n",
      "An optical microscopic examination of thinly cut stained tissue on glass\n",
      "slides prepared from a FFPE tissue blocks is the gold standard for tissue\n",
      "diagnostics. In addition, the diagnostic abilities and expertise of any\n",
      "pathologist is dependent on their direct experience with common as well as\n",
      "rarer variant morphologies. Recently, deep learning approaches have been used\n",
      "to successfully show a high level of accuracy for such tasks. However,\n",
      "obtaining expert-level annotated images is an expensive and time-consuming task\n",
      "and artificially synthesized histological images can prove greatly beneficial.\n",
      "Here, we present an approach to not only generate histological images that\n",
      "reproduce the diagnostic morphologic features of common disease but also\n",
      "provide a user ability to generate new and rare morphologies. Our approach\n",
      "involves developing a generative adversarial network model that synthesizes\n",
      "pathology images constrained by class labels. We investigated the ability of\n",
      "this framework in synthesizing realistic prostate and colon tissue images and\n",
      "assessed the utility of these images in augmenting diagnostic ability of\n",
      "machine learning methods as well as their usability by a panel of experienced\n",
      "anatomic pathologists. Synthetic data generated by our framework performed\n",
      "similar to real data in training a deep learning model for diagnosis.\n",
      "Pathologists were not able to distinguish between real and synthetic images and\n",
      "showed a similar level of inter-observer agreement for prostate cancer grading.\n",
      "We extended the approach to significantly more complex images from colon\n",
      "biopsies and showed that the complex microenvironment in such tissues can also\n",
      "be reproduced. Finally, we present the ability for a user to generate deepfake\n",
      "histological images via a simple markup of sematic labels. \n",
      "\n",
      "\n",
      "Deepfakes pose a serious threat to our digital society by fueling the spread\n",
      "of misinformation. It is essential to develop techniques that both detect them,\n",
      "and effectively alert the human user to their presence. Here, we introduce a\n",
      "novel deepfake detection framework that meets both of these needs. Our approach\n",
      "learns to generate attention maps of video artifacts, semi-supervised on human\n",
      "annotations. These maps make two contributions. First, they improve the\n",
      "accuracy and generalizability of a deepfake classifier, demonstrated across\n",
      "several deepfake detection datasets. Second, they allow us to generate an\n",
      "intuitive signal for the human user, in the form of \"Deepfake Caricatures\":\n",
      "transformations of the original deepfake video where attended artifacts are\n",
      "exacerbated to improve human recognition. Our approach, based on a mixture of\n",
      "human and artificial supervision, aims to further the development of\n",
      "countermeasures against fake visual content, and grants humans the ability to\n",
      "make their own judgment when presented with dubious visual media. \n",
      "\n",
      "\n",
      "DeepFake is becoming a real risk to society and brings potential threats to\n",
      "both individual privacy and political security due to the DeepFaked multimedia\n",
      "are realistic and convincing. However, the popular DeepFake passive detection\n",
      "is an ex-post forensics countermeasure and failed in blocking the\n",
      "disinformation spreading in advance. To address this limitation, researchers\n",
      "study the proactive defense techniques by adding adversarial noises into the\n",
      "source data to disrupt the DeepFake manipulation. However, the existing studies\n",
      "on proactive DeepFake defense via injecting adversarial noises are not robust,\n",
      "which could be easily bypassed by employing simple image reconstruction\n",
      "revealed in a recent study MagDR.\n",
      "  In this paper, we investigate the vulnerability of the existing forgery\n",
      "techniques and propose a novel \\emph{anti-forgery} technique that helps users\n",
      "protect the shared facial images from attackers who are capable of applying the\n",
      "popular forgery techniques. Our proposed method generates perceptual-aware\n",
      "perturbations in an incessant manner which is vastly different from the prior\n",
      "studies by adding adversarial noises that is sparse. Experimental results\n",
      "reveal that our perceptual-aware perturbations are robust to diverse image\n",
      "transformations, especially the competitive evasion technique, MagDR via image\n",
      "reconstruction. Our findings potentially open up a new research direction\n",
      "towards thorough understanding and investigation of perceptual-aware\n",
      "adversarial attack for protecting facial images against DeepFakes in a\n",
      "proactive and robust manner. We open-source our tool to foster future research.\n",
      "Code is available at https://github.com/AbstractTeen/AntiForgery/. \n",
      "\n",
      "\n",
      "Recently, image manipulation has achieved rapid growth due to the advancement\n",
      "of sophisticated image editing tools. A recent surge of generated fake imagery\n",
      "and videos using neural networks is DeepFake. DeepFake algorithms can create\n",
      "fake images and videos that humans cannot distinguish from authentic ones.\n",
      "(GANs) have been extensively used for creating realistic images without\n",
      "accessing the original images. Therefore, it is become essential to detect fake\n",
      "videos to avoid spreading false information. This paper presents a survey of\n",
      "methods used to detect DeepFakes and datasets available for detecting DeepFakes\n",
      "in the literature to date. We present extensive discussions and research trends\n",
      "related to DeepFake technologies. \n",
      "\n",
      "\n",
      "Increasing use of our biometrics (e.g., fingerprints, faces, or voices) to\n",
      "unlock access to and interact with online services raises concerns about the\n",
      "trade-offs between convenience, privacy, and security. Service providers must\n",
      "authenticate their users, although individuals may wish to maintain privacy and\n",
      "limit the disclosure of sensitive attributes beyond the authentication step,\n",
      "\\eg~when interacting with Voice User Interfaces (VUIs). Preserving privacy\n",
      "while performing authentication is challenging, particularly where adversaries\n",
      "can use biometric data to train transformation tools (e.g.,`deepfaked' speech)\n",
      "and use the faked output to defeat existing authentication systems. In this\n",
      "paper, we take a step towards understanding security and privacy requirements\n",
      "to establish the threat and defense boundaries. We introduce a secure, flexible\n",
      "privacy-preserving system to capture and store an on-device fingerprint of the\n",
      "users' raw signals (i.e., voice) for authentication instead of sending/sharing\n",
      "the raw biometric signals. We then analyze this fingerprint using different\n",
      "predictors, each evaluating its legitimacy from a different perspective (e.g.,\n",
      "target identity claim, spoofing attempt, and liveness). We fuse multiple\n",
      "predictors' decisions to make a final decision on whether the user input is\n",
      "legitimate or not. Validating legitimate users yields an accuracy rate of\n",
      "98.68% after cross-validation using our verification technique. The pipeline\n",
      "runs in tens of milliseconds when tested on a CPU and a single-core ARM\n",
      "processor, without specialized hardware. \n",
      "\n",
      "\n",
      "This work proposes an algorithm for fMRI data analysis for the classification\n",
      "of ADHD disorders. There have been several breakthroughs in the analysis of\n",
      "fMRI via 3D convolutional neural networks (CNNs). With these new techniques it\n",
      "is possible to preserve the 3D spatial data of fMRI data. Additionally there\n",
      "have been recent advances in the use of 3D generative adversarial neural\n",
      "networks (GANs) for the generation of normal MRI data. This work utilizes multi\n",
      "modal 3D CNNs with data augmentation from 3D GAN for ADHD prediction from fMRI.\n",
      "By leveraging a 3D-GAN it would be possible to use deepfake data to enhance the\n",
      "accuracy of 3D CNN classification of brain disorders. A comparison will be made\n",
      "between a time distributed single modal 3D CNN model for classification and the\n",
      "modified multi modal model with MRI data as well. \n",
      "\n",
      "\n",
      "Most of previous deepfake detection researches bent their efforts to describe\n",
      "and discriminate artifacts in human perceptible ways, which leave a bias in the\n",
      "learned networks of ignoring some critical invariance features intra-class and\n",
      "underperforming the robustness of internet interference. Essentially, the\n",
      "target of deepfake detection problem is to represent natural faces and fake\n",
      "faces at the representation space discriminatively, and it reminds us whether\n",
      "we could optimize the feature extraction procedure at the representation space\n",
      "through constraining intra-class consistence and inter-class inconsistence to\n",
      "bring the intra-class representations close and push the inter-class\n",
      "representations apart? Therefore, inspired by contrastive representation\n",
      "learning, we tackle the deepfake detection problem through learning the\n",
      "invariant representations of both classes and propose a novel real-centric\n",
      "consistency learning method. We constraint the representation from both the\n",
      "sample level and the feature level. At the sample level, we take the procedure\n",
      "of deepfake synthesis into consideration and propose a novel forgery\n",
      "semantical-based pairing strategy to mine latent generation-related features.\n",
      "At the feature level, based on the centers of natural faces at the\n",
      "representation space, we design a hard positive mining and synthesizing method\n",
      "to simulate the potential marginal features. Besides, a hard negative fusion\n",
      "method is designed to improve the discrimination of negative marginal features\n",
      "with the help of supervised contrastive margin loss we developed. The\n",
      "effectiveness and robustness of the proposed method has been demonstrated\n",
      "through extensive experiments. \n",
      "\n",
      "\n",
      "We present a framework that uses GAN-augmented images to complement certain\n",
      "specific attributes, usually underrepresented, for machine learning model\n",
      "training. This allows us to improve inference quality over those attributes for\n",
      "the facial recognition tasks. \n",
      "\n",
      "\n",
      "Deepfakes are a form of synthetic image generation used to generate fake\n",
      "videos of individuals for malicious purposes. The resulting videos may be used\n",
      "to spread misinformation, reduce trust in media, or as a form of blackmail.\n",
      "These threats necessitate automated methods of deepfake video detection. This\n",
      "paper investigates whether temporal information can improve the deepfake\n",
      "detection performance of deep learning models.\n",
      "  To investigate this, we propose a framework that classifies new and existing\n",
      "approaches by their defining characteristics. These are the types of feature\n",
      "extraction: automatic or manual, and the temporal relationship between frames:\n",
      "dependent or independent. We apply this framework to investigate the effect of\n",
      "temporal dependency on a model's deepfake detection performance.\n",
      "  We find that temporal dependency produces a statistically significant (p <\n",
      "0.05) increase in performance in classifying real images for the model using\n",
      "automatic feature selection, demonstrating that spatio-temporal information can\n",
      "increase the performance of deepfake video detection models. \n",
      "\n",
      "\n",
      "Deep learning algorithms are rapidly changing the way in which audiovisual\n",
      "media can be produced. Synthetic audiovisual media generated with deep learning\n",
      "- often subsumed colloquially under the label \"deepfakes\" - have a number of\n",
      "impressive characteristics; they are increasingly trivial to produce, and can\n",
      "be indistinguishable from real sounds and images recorded with a sensor. Much\n",
      "attention has been dedicated to ethical concerns raised by this technological\n",
      "development. Here, I focus instead on a set of issues related to the notion of\n",
      "synthetic audiovisual media, its place within a broader taxonomy of audiovisual\n",
      "media, and how deep learning techniques differ from more traditional approaches\n",
      "to media synthesis. After reviewing important etiological features of deep\n",
      "learning pipelines for media manipulation and generation, I argue that\n",
      "\"deepfakes\" and related synthetic media produced with such pipelines do not\n",
      "merely offer incremental improvements over previous methods, but challenge\n",
      "traditional taxonomical distinctions, and pave the way for genuinely novel\n",
      "kinds of audiovisual media. \n",
      "\n",
      "\n",
      "There have been emerging a number of benchmarks and techniques for the\n",
      "detection of deepfakes. However, very few works study the detection of\n",
      "incrementally appearing deepfakes in the real-world scenarios. To simulate the\n",
      "wild scenes, this paper suggests a continual deepfake detection benchmark\n",
      "(CDDB) over a new collection of deepfakes from both known and unknown\n",
      "generative models. The suggested CDDB designs multiple evaluations on the\n",
      "detection over easy, hard, and long sequence of deepfake tasks, with a set of\n",
      "appropriate measures. In addition, we exploit multiple approaches to adapt\n",
      "multiclass incremental learning methods, commonly used in the continual visual\n",
      "recognition, to the continual deepfake detection problem. We evaluate several\n",
      "methods, including the adapted ones, on the proposed CDDB. Within the proposed\n",
      "benchmark, we explore some commonly known essentials of standard continual\n",
      "learning. Our study provides new insights on these essentials in the context of\n",
      "continual deepfake detection. The suggested CDDB is clearly more challenging\n",
      "than the existing benchmarks, which thus offers a suitable evaluation avenue to\n",
      "the future research. Our benchmark dataset and the source code will be made\n",
      "publicly available. \n",
      "\n",
      "\n",
      "Residual-domain feature is very useful for Deepfake detection because it\n",
      "suppresses irrelevant content features and preserves key manipulation traces.\n",
      "However, inappropriate residual prediction will bring side effects on detection\n",
      "accuracy. In addition, residual-domain features are easily affected by image\n",
      "operations such as compression. Most existing works exploit either\n",
      "spatial-domain features or residual-domain features, while neglecting that two\n",
      "types of features are mutually correlated. In this paper, we propose a guided\n",
      "residuals network, namely GRnet, which fuses spatial-domain and residual-domain\n",
      "features in a mutually reinforcing way, to expose face images generated by\n",
      "Deepfake. Different from existing prediction based residual extraction methods,\n",
      "we propose a manipulation trace extractor (MTE) to directly remove the content\n",
      "features and preserve manipulation traces. MTE is a fine-grained method that\n",
      "can avoid the potential bias caused by inappropriate prediction. Moreover, an\n",
      "attention fusion mechanism (AFM) is designed to selectively emphasize feature\n",
      "channel maps and adaptively allocate the weights for two streams. The\n",
      "experimental results show that the proposed GRnet achieves better performances\n",
      "than the state-of-the-art works on four public fake face datasets including\n",
      "HFF, FaceForensics++, DFDC and Celeb-DF. Especially, GRnet achieves an average\n",
      "accuracy of 97.72% on the HFF dataset, which is at least 5.25% higher than the\n",
      "existing works. \n",
      "\n",
      "\n",
      "On the basis of DefakeHop, an enhanced lightweight Deepfake detector called\n",
      "DefakeHop++ is proposed in this work. The improvements lie in two areas. First,\n",
      "DefakeHop examines three facial regions (i.e., two eyes and mouth) while\n",
      "DefakeHop++ includes eight more landmarks for broader coverage. Second, for\n",
      "discriminant features selection, DefakeHop uses an unsupervised approach while\n",
      "DefakeHop++ adopts a more effective approach with supervision, called the\n",
      "Discriminant Feature Test (DFT). In DefakeHop++, rich spatial and spectral\n",
      "features are first derived from facial regions and landmarks automatically.\n",
      "Then, DFT is used to select a subset of discriminant features for classifier\n",
      "training. As compared with MobileNet v3 (a lightweight CNN model of 1.5M\n",
      "parameters targeting at mobile applications), DefakeHop++ has a model of 238K\n",
      "parameters, which is 16% of MobileNet v3. Furthermore, DefakeHop++ outperforms\n",
      "MobileNet v3 in Deepfake image detection performance in a weakly-supervised\n",
      "setting. \n",
      "\n",
      "\n",
      "Enabled by recent improvements in generation methodologies, DeepFakes have\n",
      "become mainstream due to their increasingly better visual quality, the increase\n",
      "in easy-to-use generation tools and the rapid dissemination through social\n",
      "media. This fact poses a severe threat to our societies with the potential to\n",
      "erode social cohesion and influence our democracies. To mitigate the threat,\n",
      "numerous DeepFake detection schemes have been introduced in the literature but\n",
      "very few provide a web service that can be used in the wild. In this paper, we\n",
      "introduce the MeVer DeepFake detection service, a web service detecting deep\n",
      "learning manipulations in images and video. We present the design and\n",
      "implementation of the proposed processing pipeline that involves a model\n",
      "ensemble scheme, and we endow the service with a model card for transparency.\n",
      "Experimental results show that our service performs robustly on the three\n",
      "benchmark datasets while being vulnerable to Adversarial Attacks. Finally, we\n",
      "outline our experience and lessons learned when deploying a research system\n",
      "into production in the hopes that it will be useful to other academic and\n",
      "industry teams. \n",
      "\n",
      "\n",
      "Design tools in the 3D industry, while powerful, are still tedious and\n",
      "labor-intensive when it comes to bringing a creative idea for a visual effect\n",
      "to life. In this position paper, we discussed how an infamous generative\n",
      "synthetic media, deepfakes, could be of use and embedded into common\n",
      "sophisticated 3D workflows to reduce user workloads in areas such as 3D model\n",
      "editing, material design, and character animation. As a case discussion, we\n",
      "also prototyped a tool to address the retargeting problem in character\n",
      "animation. Although deepfakes themselves have received a negative public image,\n",
      "the results of our interviews with field experts are unexpectedly positive in\n",
      "regard to our tool that utilizes deepfake algorithms. Lastly, we also discussed\n",
      "our experience and observed design practices to put deepfakes to good use,\n",
      "including how we could avoid potential misuses directly by design, how this\n",
      "design changes user interactions, and subsequent open issues. \n",
      "\n",
      "\n",
      "DeepFake face swapping presents a significant threat to online security and\n",
      "social media, which can replace the source face in an arbitrary photo/video\n",
      "with the target face of an entirely different person. In order to prevent this\n",
      "fraud, some researchers have begun to study the adversarial methods against\n",
      "DeepFake or face manipulation. However, existing works focus on the white-box\n",
      "setting or the black-box setting driven by abundant queries, which severely\n",
      "limits the practical application of these methods. To tackle this problem, we\n",
      "introduce a practical adversarial attack that does not require any queries to\n",
      "the facial image forgery model. Our method is built on a substitute model\n",
      "persuing for face reconstruction and then transfers adversarial examples from\n",
      "the substitute model directly to inaccessible black-box DeepFake models.\n",
      "Specially, we propose the Transferable Cycle Adversary Generative Adversarial\n",
      "Network (TCA-GAN) to construct the adversarial perturbation for disrupting\n",
      "unknown DeepFake systems. We also present a novel post-regularization module\n",
      "for enhancing the transferability of generated adversarial examples. To\n",
      "comprehensively measure the effectiveness of our approaches, we construct a\n",
      "challenging benchmark of DeepFake adversarial attacks for future development.\n",
      "Extensive experiments impressively show that the proposed adversarial attack\n",
      "method makes the visual quality of DeepFake face images plummet so that they\n",
      "are easier to be detected by humans and algorithms. Moreover, we demonstrate\n",
      "that the proposed algorithm can be generalized to offer face image protection\n",
      "against various face translation methods. \n",
      "\n",
      "\n",
      "We propose an end-to-end pipeline for both building and tracking 3D facial\n",
      "models from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)\n",
      "video data. First, we present a method for automatic data curation and\n",
      "retrieval based on a hierarchical clustering framework typical of collision\n",
      "detection algorithms in traditional computer graphics pipelines. Subsequently,\n",
      "we utilize synthetic turntables and leverage deepfake technology in order to\n",
      "build a synthetic multi-view stereo pipeline for appearance capture that is\n",
      "robust to imperfect synthetic geometry and image misalignment. The resulting\n",
      "model is fit with an animation rig, which is then used to track facial\n",
      "performances. Notably, our novel use of deepfake technology enables us to\n",
      "perform robust tracking of in-the-wild data using differentiable renderers\n",
      "despite a significant synthetic-to-real domain gap. Finally, we outline how we\n",
      "train a motion capture regressor, leveraging the aforementioned techniques to\n",
      "avoid the need for real-world ground truth data and/or a high-end calibrated\n",
      "camera capture setup. \n",
      "\n",
      "\n",
      "Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic\n",
      "media where the likeness of one person is replaced with another. There are\n",
      "growing concerns that deepfakes can be maliciously used to create misleading\n",
      "and harmful digital contents. As deepfakes become more common, there is a dire\n",
      "need for deepfake detection technology to help spot deepfake media. Present\n",
      "deepfake detection models are able to achieve outstanding accuracy (>90%).\n",
      "However, most of them are limited to within-dataset scenario, where the same\n",
      "dataset is used for training and testing. Most models do not generalise well\n",
      "enough in cross-dataset scenario, where models are tested on unseen datasets\n",
      "from another source. Furthermore, state-of-the-art deepfake detection models\n",
      "rely on neural network-based classification models that are known to be\n",
      "vulnerable to adversarial attacks. Motivated by the need for a robust deepfake\n",
      "detection model, this study adapts metamorphic testing (MT) principles to help\n",
      "identify potential factors that could influence the robustness of the examined\n",
      "model, while overcoming the test oracle problem in this domain. Metamorphic\n",
      "testing is specifically chosen as the testing technique as it fits our demand\n",
      "to address learning-based system testing with probabilistic outcomes from\n",
      "largely black-box components, based on potentially large input domains. We\n",
      "performed our evaluations on MesoInception-4 and TwoStreamNet models, which are\n",
      "the state-of-the-art deepfake detection models. This study identified makeup\n",
      "application as an adversarial attack that could fool deepfake detectors. Our\n",
      "experimental results demonstrate that both the MesoInception-4 and TwoStreamNet\n",
      "models degrade in their performance by up to 30\\% when the input data is\n",
      "perturbed with makeup. \n",
      "\n",
      "\n",
      "In this paper, we present novel synthetic training data called self-blended\n",
      "images (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source\n",
      "and target images from single pristine images, reproducing common forgery\n",
      "artifacts (e.g., blending boundaries and statistical inconsistencies between\n",
      "source and target images). The key idea behind SBIs is that more general and\n",
      "hardly recognizable fake samples encourage classifiers to learn generic and\n",
      "robust representations without overfitting to manipulation-specific artifacts.\n",
      "We compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC,\n",
      "DFDCP, and FFIW datasets by following the standard cross-dataset and\n",
      "cross-manipulation protocols. Extensive experiments show that our method\n",
      "improves the model generalization to unknown manipulations and scenes. In\n",
      "particular, on DFDC and DFDCP where existing methods suffer from the domain gap\n",
      "between the training and test sets, our approach outperforms the baseline by\n",
      "4.90% and 11.78% points in the cross-dataset evaluation, respectively. \n",
      "\n",
      "\n",
      "Due to its high societal impact, deepfake detection is getting active\n",
      "attention in the computer vision community. Most deepfake detection methods\n",
      "rely on identity, facial attribute and adversarial perturbation based\n",
      "spatio-temporal modifications at the whole video or random locations, while\n",
      "keeping the meaning of the content intact. However, a sophisticated deepfake\n",
      "may contain only a small segment of video/audio manipulation, through which the\n",
      "meaning of the content can be, for example, completely inverted from sentiment\n",
      "perspective. To address this gap, we introduce a content driven audio-visual\n",
      "deepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF),\n",
      "explicitly designed for the task of learning temporal forgery localization.\n",
      "Specifically, the content driven audio-visual manipulations are performed at\n",
      "strategic locations in order to change the sentiment polarity of the whole\n",
      "video. Our baseline method for benchmarking the proposed dataset is a 3DCNN\n",
      "model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is\n",
      "guided via contrastive, boundary matching and frame classification loss\n",
      "functions. Our extensive quantitative analysis demonstrates the strong\n",
      "performance of the proposed method for both task of temporal forgery\n",
      "localization and deepfake detection. \n",
      "\n",
      "\n",
      "Despite recent advances in Generative Adversarial Networks (GANs), with\n",
      "special focus to the Deepfake phenomenon there is no a clear understanding\n",
      "neither in terms of explainability nor of recognition of the involved models.\n",
      "In particular, the recognition of a specific GAN model that generated the\n",
      "deepfake image compared to many other possible models created by the same\n",
      "generative architecture (e.g. StyleGAN) is a task not yet completely addressed\n",
      "in the state-of-the-art. In this work, a robust processing pipeline to evaluate\n",
      "the possibility to point-out analytic fingerprints for Deepfake model\n",
      "recognition is presented. After exploiting the latent space of 50 slightly\n",
      "different models through an in-depth analysis on the generated images, a proper\n",
      "encoder was trained to discriminate among these models obtaining a\n",
      "classification accuracy of over 96%. Once demonstrated the possibility to\n",
      "discriminate extremely similar images, a dedicated metric exploiting the\n",
      "insights discovered in the latent space was introduced. By achieving a final\n",
      "accuracy of more than 94% for the Model Recognition task on images generated by\n",
      "models not employed in the training phase, this study takes an important step\n",
      "in countering the Deepfake phenomenon introducing a sort of signature in some\n",
      "sense similar to those employed in the multimedia forensics field (e.g. for\n",
      "camera source identification task, image ballistics task, etc). \n",
      "\n",
      "\n",
      "Facial manipulation by deep fake has caused major security risks and raised\n",
      "severe societal concerns. As a countermeasure, a number of deep fake detection\n",
      "methods have been proposed recently. Most of them model deep fake detection as\n",
      "a binary classification problem using a backbone convolutional neural network\n",
      "(CNN) architecture pretrained for the task. These CNN-based methods have\n",
      "demonstrated very high efficacy in deep fake detection with the Area under the\n",
      "Curve (AUC) as high as 0.99. However, the performance of these methods degrades\n",
      "significantly when evaluated across datasets. In this paper, we formulate deep\n",
      "fake detection as a hybrid combination of supervised and reinforcement learning\n",
      "(RL) to improve its cross-dataset generalization performance. The proposed\n",
      "method chooses the top-k augmentations for each test sample by an RL agent in\n",
      "an image-specific manner. The classification scores, obtained using CNN, of all\n",
      "the augmentations of each test image are averaged together for final real or\n",
      "fake classification. Through extensive experimental validation, we demonstrate\n",
      "the superiority of our method over existing published research in cross-dataset\n",
      "generalization of deep fake detectors, thus obtaining state-of-the-art\n",
      "performance. \n",
      "\n",
      "\n",
      "Face manipulation technology is advancing very rapidly, and new methods are\n",
      "being proposed day by day. The aim of this work is to propose a deepfake\n",
      "detector that can cope with the wide variety of manipulation methods and\n",
      "scenarios encountered in the real world. Our key insight is that each person\n",
      "has specific biometric characteristics that a synthetic generator cannot likely\n",
      "reproduce. Accordingly, we extract high-level audio-visual biometric features\n",
      "which characterize the identity of a person, and use them to create a\n",
      "person-of-interest (POI) deepfake detector. We leverage a contrastive learning\n",
      "paradigm to learn the moving-face and audio segments embeddings that are most\n",
      "discriminative for each identity. As a result, when the video and/or audio of a\n",
      "person is manipulated, its representation in the embedding space becomes\n",
      "inconsistent with the real identity, allowing reliable detection. Training is\n",
      "carried out exclusively on real talking-face videos, thus the detector does not\n",
      "depend on any specific manipulation method and yields the highest\n",
      "generalization ability. In addition, our method can detect both single-modality\n",
      "(audio-only, video-only) and multi-modality (audio-video) attacks, and is\n",
      "robust to low-quality or corrupted videos by building only on high-level\n",
      "semantic features. Experiments on a wide variety of datasets confirm that our\n",
      "method ensures a SOTA performance, with an average improvement in terms of AUC\n",
      "of around 3%, 10%, and 7% for high-quality, low quality and attacked videos,\n",
      "respectively. \n",
      "\n",
      "\n",
      "Deepfakes and manipulated media are becoming a prominent threat due to the\n",
      "recent advances in realistic image and video synthesis techniques. There have\n",
      "been several attempts at combating Deepfakes using machine learning\n",
      "classifiers. However, such classifiers do not generalize well to black-box\n",
      "image synthesis techniques and have been shown to be vulnerable to adversarial\n",
      "examples. To address these challenges, we introduce a deep learning based\n",
      "semi-fragile watermarking technique that allows media authentication by\n",
      "verifying an invisible secret message embedded in the image pixels. Instead of\n",
      "identifying and detecting fake media using visual artifacts, we propose to\n",
      "proactively embed a semi-fragile watermark into a real image so that we can\n",
      "prove its authenticity when needed. Our watermarking framework is designed to\n",
      "be fragile to facial manipulations or tampering while being robust to benign\n",
      "image-processing operations such as image compression, scaling, saturation,\n",
      "contrast adjustments etc. This allows images shared over the internet to retain\n",
      "the verifiable watermark as long as face-swapping or any other Deepfake\n",
      "modification technique is not applied. We demonstrate that FaceSigns can embed\n",
      "a 128 bit secret as an imperceptible image watermark that can be recovered with\n",
      "a high bit recovery accuracy at several compression levels, while being\n",
      "non-recoverable when unseen Deepfake manipulations are applied. For a set of\n",
      "unseen benign and Deepfake manipulations studied in our work, FaceSigns can\n",
      "reliably detect manipulated content with an AUC score of 0.996 which is\n",
      "significantly higher than prior image watermarking and steganography\n",
      "techniques. \n",
      "\n",
      "\n",
      "Current text-to-speech algorithms produce realistic fakes of human voices,\n",
      "making deepfake detection a much-needed area of research. While researchers\n",
      "have presented various techniques for detecting audio spoofs, it is often\n",
      "unclear exactly why these architectures are successful: Preprocessing steps,\n",
      "hyperparameter settings, and the degree of fine-tuning are not consistent\n",
      "across related work. Which factors contribute to success, and which are\n",
      "accidental? In this work, we address this problem: We systematize audio\n",
      "spoofing detection by re-implementing and uniformly evaluating architectures\n",
      "from related work. We identify overarching features for successful audio\n",
      "deepfake detection, such as using cqtspec or logspec features instead of\n",
      "melspec features, which improves performance by 37% EER on average, all other\n",
      "factors constant. Additionally, we evaluate generalization capabilities: We\n",
      "collect and publish a new dataset consisting of 37.9 hours of found audio\n",
      "recordings of celebrities and politicians, of which 17.2 hours are deepfakes.\n",
      "We find that related work performs poorly on such real-world data (performance\n",
      "degradation of up to one thousand percent). This may suggest that the community\n",
      "has tailored its solutions too closely to the prevailing ASVSpoof benchmark and\n",
      "that deepfakes are much harder to detect outside the lab than previously\n",
      "thought. \n",
      "\n",
      "\n",
      "Deepfakes are synthetically generated media often devised with malicious\n",
      "intent. They have become increasingly more convincing with large training\n",
      "datasets advanced neural networks. These fakes are readily being misused for\n",
      "slander, misinformation and fraud. For this reason, intensive research for\n",
      "developing countermeasures is also expanding. However, recent work is almost\n",
      "exclusively limited to deepfake detection - predicting if audio is real or\n",
      "fake. This is despite the fact that attribution (who created which fake?) is an\n",
      "essential building block of a larger defense strategy, as practiced in the\n",
      "field of cybersecurity for a long time. This paper considers the problem of\n",
      "deepfake attacker attribution in the domain of audio. We present several\n",
      "methods for creating attacker signatures using low-level acoustic descriptors\n",
      "and machine learning embeddings. We show that speech signal features are\n",
      "inadequate for characterizing attacker signatures. However, we also demonstrate\n",
      "that embeddings from a recurrent neural network can successfully characterize\n",
      "attacks from both known and unknown attackers. Our attack signature embeddings\n",
      "result in distinct clusters, both for seen and unseen audio deepfakes. We show\n",
      "that these embeddings can be used in downstream-tasks to high-effect, scoring\n",
      "97.10% accuracy in attacker-id classification. \n",
      "\n",
      "\n",
      "Face forgery has attracted increasing attention in recent applications of\n",
      "computer vision. Existing detection techniques using the two-branch framework\n",
      "benefit a lot from a frequency perspective, yet are restricted by their fixed\n",
      "frequency decomposition and transform. In this paper, we propose to Adaptively\n",
      "learn Frequency information in the two-branch Detection framework, dubbed AFD.\n",
      "To be specific, we automatically learn decomposition in the frequency domain by\n",
      "introducing heterogeneity constraints, and propose an attention-based module to\n",
      "adaptively incorporate frequency features into spatial clues. Then we liberate\n",
      "our network from the fixed frequency transforms, and achieve better performance\n",
      "with our data- and task-dependent transform layers. Extensive experiments show\n",
      "that AFD generally outperforms. \n",
      "\n",
      "\n",
      "With the development of the Generative Adversarial Networks (GANs) and\n",
      "DeepFakes, AI-synthesized images are now of such high quality that humans can\n",
      "hardly distinguish them from real images. It is imperative for media forensics\n",
      "to develop detectors to expose them accurately. Existing detection methods have\n",
      "shown high performance in generated images detection, but they tend to\n",
      "generalize poorly in the real-world scenarios, where the synthetic images are\n",
      "usually generated with unseen models using unknown source data. In this work,\n",
      "we emphasize the importance of combining information from the whole image and\n",
      "informative patches in improving the generalization ability of AI-synthesized\n",
      "image detection. Specifically, we design a two-branch model to combine global\n",
      "spatial information from the whole image and local informative features from\n",
      "multiple patches selected by a novel patch selection module. Multi-head\n",
      "attention mechanism is further utilized to fuse the global and local features.\n",
      "We collect a highly diverse dataset synthesized by 19 models with various\n",
      "objects and resolutions to evaluate our model. Experimental results demonstrate\n",
      "the high accuracy and good generalization ability of our method in detecting\n",
      "generated images. \n",
      "\n",
      "\n",
      "Recent studies in deepfake detection have yielded promising results when the\n",
      "training and testing face forgeries are from the same dataset. However, the\n",
      "problem remains challenging when one tries to generalize the detector to\n",
      "forgeries created by unseen methods in the training dataset. This work\n",
      "addresses the generalizable deepfake detection from a simple principle: a\n",
      "generalizable representation should be sensitive to diverse types of forgeries.\n",
      "Following this principle, we propose to enrich the \"diversity\" of forgeries by\n",
      "synthesizing augmented forgeries with a pool of forgery configurations and\n",
      "strengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\n",
      "the forgery configurations. To effectively explore the large forgery\n",
      "augmentation space, we further propose to use the adversarial training strategy\n",
      "to dynamically synthesize the most challenging forgeries to the current model.\n",
      "Through extensive experiments, we show that the proposed strategies are\n",
      "surprisingly effective (see Figure 1), and they could achieve superior\n",
      "performance than the current state-of-the-art methods. Code is available at\n",
      "\\url{https://github.com/liangchen527/SLADD}. \n",
      "\n",
      "\n",
      "Deep convolutional neural networks have achieved exceptional results on\n",
      "multiple detection and recognition tasks. However, the performance of such\n",
      "detectors are often evaluated in public benchmarks under constrained and\n",
      "non-realistic situations. The impact of conventional distortions and processing\n",
      "operations found in imaging workflows such as compression, noise, and\n",
      "enhancement are not sufficiently studied. Currently, only a few researches have\n",
      "been done to improve the detector robustness to unseen perturbations. This\n",
      "paper proposes a more effective data augmentation scheme based on real-world\n",
      "image degradation process. This novel technique is deployed for deepfake\n",
      "detection tasks and has been evaluated by a more realistic assessment\n",
      "framework. Extensive experiments show that the proposed data augmentation\n",
      "scheme improves generalization ability to unpredictable data distortions and\n",
      "unseen datasets. \n",
      "\n",
      "\n",
      "Deep convolutional neural networks have shown remarkable results on multiple\n",
      "detection tasks. Despite the significant progress, the performance of such\n",
      "detectors are often assessed in public benchmarks under non-realistic\n",
      "conditions. Specifically, impact of conventional distortions and processing\n",
      "operations such as compression, noise, and enhancement are not sufficiently\n",
      "studied. This paper proposes a rigorous framework to assess performance of\n",
      "learning-based detectors in more realistic situations. An illustrative example\n",
      "is shown under deepfake detection context. Inspired by the assessment results,\n",
      "a data augmentation strategy based on natural image degradation process is\n",
      "designed, which significantly improves the generalization ability of two\n",
      "deepfake detectors. \n",
      "\n",
      "\n",
      "DeepFakes are raising significant social concerns. Although various DeepFake\n",
      "detectors have been developed as forensic countermeasures, these detectors are\n",
      "still vulnerable to attacks. Recently, a few attacks, principally adversarial\n",
      "attacks, have succeeded in cloaking DeepFake images to evade detection.\n",
      "However, these attacks have typical detector-specific designs, which require\n",
      "prior knowledge about the detector, leading to poor transferability. Moreover,\n",
      "these attacks only consider simple security scenarios. Less is known about how\n",
      "effective they are in high-level scenarios where either the detectors or the\n",
      "attacker's knowledge varies. In this paper, we solve the above challenges with\n",
      "presenting a novel detector-agnostic trace removal attack for DeepFake\n",
      "anti-forensics. Instead of investigating the detector side, our attack looks\n",
      "into the original DeepFake creation pipeline, attempting to remove all\n",
      "detectable natural DeepFake traces to render the fake images more \"authentic\".\n",
      "To implement this attack, first, we perform a DeepFake trace discovery,\n",
      "identifying three discernible traces. Then a trace removal network (TR-Net) is\n",
      "proposed based on an adversarial learning framework involving one generator and\n",
      "multiple discriminators. Each discriminator is responsible for one individual\n",
      "trace representation to avoid cross-trace interference. These discriminators\n",
      "are arranged in parallel, which prompts the generator to remove various traces\n",
      "simultaneously. To evaluate the attack efficacy, we crafted heterogeneous\n",
      "security scenarios where the detectors were embedded with different levels of\n",
      "defense and the attackers' background knowledge of data varies. The\n",
      "experimental results show that the proposed attack can significantly compromise\n",
      "the detection accuracy of six state-of-the-art DeepFake detectors while causing\n",
      "only a negligible loss in visual quality to the original DeepFake samples. \n",
      "\n",
      "\n",
      "Most recent style-transfer techniques based on generative architectures are\n",
      "able to obtain synthetic multimedia contents, or commonly called deepfakes,\n",
      "with almost no artifacts. Researchers already demonstrated that synthetic\n",
      "images contain patterns that can determine not only if it is a deepfake but\n",
      "also the generative architecture employed to create the image data itself.\n",
      "These traces can be exploited to study problems that have never been addressed\n",
      "in the context of deepfakes. To this aim, in this paper a first approach to\n",
      "investigate the image ballistics on deepfake images subject to style-transfer\n",
      "manipulations is proposed. Specifically, this paper describes a study on\n",
      "detecting how many times a digital image has been processed by a generative\n",
      "architecture for style transfer. Moreover, in order to address and study\n",
      "accurately forensic ballistics on deepfake images, some mathematical properties\n",
      "of style-transfer operations were investigated. \n",
      "\n",
      "\n",
      "GAN-generated deepfakes as a genre of digital images are gaining ground as\n",
      "both catalysts of artistic expression and malicious forms of deception,\n",
      "therefore demanding systems to enforce and accredit their ethical use. Existing\n",
      "techniques for the source attribution of synthetic images identify subtle\n",
      "intrinsic fingerprints using multiclass classification neural nets limited in\n",
      "functionality and scalability. Hence, we redefine the deepfake detection and\n",
      "source attribution problems as a series of related binary classification tasks.\n",
      "We leverage transfer learning to rapidly adapt forgery detection networks for\n",
      "multiple independent attribution problems, by proposing a semi-decentralized\n",
      "modular design to solve them simultaneously and efficiently. Class activation\n",
      "mapping is also demonstrated as an effective means of feature localization for\n",
      "model interpretation. Our models are determined via experimentation to be\n",
      "competitive with current benchmarks, and capable of decent performance on human\n",
      "portraits in ideal conditions. Decentralized fingerprint-based attribution is\n",
      "found to retain validity in the presence of novel sources, but is more\n",
      "susceptible to type II errors that intensify with image perturbations and\n",
      "attributive uncertainty. We describe both our conceptual framework and model\n",
      "prototypes for further enhancement when investigating the technical limits of\n",
      "reactive deepfake attribution. \n",
      "\n",
      "\n",
      "How can we best address the dangerous impact that deep learning-generated\n",
      "fake audios, photographs, and videos (a.k.a. deepfakes) may have in personal\n",
      "and societal life? We foresee that the availability of cheap deepfake\n",
      "technology will create a second wave of disinformation where people will\n",
      "receive specific, personalized disinformation through different channels,\n",
      "making the current approaches to fight disinformation obsolete. We argue that\n",
      "fake media has to be seen as an upcoming cybersecurity problem, and we have to\n",
      "shift from combating its spread to a prevention and cure framework where users\n",
      "have available ways to verify, challenge, and argue against the veracity of\n",
      "each piece of media they are exposed to. To create the technologies behind this\n",
      "framework, we propose that a new Science of Disinformation is needed, one which\n",
      "creates a theoretical framework both for the processes of communication and\n",
      "consumption of false content. Key scientific and technological challenges\n",
      "facing this research agenda are listed and discussed in the light of\n",
      "state-of-art technologies for fake media generation and detection, argument\n",
      "finding and construction, and how to effectively engage users in the prevention\n",
      "and cure processes. \n",
      "\n",
      "\n",
      "Deepfakes are synthetic content generated using advanced deep learning and AI\n",
      "technologies. The advancement of technology has created opportunities for\n",
      "anyone to create and share deepfakes much easier. This may lead to societal\n",
      "concerns based on how communities engage with it. However, there is limited\n",
      "research available to understand how communities perceive deepfakes. We\n",
      "examined deepfake conversations on Reddit from 2018 to 2021 -- including major\n",
      "topics and their temporal changes as well as implications of these\n",
      "conversations. Using a mixed-method approach -- topic modeling and qualitative\n",
      "coding, we found 6,638 posts and 86,425 comments discussing concerns of the\n",
      "believable nature of deepfakes and how platforms moderate them. We also found\n",
      "Reddit conversations to be pro-deepfake and building a community that supports\n",
      "creating and sharing deepfake artifacts and building a marketplace regardless\n",
      "of the consequences. Possible implications derived from qualitative codes\n",
      "indicate that deepfake conversations raise societal concerns. We propose that\n",
      "there are implications for Human Computer Interaction (HCI) to mitigate the\n",
      "harm created from deepfakes. \n",
      "\n",
      "\n",
      "Fairness of deepfake detectors in the presence of anomalies are not well\n",
      "investigated, especially if those anomalies are more prominent in either male\n",
      "or female subjects. The primary motivation for this work is to evaluate how\n",
      "deepfake detection model behaves under such anomalies. However, due to the\n",
      "black-box nature of deep learning (DL) and artificial intelligence (AI)\n",
      "systems, it is hard to predict the performance of a model when the input data\n",
      "is modified. Crucially, if this defect is not addressed properly, it will\n",
      "adversely affect the fairness of the model and result in discrimination of\n",
      "certain sub-population unintentionally. Therefore, the objective of this work\n",
      "is to adopt metamorphic testing to examine the reliability of the selected\n",
      "deepfake detection model, and how the transformation of input variation places\n",
      "influence on the output. We have chosen MesoInception-4, a state-of-the-art\n",
      "deepfake detection model, as the target model and makeup as the anomalies.\n",
      "Makeups are applied through utilizing the Dlib library to obtain the 68 facial\n",
      "landmarks prior to filling in the RGB values. Metamorphic relations are derived\n",
      "based on the notion that realistic perturbations of the input images, such as\n",
      "makeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are\n",
      "common cosmetic appearance) applied to male and female images, should not alter\n",
      "the output of the model by a huge margin. Furthermore, we narrow down the scope\n",
      "to focus on revealing potential gender biases in DL and AI systems.\n",
      "Specifically, we are interested to examine whether MesoInception-4 model\n",
      "produces unfair decisions, which should be considered as a consequence of\n",
      "robustness issues. The findings from our work have the potential to pave the\n",
      "way for new research directions in the quality assurance and fairness in DL and\n",
      "AI systems. \n",
      "\n",
      "\n",
      "DeepFake based digital facial forgery is threatening the public media\n",
      "security, especially when lip manipulation has been used in talking face\n",
      "generation, the difficulty of fake video detection is further improved. By only\n",
      "changing lip shape to match the given speech, the facial features of identity\n",
      "is hard to be discriminated in such fake talking face videos. Together with the\n",
      "lack of attention on audio stream as the prior knowledge, the detection failure\n",
      "of fake talking face generation also becomes inevitable. Inspired by the\n",
      "decision-making mechanism of human multisensory perception system, which\n",
      "enables the auditory information to enhance post-sensory visual evidence for\n",
      "informed decisions output, in this study, a fake talking face detection\n",
      "framework FTFDNet is proposed by incorporating audio and visual representation\n",
      "to achieve more accurate fake talking face videos detection. Furthermore, an\n",
      "audio-visual attention mechanism (AVAM) is proposed to discover more\n",
      "informative features, which can be seamlessly integrated into any audio-visual\n",
      "CNN architectures by modularization. With the additional AVAM, the proposed\n",
      "FTFDNet is able to achieve a better detection performance on the established\n",
      "dataset (FTFDD). The evaluation of the proposed work has shown an excellent\n",
      "performance on the detection of fake talking face videos, which is able to\n",
      "arrive at a detection rate above 97%. \n",
      "\n",
      "\n",
      "Detecting forgery videos is highly desirable due to the abuse of deepfake.\n",
      "Existing detection approaches contribute to exploring the specific artifacts in\n",
      "deepfake videos and fit well on certain data. However, the growing technique on\n",
      "these artifacts keeps challenging the robustness of traditional deepfake\n",
      "detectors. As a result, the development of generalizability of these approaches\n",
      "has reached a blockage. To address this issue, given the empirical results that\n",
      "the identities behind voices and faces are often mismatched in deepfake videos,\n",
      "and the voices and faces have homogeneity to some extent, in this paper, we\n",
      "propose to perform the deepfake detection from an unexplored voice-face\n",
      "matching view. To this end, a voice-face matching method is devised to measure\n",
      "the matching degree of these two. Nevertheless, training on specific deepfake\n",
      "datasets makes the model overfit certain traits of deepfake algorithms. We\n",
      "instead, advocate a method that quickly adapts to untapped forgery, with a\n",
      "pre-training then fine-tuning paradigm. Specifically, we first pre-train the\n",
      "model on a generic audio-visual dataset, followed by the fine-tuning on\n",
      "downstream deepfake data. We conduct extensive experiments over three widely\n",
      "exploited deepfake datasets - DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method\n",
      "obtains significant performance gains as compared to other state-of-the-art\n",
      "competitors. It is also worth noting that our method already achieves\n",
      "competitive results when fine-tuned on limited deepfake data. \n",
      "\n",
      "\n",
      "Deepfake detection automatically recognizes the manipulated medias through\n",
      "the analysis of the difference between manipulated and non-altered videos. It\n",
      "is natural to ask which are the top performers among the existing deepfake\n",
      "detection approaches to identify promising research directions and provide\n",
      "practical guidance. Unfortunately, it's difficult to conduct a sound\n",
      "benchmarking comparison of existing detection approaches using the results in\n",
      "the literature because evaluation conditions are inconsistent across studies.\n",
      "Our objective is to establish a comprehensive and consistent benchmark, to\n",
      "develop a repeatable evaluation procedure, and to measure the performance of a\n",
      "range of detection approaches so that the results can be compared soundly. A\n",
      "challenging dataset consisting of the manipulated samples generated by more\n",
      "than 13 different methods has been collected, and 11 popular detection\n",
      "approaches (9 algorithms) from the existing literature have been implemented\n",
      "and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92\n",
      "models have been trained and 644 experiments have been performed for the\n",
      "evaluation. The results along with the shared data and evaluation methodology\n",
      "constitute a benchmark for comparing deepfake detection approaches and\n",
      "measuring progress. \n",
      "\n",
      "\n",
      "This paper describes our submitted systems to the 2022 ADD challenge withing\n",
      "the tracks 1 and 2. Our approach is based on the combination of a pre-trained\n",
      "wav2vec2 feature extractor and a downstream classifier to detect spoofed audio.\n",
      "This method exploits the contextualized speech representations at the different\n",
      "transformer layers to fully capture discriminative information. Furthermore,\n",
      "the classification model is adapted to the application scenario using different\n",
      "data augmentation techniques. We evaluate our system for audio synthesis\n",
      "detection in both the ASVspoof 2021 and the 2022 ADD challenges, showing its\n",
      "robustness and good performance in realistic challenging environments such as\n",
      "telephonic and audio codec systems, noisy audio, and partial deepfakes. \n",
      "\n",
      "\n",
      "In this work we propose Identity Consistency Transformer, a novel face\n",
      "forgery detection method that focuses on high-level semantics, specifically\n",
      "identity information, and detecting a suspect face by finding identity\n",
      "inconsistency in inner and outer face regions. The Identity Consistency\n",
      "Transformer incorporates a consistency loss for identity consistency\n",
      "determination. We show that Identity Consistency Transformer exhibits superior\n",
      "generalization ability not only across different datasets but also across\n",
      "various types of image degradation forms found in real-world applications\n",
      "including deepfake videos. The Identity Consistency Transformer can be easily\n",
      "enhanced with additional identity information when such information is\n",
      "available, and for this reason it is especially well-suited for detecting face\n",
      "forgeries involving celebrities. Code will be released at\n",
      "\\url{https://github.com/LightDXY/ICT_DeepFake} \n",
      "\n",
      "\n",
      "The fast evolution and widespread of deepfake techniques in real-world\n",
      "scenarios require stronger generalization abilities of face forgery detectors.\n",
      "Some works capture the features that are unrelated to method-specific\n",
      "artifacts, such as clues of blending boundary, accumulated up-sampling, to\n",
      "strengthen the generalization ability. However, the effectiveness of these\n",
      "methods can be easily corrupted by post-processing operations such as\n",
      "compression. Inspired by transfer learning, neural networks pre-trained on\n",
      "other large-scale face-related tasks may provide useful features for deepfake\n",
      "detection. For example, lip movement has been proved to be a kind of robust and\n",
      "good-transferring highlevel semantic feature, which can be learned from the\n",
      "lipreading task. However, the existing method pre-trains the lip feature\n",
      "extraction model in a supervised manner, which requires plenty of human\n",
      "resources in data annotation and increases the difficulty of obtaining training\n",
      "data. In this paper, we propose a self-supervised transformer based\n",
      "audio-visual contrastive learning method. The proposed method learns mouth\n",
      "motion representations by encouraging the paired video and audio\n",
      "representations to be close while unpaired ones to be diverse. After\n",
      "pre-training with our method, the model will then be partially fine-tuned for\n",
      "deepfake detection task. Extensive experiments show that our self-supervised\n",
      "method performs comparably or even better than the supervised pre-training\n",
      "counterpart. \n",
      "\n",
      "\n",
      "DeepFakes are synthetic videos generated by swapping a face of an original\n",
      "image with the face of somebody else. In this paper, we describe our work to\n",
      "develop general, deep learning-based models to classify DeepFake content. We\n",
      "propose a novel framework for using Generative Adversarial Network (GAN)-based\n",
      "models, we call MRI-GAN, that utilizes perceptual differences in images to\n",
      "detect synthesized videos. We test our MRI-GAN approach and a\n",
      "plain-frames-based model using the DeepFake Detection Challenge Dataset. Our\n",
      "plain frames-based-model achieves 91% test accuracy and a model which uses our\n",
      "MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the\n",
      "perceptual differences achieves 74% test accuracy. The results of MRI-GAN are\n",
      "preliminary and may be improved further by modifying the choice of loss\n",
      "function, tuning hyper-parameters, or by using a more advanced perceptual\n",
      "similarity metric. \n",
      "\n",
      "\n",
      "With the rapid progress of generation technology, it has become necessary to\n",
      "attribute the origin of fake images. Existing works on fake image attribution\n",
      "perform multi-class classification on several Generative Adversarial Network\n",
      "(GAN) models and obtain high accuracies. While encouraging, these works are\n",
      "restricted to model-level attribution, only capable of handling images\n",
      "generated by seen models with a specific seed, loss and dataset, which is\n",
      "limited in real-world scenarios when fake images may be generated by privately\n",
      "trained models. This motivates us to ask whether it is possible to attribute\n",
      "fake images to the source models' architectures even if they are finetuned or\n",
      "retrained under different configurations. In this work, we present the first\n",
      "study on Deepfake Network Architecture Attribution to attribute fake images on\n",
      "architecture-level. Based on an observation that GAN architecture is likely to\n",
      "leave globally consistent fingerprints while traces left by model weights vary\n",
      "in different regions, we provide a simple yet effective solution named DNA-Det\n",
      "for this problem. Extensive experiments on multiple cross-test setups and a\n",
      "large-scale dataset demonstrate the effectiveness of DNA-Det. \n",
      "\n",
      "\n",
      "Despite several years of research in deepfake and spoofing detection for\n",
      "automatic speaker verification, little is known about the artefacts that\n",
      "classifiers use to distinguish between bona fide and spoofed utterances. An\n",
      "understanding of these is crucial to the design of trustworthy, explainable\n",
      "solutions. In this paper we report an extension of our previous work to better\n",
      "understand classifier behaviour to the use of SHapley Additive exPlanations\n",
      "(SHAP) to attack analysis. Our goal is to identify the artefacts that\n",
      "characterise utterances generated by different attacks algorithms. Using a pair\n",
      "of classifiers which operate either upon raw waveforms or magnitude\n",
      "spectrograms, we show that visualisations of SHAP results can be used to\n",
      "identify attack-specific artefacts and the differences and consistencies\n",
      "between synthetic speech and converted voice spoofing attacks. \n",
      "\n",
      "\n",
      "AI-created face-swap videos, commonly known as Deepfakes, have attracted wide\n",
      "attention as powerful impersonation attacks. Existing research on Deepfakes\n",
      "mostly focuses on binary detection to distinguish between real and fake videos.\n",
      "However, it is also important to determine the specific generation model for a\n",
      "fake video, which can help attribute it to the source for forensic\n",
      "investigation. In this paper, we fill this gap by studying the model\n",
      "attribution problem of Deepfake videos. We first introduce a new dataset with\n",
      "DeepFakes from Different Models (DFDM) based on several Autoencoder models.\n",
      "Specifically, five generation models with variations in encoder, decoder,\n",
      "intermediate layer, input resolution, and compression ratio have been used to\n",
      "generate a total of 6,450 Deepfake videos based on the same input. Then we take\n",
      "Deepfakes model attribution as a multiclass classification task and propose a\n",
      "spatial and temporal attention based method to explore the differences among\n",
      "Deepfakes in the new dataset. Experimental evaluation shows that most existing\n",
      "Deepfakes detection methods failed in Deepfakes model attribution, while the\n",
      "proposed method achieved over 70% accuracy on the high-quality DFDM dataset. \n",
      "\n",
      "\n",
      "Recent advances in technology for hyper-realistic visual effects provoke the\n",
      "concern that deepfake videos of political speeches will soon be visually\n",
      "indistinguishable from authentic video recordings. The conventional wisdom in\n",
      "communications research predicts people will fall for fake news more often when\n",
      "the same version of a story is presented as a video rather than text. Here, we\n",
      "evaluate how accurately 41,822 participants distinguish real political speeches\n",
      "from fabrications in an experiment where speeches are randomized to appear as\n",
      "permutations of text, audio, and video. We find access to audio and visual\n",
      "communication modalities improve participants' accuracy. Here, human judgment\n",
      "relies more on how something is said, the audio-visual cues, than what is said,\n",
      "the speech content. However, we find that reflective reasoning moderates the\n",
      "degree to which participants consider visual information: low performance on\n",
      "the Cognitive Reflection Test is associated with an over-reliance on what is\n",
      "said. \n",
      "\n",
      "\n",
      "The performance of spoofing countermeasure systems depends fundamentally upon\n",
      "the use of sufficiently representative training data. With this usually being\n",
      "limited, current solutions typically lack generalisation to attacks encountered\n",
      "in the wild. Strategies to improve reliability in the face of uncontrolled,\n",
      "unpredictable attacks are hence needed. We report in this paper our efforts to\n",
      "use self-supervised learning in the form of a wav2vec 2.0 front-end with fine\n",
      "tuning. Despite initial base representations being learned using only bona fide\n",
      "data and no spoofed data, we obtain the lowest equal error rates reported in\n",
      "the literature for both the ASVspoof 2021 Logical Access and Deepfake\n",
      "databases. When combined with data augmentation,these results correspond to an\n",
      "improvement of almost 90% relative to our baseline system. \n",
      "\n",
      "\n",
      "Hyper-realistic face image generation and manipulation have givenrise to\n",
      "numerous unethical social issues, e.g., invasion of privacy,threat of security,\n",
      "and malicious political maneuvering, which re-sulted in the development of\n",
      "recent deepfake detection methods with the rising demands of deepfake\n",
      "forensics. Proposed deepfake detection methods to date have shown remarkable\n",
      "detection performance and robustness. However, none of the suggested deepfake\n",
      "detection methods assessed the performance of deepfakes with the facemask\n",
      "during the pandemic crisis after the outbreak of theCovid-19. In this paper, we\n",
      "thoroughly evaluate the performance of state-of-the-art deepfake detection\n",
      "models on the deepfakes with the facemask. Also, we propose two approaches to\n",
      "enhance the masked deepfakes detection: face-patch and face-crop. The\n",
      "experimental evaluations on both methods are assessed through the base-line\n",
      "deepfake detection models on the various deepfake datasets. Our extensive\n",
      "experiments show that, among the two methods, face-crop performs better than\n",
      "the face-patch, and could be a train method for deepfake detection models to\n",
      "detect fake faces with facemask in real world. \n",
      "\n",
      "\n",
      "Facial Liveness Verification (FLV) is widely used for identity authentication\n",
      "in many security-sensitive domains and offered as Platform-as-a-Service (PaaS)\n",
      "by leading cloud vendors. Yet, with the rapid advances in synthetic media\n",
      "techniques (e.g., deepfake), the security of FLV is facing unprecedented\n",
      "challenges, about which little is known thus far.\n",
      "  To bridge this gap, in this paper, we conduct the first systematic study on\n",
      "the security of FLV in real-world settings. Specifically, we present\n",
      "LiveBugger, a new deepfake-powered attack framework that enables customizable,\n",
      "automated security evaluation of FLV. Leveraging LiveBugger, we perform a\n",
      "comprehensive empirical assessment of representative FLV platforms, leading to\n",
      "a set of interesting findings. For instance, most FLV APIs do not use\n",
      "anti-deepfake detection; even for those with such defenses, their effectiveness\n",
      "is concerning (e.g., it may detect high-quality synthesized videos but fail to\n",
      "detect low-quality ones). We then conduct an in-depth analysis of the factors\n",
      "impacting the attack performance of LiveBugger: a) the bias (e.g., gender or\n",
      "race) in FLV can be exploited to select victims; b) adversarial training makes\n",
      "deepfake more effective to bypass FLV; c) the input quality has a varying\n",
      "influence on different deepfake techniques to bypass FLV. Based on these\n",
      "findings, we propose a customized, two-stage approach that can boost the attack\n",
      "success rate by up to 70%. Further, we run proof-of-concept attacks on several\n",
      "representative applications of FLV (i.e., the clients of FLV APIs) to\n",
      "illustrate the practical implications: due to the vulnerability of the APIs,\n",
      "many downstream applications are vulnerable to deepfake. Finally, we discuss\n",
      "potential countermeasures to improve the security of FLV. Our findings have\n",
      "been confirmed by the corresponding vendors. \n",
      "\n",
      "\n",
      "Audio deepfake detection is an emerging topic, which was included in the\n",
      "ASVspoof 2021. However, the recent shared tasks have not covered many real-life\n",
      "and challenging scenarios. The first Audio Deep synthesis Detection challenge\n",
      "(ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks:\n",
      "low-quality fake audio detection (LF), partially fake audio detection (PF) and\n",
      "audio fake game (FG). The LF track focuses on dealing with bona fide and fully\n",
      "fake utterances with various real-world noises etc. The PF track aims to\n",
      "distinguish the partially fake audio from the real. The FG track is a rivalry\n",
      "game, which includes two tasks: an audio generation task and an audio fake\n",
      "detection task. In this paper, we describe the datasets, evaluation metrics,\n",
      "and protocols. We also report major findings that reflect the recent advances\n",
      "in audio deepfake detection tasks. \n",
      "\n",
      "\n",
      "Recent advances in deep learning have led to substantial improvements in\n",
      "deepfake generation, resulting in fake media with a more realistic appearance.\n",
      "Although deepfake media have potential application in a wide range of areas and\n",
      "are drawing much attention from both the academic and industrial communities,\n",
      "it also leads to serious social and criminal concerns. This chapter explores\n",
      "the evolution of and challenges in deepfake generation and detection. It also\n",
      "discusses possible ways to improve the robustness of deepfake detection for a\n",
      "wide variety of media (e.g., in-the-wild images and videos). Finally, it\n",
      "suggests a focus for future fake media research. \n",
      "\n",
      "\n",
      "The fast-spreading information over the internet is essential to support the\n",
      "rapid supply of numerous public utility services and entertainment to users.\n",
      "Social networks and online media paved the way for modern,\n",
      "timely-communication-fashion and convenient access to all types of information.\n",
      "However, it also provides new chances for ill use of the massive amount of\n",
      "available data, such as spreading fake content to manipulate public opinion.\n",
      "Detection of counterfeit content has raised attention in the last few years for\n",
      "the advances in deepfake generation. The rapid growth of machine learning\n",
      "techniques, particularly deep learning, can predict fake content in several\n",
      "application domains, including fake image and video manipulation. This paper\n",
      "presents a comprehensive review of recent studies for deepfake content\n",
      "detection using deep learning-based approaches. We aim to broaden the\n",
      "state-of-the-art research by systematically reviewing the different categories\n",
      "of fake content detection. Furthermore, we report the advantages and drawbacks\n",
      "of the examined works and future directions towards the issues and shortcomings\n",
      "still unsolved on deepfake detection. \n",
      "\n",
      "\n",
      "Detecting deepfakes is an important problem, but recent work has shown that\n",
      "DNN-based deepfake detectors are brittle against adversarial deepfakes, in\n",
      "which an adversary adds imperceptible perturbations to a deepfake to evade\n",
      "detection. In this work, we show that a modification to the detection strategy\n",
      "in which we replace a single classifier with a carefully chosen ensemble, in\n",
      "which input transformations for each model in the ensemble induces pairwise\n",
      "orthogonal gradients, can significantly improve robustness beyond the de facto\n",
      "solution of adversarial training. We present theoretical results to show that\n",
      "such orthogonal gradients can help thwart a first-order adversary by reducing\n",
      "the dimensionality of the input subspace in which adversarial deepfakes lie. We\n",
      "validate the results empirically by instantiating and evaluating a randomized\n",
      "version of such \"orthogonal\" ensembles for adversarial deepfake detection and\n",
      "find that these randomized ensembles exhibit significantly higher robustness as\n",
      "deepfake detectors compared to state-of-the-art deepfake detectors against\n",
      "adversarial deepfakes, even those created using strong PGD-500 attacks. \n",
      "\n",
      "\n",
      "In the world of fake news and deepfakes, there have been an alarmingly large\n",
      "number of cases of images being tampered with and published in newspapers, used\n",
      "in court, and posted on social media for defamation purposes. Detecting these\n",
      "tampered images is an important task and one we try to tackle. In this paper,\n",
      "we focus on the methods to detect if an image has been tampered with using both\n",
      "Deep Learning and Image transformation methods and comparing the performances\n",
      "and robustness of each method. We then attempt to identify the tampered area of\n",
      "the image and predict the corresponding mask. Based on the results, suggestions\n",
      "and approaches are provided to achieve a more robust framework to detect and\n",
      "identify the forgeries. \n",
      "\n",
      "\n",
      "Various deepfake detectors have been proposed, but challenges still exist to\n",
      "detect images of unknown categories or GAN models outside of the training\n",
      "settings. Such issues arise from the overfitting issue, which we discover from\n",
      "our own analysis and the previous studies to originate from the frequency-level\n",
      "artifacts in generated images. We find that ignoring the frequency-level\n",
      "artifacts can improve the detector's generalization across various GAN models,\n",
      "but it can reduce the model's performance for the trained GAN models. Thus, we\n",
      "design a framework to generalize the deepfake detector for both the known and\n",
      "unseen GAN models. Our framework generates the frequency-level perturbation\n",
      "maps to make the generated images indistinguishable from the real images. By\n",
      "updating the deepfake detector along with the training of the perturbation\n",
      "generator, our model is trained to detect the frequency-level artifacts at the\n",
      "initial iterations and consider the image-level irregularities at the last\n",
      "iterations. For experiments, we design new test scenarios varying from the\n",
      "training settings in GAN models, color manipulations, and object categories.\n",
      "Numerous experiments validate the state-of-the-art performance of our deepfake\n",
      "detector. \n",
      "\n",
      "\n",
      "Although the deepfake detection based on convolutional neural network has\n",
      "achieved good results, the detection results show that these detectors show\n",
      "obvious performance degradation when the input images undergo some common\n",
      "transformations (like resizing, blurring), which indicates that the\n",
      "generalization ability of the detector is insufficient. In this paper, we\n",
      "propose a novel block shuffling learning method to solve this problem.\n",
      "Specifically, we divide the images into blocks and then introduce the random\n",
      "shuffling to intra-block and inter-block. Intra-block shuffling increases the\n",
      "robustness of the detector and we also propose an adversarial loss algorithm to\n",
      "overcome the over-fitting problem brought by the noise introduced by shuffling.\n",
      "Moreover, we encourage the detector to focus on finding differences among the\n",
      "local features through inter-block shuffling, and reconstruct the spatial\n",
      "layout of the blocks to model the semantic associations between them.\n",
      "Especially, our method can be easily integrated with various CNN models.\n",
      "Extensive experiments show that our proposed method achieves state-of-the-art\n",
      "performance in forgery face detection, including good generalization ability in\n",
      "the face of common image transformations. \n",
      "\n",
      "\n",
      "This essay shows the impact of deepfake technology on fan culture. The\n",
      "innovative technology provided the male audience with an instrument to express\n",
      "its ideas and plots. Which subsequently led to the rise of deepfake\n",
      "pornography. It is often seen as a part of celebrity studies; however, the\n",
      "essay shows that it could also be considered a type of fanfic and a product of\n",
      "participatory culture, sharing community origin, exploitation by commercial\n",
      "companies and deep sexualisation. These two branches of fanfic evolution can be\n",
      "connected via the genre of machinima pornography. Textual fanfics are mainly\n",
      "created by females for females, depicting males; otherwise, deepfake\n",
      "pornography and machinima are made by males and for males targeting females. \n",
      "\n",
      "\n",
      ": Deep learning methodologies have been used to create applications that can\n",
      "cause threats to privacy, democracy and national security and could be used to\n",
      "further amplify malicious activities. One of those deep learning-powered\n",
      "applications in recent times is synthesized videos of famous personalities.\n",
      "According to Forbes, Generative Adversarial Networks(GANs) generated fake\n",
      "videos growing exponentially every year and the organization known as Deeptrace\n",
      "had estimated an increase of deepfakes by 84% from the year 2018 to 2019. They\n",
      "are used to generate and modify human faces, where most of the existing fake\n",
      "videos are of prurient non-consensual nature, of which its estimates to be\n",
      "around 96% and some carried out impersonating personalities for cyber crime. In\n",
      "this paper, available video datasets are identified and a pretrained model\n",
      "BlazeFace is used to detect faces, and a ResNet and Xception ensembled\n",
      "architectured neural network trained on the dataset to achieve the goal of\n",
      "detection of fake faces in videos. The model is optimized over a loss value and\n",
      "log loss values and evaluated over its F1 score. Over a sample of data, it is\n",
      "observed that focal loss provides better accuracy, F1 score and loss as the\n",
      "gamma of the focal loss becomes a hyper parameter. This provides a k-folded\n",
      "accuracy of around 91% at its peak in a training cycle with the real world\n",
      "accuracy subjected to change over time as the model decays. \n",
      "\n",
      "\n",
      "Pre-trained encoders are general-purpose feature extractors that can be used\n",
      "for many downstream tasks. Recent progress in self-supervised learning can\n",
      "pre-train highly effective encoders using a large volume of unlabeled data,\n",
      "leading to the emerging encoder as a service (EaaS). A pre-trained encoder may\n",
      "be deemed confidential because its training requires lots of data and\n",
      "computation resources as well as its public release may facilitate misuse of\n",
      "AI, e.g., for deepfakes generation. In this paper, we propose the first attack\n",
      "called StolenEncoder to steal pre-trained image encoders. We evaluate\n",
      "StolenEncoder on multiple target encoders pre-trained by ourselves and three\n",
      "real-world target encoders including the ImageNet encoder pre-trained by\n",
      "Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding\n",
      "encoder deployed as a paid EaaS. Our results show that our stolen encoders have\n",
      "similar functionality with the target encoders. In particular, the downstream\n",
      "classifiers built upon a target encoder and a stolen one have similar accuracy.\n",
      "Moreover, stealing a target encoder using StolenEncoder requires much less data\n",
      "and computation resources than pre-training it from scratch. We also explore\n",
      "three defenses that perturb feature vectors produced by a target encoder. Our\n",
      "results show these defenses are not enough to mitigate StolenEncoder. \n",
      "\n",
      "\n",
      "Due to the advancement of Generative Adversarial Networks (GAN),\n",
      "Autoencoders, and other AI technologies, it has been much easier to create fake\n",
      "images such as \"Deepfakes\". More recent research has introduced few-shot\n",
      "learning, which uses a small amount of training data to produce fake images and\n",
      "videos more effectively. Therefore, the ease of generating manipulated images\n",
      "and the difficulty of distinguishing those images can cause a serious threat to\n",
      "our society, such as propagating fake information. However, detecting realistic\n",
      "fake images generated by the latest AI technology is challenging due to the\n",
      "reasons mentioned above. In this work, we propose Dual Attention Fake Detection\n",
      "Fine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images\n",
      "from the real face data. Our DA-FDFtNet integrates the pre-trained model with\n",
      "Fine-Tune Transformer, MBblockV3, and a channel attention module to improve the\n",
      "performance and robustness across different types of fake images. In\n",
      "particular, Fine-Tune Transformer consists of multiple numbers of an\n",
      "image-based self-attention module and a down-sampling layer. The channel\n",
      "attention module is also connected with the pre-trained model to capture the\n",
      "fake images feature space. We experiment with our DA-FDFtNet with the\n",
      "FaceForensics++ dataset and various GAN-generated datasets, and we show that\n",
      "our approach outperforms the previous baseline models. \n",
      "\n",
      "\n",
      "In today's era of digital misinformation, we are increasingly faced with new\n",
      "threats posed by video falsification techniques. Such falsifications range from\n",
      "cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\n",
      "sophisticated AI media synthesis methods), which are becoming perceptually\n",
      "indistinguishable from real videos. To tackle this challenge, we propose a\n",
      "multi-modal semantic forensic approach to discover clues that go beyond\n",
      "detecting discrepancies in visual quality, thereby handling both simpler\n",
      "cheapfakes and visually persuasive deepfakes. In this work, our goal is to\n",
      "verify that the purported person seen in the video is indeed themselves by\n",
      "detecting anomalous correspondences between their facial movements and the\n",
      "words they are saying. We leverage the idea of attribution to learn\n",
      "person-specific biometric patterns that distinguish a given speaker from\n",
      "others. We use interpretable Action Units (AUs) to capture a persons' face and\n",
      "head movement as opposed to deep CNN visual features, and we are the first to\n",
      "use word-conditioned facial motion analysis. Unlike existing person-specific\n",
      "approaches, our method is also effective against attacks that focus on lip\n",
      "manipulation. We further demonstrate our method's effectiveness on a range of\n",
      "fakes not seen in training including those without video manipulation, that\n",
      "were not addressed in prior work. \n",
      "\n",
      "\n",
      "In recent years, the spread of fake videos has brought great influence on\n",
      "individuals and even countries. It is important to provide robust and reliable\n",
      "results for fake videos. The results of conventional detection methods are not\n",
      "reliable and not robust for unseen videos. Another alternative and more\n",
      "effective way is to find the original video of the fake video. For example,\n",
      "fake videos from the Russia-Ukraine war and the Hong Kong law revision storm\n",
      "are refuted by finding the original video. We use an improved retrieval method\n",
      "to find the original video, named ViTHash. Specifically, tracing the source of\n",
      "fake videos requires finding the unique one, which is difficult when there are\n",
      "only small differences in the original videos. To solve the above problems, we\n",
      "designed a novel loss Hash Triplet Loss. In addition, we designed a tool called\n",
      "Localizator to compare the difference between the original traced video and the\n",
      "fake video. We have done extensive experiments on FaceForensics++, Celeb-DF and\n",
      "DeepFakeDetection, and we also have done additional experiments on our built\n",
      "three datasets: DAVIS2016-TL (video inpainting), VSTL (video splicing) and DFTL\n",
      "(similar videos). Experiments have shown that our performance is better than\n",
      "state-of-the-art methods, especially in cross-dataset mode. Experiments also\n",
      "demonstrated that ViTHash is effective in various forgery detection: video\n",
      "inpainting, video splicing and deepfakes. Our code and datasets have been\n",
      "released on GitHub: \\url{https://github.com/lajlksdf/vtl}. \n",
      "\n",
      "\n",
      "Despite significant advancements of deep learning-based forgery detectors for\n",
      "distinguishing manipulated deepfake images, most detection approaches suffer\n",
      "from moderate to significant performance degradation with low-quality\n",
      "compressed deepfake images. Because of the limited information in low-quality\n",
      "images, detecting low-quality deepfake remains an important challenge. In this\n",
      "work, we apply frequency domain learning and optimal transport theory in\n",
      "knowledge distillation (KD) to specifically improve the detection of\n",
      "low-quality compressed deepfake images. We explore transfer learning capability\n",
      "in KD to enable a student network to learn discriminative features from\n",
      "low-quality images effectively. In particular, we propose the Attention-based\n",
      "Deepfake detection Distiller (ADD), which consists of two novel distillations:\n",
      "1) frequency attention distillation that effectively retrieves the removed\n",
      "high-frequency components in the student network, and 2) multi-view attention\n",
      "distillation that creates multiple attention vectors by slicing the teacher's\n",
      "and student's tensors under different views to transfer the teacher tensor's\n",
      "distribution to the student more efficiently. Our extensive experimental\n",
      "results demonstrate that our approach outperforms state-of-the-art baselines in\n",
      "detecting low-quality compressed deepfake images. \n",
      "\n",
      "\n",
      "Deep learning based generative models such as deepfake have been able to\n",
      "generate amazing images and videos. However, these models may need significant\n",
      "transformation when applied to generate crystal materials structures in which\n",
      "the building blocks, the physical atoms are very different from the pixels.\n",
      "Naively transferred generative models tend to generate a large portion of\n",
      "physically infeasible crystal structures that are not stable or synthesizable.\n",
      "Herein we show that by exploiting and adding physically oriented data\n",
      "augmentation, loss function terms, and post processing, our deep adversarial\n",
      "network (GAN) based generative models can now generate crystal structures with\n",
      "higher physical feasibility and expand our previous models which can only\n",
      "create cubic structures. \n",
      "\n",
      "\n",
      "Deepfake is content or material that is generated or manipulated using AI\n",
      "methods, to pass off as real. There are four different deepfake types: audio,\n",
      "video, image and text. In this research we focus on audio deepfakes and how\n",
      "people perceive it. There are several audio deepfake generation frameworks, but\n",
      "we chose MelGAN which is a non-autoregressive and fast audio deepfake\n",
      "generating framework, requiring fewer parameters. This study tries to assess\n",
      "audio deepfake perceptions among college students from different majors. This\n",
      "study also answers the question of how their background and major can affect\n",
      "their perception towards AI generated deepfakes. We also analyzed the results\n",
      "based on different aspects of: grade level, complexity of the grammar used in\n",
      "the audio clips, length of the audio clips, those who knew the term deepfakes\n",
      "and those who did not, as well as the political angle. It is interesting that\n",
      "the results show when an audio clip has a political connotation, it can affect\n",
      "what people think about whether it is real or fake, even if the content is\n",
      "fairly similar. This study also explores the question of how background and\n",
      "major can affect perception towards deepfakes. \n",
      "\n",
      "\n",
      "Deepfake is content or material that is synthetically generated or\n",
      "manipulated using artificial intelligence (AI) methods, to be passed off as\n",
      "real and can include audio, video, image, and text synthesis. This survey has\n",
      "been conducted with a different perspective compared to existing survey papers,\n",
      "that mostly focus on just video and image deepfakes. This survey not only\n",
      "evaluates generation and detection methods in the different deepfake\n",
      "categories, but mainly focuses on audio deepfakes that are overlooked in most\n",
      "of the existing surveys. This paper critically analyzes and provides a unique\n",
      "source of audio deepfake research, mostly ranging from 2016 to 2020. To the\n",
      "best of our knowledge, this is the first survey focusing on audio deepfakes in\n",
      "English. This survey provides readers with a summary of 1) different deepfake\n",
      "categories 2) how they could be created and detected 3) the most recent trends\n",
      "in this domain and shortcomings in detection methods 4) audio deepfakes, how\n",
      "they are created and detected in more detail which is the main focus of this\n",
      "paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural\n",
      "Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and\n",
      "detecting deepfakes. In our evaluation of over 140 methods we found that the\n",
      "majority of the focus is on video deepfakes and in particular in the generation\n",
      "of video deepfakes. We found that for text deepfakes there are more generation\n",
      "methods but very few robust methods for detection, including fake news\n",
      "detection, which has become a controversial area of research because of the\n",
      "potential of heavy overlaps with human generation of fake content. This paper\n",
      "is an abbreviated version of the full survey and reveals a clear need to\n",
      "research audio deepfakes and particularly detection of audio deepfakes. \n",
      "\n",
      "\n",
      "In recent years, visual forgery has reached a level of sophistication that\n",
      "humans cannot identify fraud, which poses a significant threat to information\n",
      "security. A wide range of malicious applications have emerged, such as fake\n",
      "news, defamation or blackmailing of celebrities, impersonation of politicians\n",
      "in political warfare, and the spreading of rumours to attract views. As a\n",
      "result, a rich body of visual forensic techniques has been proposed in an\n",
      "attempt to stop this dangerous trend. In this paper, we present a benchmark\n",
      "that provides in-depth insights into visual forgery and visual forensics, using\n",
      "a comprehensive and empirical approach. More specifically, we develop an\n",
      "independent framework that integrates state-of-the-arts counterfeit generators\n",
      "and detectors, and measure the performance of these techniques using various\n",
      "criteria. We also perform an exhaustive analysis of the benchmarking results,\n",
      "to determine the characteristics of the methods that serve as a comparative\n",
      "reference in this never-ending war between measures and countermeasures. \n",
      "\n",
      "\n",
      "Self-supervised speech model is a rapid progressing research topic, and many\n",
      "pre-trained models have been released and used in various down stream tasks.\n",
      "For speech anti-spoofing, most countermeasures (CMs) use signal processing\n",
      "algorithms to extract acoustic features for classification. In this study, we\n",
      "use pre-trained self-supervised speech models as the front end of spoofing CMs.\n",
      "We investigated different back end architectures to be combined with the\n",
      "self-supervised front end, the effectiveness of fine-tuning the front end, and\n",
      "the performance of using different pre-trained self-supervised models. Our\n",
      "findings showed that, when a good pre-trained front end was fine-tuned with\n",
      "either a shallow or a deep neural network-based back end on the ASVspoof 2019\n",
      "logical access (LA) training set, the resulting CM not only achieved a low EER\n",
      "score on the 2019 LA test set but also significantly outperformed the baseline\n",
      "on the ASVspoof 2015, 2021 LA, and 2021 deepfake test sets. A sub-band analysis\n",
      "further demonstrated that the CM mainly used the information in a specific\n",
      "frequency band to discriminate the bona fide and spoofed trials across the test\n",
      "sets. \n",
      "\n",
      "\n",
      "With the rapid development of generation model, AI-based face manipulation\n",
      "technology, which called DeepFakes, has become more and more realistic. This\n",
      "means of face forgery can attack any target, which poses a new threat to\n",
      "personal privacy and property security. Moreover, the misuse of synthetic video\n",
      "shows potential dangers in many areas, such as identity harassment, pornography\n",
      "and news rumors. Inspired by the fact that the spatial coherence and temporal\n",
      "consistency of physiological signal are destroyed in the generated content, we\n",
      "attempt to find inconsistent patterns that can distinguish between real videos\n",
      "and synthetic videos from the variations of facial pixels, which are highly\n",
      "related to physiological information. Our approach first applies Eulerian Video\n",
      "Magnification (EVM) at multiple Gaussian scales to the original video to\n",
      "enlarge the physiological variations caused by the change of facial blood\n",
      "volume, and then transform the original video and magnified videos into a\n",
      "Multi-Scale Eulerian Magnified Spatial-Temporal map (MEMSTmap), which can\n",
      "represent time-varying physiological enhancement sequences on different\n",
      "octaves. Then, these maps are reshaped into frame patches in column units and\n",
      "sent to the vision Transformer to learn the spatio-time descriptors of frame\n",
      "levels. Finally, we sort out the feature embedding and output the probability\n",
      "of judging whether the video is real or fake. We validate our method on the\n",
      "FaceForensics++ and DeepFake Detection datasets. The results show that our\n",
      "model achieves excellent performance in forgery detection, and also show\n",
      "outstanding generalization capability in cross-data domain. \n",
      "\n",
      "\n",
      "Deepfakes are becoming increasingly popular in both good faith applications\n",
      "such as in entertainment and maliciously intended manipulations such as in\n",
      "image and video forgery. Primarily motivated by the latter, a large number of\n",
      "deepfake detectors have been proposed recently in order to identify such\n",
      "content. While the performance of such detectors still need further\n",
      "improvements, they are often assessed in simple if not trivial scenarios. In\n",
      "particular, the impact of benign processing operations such as transcoding,\n",
      "denoising, resizing and enhancement are not sufficiently studied. This paper\n",
      "proposes a more rigorous and systematic framework to assess the performance of\n",
      "deepfake detectors in more realistic situations. It quantitatively measures how\n",
      "and to which extent each benign processing approach impacts a state-of-the-art\n",
      "deepfake detection method. By illustrating it in a popular deepfake detector,\n",
      "our benchmark proposes a framework to assess robustness of detectors and\n",
      "provides valuable insights to design more efficient deepfake detectors. \n",
      "\n",
      "\n",
      "Deep generative modeling has the potential to cause significant harm to\n",
      "society. Recognizing this threat, a magnitude of research into detecting\n",
      "so-called \"Deepfakes\" has emerged. This research most often focuses on the\n",
      "image domain, while studies exploring generated audio signals have, so-far,\n",
      "been neglected. In this paper we make three key contributions to narrow this\n",
      "gap. First, we provide researchers with an introduction to common signal\n",
      "processing techniques used for analyzing audio signals. Second, we present a\n",
      "novel data set, for which we collected nine sample sets from five different\n",
      "network architectures, spanning two languages. Finally, we supply practitioners\n",
      "with two baseline models, adopted from the signal processing community, to\n",
      "facilitate further research in this area. \n",
      "\n",
      "\n",
      "Deepfake poses a serious threat to the reliability of judicial evidence and\n",
      "intellectual property protection. In spite of an urgent need for Deepfake\n",
      "identification, existing pixel-level detection methods are increasingly unable\n",
      "to resist the growing realism of fake videos and lack generalization. In this\n",
      "paper, we propose a scheme to expose Deepfake through faint signals hidden in\n",
      "face videos. This scheme extracts two types of minute information hidden\n",
      "between face pixels-photoplethysmography (PPG) features and auto-regressive\n",
      "(AR) features, which are used as the basis for forensics in the temporal and\n",
      "spatial domains, respectively. According to the principle of PPG, tracking the\n",
      "absorption of light by blood cells allows remote estimation of the temporal\n",
      "domains heart rate (HR) of face video, and irregular HR fluctuations can be\n",
      "seen as traces of tampering. On the other hand, AR coefficients are able to\n",
      "reflect the inter-pixel correlation, and can also reflect the traces of\n",
      "smoothing caused by up-sampling in the process of generating fake faces.\n",
      "Furthermore, the scheme combines asymmetric convolution block (ACBlock)-based\n",
      "improved densely connected networks (DenseNets) to achieve face video\n",
      "authenticity forensics. Its asymmetric convolutional structure enhances the\n",
      "robustness of network to the input feature image upside-down and left-right\n",
      "flipping, so that the sequence of feature stitching does not affect detection\n",
      "results. Simulation results show that our proposed scheme provides more\n",
      "accurate authenticity detection results on multiple deep forgery datasets and\n",
      "has better generalization compared to the benchmark strategy. \n",
      "\n",
      "\n",
      "Substantial progress in spoofing and deepfake detection has been made in\n",
      "recent years. Nonetheless, the community has yet to make notable inroads in\n",
      "providing an explanation for how a classifier produces its output. The\n",
      "dominance of black box spoofing detection solutions is at further odds with the\n",
      "drive toward trustworthy, explainable artificial intelligence. This paper\n",
      "describes our use of SHapley Additive exPlanations (SHAP) to gain new insights\n",
      "in spoofing detection. We demonstrate use of the tool in revealing unexpected\n",
      "classifier behaviour, the artefacts that contribute most to classifier outputs\n",
      "and differences in the behaviour of competing spoofing detection models. The\n",
      "tool is both efficient and flexible, being readily applicable to a host of\n",
      "different architecture models in addition to related, different applications.\n",
      "All results reported in the paper are reproducible using open-source software. \n",
      "\n",
      "\n",
      "With the successful creation of high-quality image-to-image (Img2Img)\n",
      "translation GANs comes the non-ethical applications of DeepFake and DeepNude.\n",
      "Such misuses of img2img techniques present a challenging problem for society.\n",
      "In this work, we tackle the problem by introducing the Limit-Aware Self-Guiding\n",
      "Gradient Sliding Attack (LaS-GSA). LaS-GSA follows the Nullifying Attack to\n",
      "cancel the img2img translation process under a black-box setting. In other\n",
      "words, by processing input images with the proposed LaS-GSA before publishing,\n",
      "any targeted img2img GANs can be nullified, preventing the model from\n",
      "maliciously manipulating the images. To improve efficiency, we introduce the\n",
      "limit-aware random gradient-free estimation and the gradient sliding mechanism\n",
      "to estimate the gradient that adheres to the adversarial limit, i.e., the pixel\n",
      "value limitations of the adversarial example. Theoretical justifications\n",
      "validate how the above techniques prevent inefficiency caused by the\n",
      "adversarial limit in both the direction and the step length. Furthermore, an\n",
      "effective self-guiding prior is extracted solely from the threat model and the\n",
      "target image to efficiently leverage the prior information and guide the\n",
      "gradient estimation process. Extensive experiments demonstrate that LaS-GSA\n",
      "requires fewer queries to nullify the image translation process with higher\n",
      "success rates than 4 state-of-the-art black-box methods. \n",
      "\n",
      "\n",
      "Significant advances in deep learning have obtained hallmark accuracy rates\n",
      "for various computer vision applications. However, advances in deep generative\n",
      "models have also led to the generation of very realistic fake content, also\n",
      "known as deepfakes, causing a threat to privacy, democracy, and national\n",
      "security. Most of the current deepfake detection methods are deemed as a binary\n",
      "classification problem in distinguishing authentic images or videos from fake\n",
      "ones using two-class convolutional neural networks (CNNs). These methods are\n",
      "based on detecting visual artifacts, temporal or color inconsistencies produced\n",
      "by deep generative models. However, these methods require a large amount of\n",
      "real and fake data for model training and their performance drops significantly\n",
      "in cross dataset evaluation with samples generated using advanced deepfake\n",
      "generation techniques. In this paper, we thoroughly evaluate the efficacy of\n",
      "deep face recognition in identifying deepfakes, using different loss functions\n",
      "and deepfake generation techniques. Experimental investigations on challenging\n",
      "Celeb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep\n",
      "face recognition in identifying deepfakes over two-class CNNs and the ocular\n",
      "modality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and\n",
      "an Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition\n",
      "on the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER\n",
      "obtained for the two-class CNN and the ocular modality on the Celeb-DF dataset.\n",
      "Further on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were\n",
      "obtained. The use of biometric facial recognition technology has the advantage\n",
      "of bypassing the need for a large amount of fake data for model training and\n",
      "obtaining better generalizability to evolving deepfake creation techniques. \n",
      "\n",
      "\n",
      "With the rapid development of deep learning technology, more and more face\n",
      "forgeries by deepfake are widely spread on social media, causing serious social\n",
      "concern. Face forgery detection has become a research hotspot in recent years,\n",
      "and many related methods have been proposed until now. For those images with\n",
      "low quality and/or diverse sources, however, the detection performances of\n",
      "existing methods are still far from satisfactory. In this paper, we propose an\n",
      "improved Xception with dual attention mechanism and feature fusion for face\n",
      "forgery detection. Different from the middle flow in original Xception model,\n",
      "we try to catch different high-semantic features of the face images using\n",
      "different levels of convolution, and introduce the convolutional block\n",
      "attention module and feature fusion to refine and reorganize those\n",
      "high-semantic features. In the exit flow, we employ the self-attention\n",
      "mechanism and depthwise separable convolution to learn the global information\n",
      "and local information of the fused features separately to improve the\n",
      "classification the ability of the proposed model. Experimental results\n",
      "evaluated on three Deepfake datasets demonstrate that the proposed method\n",
      "outperforms Xception as well as other related methods both in effectiveness and\n",
      "generalization ability. \n",
      "\n",
      "\n",
      "Deep generative networks in recent years have reinforced the need for caution\n",
      "while consuming various modalities of digital information. One avenue of\n",
      "deepfake creation is aligned with injection and removal of tumors from medical\n",
      "scans. Failure to detect medical deepfakes can lead to large setbacks on\n",
      "hospital resources or even loss of life. This paper attempts to address the\n",
      "detection of such attacks with a structured case study. Specifically, we\n",
      "evaluate eight different machine learning algorithms, which including three\n",
      "conventional machine learning methods, support vector machine, random forest,\n",
      "decision tree, and five deep learning models, DenseNet121, DenseNet201,\n",
      "ResNet50, ResNet101, VGG19, on distinguishing between tampered and untampered\n",
      "images.For deep learning models, the five models are used for feature\n",
      "extraction, then fine-tune for each pre-trained model is performed. The\n",
      "findings of this work show near perfect accuracy in detecting instances of\n",
      "tumor injections and removals. \n",
      "\n",
      "\n",
      "Manipulated videos, especially those where the identity of an individual has\n",
      "been modified using deep neural networks, are becoming an increasingly relevant\n",
      "threat in the modern day. In this paper, we seek to develop a generalizable,\n",
      "explainable solution to detecting these manipulated videos. To achieve this, we\n",
      "design a series of forgery detection systems that each focus on one individual\n",
      "part of the face. These parts-based detection systems, which can be combined\n",
      "and used together in a single architecture, meet all of our desired criteria -\n",
      "they generalize effectively between datasets and give us valuable insights into\n",
      "what the network is looking at when making its decision. We thus use these\n",
      "detectors to perform detailed empirical analysis on the FaceForensics++,\n",
      "Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not\n",
      "just what the detectors find but also collecting and analyzing useful related\n",
      "statistics on the datasets themselves. \n",
      "\n",
      "\n",
      "The rapid progress in the ease of creating and spreading ultra-realistic\n",
      "media over social platforms calls for an urgent need to develop a generalizable\n",
      "deepfake detection technique. It has been observed that current deepfake\n",
      "generation methods leave discriminative artifacts in the frequency spectrum of\n",
      "fake images and videos. Inspired by this observation, in this paper, we present\n",
      "a novel approach, termed as MD-CSDNetwork, for combining the features in the\n",
      "spatial and frequency domains to mine a shared discriminative representation\n",
      "for classifying \\textit{deepfakes}. MD-CSDNetwork is a novel cross-stitched\n",
      "network with two parallel branches carrying the spatial and frequency\n",
      "information, respectively. We hypothesize that these multi-domain input data\n",
      "streams can be considered as related supervisory signals. The supervision from\n",
      "both branches ensures better performance and generalization. Further, the\n",
      "concept of cross-stitch connections is utilized where they are inserted between\n",
      "the two branches to learn an optimal combination of domain-specific and shared\n",
      "representations from other domains automatically. Extensive experiments are\n",
      "conducted on the popular benchmark dataset namely FaceForeniscs++ for forgery\n",
      "classification. We report improvements over all the manipulation types in\n",
      "FaceForensics++ dataset and comparable results with state-of-the-art methods\n",
      "for cross-database evaluation on the Celeb-DF dataset and the Deepfake\n",
      "Detection Dataset. \n",
      "\n",
      "\n",
      "Existing deepfake-detection methods focus on passive detection, i.e., they\n",
      "detect fake face images via exploiting the artifacts produced during deepfake\n",
      "manipulation. A key limitation of passive detection is that it cannot detect\n",
      "fake faces that are generated by new deepfake generation methods. In this work,\n",
      "we propose FaceGuard, a proactive deepfake-detection framework. FaceGuard\n",
      "embeds a watermark into a real face image before it is published on social\n",
      "media. Given a face image that claims to be an individual (e.g., Nicolas Cage),\n",
      "FaceGuard extracts a watermark from it and predicts the face image to be fake\n",
      "if the extracted watermark does not match well with the individual's ground\n",
      "truth one. A key component of FaceGuard is a new deep-learning-based\n",
      "watermarking method, which is 1) robust to normal image post-processing such as\n",
      "JPEG compression, Gaussian blurring, cropping, and resizing, but 2) fragile to\n",
      "deepfake manipulation. Our evaluation on multiple datasets shows that FaceGuard\n",
      "can detect deepfakes accurately and outperforms existing methods. \n",
      "\n",
      "\n",
      "Deep learning has been successfully appertained to solve various complex\n",
      "problems in the area of big data analytics to computer vision. A deep\n",
      "learning-powered application recently emerged is Deep Fake. It helps to create\n",
      "fake images and videos that human cannot distinguish them from the real ones\n",
      "and are recent off-shelf manipulation technique that allows swapping two\n",
      "identities in a single video. Technology is a controversial technology with\n",
      "many wide-reaching issues impacting society. So, to counter this emerging\n",
      "problem, we introduce a dataset of 140k real and fake faces which contain 70k\n",
      "real faces from the Flickr dataset collected by Nvidia, as well as 70k fake\n",
      "faces sampled from 1 million fake faces generated by style GAN. We will train\n",
      "our model in the dataset so that our model can identify real or fake faces. \n",
      "\n",
      "\n",
      "Research on the detection of AI-generated videos has focused almost\n",
      "exclusively on face videos, usually referred to as deepfakes. Manipulations\n",
      "like face swapping, face reenactment and expression manipulation have been the\n",
      "subject of an intense research with the development of a number of efficient\n",
      "tools to distinguish artificial videos from genuine ones. Much less attention\n",
      "has been paid to the detection of artificial non-facial videos. Yet, new tools\n",
      "for the generation of such kind of videos are being developed at a fast pace\n",
      "and will soon reach the quality level of deepfake videos. The goal of this\n",
      "paper is to investigate the detectability of a new kind of AI-generated videos\n",
      "framing driving street sequences (here referred to as DeepStreets videos),\n",
      "which, by their nature, can not be analysed with the same tools used for facial\n",
      "deepfakes. Specifically, we present a simple frame-based detector, achieving\n",
      "very good performance on state-of-the-art DeepStreets videos generated by the\n",
      "Vid2vid architecture. Noticeably, the detector retains very good performance on\n",
      "compressed videos, even when the compression level used during training does\n",
      "not match that used for the test videos. \n",
      "\n",
      "\n",
      "Significant advancements made in the generation of deepfakes have caused\n",
      "security and privacy issues. Attackers can easily impersonate a person's\n",
      "identity in an image by replacing his face with the target person's face.\n",
      "Moreover, a new domain of cloning human voices using deep-learning technologies\n",
      "is also emerging. Now, an attacker can generate realistic cloned voices of\n",
      "humans using only a few seconds of audio of the target person. With the\n",
      "emerging threat of potential harm deepfakes can cause, researchers have\n",
      "proposed deepfake detection methods. However, they only focus on detecting a\n",
      "single modality, i.e., either video or audio. On the other hand, to develop a\n",
      "good deepfake detector that can cope with the recent advancements in deepfake\n",
      "generation, we need to have a detector that can detect deepfakes of multiple\n",
      "modalities, i.e., videos and audios. To build such a detector, we need a\n",
      "dataset that contains video and respective audio deepfakes. We were able to\n",
      "find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\n",
      "Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\n",
      "fake audios as well. We used this multimodal deepfake dataset and performed\n",
      "detailed baseline experiments using state-of-the-art unimodal, ensemble-based,\n",
      "and multimodal detection methods to evaluate it. We conclude through detailed\n",
      "experimentation that unimodals, addressing only a single modality, video or\n",
      "audio, do not perform well compared to ensemble-based methods. Whereas purely\n",
      "multimodal-based baselines provide the worst performance. \n",
      "\n",
      "\n",
      "The rapid advancement in deep learning makes the differentiation of authentic\n",
      "and manipulated facial images and video clips unprecedentedly harder. The\n",
      "underlying technology of manipulating facial appearances through deep\n",
      "generative approaches, enunciated as DeepFake that have emerged recently by\n",
      "promoting a vast number of malicious face manipulation applications.\n",
      "Subsequently, the need of other sort of techniques that can assess the\n",
      "integrity of digital visual content is indisputable to reduce the impact of the\n",
      "creations of DeepFake. A large body of research that are performed on DeepFake\n",
      "creation and detection create a scope of pushing each other beyond the current\n",
      "status. This study presents challenges, research trends, and directions related\n",
      "to DeepFake creation and detection techniques by reviewing the notable research\n",
      "in the DeepFake domain to facilitate the development of more robust approaches\n",
      "that could deal with the more advance DeepFake in the future. \n",
      "\n",
      "\n",
      "The rapid development of facial manipulation techniques has aroused public\n",
      "concerns in recent years. Following the success of deep learning, existing\n",
      "methods always formulate DeepFake video detection as a binary classification\n",
      "problem and develop frame-based and video-based solutions. However, little\n",
      "attention has been paid to capturing the spatial-temporal inconsistency in\n",
      "forged videos. To address this issue, we term this task as a Spatial-Temporal\n",
      "Inconsistency Learning (STIL) process and instantiate it into a novel STIL\n",
      "block, which consists of a Spatial Inconsistency Module (SIM), a Temporal\n",
      "Inconsistency Module (TIM), and an Information Supplement Module (ISM).\n",
      "Specifically, we present a novel temporal modeling paradigm in TIM by\n",
      "exploiting the temporal difference over adjacent frames along with both\n",
      "horizontal and vertical directions. And the ISM simultaneously utilizes the\n",
      "spatial information from SIM and temporal information from TIM to establish a\n",
      "more comprehensive spatial-temporal representation. Moreover, our STIL block is\n",
      "flexible and could be plugged into existing 2D CNNs. Extensive experiments and\n",
      "visualizations are presented to demonstrate the effectiveness of our method\n",
      "against the state-of-the-art competitors. \n",
      "\n",
      "\n",
      "ASVspoof 2021 is the forth edition in the series of bi-annual challenges\n",
      "which aim to promote the study of spoofing and the design of countermeasures to\n",
      "protect automatic speaker verification systems from manipulation. In addition\n",
      "to a continued focus upon logical and physical access tasks in which there are\n",
      "a number of advances compared to previous editions, ASVspoof 2021 introduces a\n",
      "new task involving deepfake speech detection. This paper describes all three\n",
      "tasks, the new databases for each of them, the evaluation metrics, four\n",
      "challenge baselines, the evaluation platform and a summary of challenge\n",
      "results. Despite the introduction of channel and compression variability which\n",
      "compound the difficulty, results for the logical access and deepfake tasks are\n",
      "close to those from previous ASVspoof editions. Results for the physical access\n",
      "task show the difficulty in detecting attacks in real, variable physical\n",
      "spaces. With ASVspoof 2021 being the first edition for which participants were\n",
      "not provided with any matched training or development data and with this\n",
      "reflecting real conditions in which the nature of spoofed and deepfake speech\n",
      "can never be predicated with confidence, the results are extremely encouraging\n",
      "and demonstrate the substantial progress made in the field in recent years. \n",
      "\n",
      "\n",
      "The automatic speaker verification spoofing and countermeasures (ASVspoof)\n",
      "challenge series is a community-led initiative which aims to promote the\n",
      "consideration of spoofing and the development of countermeasures. ASVspoof 2021\n",
      "is the 4th in a series of bi-annual, competitive challenges where the goal is\n",
      "to develop countermeasures capable of discriminating between bona fide and\n",
      "spoofed or deepfake speech. This document provides a technical description of\n",
      "the ASVspoof 2021 challenge, including details of training, development and\n",
      "evaluation data, metrics, baselines, evaluation rules, submission procedures\n",
      "and the schedule. \n",
      "\n",
      "\n",
      "Applications of deep learning to synthetic media generation allow the\n",
      "creation of convincing forgeries, called DeepFakes, with limited technical\n",
      "expertise. DeepFake detection is an increasingly active research area. In this\n",
      "paper, we analyze an existing DeepFake detection technique based on head pose\n",
      "estimation, which can be applied when fake images are generated with an\n",
      "autoencoder-based face swap. Existing literature suggests that this method is\n",
      "an effective DeepFake detector, and its motivating principles are attractively\n",
      "simple. With an eye towards using these principles to develop new DeepFake\n",
      "detectors, we conduct a reproducibility study of the existing method. We\n",
      "conclude that its merits are dramatically overstated, despite its celebrated\n",
      "status. By investigating this discrepancy we uncover a number of important and\n",
      "generalizable insights related to facial landmark detection, identity-agnostic\n",
      "head pose estimation, and algorithmic bias in DeepFake detectors. Our results\n",
      "correct the current literature's perception of state of the art performance for\n",
      "DeepFake detection. \n",
      "\n",
      "\n",
      "The DeepFakes, which are the facial manipulation techniques, is the emerging\n",
      "threat to digital society. Various DeepFake detection methods and datasets are\n",
      "proposed for detecting such data, especially for face-swapping. However, recent\n",
      "researches less consider facial animation, which is also important in the\n",
      "DeepFake attack side. It tries to animate a face image with actions provided by\n",
      "a driving video, which also leads to a concern about the security of recent\n",
      "payment systems that reply on liveness detection to authenticate real users via\n",
      "recognising a sequence of user facial actions. However, our experiments show\n",
      "that the existed datasets are not sufficient to develop reliable detection\n",
      "methods. While the current liveness detector cannot defend such videos as the\n",
      "attack. As a response, we propose a new human face animation dataset, called\n",
      "DeepFake MNIST+, generated by a SOTA image animation generator. It includes\n",
      "10,000 facial animation videos in ten different actions, which can spoof the\n",
      "recent liveness detectors. A baseline detection method and a comprehensive\n",
      "analysis of the method is also included in this paper. In addition, we analyze\n",
      "the proposed dataset's properties and reveal the difficulty and importance of\n",
      "detecting animation datasets under different types of motion and compression\n",
      "quality. \n",
      "\n",
      "\n",
      "The advancement in numerous generative models has a two-fold effect: a simple\n",
      "and easy generation of realistic synthesized images, but also an increased risk\n",
      "of malicious abuse of those images. Thus, it is important to develop a\n",
      "generalized detector for synthesized images of any GAN model or object\n",
      "category, including those unseen during the training phase. However, the\n",
      "conventional methods heavily depend on the training settings, which cause a\n",
      "dramatic decline in performance when tested with unknown domains. To resolve\n",
      "the issue and obtain a generalized detection ability, we propose Bilateral\n",
      "High-Pass Filters (BiHPF), which amplify the effect of the frequency-level\n",
      "artifacts that are known to be found in the synthesized images of generative\n",
      "models. Numerous experimental results validate that our method outperforms\n",
      "other state-of-the-art methods, even when tested with unseen domains. \n",
      "\n",
      "\n",
      "Generative neural network architectures such as GANs, may be used to generate\n",
      "synthetic instances to compensate for the lack of real data. However, they may\n",
      "be employed to create media that may cause social, political or economical\n",
      "upheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\n",
      "between such media is indispensable. In this paper, we propose a modified\n",
      "multilinear (tensor) method, a combination of linear and multilinear\n",
      "regressions for representing fake and real data. We test our approach by\n",
      "representing Deepfakes with our modified multilinear (tensor) approach and\n",
      "perform SVM classification with encouraging results. \n",
      "\n",
      "\n",
      "Face forgery by deepfake is widely spread over the internet and this raises\n",
      "severe societal concerns. In this paper, we propose a novel video transformer\n",
      "with incremental learning for detecting deepfake videos. To better align the\n",
      "input face images, we use a 3D face reconstruction method to generate UV\n",
      "texture from a single input face image. The aligned face image can also provide\n",
      "pose, eyes blink and mouth movement information that cannot be perceived in the\n",
      "UV texture image, so we use both face images and their UV texture maps to\n",
      "extract the image features. We present an incremental learning strategy to\n",
      "fine-tune the proposed model on a smaller amount of data and achieve better\n",
      "deepfake detection performance. The comprehensive experiments on various public\n",
      "deepfake datasets demonstrate that the proposed video transformer model with\n",
      "incremental learning achieves state-of-the-art performance in the deepfake\n",
      "video detection task with enhanced feature learning from the sequenced data. \n",
      "\n",
      "\n",
      "While the significant advancements have made in the generation of deepfakes\n",
      "using deep learning technologies, its misuse is a well-known issue now.\n",
      "Deepfakes can cause severe security and privacy issues as they can be used to\n",
      "impersonate a person's identity in a video by replacing his/her face with\n",
      "another person's face. Recently, a new problem of generating synthesized human\n",
      "voice of a person is emerging, where AI-based deep learning models can\n",
      "synthesize any person's voice requiring just a few seconds of audio. With the\n",
      "emerging threat of impersonation attacks using deepfake audios and videos, a\n",
      "new generation of deepfake detectors is needed to focus on both video and audio\n",
      "collectively. To develop a competent deepfake detector, a large amount of\n",
      "high-quality data is typically required to capture real-world (or practical)\n",
      "scenarios. Existing deepfake datasets either contain deepfake videos or audios,\n",
      "which are racially biased as well. As a result, it is critical to develop a\n",
      "high-quality video and audio deepfake dataset that can be used to detect both\n",
      "audio and video deepfakes simultaneously. To fill this gap, we propose a novel\n",
      "Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\n",
      "videos but also respective synthesized lip-synced fake audios. We generate this\n",
      "dataset using the most popular deepfake generation methods. We selected real\n",
      "YouTube videos of celebrities with four ethnic backgrounds to develop a more\n",
      "realistic multimodal dataset that addresses racial bias, and further help\n",
      "develop multimodal deepfake detectors. We performed several experiments using\n",
      "state-of-the-art detection methods to evaluate our deepfake dataset and\n",
      "demonstrate the challenges and usefulness of our multimodal Audio-Video\n",
      "deepfake dataset. \n",
      "\n",
      "\n",
      "Synthesizing voice with the help of machine learning techniques has made\n",
      "rapid progress over the last years [1] and first high profile fraud cases have\n",
      "been recently reported [2]. Given the current increase in using conferencing\n",
      "tools for online teaching, we question just how easy (i.e. needed data,\n",
      "hardware, skill set) it would be to create a convincing voice fake. We analyse\n",
      "how much training data a participant (e.g. a student) would actually need to\n",
      "fake another participants voice (e.g. a professor). We provide an analysis of\n",
      "the existing state of the art in creating voice deep fakes, as well as offer\n",
      "detailed technical guidance and evidence of just how much effort is needed to\n",
      "copy a voice. A user study with more than 100 participants shows how difficult\n",
      "it is to identify real and fake voice (on avg. only 37 percent can distinguish\n",
      "between real and fake voice of a professor). With a focus on German language\n",
      "and an online teaching environment we discuss the societal implications as well\n",
      "as demonstrate how to use machine learning techniques to possibly detect such\n",
      "fakes. \n",
      "\n",
      "\n",
      "The proliferation of deepfake media is raising concerns among the public and\n",
      "relevant authorities. It has become essential to develop countermeasures\n",
      "against forged faces in social media. This paper presents a comprehensive study\n",
      "on two new countermeasure tasks: multi-face forgery detection and segmentation\n",
      "in-the-wild. Localizing forged faces among multiple human faces in unrestricted\n",
      "natural scenes is far more challenging than the traditional deepfake\n",
      "recognition task. To promote these new tasks, we have created the first\n",
      "large-scale dataset posing a high level of challenges that is designed with\n",
      "face-wise rich annotations explicitly for face forgery detection and\n",
      "segmentation, namely OpenForensics. With its rich annotations, our\n",
      "OpenForensics dataset has great potentials for research in both deepfake\n",
      "prevention and general human face detection. We have also developed a suite of\n",
      "benchmarks for these tasks by conducting an extensive evaluation of\n",
      "state-of-the-art instance detection and segmentation methods on our newly\n",
      "constructed dataset in various scenarios. The dataset, benchmark results,\n",
      "codes, and supplementary materials will be publicly available on our project\n",
      "page: https://sites.google.com/view/ltnghia/research/openforensics \n",
      "\n",
      "\n",
      "Artefacts that serve to distinguish bona fide speech from spoofed or deepfake\n",
      "speech are known to reside in specific subbands and temporal segments. Various\n",
      "approaches can be used to capture and model such artefacts, however, none works\n",
      "well across a spectrum of diverse spoofing attacks. Reliable detection then\n",
      "often depends upon the fusion of multiple detection systems, each tuned to\n",
      "detect different forms of attack. In this paper we show that better performance\n",
      "can be achieved when the fusion is performed within the model itself and when\n",
      "the representation is learned automatically from raw waveform inputs. The\n",
      "principal contribution is a spectro-temporal graph attention network (GAT)\n",
      "which learns the relationship between cues spanning different sub-bands and\n",
      "temporal intervals. Using a model-level graph fusion of spectral (S) and\n",
      "temporal (T) sub-graphs and a graph pooling strategy to improve discrimination,\n",
      "the proposed RawGAT-ST model achieves an equal error rate of 1.06 % for the\n",
      "ASVspoof 2019 logical access database. This is one of the best results reported\n",
      "to date and is reproducible using an open source implementation. \n",
      "\n",
      "\n",
      "End-to-end approaches to anti-spoofing, especially those which operate\n",
      "directly upon the raw signal, are starting to be competitive with their more\n",
      "traditional counterparts. Until recently, all such approaches consider only the\n",
      "learning of network parameters; the network architecture is still hand crafted.\n",
      "This too, however, can also be learned. Described in this paper is our attempt\n",
      "to learn automatically the network architecture of a speech deepfake and\n",
      "spoofing detection solution, while jointly optimising other network components\n",
      "and parameters, such as the first convolutional layer which operates on raw\n",
      "signal inputs. The resulting raw differentiable architecture search system\n",
      "delivers a tandem detection cost function score of 0.0517 for the ASVspoof 2019\n",
      "logical access database, a result which is among the best single-system results\n",
      "reported to date. \n",
      "\n",
      "\n",
      "In this paper, we present UR-AIR system submission to the logical access (LA)\n",
      "and the speech deepfake (DF) tracks of the ASVspoof 2021 Challenge. The LA and\n",
      "DF tasks focus on synthetic speech detection (SSD), i.e. detecting\n",
      "text-to-speech and voice conversion as spoofing attacks. Different from\n",
      "previous ASVspoof challenges, the LA task this year presents codec and\n",
      "transmission channel variability, while the new task DF presents general audio\n",
      "compression. Built upon our previous research work on improving the robustness\n",
      "of the SSD systems to channel effects, we propose a channel-robust synthetic\n",
      "speech detection system for the challenge. To mitigate the channel variability\n",
      "issue, we use an acoustic simulator to apply transmission codec, compression\n",
      "codec, and convolutional impulse responses to augmenting the original datasets.\n",
      "For the neural network backbone, we propose to use Emphasized Channel\n",
      "Attention, Propagation and Aggregation Time Delay Neural Networks (ECAPA-TDNN)\n",
      "as our primary model. We also incorporate one-class learning with\n",
      "channel-robust training strategies to further learn a channel-invariant speech\n",
      "representation. Our submission achieved EER 20.33% in the DF task; EER 5.46%\n",
      "and min-tDCF 0.3094 in the LA task. \n",
      "\n",
      "\n",
      "Generative models learn the distribution of data from a sample dataset and\n",
      "can then generate new data instances. Recent advances in deep learning has\n",
      "brought forth improvements in generative model architectures, and some\n",
      "state-of-the-art models can (in some cases) produce outputs realistic enough to\n",
      "fool humans.\n",
      "  We survey recent research at the intersection of security and privacy and\n",
      "generative models. In particular, we discuss the use of generative models in\n",
      "adversarial machine learning, in helping automate or enhance existing attacks,\n",
      "and as building blocks for defenses in contexts such as intrusion detection,\n",
      "biometrics spoofing, and malware obfuscation. We also describe the use of\n",
      "generative models in diverse applications such as fairness in machine learning,\n",
      "privacy-preserving data synthesis, and steganography. Finally, we discuss new\n",
      "threats due to generative models: the creation of synthetic media such as\n",
      "deepfakes that can be used for disinformation. \n",
      "\n",
      "\n",
      "The recent emergence of deepfakes has brought manipulated and generated\n",
      "content to the forefront of machine learning research. Automatic detection of\n",
      "deepfakes has seen many new machine learning techniques, however, human\n",
      "detection capabilities are far less explored. In this paper, we present results\n",
      "from comparing the abilities of humans and machines for detecting audio\n",
      "deepfakes used to imitate someone's voice. For this, we use a web-based\n",
      "application framework formulated as a game. Participants were asked to\n",
      "distinguish between real and fake audio samples. In our experiment, 378 unique\n",
      "users competed against a state-of-the-art AI deepfake detection algorithm for\n",
      "12540 total of rounds of the game. We find that humans and deepfake detection\n",
      "algorithms share similar strengths and weaknesses, both struggling to detect\n",
      "certain types of attacks. This is in contrast to the superhuman performance of\n",
      "AI in many application areas such as object detection or face recognition.\n",
      "Concerning human success factors, we find that IT professionals have no\n",
      "advantage over non-professionals but native speakers have an advantage over\n",
      "non-native speakers. Additionally, we find that older participants tend to be\n",
      "more susceptible than younger ones. These insights may be helpful when\n",
      "designing future cybersecurity training for humans as well as developing better\n",
      "detection algorithms. \n",
      "\n",
      "\n",
      "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\n",
      "manipulations of multimedia content. Cheapfakes are known to be more prevalent\n",
      "than deepfakes. Cheapfake media can be created using editing software for\n",
      "image/video manipulations, or even without using any software, by simply\n",
      "altering the context of an image/video by sharing the media alongside\n",
      "misleading claims. This alteration of context is referred to as out-of-context\n",
      "(OOC) misuse} of media. OOC media is much harder to detect than fake media,\n",
      "since the images and videos are not tampered. In this challenge, we focus on\n",
      "detecting OOC images, and more specifically the misuse of real photographs with\n",
      "conflicting image captions in news items. The aim of this challenge is to\n",
      "develop and benchmark models that can be used to detect whether given samples\n",
      "(news image and associated captions) are OOC, based on the recently compiled\n",
      "COSMOS dataset. \n",
      "\n",
      "\n",
      "Deepfakes are the result of digital manipulation to forge realistic yet fake\n",
      "imagery. With the astonishing advances in deep generative models, fake images\n",
      "or videos are nowadays obtained using variational autoencoders (VAEs) or\n",
      "Generative Adversarial Networks (GANs). These technologies are becoming more\n",
      "accessible and accurate, resulting in fake videos that are very difficult to be\n",
      "detected. Traditionally, Convolutional Neural Networks (CNNs) have been used to\n",
      "perform video deepfake detection, with the best results obtained using methods\n",
      "based on EfficientNet B7. In this study, we focus on video deep fake detection\n",
      "on faces, given that most methods are becoming extremely accurate in the\n",
      "generation of realistic human faces. Specifically, we combine various types of\n",
      "Vision Transformers with a convolutional EfficientNet B0 used as a feature\n",
      "extractor, obtaining comparable results with some very recent methods that use\n",
      "Vision Transformers. Differently from the state-of-the-art approaches, we use\n",
      "neither distillation nor ensemble methods. Furthermore, we present a\n",
      "straightforward inference procedure based on a simple voting scheme for\n",
      "handling multiple faces in the same video shot. The best model achieved an AUC\n",
      "of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the\n",
      "DeepFake Detection Challenge (DFDC). \n",
      "\n",
      "\n",
      "Over the last few decades, artificial intelligence research has made\n",
      "tremendous strides, but it still heavily relies on fixed datasets in stationary\n",
      "environments. Continual learning is a growing field of research that examines\n",
      "how AI systems can learn sequentially from a continuous stream of linked data\n",
      "in the same way that biological systems do. Simultaneously, fake media such as\n",
      "deepfakes and synthetic face images have emerged as significant to current\n",
      "multimedia technologies. Recently, numerous method has been proposed which can\n",
      "detect deepfakes with high accuracy. However, they suffer significantly due to\n",
      "their reliance on fixed datasets in limited evaluation settings. Therefore, in\n",
      "this work, we apply continuous learning to neural networks' learning dynamics,\n",
      "emphasizing its potential to increase data efficiency significantly. We propose\n",
      "Continual Representation using Distillation (CoReD) method that employs the\n",
      "concept of Continual Learning (CL), Representation Learning (RL), and Knowledge\n",
      "Distillation (KD). We design CoReD to perform sequential domain adaptation\n",
      "tasks on new deepfake and GAN-generated synthetic face datasets, while\n",
      "effectively minimizing the catastrophic forgetting in a teacher-student model\n",
      "setting. Our extensive experimental results demonstrate that our method is\n",
      "efficient at domain adaptation to detect low-quality deepfakes videos and\n",
      "GAN-generated images from several datasets, outperforming the-state-of-art\n",
      "baseline methods. \n",
      "\n",
      "\n",
      "Deepfakes pose growing challenges to the trust of information on the\n",
      "Internet. Thus, detecting deepfakes has attracted increasing attentions from\n",
      "both academia and industry. State-of-the-art deepfake detection methods consist\n",
      "of two key components, i.e., face extractor and face classifier, which extract\n",
      "the face region in an image and classify it to be real/fake, respectively.\n",
      "Existing studies mainly focused on improving the detection performance in\n",
      "non-adversarial settings, leaving security of deepfake detection in adversarial\n",
      "settings largely unexplored. In this work, we aim to bridge the gap. In\n",
      "particular, we perform a systematic measurement study to understand the\n",
      "security of the state-of-the-art deepfake detection methods in adversarial\n",
      "settings. We use two large-scale public deepfakes data sources including\n",
      "FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\n",
      "are fake face images; and we train state-of-the-art deepfake detection methods.\n",
      "These detection methods can achieve 0.94--0.99 accuracies in non-adversarial\n",
      "settings on these datasets. However, our measurement results uncover multiple\n",
      "security limitations of the deepfake detection methods in adversarial settings.\n",
      "First, we find that an attacker can evade a face extractor, i.e., the face\n",
      "extractor fails to extract the correct face regions, via adding small Gaussian\n",
      "noise to its deepfake images. Second, we find that a face classifier trained\n",
      "using deepfakes generated by one method cannot detect deepfakes generated by\n",
      "another method, i.e., an attacker can evade detection via generating deepfakes\n",
      "using a new method. Third, we find that an attacker can leverage backdoor\n",
      "attacks developed by the adversarial machine learning community to evade a face\n",
      "classifier. Our results highlight that deepfake detection should consider the\n",
      "adversarial nature of the problem. \n",
      "\n",
      "\n",
      "The internet is filled with fake face images and videos synthesized by deep\n",
      "generative models. These realistic DeepFakes pose a challenge to determine the\n",
      "authenticity of multimedia content. As countermeasures, artifact-based\n",
      "detection methods suffer from insufficiently fine-grained features that lead to\n",
      "limited detection performance. DNN-based detection methods are not efficient\n",
      "enough, given that a DeepFake can be created easily by mobile apps and\n",
      "DNN-based models require high computational resources. For the first time, we\n",
      "show that DeepFake faces have fewer feature points than real ones, especially\n",
      "in certain facial regions. Inspired by feature point detector-descriptors to\n",
      "extract discriminative features at the pixel level, we propose the Fused Facial\n",
      "Region_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection.\n",
      "FFR_FD is only a vector extracted from the face, and it can be constructed from\n",
      "any feature point detector-descriptors. We train a random forest classifier\n",
      "with FFR_FD and conduct extensive experiments on six large-scale DeepFake\n",
      "datasets, whose results demonstrate that our method is superior to most state\n",
      "of the art DNN-based models. \n",
      "\n",
      "\n",
      "With the rapid progress of deepfake techniques in recent years, facial video\n",
      "forgery can generate highly deceptive video contents and bring severe security\n",
      "threats. And detection of such forgery videos is much more urgent and\n",
      "challenging. Most existing detection methods treat the problem as a vanilla\n",
      "binary classification problem. In this paper, the problem is treated as a\n",
      "special fine-grained classification problem since the differences between fake\n",
      "and real faces are very subtle. It is observed that most existing face forgery\n",
      "methods left some common artifacts in the spatial domain and time domain,\n",
      "including generative defects in the spatial domain and inter-frame\n",
      "inconsistencies in the time domain. And a spatial-temporal model is proposed\n",
      "which has two components for capturing spatial and temporal forgery traces in\n",
      "global perspective respectively. The two components are designed using a novel\n",
      "long distance attention mechanism. The one component of the spatial domain is\n",
      "used to capture artifacts in a single frame, and the other component of the\n",
      "time domain is used to capture artifacts in consecutive frames. They generate\n",
      "attention maps in the form of patches. The attention method has a broader\n",
      "vision which contributes to better assembling global information and extracting\n",
      "local statistic information. Finally, the attention maps are used to guide the\n",
      "network to focus on pivotal parts of the face, just like other fine-grained\n",
      "classification methods. The experimental results on different public datasets\n",
      "demonstrate that the proposed method achieves the state-of-the-art performance,\n",
      "and the proposed long distance attention method can effectively capture pivotal\n",
      "parts for face forgery. \n",
      "\n",
      "\n",
      "In our research, we focus on the response to the non-consensual distribution\n",
      "of intimate or sexually explicit digital images of adults, also referred as\n",
      "revenge porn, from the point of view of the victims. In this paper, we present\n",
      "a preliminary expert analysis of the process for reporting revenge porn abuses\n",
      "in selected content sharing platforms. Among these, we included social\n",
      "networks, image hosting websites, video hosting platforms, forums, and\n",
      "pornographic sites. We looked at the way to report abuse, concerning both the\n",
      "non-consensual online distribution of private sexual image or video (revenge\n",
      "pornography), as well as the use of deepfake techniques, where the face of a\n",
      "person can be replaced on original visual content with the aim of portraying\n",
      "the victim in the context of sexual behaviours. This preliminary analysis is\n",
      "directed to understand the current practices and potential issues in the\n",
      "procedures designed by the providers for reporting these abuses. \n",
      "\n",
      "\n",
      "In this paper, we propose to utilize Automated Machine Learning to adaptively\n",
      "search a neural architecture for deepfake detection. This is the first time to\n",
      "employ automated machine learning for deepfake detection. Based on our explored\n",
      "search space, our proposed method achieves competitive prediction accuracy\n",
      "compared to previous methods. To improve the generalizability of our method,\n",
      "especially when training data and testing data are manipulated by different\n",
      "methods, we propose a simple yet effective strategy in our network learning\n",
      "process: making it to estimate potential manipulation regions besides\n",
      "predicting the real/fake labels. Unlike previous works manually design neural\n",
      "networks, our method can relieve us from the high labor cost in network\n",
      "construction. More than that, compared to previous works, our method depends\n",
      "much less on prior knowledge, e.g., which manipulation method is utilized or\n",
      "where exactly the fake image is manipulated. Extensive experimental results on\n",
      "two benchmark datasets demonstrate the effectiveness of our proposed method for\n",
      "deepfake detection. \n",
      "\n",
      "\n",
      "As neural networks become able to generate realistic artificial images, they\n",
      "have the potential to improve movies, music, video games and make the internet\n",
      "an even more creative and inspiring place. Yet, the latest technology\n",
      "potentially enables new digital ways to lie. In response, the need for a\n",
      "diverse and reliable method toolbox arises to identify artificial images and\n",
      "other content. Previous work primarily relies on pixel-space CNNs or the\n",
      "Fourier transform. To the best of our knowledge, synthesized fake image\n",
      "analysis and detection methods based on a multi-scale wavelet representation,\n",
      "localized in both space and frequency, have been absent thus far. The wavelet\n",
      "transform conserves spatial information to a degree, which allows us to present\n",
      "a new analysis. Comparing the wavelet coefficients of real and fake images\n",
      "allows interpretation. Significant differences are identified. Additionally,\n",
      "this paper proposes to learn a model for the detection of synthetic images\n",
      "based on the wavelet-packet representation of natural and GAN-generated images.\n",
      "Our lightweight forensic classifiers exhibit competitive or improved\n",
      "performance at comparatively small network sizes, as we demonstrate on the\n",
      "FFHQ, CelebA and LSUN source identification problems. Furthermore, we study the\n",
      "binary FaceForensics++ fake-detection problem. \n",
      "\n",
      "\n",
      "State-of-the-art (SOTA) Generative Models (GMs) can synthesize\n",
      "photo-realistic images that are hard for humans to distinguish from genuine\n",
      "photos. Identifying and understanding manipulated media are crucial to mitigate\n",
      "the social concerns on the potential misuse of GMs. We propose to perform\n",
      "reverse engineering of GMs to infer model hyperparameters from the images\n",
      "generated by these models. We define a novel problem, \"model parsing\", as\n",
      "estimating GM network architectures and training loss functions by examining\n",
      "their generated images - a task seemingly impossible for human beings. To\n",
      "tackle this problem, we propose a framework with two components: a Fingerprint\n",
      "Estimation Network (FEN), which estimates a GM fingerprint from a generated\n",
      "image by training with four constraints to encourage the fingerprint to have\n",
      "desired properties, and a Parsing Network (PN), which predicts network\n",
      "architecture and loss functions from the estimated fingerprints. To evaluate\n",
      "our approach, we collect a fake image dataset with 100K images generated by 116\n",
      "different GMs. Extensive experiments show encouraging results in parsing the\n",
      "hyperparameters of the unseen models. Finally, our fingerprint estimation can\n",
      "be leveraged for deepfake detection and image attribution, as we show by\n",
      "reporting SOTA results on both the deepfake detection (Celeb-DF) and image\n",
      "attribution benchmarks. \n",
      "\n",
      "\n",
      "Fooling people with highly realistic fake images generated with Deepfake or\n",
      "GANs brings a great social disturbance to our society. Many methods have been\n",
      "proposed to detect fake images, but they are vulnerable to adversarial\n",
      "perturbations -- intentionally designed noises that can lead to the wrong\n",
      "prediction. Existing methods of attacking fake image detectors usually generate\n",
      "adversarial perturbations to perturb almost the entire image. This is redundant\n",
      "and increases the perceptibility of perturbations. In this paper, we propose a\n",
      "novel method to disrupt the fake image detection by determining key pixels to a\n",
      "fake image detector and attacking only the key pixels, which results in the\n",
      "$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of\n",
      "existing works. Experiments on two public datasets with three fake image\n",
      "detectors indicate that our proposed method achieves state-of-the-art\n",
      "performance in both white-box and black-box attacks. \n",
      "\n",
      "\n",
      "This paper presents a summary of the DFGC 2021 competition. DeepFake\n",
      "technology is developing fast, and realistic face-swaps are increasingly\n",
      "deceiving and hard to detect. At the same time, DeepFake detection methods are\n",
      "also improving. There is a two-party game between DeepFake creators and\n",
      "detectors. This competition provides a common platform for benchmarking the\n",
      "adversarial game between current state-of-the-art DeepFake creation and\n",
      "detection methods. In this paper, we present the organization, results and top\n",
      "solutions of this competition and also share our insights obtained during this\n",
      "event. We also release the DFGC-21 testing dataset collected from our\n",
      "participants to further benefit the research community. \n",
      "\n",
      "\n",
      "The rapid advances in deep generative models over the past years have led to\n",
      "highly {realistic media, known as deepfakes,} that are commonly\n",
      "indistinguishable from real to human eyes. These advances make assessing the\n",
      "authenticity of visual data increasingly difficult and pose a misinformation\n",
      "threat to the trustworthiness of visual content in general. Although recent\n",
      "work has shown strong detection accuracy of such deepfakes, the success largely\n",
      "relies on identifying frequency artifacts in the generated images, which will\n",
      "not yield a sustainable detection approach as generative models continue\n",
      "evolving and closing the gap to real images. In order to overcome this issue,\n",
      "we propose a novel fake detection that is designed to re-synthesize testing\n",
      "images and extract visual cues for detection. The re-synthesis procedure is\n",
      "flexible, allowing us to incorporate a series of visual tasks - we adopt\n",
      "super-resolution, denoising and colorization as the re-synthesis. We\n",
      "demonstrate the improved effectiveness, cross-GAN generalization, and\n",
      "robustness against perturbations of our approach in a variety of detection\n",
      "scenarios involving multiple generators over CelebA-HQ, FFHQ, and LSUN\n",
      "datasets. Source code is available at\n",
      "https://github.com/SSAW14/BeyondtheSpectrum. \n",
      "\n",
      "\n",
      "As GAN-based video and image manipulation technologies become more\n",
      "sophisticated and easily accessible, there is an urgent need for effective\n",
      "deepfake detection technologies. Moreover, various deepfake generation\n",
      "techniques have emerged over the past few years. While many deepfake detection\n",
      "methods have been proposed, their performance suffers from new types of\n",
      "deepfake methods on which they are not sufficiently trained. To detect new\n",
      "types of deepfakes, the model should learn from additional data without losing\n",
      "its prior knowledge about deepfakes (catastrophic forgetting), especially when\n",
      "new deepfakes are significantly different. In this work, we employ the\n",
      "Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to\n",
      "introduce a transfer learning-based Feature Representation Transfer Adaptation\n",
      "Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on\n",
      "new deepfake datasets while minimizing catastrophic forgetting. Our student\n",
      "model can quickly adapt to new types of deepfake by distilling knowledge from a\n",
      "pre-trained teacher model and applying transfer learning without using source\n",
      "domain data during domain adaptation. Through experiments on FaceForensics++\n",
      "datasets, we demonstrate that FReTAL outperforms all baselines on the domain\n",
      "adaptation task with up to 86.97% accuracy on low-quality deepfakes. \n",
      "\n",
      "\n",
      "As computer-generated content and deepfakes make steady improvements,\n",
      "semantic approaches to multimedia forensics will become more important. In this\n",
      "paper, we introduce a novel classification architecture for identifying\n",
      "semantic inconsistencies between video appearance and text caption in social\n",
      "media news posts. We develop a multi-modal fusion framework to identify\n",
      "mismatches between videos and captions in social media posts by leveraging an\n",
      "ensemble method based on textual analysis of the caption, automatic audio\n",
      "transcription, semantic video analysis, object detection, named entity\n",
      "consistency, and facial verification. To train and test our approach, we curate\n",
      "a new video-based dataset of 4,000 real-world Facebook news posts for analysis.\n",
      "Our multi-modal approach achieves 60.5% classification accuracy on random\n",
      "mismatches between caption and appearance, compared to accuracy below 50% for\n",
      "uni-modal models. Further ablation studies confirm the necessity of fusion\n",
      "across modalities for correctly identifying semantic inconsistencies. \n",
      "\n",
      "\n",
      "Malicious applications of deepfakes (i.e., technologies generating target\n",
      "facial attributes or entire faces from facial images) have posed a huge threat\n",
      "to individuals' reputation and security. To mitigate these threats, recent\n",
      "studies have proposed adversarial watermarks to combat deepfake models, leading\n",
      "them to generate distorted outputs. Despite achieving impressive results, these\n",
      "adversarial watermarks have low image-level and model-level transferability,\n",
      "meaning that they can protect only one facial image from one specific deepfake\n",
      "model. To address these issues, we propose a novel solution that can generate a\n",
      "Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a\n",
      "large number of facial images from multiple deepfake models. Specifically, we\n",
      "begin by proposing a cross-model universal attack pipeline that attacks\n",
      "multiple deepfake models iteratively. Then, we design a two-level perturbation\n",
      "fusion strategy to alleviate the conflict between the adversarial watermarks\n",
      "generated by different facial images and models. Moreover, we address the key\n",
      "problem in cross-model optimization with a heuristic approach to automatically\n",
      "find the suitable attack step sizes for different models, further weakening the\n",
      "model-level conflict. Finally, we introduce a more reasonable and comprehensive\n",
      "evaluation method to fully test the proposed method and compare it with\n",
      "existing ones. Extensive experimental results demonstrate that the proposed\n",
      "CMUA-Watermark can effectively distort the fake facial images generated by\n",
      "multiple deepfake models while achieving a better performance than existing\n",
      "methods. \n",
      "\n",
      "\n",
      "The recent emergence of machine-manipulated media raises an important\n",
      "societal question: how can we know if a video that we watch is real or fake? In\n",
      "two online studies with 15,016 participants, we present authentic videos and\n",
      "deepfakes and ask participants to identify which is which. We compare the\n",
      "performance of ordinary human observers against the leading computer vision\n",
      "deepfake detection model and find them similarly accurate while making\n",
      "different kinds of mistakes. Together, participants with access to the model's\n",
      "prediction are more accurate than either alone, but inaccurate model\n",
      "predictions often decrease participants' accuracy. To probe the relative\n",
      "strengths and weaknesses of humans and machines as detectors of deepfakes, we\n",
      "examine human and machine performance across video-level features, and we\n",
      "evaluate the impact of pre-registered randomized interventions on deepfake\n",
      "detection. We find that manipulations designed to disrupt visual processing of\n",
      "faces hinder human participants' performance while mostly not affecting the\n",
      "model's performance, suggesting a role for specialized cognitive capacities in\n",
      "explaining human deepfake detection performance. \n",
      "\n",
      "\n",
      "Deepfakes have become a critical social problem, and detecting them is of\n",
      "utmost importance. Also, deepfake generation methods are advancing, and it is\n",
      "becoming harder to detect. While many deepfake detection models can detect\n",
      "different types of deepfakes separately, they perform poorly on generalizing\n",
      "the detection performance over multiple types of deepfake. This motivates us to\n",
      "develop a generalized model to detect different types of deepfakes. Therefore,\n",
      "in this work, we introduce a practical digital forensic tool to detect\n",
      "different types of deepfakes simultaneously and propose Transfer learning-based\n",
      "Autoencoder with Residuals (TAR). The ultimate goal of our work is to develop a\n",
      "unified model to detect various types of deepfake videos with high accuracy,\n",
      "with only a small number of training samples that can work well in real-world\n",
      "settings. We develop an autoencoder-based detection model with Residual blocks\n",
      "and sequentially perform transfer learning to detect different types of\n",
      "deepfakes simultaneously. Our approach achieves a much higher generalized\n",
      "detection performance than the state-of-the-art methods on the FaceForensics++\n",
      "dataset. In addition, we evaluate our model on 200 real-world\n",
      "Deepfake-in-the-Wild (DW) videos of 50 celebrities available on the Internet\n",
      "and achieve 89.49% zero-shot accuracy, which is significantly higher than the\n",
      "best baseline model (gaining 10.77%), demonstrating and validating the\n",
      "practicability of our approach. \n",
      "\n",
      "\n",
      "Deepfakes are computer manipulated videos where the face of an individual has\n",
      "been replaced with that of another. Software for creating such forgeries is\n",
      "easy to use and ever more popular, causing serious threats to personal\n",
      "reputation and public security. The quality of classifiers for detecting\n",
      "deepfakes has improved with the releasing of ever larger datasets, but the\n",
      "understanding of why a particular video has been labelled as fake has not kept\n",
      "pace.\n",
      "  In this work we develop, extend and compare white-box, black-box and\n",
      "model-specific techniques for explaining the labelling of real and fake videos.\n",
      "In particular, we adapt SHAP, GradCAM and self-attention models to the task of\n",
      "explaining the predictions of state-of-the-art detectors based on EfficientNet,\n",
      "trained on the Deepfake Detection Challenge (DFDC) dataset. We compare the\n",
      "obtained explanations, proposing metrics to quantify their visual features and\n",
      "desirable characteristics, and also perform a user survey collecting users'\n",
      "opinions regarding the usefulness of the explainers. \n",
      "\n",
      "\n",
      "Face swapping has both positive applications such as entertainment,\n",
      "human-computer interaction, etc., and negative applications such as DeepFake\n",
      "threats to politics, economics, etc. Nevertheless, it is necessary to\n",
      "understand the scheme of advanced methods for high-quality face swapping and\n",
      "generate enough and representative face swapping images to train DeepFake\n",
      "detection algorithms. This paper proposes the first Megapixel level method for\n",
      "one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face\n",
      "representation hierarchically by the proposed Hierarchical Representation Face\n",
      "Encoder (HieRFE) in an extended latent space to maintain more facial details,\n",
      "rather than compressed representation in previous face swapping methods.\n",
      "Secondly, a carefully designed Face Transfer Module (FTM) is proposed to\n",
      "transfer the identity from a source image to the target by a non-linear\n",
      "trajectory without explicit feature disentanglement. Finally, the swapped faces\n",
      "can be synthesized by StyleGAN2 with the benefits of its training stability and\n",
      "powerful generative capability. Each part of MegaFS can be trained separately\n",
      "so the requirement of our model for GPU memory can be satisfied for megapixel\n",
      "face swapping. In summary, complete face representation, stable training, and\n",
      "limited memory usage are the three novel contributions to the success of our\n",
      "method. Extensive experiments demonstrate the superiority of MegaFS and the\n",
      "first megapixel level face swapping database is released for research on\n",
      "DeepFake detection and face image editing in the public domain. The dataset is\n",
      "at this link. \n",
      "\n",
      "\n",
      "Recent studies have demonstrated that deep learning models can discriminate\n",
      "based on protected classes like race and gender. In this work, we evaluate bias\n",
      "present in deepfake datasets and detection models across protected subgroups.\n",
      "Using facial datasets balanced by race and gender, we examine three popular\n",
      "deepfake detectors and find large disparities in predictive performances across\n",
      "races, with up to 10.7% difference in error rate between subgroups. A closer\n",
      "look reveals that the widely used FaceForensics++ dataset is overwhelmingly\n",
      "composed of Caucasian subjects, with the majority being female Caucasians. Our\n",
      "investigation of the racial distribution of deepfakes reveals that the methods\n",
      "used to create deepfakes as positive training signals tend to produce\n",
      "\"irregular\" faces - when a person's face is swapped onto another person of a\n",
      "different race or gender. This causes detectors to learn spurious correlations\n",
      "between the foreground faces and fakeness. Moreover, when detectors are trained\n",
      "with the Blended Image (BI) dataset from Face X-Rays, we find that those\n",
      "detectors develop systematic discrimination towards certain racial subgroups,\n",
      "primarily female Asians. \n",
      "\n",
      "\n",
      "Under the aegis of computer vision and deep learning technology, a new\n",
      "emerging techniques has introduced that anyone can make highly realistic but\n",
      "fake videos, images even can manipulates the voices. This technology is widely\n",
      "known as Deepfake Technology. Although it seems interesting techniques to make\n",
      "fake videos or image of something or some individuals but it could spread as\n",
      "misinformation via internet. Deepfake contents could be dangerous for\n",
      "individuals as well as for our communities, organizations, countries religions\n",
      "etc. As Deepfake content creation involve a high level expertise with\n",
      "combination of several algorithms of deep learning, it seems almost real and\n",
      "genuine and difficult to differentiate. In this paper, a wide range of articles\n",
      "have been examined to understand Deepfake technology more extensively. We have\n",
      "examined several articles to find some insights such as what is Deepfake, who\n",
      "are responsible for this, is there any benefits of Deepfake and what are the\n",
      "challenges of this technology. We have also examined several creation and\n",
      "detection techniques. Our study revealed that although Deepfake is a threat to\n",
      "our societies, proper measures and strict regulations could prevent this. \n",
      "\n",
      "\n",
      "Deep learning-based video manipulation methods have become widely accessible\n",
      "to the masses. With little to no effort, people can quickly learn how to\n",
      "generate deepfake (DF) videos. While deep learning-based detection methods have\n",
      "been proposed to identify specific types of DFs, their performance suffers for\n",
      "other types of deepfake methods, including real-world deepfakes, on which they\n",
      "are not sufficiently trained. In other words, most of the proposed deep\n",
      "learning-based detection methods lack transferability and generalizability.\n",
      "Beyond detecting a single type of DF from benchmark deepfake datasets, we focus\n",
      "on developing a generalized approach to detect multiple types of DFs, including\n",
      "deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW)\n",
      "videos. To better cope with unknown and unseen deepfakes, we introduce a\n",
      "Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model\n",
      "training strategy and explores spatial as well as the temporal information in\n",
      "deepfakes. Through extensive experiments, we show that existing defense methods\n",
      "are not ready for real-world deployment. Whereas our defense method (CLRNet)\n",
      "achieves far better generalization when detecting various benchmark deepfake\n",
      "methods (97.57% on average). Furthermore, we evaluate our approach with a\n",
      "high-quality DeepFake-in-the-Wild dataset, collected from the Internet\n",
      "containing numerous videos and having more than 150,000 frames. Our CLRNet\n",
      "model demonstrated that it generalizes well against high-quality DFW videos by\n",
      "achieving 93.86% detection accuracy, outperforming existing state-of-the-art\n",
      "defense methods by a considerable margin. \n",
      "\n",
      "\n",
      "Face deepfake detection has seen impressive results recently. Nearly all\n",
      "existing deep learning techniques for face deepfake detection are fully\n",
      "supervised and require labels during training. In this paper, we design a novel\n",
      "deepfake detection method via unsupervised contrastive learning. We first\n",
      "generate two different transformed versions of an image and feed them into two\n",
      "sequential sub-networks, i.e., an encoder and a projection head. The\n",
      "unsupervised training is achieved by maximizing the correspondence degree of\n",
      "the outputs of the projection head. To evaluate the detection performance of\n",
      "our unsupervised method, we further use the unsupervised features to train an\n",
      "efficient linear classification network. Extensive experiments show that our\n",
      "unsupervised learning method enables comparable detection performance to\n",
      "state-of-the-art supervised techniques, in both the intra- and inter-dataset\n",
      "settings. We also conduct ablation studies for our method. \n",
      "\n",
      "\n",
      "The widespread dissemination of Deepfakes demands effective approaches that\n",
      "can detect perceptually convincing forged images. In this paper, we aim to\n",
      "capture the subtle manipulation artifacts at different scales using transformer\n",
      "models. In particular, we introduce a Multi-modal Multi-scale TRansformer\n",
      "(M2TR), which operates on patches of different sizes to detect local\n",
      "inconsistencies in images at different spatial levels. M2TR further learns to\n",
      "detect forgery artifacts in the frequency domain to complement RGB information\n",
      "through a carefully designed cross modality fusion block. In addition, to\n",
      "stimulate Deepfake detection research, we introduce a high-quality Deepfake\n",
      "dataset, SR-DF, which consists of 4,000 DeepFake videos generated by\n",
      "state-of-the-art face swapping and facial reenactment methods. We conduct\n",
      "extensive experiments to verify the effectiveness of the proposed method, which\n",
      "outperforms state-of-the-art Deepfake detection methods by clear margins. \n",
      "\n",
      "\n",
      "Deepfakes is a branch of malicious techniques that transplant a target face\n",
      "to the original one in videos, resulting in serious problems such as\n",
      "infringement of copyright, confusion of information, or even public panic.\n",
      "Previous efforts for Deepfakes videos detection mainly focused on appearance\n",
      "features, which have a risk of being bypassed by sophisticated manipulation,\n",
      "also resulting in high model complexity and sensitiveness to noise. Besides,\n",
      "how to mine the temporal features of manipulated videos and exploit them is\n",
      "still an open question. We propose an efficient and robust framework named\n",
      "LRNet for detecting Deepfakes videos through temporal modeling on precise\n",
      "geometric features. A novel calibration module is devised to enhance the\n",
      "precision of geometric features, making it more discriminative, and a\n",
      "two-stream Recurrent Neural Network (RNN) is constructed for sufficient\n",
      "exploitation of temporal features. Compared to previous methods, our proposed\n",
      "method is lighter-weighted and easier to train. Moreover, our method has shown\n",
      "robustness in detecting highly compressed or noise corrupted videos. Our model\n",
      "achieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful\n",
      "decline in performance (-0.042 AUC) when faced with highly compressed videos. \n",
      "\n",
      "\n",
      "State-of-the-art methods for audio generation suffer from fingerprint\n",
      "artifacts and repeated inconsistencies across temporal and spectral domains.\n",
      "Such artifacts could be well captured by the frequency domain analysis over the\n",
      "spectrogram. Thus, we propose a novel use of long-range spectro-temporal\n",
      "modulation feature -- 2D DCT over log-Mel spectrogram for the audio deepfake\n",
      "detection. We show that this feature works better than log-Mel spectrogram,\n",
      "CQCC, MFCC, as a suitable candidate to capture such artifacts. We employ\n",
      "spectrum augmentation and feature normalization to decrease overfitting and\n",
      "bridge the gap between training and test dataset along with this novel feature\n",
      "introduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF\n",
      "and outperformed the previously top single systems reported in the ASVspoof\n",
      "2019 challenge. Finally, by combining our baseline with our proposed 2D DCT\n",
      "spectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737,\n",
      "making it a state-of-the-art system for spoofing detection. Furthermore, we\n",
      "evaluate our model using two external datasets, showing the proposed feature's\n",
      "generalization ability. We also provide analysis and ablation studies for our\n",
      "proposed feature and results. \n",
      "\n",
      "\n",
      "This paper reports the first successful application of a differentiable\n",
      "architecture search (DARTS) approach to the deepfake and spoofing detection\n",
      "problems. An example of neural architecture search, DARTS operates upon a\n",
      "continuous, differentiable search space which enables both the architecture and\n",
      "parameters to be optimised via gradient descent. Solutions based on\n",
      "partially-connected DARTS use random channel masking in the search space to\n",
      "reduce GPU time and automatically learn and optimise complex neural\n",
      "architectures composed of convolutional operations and residual blocks. Despite\n",
      "being learned quickly with little human effort, the resulting networks are\n",
      "competitive with the best performing systems reported in the literature. Some\n",
      "are also far less complex, containing 85% fewer parameters than a Res2Net\n",
      "competitor. \n",
      "\n",
      "\n",
      "Contrastive learning has delivered impressive results for various tasks in\n",
      "the self-supervised regime. However, existing approaches optimize for learning\n",
      "representations specific to downstream scenarios, i.e., \\textit{global}\n",
      "representations suitable for tasks such as classification or \\textit{local}\n",
      "representations for tasks such as detection and localization. While they\n",
      "produce satisfactory results in the intended downstream scenarios, they often\n",
      "fail to generalize to tasks that they were not originally designed for. In this\n",
      "work, we propose to learn video representations that generalize to both the\n",
      "tasks which require global semantic information (e.g., classification) and the\n",
      "tasks that require local fine-grained spatio-temporal information (e.g.,\n",
      "localization). We achieve this by optimizing two contrastive objectives that\n",
      "together encourage our model to learn global-local visual information given\n",
      "audio signals. We show that the two objectives mutually improve the\n",
      "generalizability of the learned global-local representations, significantly\n",
      "outperforming their disjointly learned counterparts. We demonstrate our\n",
      "approach on various tasks including action/sound classification, lip reading,\n",
      "deepfake detection, event and sound localization\n",
      "(https://github.com/yunyikristy/global\\_local). \n",
      "\n",
      "\n",
      "This paper introduces a novel dataset to help researchers evaluate their\n",
      "computer vision and audio models for accuracy across a diverse set of age,\n",
      "genders, apparent skin tones and ambient lighting conditions. Our dataset is\n",
      "composed of 3,011 subjects and contains over 45,000 videos, with an average of\n",
      "15 videos per person. The videos were recorded in multiple U.S. states with a\n",
      "diverse set of adults in various age, gender and apparent skin tone groups. A\n",
      "key feature is that each subject agreed to participate for their likenesses to\n",
      "be used. Additionally, our age and gender annotations are provided by the\n",
      "subjects themselves. A group of trained annotators labeled the subjects'\n",
      "apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\n",
      "for videos recorded in low ambient lighting are also provided. As an\n",
      "application to measure robustness of predictions across certain attributes, we\n",
      "provide a comprehensive study on the top five winners of the DeepFake Detection\n",
      "Challenge (DFDC). Experimental evaluation shows that the winning models are\n",
      "less performant on some specific groups of people, such as subjects with darker\n",
      "skin tones and thus may not generalize to all people. In addition, we also\n",
      "evaluate the state-of-the-art apparent age and gender classification methods.\n",
      "Our experiments provides a thorough analysis on these models in terms of fair\n",
      "treatment of people from various backgrounds. \n",
      "\n",
      "\n",
      "Deepfake is the manipulated video made with a generative deep learning\n",
      "technique such as Generative Adversarial Networks (GANs) or Auto Encoder that\n",
      "anyone can utilize. Recently, with the increase of Deepfake videos, some\n",
      "classifiers consisting of the convolutional neural network that can distinguish\n",
      "fake videos as well as deepfake datasets have been actively created. However,\n",
      "the previous studies based on the CNN structure have the problem of not only\n",
      "overfitting, but also considerable misjudging fake video as real ones. In this\n",
      "paper, we propose a Vision Transformer model with distillation methodology for\n",
      "detecting fake videos. We design that a CNN features and patch-based\n",
      "positioning model learns to interact with all positions to find the artifact\n",
      "region for solving false negative problem. Through comparative analysis on\n",
      "Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with\n",
      "patch embedding as input outperforms the state-of-the-art using the combined\n",
      "CNN features. Without ensemble technique, our model obtains 0.978 of AUC and\n",
      "91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1\n",
      "score on the same condition. \n",
      "\n",
      "\n",
      "Deepfakes raised serious concerns on the authenticity of visual contents.\n",
      "Prior works revealed the possibility to disrupt deepfakes by adding adversarial\n",
      "perturbations to the source data, but we argue that the threat has not been\n",
      "eliminated yet. This paper presents MagDR, a mask-guided detection and\n",
      "reconstruction pipeline for defending deepfakes from adversarial attacks. MagDR\n",
      "starts with a detection module that defines a few criteria to judge the\n",
      "abnormality of the output of deepfakes, and then uses it to guide a learnable\n",
      "reconstruction procedure. Adaptive masks are extracted to capture the change in\n",
      "local facial regions. In experiments, MagDR defends three main tasks of\n",
      "deepfakes, and the learned reconstruction pipeline transfers across input data,\n",
      "showing promising performance in defending both black-box and white-box\n",
      "attacks. \n",
      "\n",
      "\n",
      "With the progress in AI-based facial forgery (i.e., deepfake), people are\n",
      "increasingly concerned about its abuse. Albeit effort has been made for\n",
      "training classification (also known as deepfake detection) models to recognize\n",
      "such forgeries, existing models suffer from poor generalization to unseen\n",
      "forgery technologies and high sensitivity to changes in image/video quality. In\n",
      "this paper, we advocate adversarial training for improving the generalization\n",
      "ability to both unseen facial forgeries and unseen image/video qualities. We\n",
      "believe training with samples that are adversarially crafted to attack the\n",
      "classification models improves the generalization ability considerably.\n",
      "Considering that AI-based face manipulation often leads to high-frequency\n",
      "artifacts that can be easily spotted by models yet difficult to generalize, we\n",
      "further propose a new adversarial training method that attempts to blur out\n",
      "these specific artifacts, by introducing pixel-wise Gaussian blurring models.\n",
      "With adversarial training, the classification models are forced to learn more\n",
      "discriminative and generalizable features, and the effectiveness of our method\n",
      "can be verified by plenty of empirical evidence. Our code will be made publicly\n",
      "available. \n",
      "\n",
      "\n",
      "A variety of effective face-swap and face-reenactment methods have been\n",
      "publicized in recent years, democratizing the face synthesis technology to a\n",
      "great extent. Videos generated as such have come to be called deepfakes with a\n",
      "negative connotation, for various social problems they have caused. Facing the\n",
      "emerging threat of deepfakes, we have built the Korean DeepFake Detection\n",
      "Dataset (KoDF), a large-scale collection of synthesized and real videos focused\n",
      "on Korean subjects. In this paper, we provide a detailed description of methods\n",
      "used to construct the dataset, experimentally show the discrepancy between the\n",
      "distributions of KoDF and existing deepfake detection datasets, and underline\n",
      "the importance of using multiple datasets for real-world generalization. KoDF\n",
      "is publicly available at https://moneybrain-research.github.io/kodf in its\n",
      "entirety (i.e. real clips, synthesized clips, clips with adversarial attack,\n",
      "and metadata). \n",
      "\n",
      "\n",
      "This work is an update of a previous paper on the same topic published a few\n",
      "years ago. With the dramatic progress in generative modeling, a suite of new\n",
      "quantitative and qualitative techniques to evaluate models has emerged.\n",
      "Although some measures such as Inception Score, Frechet Inception Distance,\n",
      "Precision-Recall, and Perceptual Path Length are relatively more popular, GAN\n",
      "evaluation is not a settled issue and there is still room for improvement.\n",
      "Here, I describe new dimensions that are becoming important in assessing models\n",
      "(e.g. bias and fairness) and discuss the connection between GAN evaluation and\n",
      "deepfakes. These are important areas of concern in the machine learning\n",
      "community today and progress in GAN evaluation can help mitigate them. \n",
      "\n",
      "\n",
      "A light-weight high-performance Deepfake detection method, called DefakeHop,\n",
      "is proposed in this work. State-of-the-art Deepfake detection methods are built\n",
      "upon deep neural networks. DefakeHop extracts features automatically using the\n",
      "successive subspace learning (SSL) principle from various parts of face images.\n",
      "The features are extracted by c/w Saab transform and further processed by our\n",
      "feature distillation module using spatial dimension reduction and soft\n",
      "classification for each channel to get a more concise description of the face.\n",
      "Extensive experiments are conducted to demonstrate the effectiveness of the\n",
      "proposed DefakeHop method. With a small model size of 42,845 parameters,\n",
      "DefakeHop achieves state-of-the-art performance with the area under the ROC\n",
      "curve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2\n",
      "datasets, respectively. \n",
      "\n",
      "\n",
      "AI-manipulated videos, commonly known as deepfakes, are an emerging problem.\n",
      "Recently, researchers in academia and industry have contributed several\n",
      "(self-created) benchmark deepfake datasets, and deepfake detection algorithms.\n",
      "However, little effort has gone towards understanding deepfake videos in the\n",
      "wild, leading to a limited understanding of the real-world applicability of\n",
      "research contributions in this space. Even if detection schemes are shown to\n",
      "perform well on existing datasets, it is unclear how well the methods\n",
      "generalize to real-world deepfakes. To bridge this gap in knowledge, we make\n",
      "the following contributions: First, we collect and present the largest dataset\n",
      "of deepfake videos in the wild, containing 1,869 videos from YouTube and\n",
      "Bilibili, and extract over 4.8M frames of content. Second, we present a\n",
      "comprehensive analysis of the growth patterns, popularity, creators,\n",
      "manipulation strategies, and production methods of deepfake content in the\n",
      "real-world. Third, we systematically evaluate existing defenses using our new\n",
      "dataset, and observe that they are not ready for deployment in the real-world.\n",
      "Fourth, we explore the potential for transfer learning schemes and\n",
      "competition-winning techniques to improve defenses. \n",
      "\n",
      "\n",
      "Face forgery by deepfake is widely spread over the internet and has raised\n",
      "severe societal concerns. Recently, how to detect such forgery contents has\n",
      "become a hot research topic and many deepfake detection methods have been\n",
      "proposed. Most of them model deepfake detection as a vanilla binary\n",
      "classification problem, i.e, first use a backbone network to extract a global\n",
      "feature and then feed it into a binary classifier (real/fake). But since the\n",
      "difference between the real and fake images in this task is often subtle and\n",
      "local, we argue this vanilla solution is not optimal. In this paper, we instead\n",
      "formulate deepfake detection as a fine-grained classification problem and\n",
      "propose a new multi-attentional deepfake detection network. Specifically, it\n",
      "consists of three key components: 1) multiple spatial attention heads to make\n",
      "the network attend to different local parts; 2) textural feature enhancement\n",
      "block to zoom in the subtle artifacts in shallow features; 3) aggregate the\n",
      "low-level textural feature and high-level semantic features guided by the\n",
      "attention maps. Moreover, to address the learning difficulty of this network,\n",
      "we further introduce a new regional independence loss and an attention guided\n",
      "data augmentation strategy. Through extensive experiments on different\n",
      "datasets, we demonstrate the superiority of our method over the vanilla binary\n",
      "classifier counterparts, and achieve state-of-the-art performance. \n",
      "\n",
      "\n",
      "In recent years, the advent of deep learning-based techniques and the\n",
      "significant reduction in the cost of computation resulted in the feasibility of\n",
      "creating realistic videos of human faces, commonly known as DeepFakes. The\n",
      "availability of open-source tools to create DeepFakes poses as a threat to the\n",
      "trustworthiness of the online media. In this work, we develop an open-source\n",
      "online platform, known as DeepFake-o-meter, that integrates state-of-the-art\n",
      "DeepFake detection methods and provide a convenient interface for the users. We\n",
      "describe the design and function of DeepFake-o-meter in this work. \n",
      "\n",
      "\n",
      "Recently, significant advancements have been made in face recognition\n",
      "technologies using Deep Neural Networks. As a result, companies such as\n",
      "Microsoft, Amazon, and Naver offer highly accurate commercial face recognition\n",
      "web services for diverse applications to meet the end-user needs. Naturally,\n",
      "however, such technologies are threatened persistently, as virtually any\n",
      "individual can quickly implement impersonation attacks. In particular, these\n",
      "attacks can be a significant threat for authentication and identification\n",
      "services, which heavily rely on their underlying face recognition technologies'\n",
      "accuracy and robustness. Despite its gravity, the issue regarding deepfake\n",
      "abuse using commercial web APIs and their robustness has not yet been\n",
      "thoroughly investigated. This work provides a measurement study on the\n",
      "robustness of black-box commercial face recognition APIs against Deepfake\n",
      "Impersonation (DI) attacks using celebrity recognition APIs as an example case\n",
      "study. We use five deepfake datasets, two of which are created by us and\n",
      "planned to be released. More specifically, we measure attack performance based\n",
      "on two scenarios (targeted and non-targeted) and further analyze the differing\n",
      "system behaviors using fidelity, confidence, and similarity metrics.\n",
      "Accordingly, we demonstrate how vulnerable face recognition technologies from\n",
      "popular companies are to DI attack, achieving maximum success rates of 78.0%\n",
      "and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with\n",
      "any celebrity) attacks, respectively. Moreover, we propose practical defense\n",
      "strategies to mitigate DI attacks, reducing the attack success rates to as low\n",
      "as 0% and 0.02% for targeted and non-targeted attacks, respectively. \n",
      "\n",
      "\n",
      "The creation or manipulation of facial appearance through deep generative\n",
      "approaches, known as DeepFake, have achieved significant progress and promoted\n",
      "a wide range of benign and malicious applications, e.g., visual effect\n",
      "assistance in movie and misinformation generation by faking famous persons. The\n",
      "evil side of this new technique poses another popular study, i.e., DeepFake\n",
      "detection aiming to identify the fake faces from the real ones. With the rapid\n",
      "development of the DeepFake-related studies in the community, both sides have\n",
      "formed the relationship of battleground, pushing the improvements of each other\n",
      "and inspiring new directions, e.g., the evasion of DeepFake detection.\n",
      "Nevertheless, the overview of such battleground and the new direction is\n",
      "unclear and neglected by recent surveys due to the rapid increase of related\n",
      "publications, limiting the in-depth understanding of the tendency and future\n",
      "works. To fill this gap, in this paper, we provide a comprehensive overview and\n",
      "detailed analysis of the research work on the topic of DeepFake generation,\n",
      "DeepFake detection as well as evasion of DeepFake detection, with more than 318\n",
      "research papers carefully surveyed. We present the taxonomy of various DeepFake\n",
      "generation methods and the categorization of various DeepFake detection\n",
      "methods, and more importantly, we showcase the battleground between the two\n",
      "parties with detailed interactions between the adversaries (DeepFake\n",
      "generation) and the defenders (DeepFake detection). The battleground allows\n",
      "fresh perspective into the latest landscape of the DeepFake research and can\n",
      "provide valuable analysis towards the research challenges and opportunities as\n",
      "well as research trends and future directions. We also elaborately design\n",
      "interactive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to\n",
      "explore their own interests on popular DeepFake generators or detectors. \n",
      "\n",
      "\n",
      "Easy access to audio-visual content on social media, combined with the\n",
      "availability of modern tools such as Tensorflow or Keras, open-source trained\n",
      "models, and economical computing infrastructure, and the rapid evolution of\n",
      "deep-learning (DL) methods, especially Generative Adversarial Networks (GAN),\n",
      "have made it possible to generate deepfakes to disseminate disinformation,\n",
      "revenge porn, financial frauds, hoaxes, and to disrupt government functioning.\n",
      "The existing surveys have mainly focused on the detection of deepfake images\n",
      "and videos. This paper provides a comprehensive review and detailed analysis of\n",
      "existing tools and machine learning (ML) based approaches for deepfake\n",
      "generation and the methodologies used to detect such manipulations for both\n",
      "audio and visual deepfakes. For each category of deepfake, we discuss\n",
      "information related to manipulation approaches, current public datasets, and\n",
      "key standards for the performance evaluation of deepfake detection techniques\n",
      "along with their results. Additionally, we also discuss open challenges and\n",
      "enumerate future directions to guide future researchers on issues that need to\n",
      "be considered to improve the domains of both deepfake generation and detection.\n",
      "This work is expected to assist the readers in understanding the creation and\n",
      "detection mechanisms of deepfakes, along with their current limitations and\n",
      "future direction. \n",
      "\n",
      "\n",
      "The rapid advancement of deep learning models that can generate and synthesis\n",
      "hyper-realistic videos known as Deepfakes and their ease of access to the\n",
      "general public have raised concern from all concerned bodies to their possible\n",
      "malicious intent use. Deep learning techniques can now generate faces, swap\n",
      "faces between two subjects in a video, alter facial expressions, change gender,\n",
      "and alter facial features, to list a few. These powerful video manipulation\n",
      "methods have potential use in many fields. However, they also pose a looming\n",
      "threat to everyone if used for harmful purposes such as identity theft,\n",
      "phishing, and scam. In this work, we propose a Convolutional Vision Transformer\n",
      "for the detection of Deepfakes. The Convolutional Vision Transformer has two\n",
      "components: Convolutional Neural Network (CNN) and Vision Transformer (ViT).\n",
      "The CNN extracts learnable features while the ViT takes in the learned features\n",
      "as input and categorizes them using an attention mechanism. We trained our\n",
      "model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5\n",
      "percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our\n",
      "contribution is that we have added a CNN module to the ViT architecture and\n",
      "have achieved a competitive result on the DFDC dataset. \n",
      "\n",
      "\n",
      "The creation of altered and manipulated faces has become more common due to\n",
      "the improvement of DeepFake generation methods. Simultaneously, we have seen\n",
      "detection models' development for differentiating between a manipulated and\n",
      "original face from image or video content. In this paper, we focus on\n",
      "identifying the limitations and shortcomings of existing deepfake detection\n",
      "frameworks. We identified some key problems surrounding deepfake detection\n",
      "through quantitative and qualitative analysis of existing methods and datasets.\n",
      "We found that deepfake datasets are highly oversampled, causing models to\n",
      "become easily overfitted. The datasets are created using a small set of real\n",
      "faces to generate multiple fake samples. When trained on these datasets, models\n",
      "tend to memorize the actors' faces and labels instead of learning fake\n",
      "features. To mitigate this problem, we propose a simple data augmentation\n",
      "method termed Face-Cutout. Our method dynamically cuts out regions of an image\n",
      "using the face landmark information. It helps the model selectively attend to\n",
      "only the relevant regions of the input. Our evaluation experiments show that\n",
      "Face-Cutout can successfully improve the data variation and alleviate the\n",
      "problem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to\n",
      "35.3% on different datasets, compared to other occlusion-based techniques.\n",
      "Moreover, we also propose a general-purpose data pre-processing guideline to\n",
      "train and evaluate existing architectures allowing us to improve the\n",
      "generalizability of these models for deepfake detection. \n",
      "\n",
      "\n",
      "Computer mediated conversations (e.g., videoconferencing) is now the new\n",
      "mainstream media. How would credibility be impacted if one could change their\n",
      "race on the fly in these environments? We propose an approach using Deepfakes\n",
      "and a supporting GAN architecture to isolate visual features and alter racial\n",
      "perception. We then crowd-sourced over 800 survey responses to measure how\n",
      "credibility was influenced by changing the perceived race. We evaluate the\n",
      "effect of showing a still image of a Black person versus a still image of a\n",
      "White person using the same audio clip for each survey. We also test the effect\n",
      "of showing either an original video or an altered video where the appearance of\n",
      "the person in the original video is modified to appear more White. We measure\n",
      "credibility as the percent of participant responses who believed the speaker\n",
      "was telling the truth. We found that changing the race of a person in a static\n",
      "image has negligible impact on credibility. However, the same manipulation of\n",
      "race on a video increases credibility significantly (61\\% to 73\\% with p $<$\n",
      "0.05). Furthermore, a VADER sentiment analysis over the free response survey\n",
      "questions reveals that more positive sentiment is used to justify the\n",
      "credibility of a White individual in a video. \n",
      "\n",
      "\n",
      "Synthetic media detection technologies label media as either synthetic or\n",
      "non-synthetic and are increasingly used by journalists, web platforms, and the\n",
      "general public to identify misinformation and other forms of problematic\n",
      "content. As both well-resourced organizations and the non-technical general\n",
      "public generate more sophisticated synthetic media, the capacity for purveyors\n",
      "of problematic content to adapt induces a \\newterm{detection dilemma}: as\n",
      "detection practices become more accessible, they become more easily\n",
      "circumvented. This paper describes how a multistakeholder cohort from academia,\n",
      "technology platforms, media entities, and civil society organizations active in\n",
      "synthetic media detection and its socio-technical implications evaluates the\n",
      "detection dilemma. Specifically, we offer an assessment of detection contexts\n",
      "and adversary capacities sourced from the broader, global AI and media\n",
      "integrity community concerned with mitigating the spread of harmful synthetic\n",
      "media. A collection of personas illustrates the intersection between\n",
      "unsophisticated and highly-resourced sponsors of misinformation in the context\n",
      "of their technical capacities. This work concludes that there is no \"best\"\n",
      "approach to navigating the detector dilemma, but derives a set of implications\n",
      "from multistakeholder input to better inform detection process decisions and\n",
      "policies, in practice. \n",
      "\n",
      "\n",
      "Deepfakes are synthetically generated images, videos or audios, which\n",
      "fraudsters use to manipulate legitimate information. Current deepfake detection\n",
      "systems struggle against unseen data. To address this, we employ three\n",
      "different deep Convolutional Neural Network (CNN) models, (1) VGG16, (2)\n",
      "InceptionV3, and (3) XceptionNet to classify fake and real images extracted\n",
      "from videos. We also constructed a fusion of the deep CNN models to improve the\n",
      "robustness and generalisation capability. The proposed technique outperforms\n",
      "state-of-the-art models with 96.5% accuracy, when tested on publicly available\n",
      "DeepFake Detection Challenge (DFDC) test data, comprising of 400 videos. The\n",
      "fusion model achieves 99% accuracy on lower quality DeepFake-TIMIT dataset\n",
      "videos and 91.88% on higher quality DeepFake-TIMIT videos. In addition to this,\n",
      "we prove that prediction fusion is more robust against adversarial attacks. If\n",
      "one model is compromised by an adversarial attack, the prediction fusion does\n",
      "not let it affect the overall classification. \n",
      "\n",
      "\n",
      "The recent development of Deep Neural Networks (DNN) has significantly\n",
      "increased the realism of AI-synthesized faces, with the most notable examples\n",
      "being the DeepFakes. The DeepFake technology can synthesize a face of target\n",
      "subject from a face of another subject, while retains the same face attributes.\n",
      "With the rapidly increased social media portals (Facebook, Instagram, etc),\n",
      "these realistic fake faces rapidly spread though the Internet, causing a broad\n",
      "negative impact to the society. In this paper, we describe Landmark Breaker,\n",
      "the first dedicated method to disrupt facial landmark extraction, and apply it\n",
      "to the obstruction of the generation of DeepFake videos.Our motivation is that\n",
      "disrupting the facial landmark extraction can affect the alignment of input\n",
      "face so as to degrade the DeepFake quality. Our method is achieved using\n",
      "adversarial perturbations. Compared to the detection methods that only work\n",
      "after DeepFake generation, Landmark Breaker goes one step ahead to prevent\n",
      "DeepFake generation. The experiments are conducted on three state-of-the-art\n",
      "facial landmark extractors using the recent Celeb-DF dataset. \n",
      "\n",
      "\n",
      "Recent advances in artificial intelligence make it progressively hard to\n",
      "distinguish between genuine and counterfeit media, especially images and\n",
      "videos. One recent development is the rise of deepfake videos, based on\n",
      "manipulating videos using advanced machine learning techniques. This involves\n",
      "replacing the face of an individual from a source video with the face of a\n",
      "second person, in the destination video. This idea is becoming progressively\n",
      "refined as deepfakes are getting progressively seamless and simpler to compute.\n",
      "Combined with the outreach and speed of social media, deepfakes could easily\n",
      "fool individuals when depicting someone saying things that never happened and\n",
      "thus could persuade people in believing fictional scenarios, creating distress,\n",
      "and spreading fake news. In this paper, we examine a technique for possible\n",
      "identification of deepfake videos. We use Euler video magnification which\n",
      "applies spatial decomposition and temporal filtering on video data to highlight\n",
      "and magnify hidden features like skin pulsation and subtle motions. Our\n",
      "approach uses features extracted from the Euler technique to train three models\n",
      "to classify counterfeit and unaltered videos and compare the results with\n",
      "existing techniques. \n",
      "\n",
      "\n",
      "To properly contrast the Deepfake phenomenon the need to design new Deepfake\n",
      "detection algorithms arises; the misuse of this formidable A.I. technology\n",
      "brings serious consequences in the private life of every involved person.\n",
      "State-of-the-art proliferates with solutions using deep neural networks to\n",
      "detect a fake multimedia content but unfortunately these algorithms appear to\n",
      "be neither generalizable nor explainable. However, traces left by Generative\n",
      "Adversarial Network (GAN) engines during the creation of the Deepfakes can be\n",
      "detected by analyzing ad-hoc frequencies. For this reason, in this paper we\n",
      "propose a new pipeline able to detect the so-called GAN Specific Frequencies\n",
      "(GSF) representing a unique fingerprint of the different generative\n",
      "architectures. By employing Discrete Cosine Transform (DCT), anomalous\n",
      "frequencies were detected. The \\BETA statistics inferred by the AC coefficients\n",
      "distribution have been the key to recognize GAN-engine generated data.\n",
      "Robustness tests were also carried out in order to demonstrate the\n",
      "effectiveness of the technique using different attacks on images such as JPEG\n",
      "Compression, mirroring, rotation, scaling, addition of random sized rectangles.\n",
      "Experiments demonstrated that the method is innovative, exceeds the state of\n",
      "the art and also give many insights in terms of explainability. \n",
      "\n",
      "\n",
      "During the last two decades, we have progressively turned to the Internet and\n",
      "social media to find news, entertain conversations and share opinion. Recently,\n",
      "OpenAI has developed a ma-chine learning system called GPT-2 for Generative\n",
      "Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate\n",
      "blocks of text based on brief writing prompts that look like they were written\n",
      "by humans, facilitating the spread false or auto-generated text. In line with\n",
      "this progress, and in order to counteract potential dangers, several methods\n",
      "have been pro-posed for detecting text written by these language models. In\n",
      "this paper, we propose a transfer learning based model that will be able to\n",
      "detect if an Arabic sentence is written by humans or automatically generated by\n",
      "bots. Our dataset is based on tweets from a previous work, which we have\n",
      "crawled and extended using the Twitter API. We used GPT2-Small-Arabic to\n",
      "generate fake Arabic Sentences. For evaluation, we compared different recurrent\n",
      "neural network (RNN) word embeddings based baseline models, namely: LSTM,\n",
      "BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new\n",
      "transfer-learning model has obtained an accuracy up to 98%. To the best of our\n",
      "knowledge, this work is the first study where ARABERT and GPT2 were combined to\n",
      "detect and classify the Arabic auto-generated texts. \n",
      "\n",
      "\n",
      "Alarmed by the volume of disinformation that was assumed to have taken place\n",
      "during the 2016 US elections, scholars, politics and journalists predicted the\n",
      "worst when the first deepfakes began to emerge in 2018. After all, US Elections\n",
      "2020 were believed to be the most secure in American history. This paper seeks\n",
      "explanations for an apparent contradiction: we believe that it was precisely\n",
      "the multiplication and conjugation of different types of warnings and fears\n",
      "that created the conditions that prevented malicious political deepfakes from\n",
      "affecting the 2020 US elections. From these warnings, we identified four\n",
      "factors (more active role of social networks, new laws, difficulties in\n",
      "accessing Artificial Intelligence and better awareness of society). But while\n",
      "this formula has proven to be effective in the case of the United States, 2020,\n",
      "it is not correct to assume that it can be repeated in other political\n",
      "contexts. \n",
      "\n",
      "\n",
      "Despite the recent attention to DeepFakes, one of the most prevalent ways to\n",
      "mislead audiences on social media is the use of unaltered images in a new but\n",
      "false context. To address these challenges and support fact-checkers, we\n",
      "propose a new method that automatically detects out-of-context image and text\n",
      "pairs. Our key insight is to leverage the grounding of image with text to\n",
      "distinguish out-of-context scenarios that cannot be disambiguated with language\n",
      "alone. We propose a self-supervised training strategy where we only need a set\n",
      "of captioned images. At train time, our method learns to selectively align\n",
      "individual objects in an image with textual claims, without explicit\n",
      "supervision. At test time, we check if both captions correspond to the same\n",
      "object(s) in the image but are semantically different, which allows us to make\n",
      "fairly accurate out-of-context predictions. Our method achieves 85%\n",
      "out-of-context detection accuracy. To facilitate benchmarking of this task, we\n",
      "create a large-scale dataset of 200K images with 450K textual captions from a\n",
      "variety of news websites, blogs, and social media posts. The dataset and source\n",
      "code is publicly available at\n",
      "https://shivangi-aneja.github.io/projects/cosmos/. \n",
      "\n",
      "\n",
      "This paper proposes a new DeepFake detector FakeBuster for detecting\n",
      "impostors during video conferencing and manipulated faces on social media.\n",
      "FakeBuster is a standalone deep learning based solution, which enables a user\n",
      "to detect if another person's video is manipulated or spoofed during a video\n",
      "conferencing based meeting. This tool is independent of video conferencing\n",
      "solutions and has been tested with Zoom and Skype applications. It uses a 3D\n",
      "convolutional neural network for predicting video segment-wise fakeness scores.\n",
      "The network is trained on a combination of datasets such as Deeperforensics,\n",
      "DFDC, VoxCeleb, and deepfake videos created using locally captured (for video\n",
      "conferencing scenarios) images. This leads to different environments and\n",
      "perturbations in the dataset, which improves the generalization of the deepfake\n",
      "network. \n",
      "\n",
      "\n",
      "Images synthesized by powerful generative adversarial network (GAN) based\n",
      "methods have drawn moral and privacy concerns. Although image forensic models\n",
      "have reached great performance in detecting fake images from real ones, these\n",
      "models can be easily fooled with a simple adversarial attack. But, the noise\n",
      "adding adversarial samples are also arousing suspicion. In this paper, instead\n",
      "of adding adversarial noise, we optimally search adversarial points on face\n",
      "manifold to generate anti-forensic fake face images. We iteratively do a\n",
      "gradient-descent with each small step in the latent space of a generative\n",
      "model, e.g. Style-GAN, to find an adversarial latent vector, which is similar\n",
      "to norm-based adversarial attack but in latent space. Then, the generated fake\n",
      "images driven by the adversarial latent vectors with the help of GANs can\n",
      "defeat main-stream forensic models. For examples, they make the accuracy of\n",
      "deepfake detection models based on Xception or EfficientNet drop from over 90%\n",
      "to nearly 0%, meanwhile maintaining high visual quality. In addition, we find\n",
      "manipulating style vector $z$ or noise vectors $n$ at different levels have\n",
      "impacts on attack success rate. The generated adversarial images mainly have\n",
      "facial texture or face attributes changing. \n",
      "\n",
      "\n",
      "In recent years, the abuse of a face swap technique called deepfake Deepfake\n",
      "has raised enormous public concerns. So far, a large number of deepfake videos\n",
      "(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\n",
      "for effective countermeasures. One promising countermeasure against deepfakes\n",
      "is deepfake detection. Several deepfake datasets have been released to support\n",
      "the training and testing of deepfake detectors, such as DeepfakeDetection and\n",
      "FaceForensics++. While this has greatly advanced deepfake detection, most of\n",
      "the real videos in these datasets are filmed with a few volunteer actors in\n",
      "limited scenes, and the fake videos are crafted by researchers using a few\n",
      "popular deepfake softwares. Detectors developed on these datasets may become\n",
      "less effective against real-world deepfakes on the internet. To better support\n",
      "detection against real-world deepfakes, in this paper, we introduce a new\n",
      "dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707\n",
      "deepfake videos collected completely from the internet. WildDeepfake is a small\n",
      "dataset that can be used, in addition to existing datasets, to develop and test\n",
      "the effectiveness of deepfake detectors against real-world deepfakes. We\n",
      "conduct a systematic evaluation of a set of baseline detection networks on both\n",
      "existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a\n",
      "more challenging dataset, where the detection performance can decrease\n",
      "drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\n",
      "Detection Networks (ADDNets) to leverage the attention masks on real/fake faces\n",
      "for improved detection. We empirically verify the effectiveness of ADDNets on\n",
      "both existing datasets and WildDeepfake. The dataset is available\n",
      "at:https://github.com/deepfakeinthewild/deepfake-in-the-wild. \n",
      "\n",
      "\n",
      "Existing deepfake detection methods have reported promising in-distribution\n",
      "results, by accessing published large-scale dataset. However, due to the\n",
      "non-smooth synthesis method, the fake samples in this dataset may expose\n",
      "obvious artifacts (e.g., stark visual contrast, non-smooth boundary), which\n",
      "were heavily relied on by most of the frame-level detection methods above. As\n",
      "these artifacts do not come up in real media forgeries, the above methods can\n",
      "suffer from a large degradation when applied to fake images that close to\n",
      "reality. To improve the robustness for high-realism fake data, we propose the\n",
      "Invariant Texture Learning (InTeLe) framework, which only accesses the\n",
      "published dataset with low visual quality. Our method is based on the prior\n",
      "that the microscopic facial texture of the source face is inevitably violated\n",
      "by the texture transferred from the target person, which can hence be regarded\n",
      "as the invariant characterization shared among all fake images. To learn such\n",
      "an invariance for deepfake detection, our InTeLe introduces an auto-encoder\n",
      "framework with different decoders for pristine and fake images, which are\n",
      "further appended with a shallow classifier in order to separate out the obvious\n",
      "artifact-effect. Equipped with such a separation, the extracted embedding by\n",
      "encoder can capture the texture violation in fake images, followed by the\n",
      "classifier for the final pristine/fake prediction. As a theoretical guarantee,\n",
      "we prove the identifiability of such an invariance texture violation, i.e., to\n",
      "be precisely inferred from observational data. The effectiveness and utility of\n",
      "our method are demonstrated by promising generalization ability from\n",
      "low-quality images with obvious artifacts to fake images with high realism. \n",
      "\n",
      "\n",
      "We propose a new method to detect deepfake images using the cue of the source\n",
      "feature inconsistency within the forged images. It is based on the hypothesis\n",
      "that images' distinct source features can be preserved and extracted after\n",
      "going through state-of-the-art deepfake generation processes. We introduce a\n",
      "novel representation learning approach, called pair-wise self-consistency\n",
      "learning (PCL), for training ConvNets to extract these source features and\n",
      "detect deepfake images. It is accompanied by a new image synthesis approach,\n",
      "called inconsistency image generator (I2G), to provide richly annotated\n",
      "training data for PCL. Experimental results on seven popular datasets show that\n",
      "our models improve averaged AUC over the state of the art from 96.45% to 98.05%\n",
      "in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\n",
      "evaluation. \n",
      "\n",
      "\n",
      "Deepfake technology (DT) has taken a new level of sophistication.\n",
      "Cybercriminals now can manipulate sounds, images, and videos to defraud and\n",
      "misinform individuals and businesses. This represents a growing threat to\n",
      "international institutions and individuals which needs to be addressed. This\n",
      "paper provides an overview of deepfakes, their benefits to society, and how DT\n",
      "works. Highlights the threats that are presented by deepfakes to businesses,\n",
      "politics, and judicial systems worldwide. Additionally, the paper will explore\n",
      "potential solutions to deepfakes and conclude with future research direction. \n",
      "\n",
      "\n",
      "Multimodal disinformation, from `deepfakes' to simple edits that deceive, is\n",
      "an important societal problem. Yet at the same time, the vast majority of media\n",
      "edits are harmless -- such as a filtered vacation photo. The difference between\n",
      "this example, and harmful edits that spread disinformation, is one of intent.\n",
      "Recognizing and describing this intent is a major challenge for today's AI\n",
      "systems.\n",
      "  We present the task of Edited Media Understanding, requiring models to answer\n",
      "open-ended questions that capture the intent and implications of an image edit.\n",
      "We introduce a dataset for our task, EMU, with 48k question-answer pairs\n",
      "written in rich natural language. We evaluate a wide variety of\n",
      "vision-and-language models for our task, and introduce a new model PELICAN,\n",
      "which builds upon recent progress in pretrained multimodal representations. Our\n",
      "model obtains promising results on our dataset, with humans rating its answers\n",
      "as accurate 40.35% of the time. At the same time, there is still much work to\n",
      "be done -- humans prefer human-annotated captions 93.56% of the time -- and we\n",
      "provide analysis that highlights areas for further progress. \n",
      "\n",
      "\n",
      "Since the invention of cinema, the manipulated videos have existed. But\n",
      "generating manipulated videos that can fool the viewer has been a\n",
      "time-consuming endeavor. With the dramatic improvements in the deep generative\n",
      "modeling, generating believable looking fake videos has become a reality. In\n",
      "the present work, we concentrate on the so-called deepfake videos, where the\n",
      "source face is swapped with the targets. We argue that deepfake detection task\n",
      "should be viewed as a screening task, where the user, such as the video\n",
      "streaming platform, will screen a large number of videos daily. It is clear\n",
      "then that only a small fraction of the uploaded videos are deepfakes, so the\n",
      "detection performance needs to be measured in a cost-sensitive way. Preferably,\n",
      "the model parameters also need to be estimated in the same way. This is\n",
      "precisely what we propose here. \n",
      "\n",
      "\n",
      "A person's appearance, identity, and other nonverbal cues can substantially\n",
      "influence how one is perceived by a negotiation counterpart, potentially\n",
      "impacting the outcome of the negotiation. With recent advances in technology,\n",
      "it is now possible to alter such cues through real-time video communication. In\n",
      "many cases, a person's physical presence can explicitly be replaced by 2D/3D\n",
      "representations in live interactive media. In other cases, technologies such as\n",
      "deepfake can subtly and implicitly alter many nonverbal cues -- including a\n",
      "person's appearance and identity -- in real-time. In this article, we look at\n",
      "some state-of-the-art technological advances that can enable such explicit and\n",
      "implicit alteration of nonverbal cues. We also discuss the implications of such\n",
      "technology for the negotiation landscape and highlight ethical considerations\n",
      "that warrant deep, ongoing attention from stakeholders. \n",
      "\n",
      "\n",
      "DeepFake detection has so far been dominated by ``artifact-driven'' methods\n",
      "and the detection performance significantly degrades when either the type of\n",
      "image artifacts is unknown or the artifacts are simply too hard to find. In\n",
      "this work, we present an alternative approach: Identity-Driven DeepFake\n",
      "Detection. Our approach takes as input the suspect image/video as well as the\n",
      "target identity information (a reference image or video). We output a decision\n",
      "on whether the identity in the suspect image/video is the same as the target\n",
      "identity. Our motivation is to prevent the most common and harmful DeepFakes\n",
      "that spread false information of a targeted person. The identity-based approach\n",
      "is fundamentally different in that it does not attempt to detect image\n",
      "artifacts. Instead, it focuses on whether the identity in the suspect\n",
      "image/video is true. To facilitate research on identity-based detection, we\n",
      "present a new large scale dataset ``Vox-DeepFake\", in which each suspect\n",
      "content is associated with multiple reference images collected from videos of a\n",
      "target identity. We also present a simple identity-based detection algorithm\n",
      "called the OuterFace, which may serve as a baseline for further research. Even\n",
      "trained without fake videos, the OuterFace algorithm achieves superior\n",
      "detection accuracy and generalizes well to different DeepFake methods, and is\n",
      "robust with respect to video degradation techniques -- a performance not\n",
      "achievable with existing detection algorithms. \n",
      "\n",
      "\n",
      "A major challenge in DeepFake forgery detection is that state-of-the-art\n",
      "algorithms are mostly trained to detect a specific fake method. As a result,\n",
      "these approaches show poor generalization across different types of facial\n",
      "manipulations, e.g., from face swapping to facial reenactment. To this end, we\n",
      "introduce ID-Reveal, a new approach that learns temporal facial features,\n",
      "specific of how a person moves while talking, by means of metric learning\n",
      "coupled with an adversarial training strategy. The advantage is that we do not\n",
      "need any training data of fakes, but only train on real videos. Moreover, we\n",
      "utilize high-level semantic features, which enables robustness to widespread\n",
      "and disruptive forms of post-processing. We perform a thorough experimental\n",
      "analysis on several publicly available benchmarks. Compared to state of the\n",
      "art, our method improves generalization and is more robust to low-quality\n",
      "videos, that are usually spread over social networks. In particular, we obtain\n",
      "an average improvement of more than 15% in terms of accuracy for facial\n",
      "reenactment on high compressed videos. \n",
      "\n",
      "\n",
      "Facially manipulated images and videos or DeepFakes can be used maliciously\n",
      "to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is\n",
      "crucial to increase the credibility of social media platforms and other media\n",
      "sharing web sites. State-of-the art DeepFake detection techniques rely on\n",
      "neural network based classification models which are known to be vulnerable to\n",
      "adversarial examples. In this work, we study the vulnerabilities of\n",
      "state-of-the-art DeepFake detection methods from a practical stand point. We\n",
      "perform adversarial attacks on DeepFake detectors in a black box setting where\n",
      "the adversary does not have complete knowledge of the classification models. We\n",
      "study the extent to which adversarial perturbations transfer across different\n",
      "models and propose techniques to improve the transferability of adversarial\n",
      "examples. We also create more accessible attacks using Universal Adversarial\n",
      "Perturbations which pose a very feasible attack scenario since they can be\n",
      "easily shared amongst attackers. We perform our evaluations on the winning\n",
      "entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they\n",
      "can be easily bypassed in a practical attack scenario by designing transferable\n",
      "and accessible adversarial attacks. \n",
      "\n",
      "\n",
      "The fast and continuous growth in number and quality of deepfake videos calls\n",
      "for the development of reliable detection systems capable of automatically\n",
      "warning users on social media and on the Internet about the potential\n",
      "untruthfulness of such contents. While algorithms, software, and smartphone\n",
      "apps are getting better every day in generating manipulated videos and swapping\n",
      "faces, the accuracy of automated systems for face forgery detection in videos\n",
      "is still quite limited and generally biased toward the dataset used to design\n",
      "and train a specific detection system. In this paper we analyze how different\n",
      "training strategies and data augmentation techniques affect CNN-based deepfake\n",
      "detectors when training and testing on the same dataset or across different\n",
      "datasets. \n",
      "\n",
      "\n",
      "There are many applications of Generative Adversarial Networks (GANs) in\n",
      "fields like computer vision, natural language processing, speech synthesis, and\n",
      "more. Undoubtedly the most notable results have been in the area of image\n",
      "synthesis and in particular in the generation of deepfake videos. While\n",
      "deepfakes have received much negative media coverage, they can be a useful\n",
      "technology in applications like entertainment, customer relations, or even\n",
      "assistive care. One problem with generating deepfakes is the requirement for a\n",
      "lot of image training data of the subject which is not an issue if the subject\n",
      "is a celebrity for whom many images already exist. If there are only a small\n",
      "number of training images then the quality of the deepfake will be poor. Some\n",
      "media reports have indicated that a good deepfake can be produced with as few\n",
      "as 500 images but in practice, quality deepfakes require many thousands of\n",
      "images, one of the reasons why deepfakes of celebrities and politicians have\n",
      "become so popular. In this study, we exploit the property of a GAN to produce\n",
      "images of an individual with variable facial expressions which we then use to\n",
      "generate a deepfake. We observe that with such variability in facial\n",
      "expressions of synthetic GAN-generated training images and a reduced quantity\n",
      "of them, we can produce a near-realistic deepfake videos. \n",
      "\n",
      "\n",
      "Automatic speaker verification (ASV) systems utilize the biometric\n",
      "information in human speech to verify the speaker's identity. The techniques\n",
      "used for performing speaker verification are often vulnerable to malicious\n",
      "attacks that attempt to induce the ASV system to return wrong results, allowing\n",
      "an impostor to bypass the system and gain access. Attackers use a multitude of\n",
      "spoofing techniques for this, such as voice conversion, audio replay, speech\n",
      "synthesis, etc. In recent years, easily available tools to generate deepfaked\n",
      "audio have increased the potential threat to ASV systems. In this paper, we\n",
      "compare the potential of human impersonation (voice disguise) based attacks\n",
      "with attacks based on machine-generated speech, on black-box and white-box ASV\n",
      "systems. We also study countermeasures by using features that capture the\n",
      "unique aspects of human speech production, under the hypothesis that machines\n",
      "cannot emulate many of the fine-level intricacies of the human speech\n",
      "production mechanism. We show that fundamental frequency sequence-related\n",
      "entropy, spectral envelope, and aperiodic parameters are promising candidates\n",
      "for robust detection of deepfaked speech generated by unknown methods. \n",
      "\n",
      "\n",
      "Recent studies have shown that the performance of forgery detection can be\n",
      "improved with diverse and challenging Deepfakes datasets. However, due to the\n",
      "lack of Deepfakes datasets with large variance in appearance, which can be\n",
      "hardly produced by recent identity swapping methods, the detection algorithm\n",
      "may fail in this situation. In this work, we provide a new identity swapping\n",
      "algorithm with large differences in appearance for face forgery detection. The\n",
      "appearance gaps mainly arise from the large discrepancies in illuminations and\n",
      "skin colors that widely exist in real-world scenarios. However, due to the\n",
      "difficulties of modeling the complex appearance mapping, it is challenging to\n",
      "transfer fine-grained appearances adaptively while preserving identity traits.\n",
      "This paper formulates appearance mapping as an optimal transport problem and\n",
      "proposes an Appearance Optimal Transport model (AOT) to formulate it in both\n",
      "latent and pixel space. Specifically, a relighting generator is designed to\n",
      "simulate the optimal transport plan. It is solved via minimizing the\n",
      "Wasserstein distance of the learned features in the latent space, enabling\n",
      "better performance and less computation than conventional optimization. To\n",
      "further refine the solution of the optimal transport plan, we develop a\n",
      "segmentation game to minimize the Wasserstein distance in the pixel space. A\n",
      "discriminator is introduced to distinguish the fake parts from a mix of real\n",
      "and fake image patches. Extensive experiments reveal that the superiority of\n",
      "our method when compared with state-of-the-art methods and the ability of our\n",
      "generated data to improve the performance of face forgery detection. \n",
      "\n",
      "\n",
      "For deepfake detection, video-level detectors have not been explored as\n",
      "extensively as image-level detectors, which do not exploit temporal data. In\n",
      "this paper, we empirically show that existing approaches on image and sequence\n",
      "classifiers generalize poorly to new manipulation techniques. To this end, we\n",
      "propose spatio-temporal features, modeled by 3D CNNs, to extend the\n",
      "generalization capabilities to detect new sorts of deepfake videos. We show\n",
      "that spatial features learn distinct deepfake-method-specific attributes, while\n",
      "spatio-temporal features capture shared attributes between deepfake methods. We\n",
      "provide an in-depth analysis of how the sequential and spatio-temporal video\n",
      "encoders are utilizing temporal information using DFDC dataset\n",
      "arXiv:2006.07397. Thus, we unravel that our approach captures local\n",
      "spatio-temporal relations and inconsistencies in the deepfake videos while\n",
      "existing sequence encoders are indifferent to it. Through large scale\n",
      "experiments conducted on the FaceForensics++ arXiv:1901.08971 and Deeper\n",
      "Forensics arXiv:2001.03024 datasets, we show that our approach outperforms\n",
      "existing methods in terms of generalization capabilities. \n",
      "\n",
      "\n",
      "Deepfake detection, the task of automatically discriminating\n",
      "machine-generated text, is increasingly critical with recent advances in\n",
      "natural language generative models. Existing approaches to deepfake detection\n",
      "typically represent documents with coarse-grained representations. However,\n",
      "they struggle to capture factual structures of documents, which is a\n",
      "discriminative factor between machine-generated and human-written text\n",
      "according to our statistical analysis. To address this, we propose a\n",
      "graph-based model that utilizes the factual structure of a document for\n",
      "deepfake detection of text. Our approach represents the factual structure of a\n",
      "given document as an entity graph, which is further utilized to learn sentence\n",
      "representations with a graph neural network. Sentence representations are then\n",
      "composed to a document representation for making predictions, where consistent\n",
      "relations between neighboring sentences are sequentially modeled. Results of\n",
      "experiments on two public deepfake datasets show that our approach\n",
      "significantly improves strong base models built with RoBERTa. Model analysis\n",
      "further indicates that our model can distinguish the difference in the factual\n",
      "structure between machine-generated text and human-written text. \n",
      "\n",
      "\n",
      "This work introduces a novel DeepFake detection framework based on\n",
      "physiological measurement. In particular, we consider information related to\n",
      "the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze\n",
      "video sequences looking for subtle color changes in the human skin, revealing\n",
      "the presence of human blood under the tissues. In this work we investigate to\n",
      "what extent rPPG is useful for the detection of DeepFake videos.\n",
      "  The proposed fake detector named DeepFakesON-Phys uses a Convolutional\n",
      "Attention Network (CAN), which extracts spatial and temporal information from\n",
      "video frames, analyzing and combining both sources to better detect fake\n",
      "videos. This detection approach has been experimentally evaluated using the\n",
      "latest public databases in the field: Celeb-DF and DFDC. The results achieved,\n",
      "above 98% AUC (Area Under the Curve) on both databases, outperform the state of\n",
      "the art and prove the success of fake detectors based on physiological\n",
      "measurement to detect the latest DeepFake videos. \n",
      "\n",
      "\n",
      "In recent years, DeepFake is becoming a common threat to our society, due to\n",
      "the remarkable progress of generative adversarial networks (GAN) in image\n",
      "synthesis. Unfortunately, existing studies that propose various approaches, in\n",
      "fighting against DeepFake and determining if the facial image is real or fake,\n",
      "is still at an early stage. Obviously, the current DeepFake detection method\n",
      "struggles to catch the rapid progress of GANs, especially in the adversarial\n",
      "scenarios where attackers can evade the detection intentionally, such as adding\n",
      "perturbations to fool the DNN-based detectors. While passive detection simply\n",
      "tells whether the image is fake or real, DeepFake provenance, on the other\n",
      "hand, provides clues for tracking the sources in DeepFake forensics. Thus, the\n",
      "tracked fake images could be blocked immediately by administrators and avoid\n",
      "further spread in social networks.\n",
      "  In this paper, we investigate the potentials of image tagging in serving the\n",
      "DeepFake provenance tracking. Specifically, we devise a deep learning-based\n",
      "approach, named FakeTagger, with a simple yet effective encoder and decoder\n",
      "design along with channel coding to embed message to the facial image, which is\n",
      "to recover the embedded message after various drastic GAN-based DeepFake\n",
      "transformation with high confidence. The embedded message could be employed to\n",
      "represent the identity of facial images, which further contributed to DeepFake\n",
      "detection and provenance. Experimental results demonstrate that our proposed\n",
      "approach could recover the embedded message with an average accuracy of more\n",
      "than 95% over the four common types of DeepFakes. Our research finding confirms\n",
      "effective privacy-preserving techniques for protecting personal photos from\n",
      "being DeepFaked. \n",
      "\n",
      "\n",
      "The current high-fidelity generation and high-precision detection of DeepFake\n",
      "images are at an arms race. We believe that producing DeepFakes that are highly\n",
      "realistic and ``detection evasive'' can serve the ultimate goal of improving\n",
      "future generation DeepFake detection capabilities. In this paper, we propose a\n",
      "simple yet powerful pipeline to reduce the artifact patterns of fake images\n",
      "without hurting image quality by performing implicit spatial-domain notch\n",
      "filtering. We first demonstrate that frequency-domain notch filtering, although\n",
      "famously shown to be effective in removing periodic noise in the spatial\n",
      "domain, is infeasible for our task at hand due to manual designs required for\n",
      "the notch filters. We, therefore, resort to a learning-based approach to\n",
      "reproduce the notch filtering effects, but solely in the spatial domain. We\n",
      "adopt a combination of adding overwhelming spatial noise for breaking the\n",
      "periodic noise pattern and deep image filtering to reconstruct the noise-free\n",
      "fake images, and we name our method DeepNotch. Deep image filtering provides a\n",
      "specialized filter for each pixel in the noisy image, producing filtered images\n",
      "with high fidelity compared to their DeepFake counterparts. Moreover, we also\n",
      "use the semantic information of the image to generate an adversarial guidance\n",
      "map to add noise intelligently. Our large-scale evaluation on 3 representative\n",
      "state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\n",
      "has demonstrated that our technique significantly reduces the accuracy of these\n",
      "3 fake image detection methods, 36.79% on average and up to 97.02% in the best\n",
      "case. \n",
      "\n",
      "\n",
      "In recent years, deep learning-based video manipulation methods have become\n",
      "widely accessible to masses. With little to no effort, people can easily learn\n",
      "how to generate deepfake videos with only a few victims or target images. This\n",
      "creates a significant social problem for everyone whose photos are publicly\n",
      "available on the Internet, especially on social media websites. Several deep\n",
      "learning-based detection methods have been developed to identify these\n",
      "deepfakes. However, these methods lack generalizability, because they perform\n",
      "well only for a specific type of deepfake method. Therefore, those methods are\n",
      "not transferable to detect other deepfake methods. Also, they do not take\n",
      "advantage of the temporal information of the video. In this paper, we addressed\n",
      "these limitations. We developed a Convolutional LSTM based Residual Network\n",
      "(CLRNet), which takes a sequence of consecutive images as an input from a video\n",
      "to learn the temporal information that helps in detecting unnatural looking\n",
      "artifacts that are present between frames of deepfake videos. We also propose a\n",
      "transfer learning-based approach to generalize different deepfake methods.\n",
      "Through rigorous experimentations using the FaceForensics++ dataset, we showed\n",
      "that our method outperforms five of the previously proposed state-of-the-art\n",
      "deepfake detection methods by better generalizing at detecting different\n",
      "deepfake methods using the same model. \n",
      "\n",
      "\n",
      "Deepfake videos, where a person's face is automatically swapped with a face\n",
      "of someone else, are becoming easier to generate with more realistic results.\n",
      "In response to the threat such manipulations can pose to our trust in video\n",
      "evidence, several large datasets of deepfake videos and many methods to detect\n",
      "them were proposed recently. However, it is still unclear how realistic\n",
      "deepfake videos are for an average person and whether the algorithms are\n",
      "significantly better than humans at detecting them. In this paper, we present a\n",
      "subjective study conducted in a crowdsourcing-like scenario, which\n",
      "systematically evaluates how hard it is for humans to see if the video is\n",
      "deepfake or not. For the evaluation, we used 120 different videos (60 deepfakes\n",
      "and 60 originals) manually pre-selected from the Facebook deepfake database,\n",
      "which was provided in the Kaggle's Deepfake Detection Challenge 2020. For each\n",
      "video, a simple question: \"Is face of the person in the video real of fake?\"\n",
      "was answered on average by 19 na\\\"ive subjects. The results of the subjective\n",
      "evaluation were compared with the performance of two different state of the art\n",
      "deepfake detection methods, based on Xception and EfficientNets (B4 variant)\n",
      "neural networks, which were pre-trained on two other large public databases:\n",
      "the Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The\n",
      "evaluation demonstrates that while the human perception is very different from\n",
      "the perception of a machine, both successfully but in different ways are fooled\n",
      "by deepfakes. Specifically, algorithms struggle to detect those deepfake\n",
      "videos, which human subjects found to be very easy to spot. \n",
      "\n",
      "\n",
      "We propose a method for detecting face swapping and other identity\n",
      "manipulations in single images. Face swapping methods, such as DeepFake,\n",
      "manipulate the face region, aiming to adjust the face to the appearance of its\n",
      "context, while leaving the context unchanged. We show that this modus operandi\n",
      "produces discrepancies between the two regions. These discrepancies offer\n",
      "exploitable telltale signs of manipulation. Our approach involves two networks:\n",
      "(i) a face identification network that considers the face region bounded by a\n",
      "tight semantic segmentation, and (ii) a context recognition network that\n",
      "considers the face context (e.g., hair, ears, neck). We describe a method which\n",
      "uses the recognition signals from our two networks to detect such\n",
      "discrepancies, providing a complementary detection signal that improves\n",
      "conventional real vs. fake classifiers commonly used for detecting fake images.\n",
      "Our method achieves state of the art results on the FaceForensics++,\n",
      "Celeb-DF-v2, and DFDC benchmarks for face manipulation detection, and even\n",
      "generalizes to detect fakes produced by unseen methods. \n",
      "\n",
      "\n",
      "Image sharing on online social networks (OSNs) has become an indispensable\n",
      "part of daily social activities, but it has also led to an increased risk of\n",
      "privacy invasion. The recent image leaks from popular OSN services and the\n",
      "abuse of personal photos using advanced algorithms (e.g. DeepFake) have\n",
      "prompted the public to rethink individual privacy needs in OSN image sharing.\n",
      "However, OSN image privacy itself is quite complicated, and solutions currently\n",
      "in place for privacy management in reality are insufficient to provide\n",
      "personalized, accurate and flexible privacy protection. A more intelligent\n",
      "environment for privacy-friendly OSN image sharing is in demand. To fill the\n",
      "gap, we contribute a survey of \"privacy intelligence\" that targets modern\n",
      "privacy issues in dynamic OSN image sharing from a user-centric perspective.\n",
      "Specifically, we present a definition and a taxonomy of OSN image privacy, and\n",
      "a high-level privacy analysis framework based on the lifecycle of OSN image\n",
      "sharing. The framework consists of three stages with different principles of\n",
      "privacy by design. At each stage, we identify typical user behaviors in OSN\n",
      "image sharing and the privacy issues associated with these behaviors. Then a\n",
      "systematic review on the representative intelligent solutions targeting those\n",
      "privacy issues is conducted, also in a stage-based manner. The resulting\n",
      "analysis describes an intelligent privacy firewall for closed-loop privacy\n",
      "management. We also discuss the challenges and future directions in this area. \n",
      "\n",
      "\n",
      "Progress in generative modelling, especially generative adversarial networks,\n",
      "have made it possible to efficiently synthesize and alter media at scale.\n",
      "Malicious individuals now rely on these machine-generated media, or deepfakes,\n",
      "to manipulate social discourse. In order to ensure media authenticity, existing\n",
      "research is focused on deepfake detection. Yet, the adversarial nature of\n",
      "frameworks used for generative modeling suggests that progress towards\n",
      "detecting deepfakes will enable more realistic deepfake generation. Therefore,\n",
      "it comes at no surprise that developers of generative models are under the\n",
      "scrutiny of stakeholders dealing with misinformation campaigns. At the same\n",
      "time, generative models have a lot of positive applications. As such, there is\n",
      "a clear need to develop tools that ensure the transparent use of generative\n",
      "modeling, while minimizing the harm caused by malicious applications.\n",
      "  Our technique optimizes over the source of entropy of each generative model\n",
      "to probabilistically attribute a deepfake to one of the models. We evaluate our\n",
      "method on the seminal example of face synthesis, demonstrating that our\n",
      "approach achieves 97.62% attribution accuracy, and is less sensitive to\n",
      "perturbations and adversarial examples. We discuss the ethical implications of\n",
      "our work, identify where our technique can be used, and highlight that a more\n",
      "meaningful legislative framework is required for a more transparent and ethical\n",
      "use of generative modeling. Finally, we argue that model developers should be\n",
      "capable of claiming plausible deniability and propose a second framework to do\n",
      "so -- this allows a model developer to produce evidence that they did not\n",
      "produce media that they are being accused of having produced. \n",
      "\n",
      "\n",
      "Recent deep learning based video synthesis approaches, in particular with\n",
      "applications that can forge identities such as \"DeepFake\", have raised great\n",
      "security concerns. Therefore, corresponding deep forensic methods are proposed\n",
      "to tackle this problem. However, existing methods are either based on\n",
      "unexplainable deep networks which greatly degrades the principal\n",
      "interpretability factor to media forensic, or rely on fragile image statistics\n",
      "such as noise pattern, which in real-world scenarios can be easily deteriorated\n",
      "by data compression. In this paper, we propose an fully-interpretable video\n",
      "forensic method that is designed specifically to expose deep-faked videos. To\n",
      "enhance generalizability on videos with various content, we model the temporal\n",
      "motion of multiple specific spatial locations in the videos to extract a robust\n",
      "and reliable representation, called Co-Motion Pattern. Such kind of conjoint\n",
      "pattern is mined across local motion features which is independent of the video\n",
      "contents so that the instance-wise variation can also be largely alleviated.\n",
      "More importantly, our proposed co-motion pattern possesses both superior\n",
      "interpretability and sufficient robustness against data compression for\n",
      "deep-faked videos. We conduct extensive experiments to empirically demonstrate\n",
      "the superiority and effectiveness of our approach under both classification and\n",
      "anomaly detection evaluation settings against the state-of-the-art deep\n",
      "forensic methods. \n",
      "\n",
      "\n",
      "With the rapid development of facial manipulation techniques, face forgery\n",
      "has received considerable attention in multimedia and computer vision community\n",
      "due to security concerns. Existing methods are mostly designed for single-frame\n",
      "detection trained with precise image-level labels or for video-level prediction\n",
      "by only modeling the inter-frame inconsistency, leaving potential high risks\n",
      "for DeepFake attackers. In this paper, we introduce a new problem of partial\n",
      "face attack in DeepFake video, where only video-level labels are provided but\n",
      "not all the faces in the fake videos are manipulated. We address this problem\n",
      "by multiple instance learning framework, treating faces and input video as\n",
      "instances and bag respectively. A sharp MIL (S-MIL) is proposed which builds\n",
      "direct mapping from instance embeddings to bag prediction, rather than from\n",
      "instance embeddings to instance prediction and then to bag prediction in\n",
      "traditional MIL. Theoretical analysis proves that the gradient vanishing in\n",
      "traditional MIL is relieved in S-MIL. To generate instances that can accurately\n",
      "incorporate the partially manipulated faces, spatial-temporal encoded instance\n",
      "is designed to fully model the intra-frame and inter-frame inconsistency, which\n",
      "further helps to promote the detection performance. We also construct a new\n",
      "dataset FFPMS for partially attacked DeepFake video detection, which can\n",
      "benefit the evaluation of different methods at both frame and video levels.\n",
      "Experiments on FFPMS and the widely used DFDC dataset verify that S-MIL is\n",
      "superior to other counterparts for partially attacked DeepFake video detection.\n",
      "In addition, S-MIL can also be adapted to traditional DeepFake image detection\n",
      "tasks and achieve state-of-the-art performance on single-frame datasets. \n",
      "\n",
      "\n",
      "The current spike of hyper-realistic faces artificially generated using\n",
      "deepfakes calls for media forensics solutions that are tailored to video\n",
      "streams and work reliably with a low false alarm rate at the video level. We\n",
      "present a method for deepfake detection based on a two-branch network structure\n",
      "that isolates digitally manipulated faces by learning to amplify artifacts\n",
      "while suppressing the high-level face content. Unlike current methods that\n",
      "extract spatial frequencies as a preprocessing step, we propose a two-branch\n",
      "structure: one branch propagates the original information, while the other\n",
      "branch suppresses the face content yet amplifies multi-band frequencies using a\n",
      "Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate\n",
      "manipulated faces, we derive a novel cost function that, unlike regular\n",
      "classification, compresses the variability of natural faces and pushes away the\n",
      "unrealistic facial samples in the feature space. Our two novel components show\n",
      "promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview\n",
      "benchmarks, when compared to prior work. We then offer a full, detailed\n",
      "ablation study of our network architecture and cost function. Finally, although\n",
      "the bar is still high to get very remarkable figures at a very low false alarm\n",
      "rate, our study shows that we can achieve good video-level performance when\n",
      "cross-testing in terms of video-level AUC. \n",
      "\n",
      "\n",
      "Advances in Artificial Intelligence and Image Processing are changing the way\n",
      "people interacts with digital images and video. Widespread mobile apps like\n",
      "FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\n",
      "produce extreme transformations on human face photos such gender swap, aging,\n",
      "etc. The results are utterly realistic and extremely easy to be exploited even\n",
      "for non-experienced users. This kind of media object took the name of Deepfake\n",
      "and raised a new challenge in the multimedia forensics field: the Deepfake\n",
      "detection challenge. Indeed, discriminating a Deepfake from a real image could\n",
      "be a difficult task even for human eyes but recent works are trying to apply\n",
      "the same technology used for generating images for discriminating them with\n",
      "preliminary good results but with many limitations: employed Convolutional\n",
      "Neural Networks are not so robust, demonstrate to be specific to the context\n",
      "and tend to extract semantics from images. In this paper, a new approach aimed\n",
      "to extract a Deepfake fingerprint from images is proposed. The method is based\n",
      "on the Expectation-Maximization algorithm trained to detect and extract a\n",
      "fingerprint that represents the Convolutional Traces (CT) left by GANs during\n",
      "image generation. The CT demonstrates to have high discriminative power\n",
      "achieving better results than state-of-the-art in the Deepfake detection task\n",
      "also proving to be robust to different attacks. Achieving an overall\n",
      "classification accuracy of over 98%, considering Deepfakes from 10 different\n",
      "GAN architectures not only involved in images of faces, the CT demonstrates to\n",
      "be reliable and without any dependence on image semantic. Finally, tests\n",
      "carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\n",
      "fake detection task, demonstrated the effectiveness of the proposed technique\n",
      "on a real-case scenario. \n",
      "\n",
      "\n",
      "The recent advances in language modeling significantly improved the\n",
      "generative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a\n",
      "pre-trained language model that can autonomously generate coherent, non-trivial\n",
      "and human-like text samples. Since then, ever more powerful text generative\n",
      "models have been developed. Adversaries can exploit these tremendous generative\n",
      "capabilities to enhance social bots that will have the ability to write\n",
      "plausible deepfake messages, hoping to contaminate public debate. To prevent\n",
      "this, it is crucial to develop deepfake social media messages detection\n",
      "systems. However, to the best of our knowledge no one has ever addressed the\n",
      "detection of machine-generated texts on social networks like Twitter or\n",
      "Facebook. With the aim of helping the research in this detection field, we\n",
      "collected the first dataset of \\real deepfake tweets, TweepFake. It is real in\n",
      "the sense that each deepfake tweet was actually posted on Twitter. We collected\n",
      "tweets from a total of 23 bots, imitating 17 human accounts. The bots are based\n",
      "on various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM,\n",
      "GPT-2. We also randomly selected tweets from the humans imitated by the bots to\n",
      "have an overall balanced dataset of 25,572 tweets (half human and half bots\n",
      "generated). The dataset is publicly available on Kaggle. Lastly, we evaluated\n",
      "13 deepfake text detection methods (based on various state-of-the-art\n",
      "approaches) to both demonstrate the challenges that Tweepfake poses and create\n",
      "a solid baseline of detection techniques. We hope that TweepFake can offer the\n",
      "opportunity to tackle the deepfake detection on social media messages as well. \n",
      "\n",
      "\n",
      "Photorealistic image generation has reached a new level of quality due to the\n",
      "breakthroughs of generative adversarial networks (GANs). Yet, the dark side of\n",
      "such deepfakes, the malicious use of generated media, raises concerns about\n",
      "visual misinformation. While existing research work on deepfake detection\n",
      "demonstrates high accuracy, it is subject to advances in generation techniques\n",
      "and adversarial iterations on detection countermeasure techniques. Thus, we\n",
      "seek a proactive and sustainable solution on deepfake detection, that is\n",
      "agnostic to the evolution of generative models, by introducing artificial\n",
      "fingerprints into the models.\n",
      "  Our approach is simple and effective. We first embed artificial fingerprints\n",
      "into training data, then validate a surprising discovery on the transferability\n",
      "of such fingerprints from training data to generative models, which in turn\n",
      "appears in the generated deepfakes. Experiments show that our fingerprinting\n",
      "solution (1) holds for a variety of cutting-edge generative models, (2) leads\n",
      "to a negligible side effect on generation quality, (3) stays robust against\n",
      "image-level and model-level perturbations, (4) stays hard to be detected by\n",
      "adversaries, and (5) converts deepfake detection and attribution into trivial\n",
      "tasks and outperforms the recent state-of-the-art baselines. Our solution\n",
      "closes the responsibility loop between publishing pre-trained generative model\n",
      "inventions and their possible misuses, which makes it independent of the\n",
      "current arms race. Code and models are available at\n",
      "https://github.com/ningyu1991/ArtificialGANFingerprints . \n",
      "\n",
      "\n",
      "Recent advances in deepfake generating algorithms that produce manipulated\n",
      "media have had dangerous implications in privacy, security and mass\n",
      "communication. Efforts to combat this issue have risen in the form of\n",
      "competitions and funding for research to detect deepfakes. This paper presents\n",
      "three techniques and algorithms: convolutional LSTM, eye blink detection and\n",
      "grayscale histograms-pursued while participating in the Deepfake Detection\n",
      "Challenge. We assessed the current knowledge about deepfake videos, a more\n",
      "severe version of manipulated media, and previous methods used, and found\n",
      "relevance in the grayscale histogram technique over others. We discussed the\n",
      "implications of each method developed and provided further steps to improve the\n",
      "given findings. \n",
      "\n",
      "\n",
      "In this paper we propose a novel human-centered approach for detecting\n",
      "forgery in face images, using dynamic prototypes as a form of visual\n",
      "explanations. Currently, most state-of-the-art deepfake detections are based on\n",
      "black-box models that process videos frame-by-frame for inference, and few\n",
      "closely examine their temporal inconsistencies. However, the existence of such\n",
      "temporal artifacts within deepfake videos is key in detecting and explaining\n",
      "deepfakes to a supervising human. To this end, we propose Dynamic Prototype\n",
      "Network (DPNet) -- an interpretable and effective solution that utilizes\n",
      "dynamic representations (i.e., prototypes) to explain deepfake temporal\n",
      "artifacts. Extensive experimental results show that DPNet achieves competitive\n",
      "predictive performance, even on unseen testing datasets such as Google's\n",
      "DeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy\n",
      "referential explanations of deepfake dynamics. On top of DPNet's prototypical\n",
      "framework, we further formulate temporal logic specifications based on these\n",
      "dynamics to check our model's compliance to desired temporal behaviors, hence\n",
      "providing trustworthiness for such critical detection systems. \n",
      "\n",
      "\n",
      "Better generative models and larger datasets have led to more realistic fake\n",
      "videos that can fool the human eye but produce temporal and spatial artifacts\n",
      "that deep learning approaches can detect. Most current Deepfake detection\n",
      "methods only use individual video frames and therefore fail to learn from\n",
      "temporal information. We created a benchmark of the performance of\n",
      "spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods\n",
      "outperformed state-of-the-art frame-based detection methods. Code for our paper\n",
      "is publicly available at https://github.com/oidelima/Deepfake-Detection. \n",
      "\n",
      "\n",
      "Recent advances in autoencoders and generative models have given rise to\n",
      "effective video forgery methods, used for generating so-called \"deepfakes\".\n",
      "Mitigation research is mostly focused on post-factum deepfake detection and not\n",
      "on prevention. We complement these efforts by introducing a novel class of\n",
      "adversarial attacks---training-resistant attacks---which can disrupt\n",
      "face-swapping autoencoders whether or not its adversarial images have been\n",
      "included in the training set of said autoencoders. We propose the Oscillating\n",
      "GAN (OGAN) attack, a novel attack optimized to be training-resistant, which\n",
      "introduces spatial-temporal distortions to the output of face-swapping\n",
      "autoencoders. To implement OGAN, we construct a bilevel optimization problem,\n",
      "where we train a generator and a face-swapping model instance against each\n",
      "other. Specifically, we pair each input image with a target distortion, and\n",
      "feed them into a generator that produces an adversarial image. This image will\n",
      "exhibit the distortion when a face-swapping autoencoder is applied to it. We\n",
      "solve the optimization problem by training the generator and the face-swapping\n",
      "model simultaneously using an iterative process of alternating optimization.\n",
      "Next, we analyze the previously published Distorting Attack and show it is\n",
      "training-resistant, though it is outperformed by our suggested OGAN. Finally,\n",
      "we validate both attacks using a popular implementation of FaceSwap, and show\n",
      "that they transfer across different target models and target faces, including\n",
      "faces the adversarial attacks were not trained on. More broadly, these results\n",
      "demonstrate the existence of training-resistant adversarial attacks,\n",
      "potentially applicable to a wide range of domains. \n",
      "\n",
      "\n",
      "As the GAN-based face image and video generation techniques, widely known as\n",
      "DeepFakes, have become more and more matured and realistic, there comes a\n",
      "pressing and urgent demand for effective DeepFakes detectors. Motivated by the\n",
      "fact that remote visual photoplethysmography (PPG) is made possible by\n",
      "monitoring the minuscule periodic changes of skin color due to blood pumping\n",
      "through the face, we conjecture that normal heartbeat rhythms found in the real\n",
      "face videos will be disrupted or even entirely broken in a DeepFake video,\n",
      "making it a potentially powerful indicator for DeepFake detection. In this\n",
      "work, we propose DeepRhythm, a DeepFake detection technique that exposes\n",
      "DeepFakes by monitoring the heartbeat rhythms. DeepRhythm utilizes\n",
      "dual-spatial-temporal attention to adapt to dynamically changing face and fake\n",
      "types. Extensive experiments on FaceForensics++ and DFDC-preview datasets have\n",
      "confirmed our conjecture and demonstrated not only the effectiveness, but also\n",
      "the generalization capability of \\emph{DeepRhythm} over different datasets by\n",
      "various DeepFakes generation techniques and multifarious challenging\n",
      "degradations. \n",
      "\n",
      "\n",
      "At this moment, GAN-based image generation methods are still imperfect, whose\n",
      "upsampling design has limitations in leaving some certain artifact patterns in\n",
      "the synthesized image. Such artifact patterns can be easily exploited (by\n",
      "recent methods) for difference detection of real and GAN-synthesized images.\n",
      "However, the existing detection methods put much emphasis on the artifact\n",
      "patterns, which can become futile if such artifact patterns were reduced.\n",
      "Towards reducing the artifacts in the synthesized images, in this paper, we\n",
      "devise a simple yet powerful approach termed FakePolisher that performs shallow\n",
      "reconstruction of fake images through a learned linear dictionary, intending to\n",
      "effectively and efficiently reduce the artifacts introduced during image\n",
      "synthesis. The comprehensive evaluation on 3 state-of-the-art DeepFake\n",
      "detection methods and fake images generated by 16 popular GAN-based fake image\n",
      "generation techniques, demonstrates the effectiveness of our technique.Overall,\n",
      "through reducing artifact patterns, our technique significantly reduces the\n",
      "accuracy of the 3 state-of-the-art fake image detection methods, i.e., 47% on\n",
      "average and up to 93% in the worst case. \n",
      "\n",
      "\n",
      "Deepfake represents a category of face-swapping attacks that leverage machine\n",
      "learning models such as autoencoders or generative adversarial networks.\n",
      "Although the concept of the face-swapping is not new, its recent technical\n",
      "advances make fake content (e.g., images, videos) more realistic and\n",
      "imperceptible to Humans. Various detection techniques for Deepfake attacks have\n",
      "been explored. These methods, however, are passive measures against Deepfakes\n",
      "as they are mitigation strategies after the high-quality fake content is\n",
      "generated. More importantly, we would like to think ahead of the attackers with\n",
      "robust defenses. This work aims to take an offensive measure to impede the\n",
      "generation of high-quality fake images or videos. Specifically, we propose to\n",
      "use novel transformation-aware adversarially perturbed faces as a defense\n",
      "against GAN-based Deepfake attacks. Different from the naive adversarial faces,\n",
      "our proposed approach leverages differentiable random image transformations\n",
      "during the generation. We also propose to use an ensemble-based approach to\n",
      "enhance the defense robustness against GAN-based Deepfake variants under the\n",
      "black-box setting. We show that training a Deepfake model with adversarial\n",
      "faces can lead to a significant degradation in the quality of synthesized\n",
      "faces. This degradation is twofold. On the one hand, the quality of the\n",
      "synthesized faces is reduced with more visual artifacts such that the\n",
      "synthesized faces are more obviously fake or less convincing to human\n",
      "observers. On the other hand, the synthesized faces can easily be detected\n",
      "based on various metrics. \n",
      "\n",
      "\n",
      "Deepfakes are a recent off-the-shelf manipulation technique that allows\n",
      "anyone to swap two identities in a single video. In addition to Deepfakes, a\n",
      "variety of GAN-based face swapping methods have also been published with\n",
      "accompanying code. To counter this emerging threat, we have constructed an\n",
      "extremely large face swap video dataset to enable the training of detection\n",
      "models, and organized the accompanying DeepFake Detection Challenge (DFDC)\n",
      "Kaggle competition. Importantly, all recorded subjects agreed to participate in\n",
      "and have their likenesses modified during the construction of the face-swapped\n",
      "dataset. The DFDC dataset is by far the largest currently and publicly\n",
      "available face swap video dataset, with over 100,000 total clips sourced from\n",
      "3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned\n",
      "methods. In addition to describing the methods used to construct the dataset,\n",
      "we provide a detailed analysis of the top submissions from the Kaggle contest.\n",
      "We show although Deepfake detection is extremely difficult and still an\n",
      "unsolved problem, a Deepfake detection model trained only on the DFDC can\n",
      "generalize to real \"in-the-wild\" Deepfake videos, and such a model can be a\n",
      "valuable analysis tool when analyzing potentially Deepfaked videos. Training,\n",
      "validation and testing corpuses can be downloaded from\n",
      "https://ai.facebook.com/datasets/dfdc. \n",
      "\n",
      "\n",
      "Recent advances in content generation technologies (widely known as\n",
      "DeepFakes) along with the online proliferation of manipulated media content\n",
      "render the detection of such manipulations a task of increasing importance.\n",
      "Even though there are many DeepFake detection methods, only a few focus on the\n",
      "impact of dataset preprocessing and the aggregation of frame-level to\n",
      "video-level prediction on model performance. In this paper, we propose a\n",
      "pre-processing step to improve the training data quality and examine its effect\n",
      "on the performance of DeepFake detection. We also propose and evaluate the\n",
      "effect of video-level prediction aggregation approaches. Experimental results\n",
      "show that the proposed pre-processing approach leads to considerable\n",
      "improvements in the performance of detection models, and the proposed\n",
      "prediction aggregation scheme further boosts the detection efficiency in cases\n",
      "where there are multiple faces in a video. \n",
      "\n",
      "\n",
      "We present \\textbf{FakeET}-- an eye-tracking database to understand human\n",
      "visual perception of \\emph{deepfake} videos. Given that the principal purpose\n",
      "of deepfakes is to deceive human observers, FakeET is designed to understand\n",
      "and evaluate the ease with which viewers can detect synthetic video artifacts.\n",
      "FakeET contains viewing patterns compiled from 40 users via the \\emph{Tobii}\n",
      "desktop eye-tracker for 811 videos from the \\textit{Google Deepfake} dataset,\n",
      "with a minimum of two viewings per video. Additionally, EEG responses acquired\n",
      "via the \\emph{Emotiv} sensor are also available. The compiled data confirms (a)\n",
      "distinct eye movement characteristics for \\emph{real} vs \\emph{fake} videos;\n",
      "(b) utility of the eye-track saliency maps for spatial forgery localization and\n",
      "detection, and (c) Error Related Negativity (ERN) triggers in the EEG\n",
      "responses, and the ability of the \\emph{raw} EEG signal to distinguish between\n",
      "\\emph{real} and \\emph{fake} videos. \n",
      "\n",
      "\n",
      "In this work, we develop efficient disruptions of black-box image translation\n",
      "deepfake generation systems. We are the first to demonstrate black-box deepfake\n",
      "generation disruption by presenting image translation formulations of attacks\n",
      "initially proposed for classification models. Nevertheless, a naive adaptation\n",
      "of classification black-box attacks results in a prohibitive number of queries\n",
      "for image translation systems in the real-world. We present a frustratingly\n",
      "simple yet highly effective algorithm Leaking Universal Perturbations (LUP),\n",
      "that significantly reduces the number of queries needed to attack an image. LUP\n",
      "consists of two phases: (1) a short leaking phase where we attack the network\n",
      "using traditional black-box attacks and gather information on successful\n",
      "attacks on a small dataset and (2) and an exploitation phase where we leverage\n",
      "said information to subsequently attack the network with improved efficiency.\n",
      "Our attack reduces the total number of queries necessary to attack GANimation\n",
      "and StarGAN by 30%. \n",
      "\n",
      "\n",
      "Deepfakes are videos that include changes, quite often substituting face of a\n",
      "portrayed individual with a different face using neural networks. Even though\n",
      "the technology gained its popularity as a carrier of jokes and parodies it\n",
      "raises a serious threat to ones security - via biometric impersonation or\n",
      "besmearing. In this paper we present two methods that allow detecting Deepfakes\n",
      "for a user without significant computational power. In particular, we enhance\n",
      "MesoNet by replacing the original activation functions allowing a nearly 1%\n",
      "improvement as well as increasing the consistency of the results. Moreover, we\n",
      "introduced and verified a new activation function - Pish that at the cost of\n",
      "slight time overhead allows even higher consistency.\n",
      "  Additionally, we present a preliminary results of Deepfake detection method\n",
      "based on Local Feature Descriptors (LFD), that allows setting up the system\n",
      "even faster and without resorting to GPU computation. Our method achieved Equal\n",
      "Error Rate of 0.28, with both accuracy and recall exceeding 0.7. \n",
      "\n",
      "\n",
      "We propose detection of deepfake videos based on the dissimilarity between\n",
      "the audio and visual modalities, termed as the Modality Dissonance Score (MDS).\n",
      "We hypothesize that manipulation of either modality will lead to dis-harmony\n",
      "between the two modalities, eg, loss of lip-sync, unnatural facial and lip\n",
      "movements, etc. MDS is computed as an aggregate of dissimilarity scores between\n",
      "audio and visual segments in a video. Discriminative features are learnt for\n",
      "the audio and visual channels in a chunk-wise manner, employing the\n",
      "cross-entropy loss for individual modalities, and a contrastive loss that\n",
      "models inter-modality similarity. Extensive experiments on the DFDC and\n",
      "DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art\n",
      "by up to 7%. We also demonstrate temporal forgery localization, and show how\n",
      "our technique identifies the manipulated video segments. \n",
      "\n",
      "\n",
      "With the recent advances in voice synthesis, AI-synthesized fake voices are\n",
      "indistinguishable to human ears and widely are applied to produce realistic and\n",
      "natural DeepFakes, exhibiting real threats to our society. However, effective\n",
      "and robust detectors for synthesized fake voices are still in their infancy and\n",
      "are not ready to fully tackle this emerging threat. In this paper, we devise a\n",
      "novel approach, named \\emph{DeepSonar}, based on monitoring neuron behaviors of\n",
      "speaker recognition (SR) system, \\ie, a deep neural network (DNN), to discern\n",
      "AI-synthesized fake voices. Layer-wise neuron behaviors provide an important\n",
      "insight to meticulously catch the differences among inputs, which are widely\n",
      "employed for building safety, robust, and interpretable DNNs. In this work, we\n",
      "leverage the power of layer-wise neuron activation patterns with a conjecture\n",
      "that they can capture the subtle differences between real and AI-synthesized\n",
      "fake voices, in providing a cleaner signal to classifiers than raw inputs.\n",
      "Experiments are conducted on three datasets (including commercial products from\n",
      "Google, Baidu, \\etc) containing both English and Chinese languages to\n",
      "corroborate the high detection rates (98.1\\% average accuracy) and low false\n",
      "alarm rates (about 2\\% error rate) of DeepSonar in discerning fake voices.\n",
      "Furthermore, extensive experimental results also demonstrate its robustness\n",
      "against manipulation attacks (\\eg, voice conversion and additive real-world\n",
      "noises). Our work further poses a new insight into adopting neuron behaviors\n",
      "for effective and robust AI aided multimedia fakes forensics as an inside-out\n",
      "approach instead of being motivated and swayed by various artifacts introduced\n",
      "in synthesizing fakes. \n",
      "\n",
      "\n",
      "In this paper, we introduce DeepFake, a novel deep reinforcement\n",
      "learning-based deception strategy to deal with reactive jamming attacks. In\n",
      "particular, for a smart and reactive jamming attack, the jammer is able to\n",
      "sense the channel and attack the channel if it detects communications from the\n",
      "legitimate transmitter. To deal with such attacks, we propose an intelligent\n",
      "deception strategy which allows the legitimate transmitter to transmit \"fake\"\n",
      "signals to attract the jammer. Then, if the jammer attacks the channel, the\n",
      "transmitter can leverage the strong jamming signals to transmit data by using\n",
      "ambient backscatter communication technology or harvest energy from the strong\n",
      "jamming signals for future use. By doing so, we can not only undermine the\n",
      "attack ability of the jammer, but also utilize jamming signals to improve the\n",
      "system performance. To effectively learn from and adapt to the dynamic and\n",
      "uncertainty of jamming attacks, we develop a novel deep reinforcement learning\n",
      "algorithm using the deep dueling neural network architecture to obtain the\n",
      "optimal policy with thousand times faster than those of the conventional\n",
      "reinforcement algorithms. Extensive simulation results reveal that our proposed\n",
      "DeepFake framework is superior to other anti-jamming strategies in terms of\n",
      "throughput, packet loss, and learning rate. \n",
      "\n",
      "\n",
      "Deepfake defense not only requires the research of detection but also\n",
      "requires the efforts of generation methods. However, current deepfake methods\n",
      "suffer the effects of obscure workflow and poor performance. To solve this\n",
      "problem, we present DeepFaceLab, the current dominant deepfake framework for\n",
      "face-swapping. It provides the necessary tools as well as an easy-to-use way to\n",
      "conduct high-quality face-swapping. It also offers a flexible and loose\n",
      "coupling structure for people who need to strengthen their pipeline with other\n",
      "features without writing complicated boilerplate code. We detail the principles\n",
      "that drive the implementation of DeepFaceLab and introduce its pipeline,\n",
      "through which every aspect of the pipeline can be modified painlessly by users\n",
      "to achieve their customization purpose. It is noteworthy that DeepFaceLab could\n",
      "achieve cinema-quality results with high fidelity. We demonstrate the advantage\n",
      "of our system by comparing our approach with other face-swapping methods.For\n",
      "more information, please visit:https://github.com/iperov/DeepFaceLab/. \n",
      "\n",
      "\n",
      "With the proliferation of face image manipulation (FIM) techniques such as\n",
      "Face2Face and Deepfake, more fake face images are spreading over the internet,\n",
      "which brings serious challenges to public confidence. Face image forgery\n",
      "detection has made considerable progresses in exposing specific FIM, but it is\n",
      "still in scarcity of a robust fake face detector to expose face image forgeries\n",
      "under complex scenarios such as with further compression, blurring, scaling,\n",
      "etc. Due to the relatively fixed structure, convolutional neural network (CNN)\n",
      "tends to learn image content representations. However, CNN should learn subtle\n",
      "manipulation traces for image forensics tasks. Thus, we propose an adaptive\n",
      "manipulation traces extraction network (AMTEN), which serves as pre-processing\n",
      "to suppress image content and highlight manipulation traces. AMTEN exploits an\n",
      "adaptive convolution layer to predict manipulation traces in the image, which\n",
      "are reused in subsequent layers to maximize manipulation artifacts by updating\n",
      "weights during the back-propagation pass. A fake face detector, namely\n",
      "AMTENnet, is constructed by integrating AMTEN with CNN. Experimental results\n",
      "prove that the proposed AMTEN achieves desirable pre-processing. When detecting\n",
      "fake face images generated by various FIM techniques, AMTENnet achieves an\n",
      "average accuracy up to 98.52%, which outperforms the state-of-the-art works.\n",
      "When detecting face images with unknown post-processing operations, the\n",
      "detector also achieves an average accuracy of 95.17%. \n",
      "\n",
      "\n",
      "As of late an AI based free programming device has made it simple to make\n",
      "authentic face swaps in recordings that leaves barely any hints of control, in\n",
      "what are known as \"deepfake\" recordings. Situations where these genuine istic\n",
      "counterfeit recordings are utilized to make political pain, extort somebody or\n",
      "phony fear based oppression occasions are effectively imagined. This paper\n",
      "proposes a transient mindful pipeline to automat-ically recognize deepfake\n",
      "recordings. Our framework utilizes a convolutional neural system (CNN) to\n",
      "remove outline level highlights. These highlights are then used to prepare a\n",
      "repetitive neural net-work (RNN) that figures out how to characterize if a\n",
      "video has been sub-ject to control or not. We assess our technique against a\n",
      "huge arrangement of deepfake recordings gathered from different video sites. We\n",
      "show how our framework can accomplish aggressive outcomes in this assignment\n",
      "while utilizing a basic design. \n",
      "\n",
      "\n",
      "Deeplearning has been used to solve complex problems in various domains. As\n",
      "it advances, it also creates applications which become a major threat to our\n",
      "privacy, security and even to our Democracy. Such an application which is being\n",
      "developed recently is the \"Deepfake\". Deepfake models can create fake images\n",
      "and videos that humans cannot differentiate them from the genuine ones.\n",
      "Therefore, the counter application to automatically detect and analyze the\n",
      "digital visual media is necessary in today world. This paper details retraining\n",
      "the image classification models to apprehend the features from each deepfake\n",
      "video frames. After feeding different sets of deepfake clips of video fringes\n",
      "through a pretrained layer of bottleneck in the neural network is made for\n",
      "every video frame, already stated layer contains condense data for all images\n",
      "and exposes artificial manipulations in Deepfake videos. When checking Deepfake\n",
      "videos, this technique received more than 87 per cent accuracy. This technique\n",
      "has been tested on the Face Forensics dataset and obtained good accuracy in\n",
      "detection. \n",
      "\n",
      "\n",
      "One of the most terrifying phenomenon nowadays is the DeepFake: the\n",
      "possibility to automatically replace a person's face in images and videos by\n",
      "exploiting algorithms based on deep learning. This paper will present a brief\n",
      "overview of technologies able to produce DeepFake images of faces. A forensics\n",
      "analysis of those images with standard methods will be presented: not\n",
      "surprisingly state of the art techniques are not completely able to detect the\n",
      "fakeness. To solve this, a preliminary idea on how to fight DeepFake images of\n",
      "faces will be presented by analysing anomalies in the frequency domain. \n",
      "\n",
      "\n",
      "Altered and manipulated multimedia is increasingly present and widely\n",
      "distributed via social media platforms. Advanced video manipulation tools\n",
      "enable the generation of highly realistic-looking altered multimedia. While\n",
      "many methods have been presented to detect manipulations, most of them fail\n",
      "when evaluated with data outside of the datasets used in research environments.\n",
      "In order to address this problem, the Deepfake Detection Challenge (DFDC)\n",
      "provides a large dataset of videos containing realistic manipulations and an\n",
      "evaluation system that ensures that methods work quickly and accurately, even\n",
      "when faced with challenging data. In this paper, we introduce a method based on\n",
      "convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that\n",
      "extracts visual and temporal features from faces present in videos to\n",
      "accurately detect manipulations. The method is evaluated with the DFDC dataset,\n",
      "providing competitive results compared to other techniques. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generative deep learning algorithms have progressed to a point where it is\n",
      "difficult to tell the difference between what is real and what is fake. In\n",
      "2018, it was discovered how easy it is to use this technology for unethical and\n",
      "malicious applications, such as the spread of misinformation, impersonation of\n",
      "political leaders, and the defamation of innocent individuals. Since then,\n",
      "these `deepfakes' have advanced significantly.\n",
      "  In this paper, we explore the creation and detection of deepfakes and provide\n",
      "an in-depth view of how these architectures work. The purpose of this survey is\n",
      "to provide the reader with a deeper understanding of (1) how deepfakes are\n",
      "created and detected, (2) the current trends and advancements in this domain,\n",
      "(3) the shortcomings of the current defense solutions, and (4) the areas which\n",
      "require further research and attention. \n",
      "\n",
      "\n",
      "The Deepfake phenomenon has become very popular nowadays thanks to the\n",
      "possibility to create incredibly realistic images using deep learning tools,\n",
      "based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we\n",
      "focus on the analysis of Deepfakes of human faces with the objective of\n",
      "creating a new detection method able to detect a forensics trace hidden in\n",
      "images: a sort of fingerprint left in the image generation process. The\n",
      "proposed technique, by means of an Expectation Maximization (EM) algorithm,\n",
      "extracts a set of local features specifically addressed to model the underlying\n",
      "convolutional generative process. Ad-hoc validation has been employed through\n",
      "experimental tests with naive classifiers on five different architectures\n",
      "(GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as\n",
      "ground-truth for non-fakes. Results demonstrated the effectiveness of the\n",
      "technique in distinguishing the different architectures and the corresponding\n",
      "generation process. \n",
      "\n",
      "\n",
      "In the last few years, several techniques for facial manipulation in videos\n",
      "have been successfully developed and made available to the masses (i.e.,\n",
      "FaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\n",
      "video sequences with incredibly realistic results and a very little effort.\n",
      "Despite the usefulness of these tools in many fields, if used maliciously, they\n",
      "can have a significantly bad impact on society (e.g., fake news spreading,\n",
      "cyber bullying through fake revenge porn). The ability of objectively detecting\n",
      "whether a face has been manipulated in a video sequence is then a task of\n",
      "utmost importance. In this paper, we tackle the problem of face manipulation\n",
      "detection in video sequences targeting modern facial manipulation techniques.\n",
      "In particular, we study the ensembling of different trained Convolutional\n",
      "Neural Network (CNN) models. In the proposed solution, different models are\n",
      "obtained starting from a base network (i.e., EfficientNetB4) making use of two\n",
      "different concepts: (i) attention layers; (ii) siamese training. We show that\n",
      "combining these networks leads to promising face manipulation detection results\n",
      "on two publicly available datasets with more than 119000 videos. \n",
      "\n",
      "\n",
      "Media forensics has attracted a lot of attention in the last years in part\n",
      "due to the increasing concerns around DeepFakes. Since the initial DeepFake\n",
      "databases from the 1st generation such as UADFV and FaceForensics++ up to the\n",
      "latest databases of the 2nd generation such as Celeb-DF and DFDC, many visual\n",
      "improvements have been carried out, making fake videos almost indistinguishable\n",
      "to the human eye. This study provides an exhaustive analysis of both 1st and\n",
      "2nd DeepFake generations in terms of facial regions and fake detection\n",
      "performance. Two different methods are considered in our experimental\n",
      "framework: i) the traditional one followed in the literature and based on\n",
      "selecting the entire face as input to the fake detection system, and ii) a\n",
      "novel approach based on the selection of specific facial regions as input to\n",
      "the fake detection system.\n",
      "  Among all the findings resulting from our experiments, we highlight the poor\n",
      "fake detection results achieved even by the strongest state-of-the-art fake\n",
      "detectors in the latest DeepFake databases of the 2nd generation, with Equal\n",
      "Error Rate results ranging from 15% to 30%. These results remark the necessity\n",
      "of further research to develop more sophisticated fake detectors. \n",
      "\n",
      "\n",
      "It is now possible to synthesize highly realistic images of people who don't\n",
      "exist. Such content has, for example, been implicated in the creation of\n",
      "fraudulent social-media profiles responsible for dis-information campaigns.\n",
      "Significant efforts are, therefore, being deployed to detect\n",
      "synthetically-generated content. One popular forensic approach trains a neural\n",
      "network to distinguish real from synthetic content.\n",
      "  We show that such forensic classifiers are vulnerable to a range of attacks\n",
      "that reduce the classifier to near-0% accuracy. We develop five attack case\n",
      "studies on a state-of-the-art classifier that achieves an area under the ROC\n",
      "curve (AUC) of 0.95 on almost all existing image generators, when only trained\n",
      "on one generator. With full access to the classifier, we can flip the lowest\n",
      "bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\n",
      "1% of the image area to reduce the classifier's AUC to 0.08; or add a single\n",
      "noise pattern in the synthesizer's latent space to reduce the classifier's AUC\n",
      "to 0.17. We also develop a black-box attack that, with no access to the target\n",
      "classifier, reduces the AUC to 0.22. These attacks reveal significant\n",
      "vulnerabilities of certain image-forensic classifiers. \n",
      "\n",
      "\n",
      "This work uses adversarial perturbations to enhance deepfake images and fool\n",
      "common deepfake detectors. We created adversarial perturbations using the Fast\n",
      "Gradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox\n",
      "and whitebox settings. Detectors achieved over 95% accuracy on unperturbed\n",
      "deepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore\n",
      "two improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)\n",
      "Deep Image Prior (DIP). Lipschitz regularization constrains the gradient of the\n",
      "detector with respect to the input in order to increase robustness to input\n",
      "perturbations. The DIP defense removes perturbations using generative\n",
      "convolutional neural networks in an unsupervised manner. Regularization\n",
      "improved the detection of perturbed deepfakes on average, including a 10%\n",
      "accuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on\n",
      "perturbed deepfakes that fooled the original detector, while retaining 98%\n",
      "accuracy in other cases on a 100 image subsample. \n",
      "\n",
      "\n",
      "With the arrival of several face-swapping applications such as FaceApp,\n",
      "SnapChat, MixBooth, FaceBlender and many more, the authenticity of digital\n",
      "media content is hanging on a very loose thread. On social media platforms,\n",
      "videos are widely circulated often at a high compression factor. In this work,\n",
      "we analyze several deep learning approaches in the context of deepfakes\n",
      "classification in high compression scenario and demonstrate that a proposed\n",
      "approach based on metric learning can be very effective in performing such a\n",
      "classification. Using less number of frames per video to assess its realism,\n",
      "the metric learning approach using a triplet network architecture proves to be\n",
      "fruitful. It learns to enhance the feature space distance between the cluster\n",
      "of real and fake videos embedding vectors. We validated our approaches on two\n",
      "datasets to analyze the behavior in different environments. We achieved a\n",
      "state-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of\n",
      "90.71% on a highly compressed Neural Texture dataset. Our approach is\n",
      "especially helpful on social media platforms where data compression is\n",
      "inevitable. \n",
      "\n",
      "\n",
      "We present a learning-based method for detecting real and fake deepfake\n",
      "multimedia content. To maximize information for learning, we extract and\n",
      "analyze the similarity between the two audio and visual modalities from within\n",
      "the same video. Additionally, we extract and compare affective cues\n",
      "corresponding to perceived emotion from the two modalities within a video to\n",
      "infer whether the input video is \"real\" or \"fake\". We propose a deep learning\n",
      "network, inspired by the Siamese network architecture and the triplet loss. To\n",
      "validate our model, we report the AUC metric on two large-scale deepfake\n",
      "detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach\n",
      "with several SOTA deepfake detection methods and report per-video AUC of 84.4%\n",
      "on the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of\n",
      "our knowledge, ours is the first approach that simultaneously exploits audio\n",
      "and video modalities and also perceived emotions from the two modalities for\n",
      "deepfake detection. \n",
      "\n",
      "\n",
      "High quality fake videos and audios generated by AI-algorithms (the deep\n",
      "fakes) have started to challenge the status of videos and audios as definitive\n",
      "evidence of events. In this paper, we highlight a few of these challenges and\n",
      "discuss the research opportunities in this direction. \n",
      "\n",
      "\n",
      "Generative convolutional deep neural networks, e.g. popular GAN\n",
      "architectures, are relying on convolution based up-sampling methods to produce\n",
      "non-scalar outputs like images or video sequences. In this paper, we show that\n",
      "common up-sampling methods, i.e. known as up-convolution or transposed\n",
      "convolution, are causing the inability of such models to reproduce spectral\n",
      "distributions of natural training data correctly. This effect is independent of\n",
      "the underlying architecture and we show that it can be used to easily detect\n",
      "generated data like deepfakes with up to 100% accuracy on public benchmarks.\n",
      "  To overcome this drawback of current generative models, we propose to add a\n",
      "novel spectral regularization term to the training optimization objective. We\n",
      "show that this approach not only allows to train spectral consistent GANs that\n",
      "are avoiding high frequency errors. Also, we show that a correct approximation\n",
      "of the frequency spectrum has positive effects on the training stability and\n",
      "output quality of generative networks. \n",
      "\n",
      "\n",
      "Face modification systems using deep learning have become increasingly\n",
      "powerful and accessible. Given images of a person's face, such systems can\n",
      "generate new images of that same person under different expressions and poses.\n",
      "Some systems can also modify targeted attributes such as hair color or age.\n",
      "This type of manipulated images and video have been coined Deepfakes. In order\n",
      "to prevent a malicious user from generating modified images of a person without\n",
      "their consent we tackle the new problem of generating adversarial attacks\n",
      "against such image translation systems, which disrupt the resulting output\n",
      "image. We call this problem disrupting deepfakes. Most image translation\n",
      "architectures are generative models conditioned on an attribute (e.g. put a\n",
      "smile on this person's face). We are first to propose and successfully apply\n",
      "(1) class transferable adversarial attacks that generalize to different\n",
      "classes, which means that the attacker does not need to have knowledge about\n",
      "the conditioning class, and (2) adversarial training for generative adversarial\n",
      "networks (GANs) as a first step towards robust image translation networks.\n",
      "Finally, in gray-box scenarios, blurring can mount a successful defense against\n",
      "disruption. We present a spread-spectrum adversarial attack, which evades blur\n",
      "defenses. Our open-source code can be found at\n",
      "https://github.com/natanielruiz/disrupting-deepfakes. \n",
      "\n",
      "\n",
      "Deep neural networks have become remarkably good at producing realistic\n",
      "deepfakes, images of people that (to the untrained eye) are indistinguishable\n",
      "from real images. Deepfakes are produced by algorithms that learn to\n",
      "distinguish between real and fake images and are optimised to generate samples\n",
      "that the system deems realistic. This paper, and the resulting series of\n",
      "artworks Being Foiled explore the aesthetic outcome of inverting this process,\n",
      "instead optimising the system to generate images that it predicts as being\n",
      "fake. This maximises the unlikelihood of the data and in turn, amplifies the\n",
      "uncanny nature of these machine hallucinations. \n",
      "\n",
      "\n",
      "Recent advances in video manipulation techniques have made the generation of\n",
      "fake videos more accessible than ever before. Manipulated videos can fuel\n",
      "disinformation and reduce trust in media. Therefore detection of fake videos\n",
      "has garnered immense interest in academia and industry. Recently developed\n",
      "Deepfake detection methods rely on deep neural networks (DNNs) to distinguish\n",
      "AI-generated fake videos from real videos. In this work, we demonstrate that it\n",
      "is possible to bypass such detectors by adversarially modifying fake videos\n",
      "synthesized using existing Deepfake generation methods. We further demonstrate\n",
      "that our adversarial perturbations are robust to image and video compression\n",
      "codecs, making them a real-world threat. We present pipelines in both white-box\n",
      "and black-box attack scenarios that can fool DNN based Deepfake detectors into\n",
      "classifying fake videos as real. \n",
      "\n",
      "\n",
      "Data sharing for medical research has been difficult as open-sourcing\n",
      "clinical data may violate patient privacy. Traditional methods for face\n",
      "de-identification wipe out facial information entirely, making it impossible to\n",
      "analyze facial behavior. Recent advancements on whole-body keypoints detection\n",
      "also rely on facial input to estimate body keypoints. Both facial and body\n",
      "keypoints are critical in some medical diagnoses, and keypoints invariability\n",
      "after de-identification is of great importance. Here, we propose a solution\n",
      "using deepfake technology, the face swapping technique. While this swapping\n",
      "method has been criticized for invading privacy and portraiture right, it could\n",
      "conversely protect privacy in medical video: patients' faces could be swapped\n",
      "to a proper target face and become unrecognizable. However, it remained an open\n",
      "question that to what extent the swapping de-identification method could affect\n",
      "the automatic detection of body keypoints. In this study, we apply deepfake\n",
      "technology to Parkinson's disease examination videos to de-identify subjects,\n",
      "and quantitatively show that: face-swapping as a de-identification approach is\n",
      "reliable, and it keeps the keypoints almost invariant, significantly better\n",
      "than traditional methods. This study proposes a pipeline for video\n",
      "de-identification and keypoint preservation, clearing up some ethical\n",
      "restrictions for medical data sharing. This work could make open-source high\n",
      "quality medical video datasets more feasible and promote future medical\n",
      "research that benefits our society. \n",
      "\n",
      "\n",
      "Full face synthesis and partial face manipulation by virtue of the generative\n",
      "adversarial networks (GANs) and its variants have raised wide public concerns.\n",
      "In the multi-media forensics area, detecting and ultimately locating the image\n",
      "forgery has become an imperative task. In this work, we investigate the\n",
      "architecture of existing GAN-based face manipulation methods and observe that\n",
      "the imperfection of upsampling methods therewithin could be served as an\n",
      "important asset for GAN-synthesized fake image detection and forgery\n",
      "localization. Based on this basic observation, we have proposed a novel\n",
      "approach, termed FakeLocator, to obtain high localization accuracy, at full\n",
      "resolution, on manipulated facial images. To the best of our knowledge, this is\n",
      "the very first attempt to solve the GAN-based fake localization problem with a\n",
      "gray-scale fakeness map that preserves more information of fake regions. To\n",
      "improve the universality of FakeLocator across multifarious facial attributes,\n",
      "we introduce an attention mechanism to guide the training of the model. To\n",
      "improve the universality of FakeLocator across different DeepFake methods, we\n",
      "propose partial data augmentation and single sample clustering on the training\n",
      "images. Experimental results on popular FaceForensics++, DFFD datasets and\n",
      "seven different state-of-the-art GAN-based face generation methods have shown\n",
      "the effectiveness of our method. Compared with the baselines, our method\n",
      "performs better on various metrics. Moreover, the proposed method is robust\n",
      "against various real-world facial image degradations such as JPEG compression,\n",
      "low-resolution, noise, and blur. \n",
      "\n",
      "\n",
      "Visual content has become the primary source of information, as evident in\n",
      "the billions of images and videos, shared and uploaded on the Internet every\n",
      "single day. This has led to an increase in alterations in images and videos to\n",
      "make them more informative and eye-catching for the viewers worldwide. Some of\n",
      "these alterations are simple, like copy-move, and are easily detectable, while\n",
      "other sophisticated alterations like reenactment based DeepFakes are hard to\n",
      "detect. Reenactment alterations allow the source to change the target\n",
      "expressions and create photo-realistic images and videos. While technology can\n",
      "be potentially used for several applications, the malicious usage of automatic\n",
      "reenactment has a very large social implication. It is therefore important to\n",
      "develop detection techniques to distinguish real images and videos with the\n",
      "altered ones. This research proposes a learning-based algorithm for detecting\n",
      "reenactment based alterations. The proposed algorithm uses a multi-stream\n",
      "network that learns regional artifacts and provides a robust performance at\n",
      "various compression levels. We also propose a loss function for the balanced\n",
      "learning of the streams for the proposed network. The performance is evaluated\n",
      "on the publicly available FaceForensics dataset. The results show\n",
      "state-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no,\n",
      "easy, and hard compression factors, respectively. \n",
      "\n",
      "\n",
      "With the rapid progress of recent years, techniques that generate and\n",
      "manipulate multimedia content can now guarantee a very advanced level of\n",
      "realism. The boundary between real and synthetic media has become very thin. On\n",
      "the one hand, this opens the door to a series of exciting applications in\n",
      "different fields such as creative arts, advertising, film production, video\n",
      "games. On the other hand, it poses enormous security threats. Software packages\n",
      "freely available on the web allow any individual, without special skills, to\n",
      "create very realistic fake images and videos. So-called deepfakes can be used\n",
      "to manipulate public opinion during elections, commit fraud, discredit or\n",
      "blackmail people. Potential abuses are limited only by human imagination.\n",
      "Therefore, there is an urgent need for automated tools capable of detecting\n",
      "false multimedia content and avoiding the spread of dangerous false\n",
      "information. This review paper aims to present an analysis of the methods for\n",
      "visual media integrity verification, that is, the detection of manipulated\n",
      "images and videos. Special emphasis will be placed on the emerging phenomenon\n",
      "of deepfakes and, from the point of view of the forensic analyst, on modern\n",
      "data-driven forensic methods. The analysis will help to highlight the limits of\n",
      "current forensic tools, the most relevant issues, the upcoming challenges, and\n",
      "suggest future directions for research. \n",
      "\n",
      "\n",
      "In recent years, neural networks have been extensively deployed for computer\n",
      "vision tasks, particularly visual classification problems, where new algorithms\n",
      "reported to achieve or even surpass the human performance. Recent studies have\n",
      "shown that they are all vulnerable to the attack of adversarial examples. Small\n",
      "and often imperceptible perturbations to the input images are sufficient to\n",
      "fool the most powerful neural networks. \\emph{Advbox} is a toolbox to generate\n",
      "adversarial examples that fool neural networks in PaddlePaddle, PyTorch,\n",
      "Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine\n",
      "learning models. Compared to previous work, our platform supports black box\n",
      "attacks on Machine-Learning-as-a-service, as well as more attack scenarios,\n",
      "such as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The\n",
      "code is licensed under the Apache 2.0 and is openly available at\n",
      "https://github.com/advboxes/AdvBox. Advbox now supports Python 3. \n",
      "\n",
      "\n",
      "Creating fake images and videos such as \"Deepfake\" has become much easier\n",
      "these days due to the advancement in Generative Adversarial Networks (GANs).\n",
      "Moreover, recent research such as the few-shot learning can create highly\n",
      "realistic personalized fake images with only a few images. Therefore, the\n",
      "threat of Deepfake to be used for a variety of malicious intents such as\n",
      "propagating fake images and videos becomes prevalent. And detecting these\n",
      "machine-generated fake images has been quite challenging than ever. In this\n",
      "work, we propose a light-weight robust fine-tuning neural network-based\n",
      "classifier architecture called Fake Detection Fine-tuning Network (FDFtNet),\n",
      "which is capable of detecting many of the new fake face image generation\n",
      "models, and can be easily combined with existing image classification networks\n",
      "and finetuned on a few datasets. In contrast to many existing methods, our\n",
      "approach aims to reuse popular pre-trained models with only a few images for\n",
      "fine-tuning to effectively detect fake images. The core of our approach is to\n",
      "introduce an image-based self-attention module called Fine-Tune Transformer\n",
      "that uses only the attention module and the down-sampling layer. This module is\n",
      "added to the pre-trained model and fine-tuned on a few data to search for new\n",
      "sets of feature space to detect fake images. We experiment with our FDFtNet on\n",
      "the GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset\n",
      "(Deepfake and Face2Face) with a small input image resolution of 64x64 that\n",
      "complicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in\n",
      "detecting fake images generated from the GANs-based dataset, outperforming the\n",
      "state-of-the-art. \n",
      "\n",
      "\n",
      "The free access to large-scale public databases, together with the fast\n",
      "progress of deep learning techniques, in particular Generative Adversarial\n",
      "Networks, have led to the generation of very realistic fake content with its\n",
      "corresponding implications towards society in this era of fake news. This\n",
      "survey provides a thorough review of techniques for manipulating face images\n",
      "including DeepFake methods, and methods to detect such manipulations. In\n",
      "particular, four types of facial manipulation are reviewed: i) entire face\n",
      "synthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv)\n",
      "expression swap. For each manipulation group, we provide details regarding\n",
      "manipulation techniques, existing public databases, and key benchmarks for\n",
      "technology evaluation of fake detection methods, including a summary of results\n",
      "from those evaluations. Among all the aspects discussed in the survey, we pay\n",
      "special attention to the latest generation of DeepFakes, highlighting its\n",
      "improvements and challenges for fake detection.\n",
      "  In addition to the survey information, we also discuss open issues and future\n",
      "trends that should be considered to advance in the field. \n",
      "\n",
      "\n",
      "In this paper we propose a novel image representation called face X-ray for\n",
      "detecting forgery in face images. The face X-ray of an input face image is a\n",
      "greyscale image that reveals whether the input image can be decomposed into the\n",
      "blending of two images from different sources. It does so by showing the\n",
      "blending boundary for a forged image and the absence of blending for a real\n",
      "image. We observe that most existing face manipulation methods share a common\n",
      "step: blending the altered face into an existing background image. For this\n",
      "reason, face X-ray provides an effective way for detecting forgery generated by\n",
      "most existing face manipulation algorithms. Face X-ray is general in the sense\n",
      "that it only assumes the existence of a blending step and does not rely on any\n",
      "knowledge of the artifacts associated with a specific face manipulation\n",
      "technique. Indeed, the algorithm for computing face X-ray can be trained\n",
      "without fake images generated by any of the state-of-the-art face manipulation\n",
      "methods. Extensive experiments show that face X-ray remains effective when\n",
      "applied to forgery generated by unseen face manipulation techniques, while most\n",
      "existing face forgery detection or deepfake detection algorithms experience a\n",
      "significant performance drop. \n",
      "\n",
      "\n",
      "In this work we ask whether it is possible to create a \"universal\" detector\n",
      "for telling apart real images from these generated by a CNN, regardless of\n",
      "architecture or dataset used. To test this, we collect a dataset consisting of\n",
      "fake images generated by 11 different CNN-based image generator models, chosen\n",
      "to span the space of commonly used architectures today (ProGAN, StyleGAN,\n",
      "BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks,\n",
      "implicit maximum likelihood estimation, second-order attention\n",
      "super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre-\n",
      "and post-processing and data augmentation, a standard image classifier trained\n",
      "on only one specific CNN generator (ProGAN) is able to generalize surprisingly\n",
      "well to unseen architectures, datasets, and training methods (including the\n",
      "just released StyleGAN2). Our findings suggest the intriguing possibility that\n",
      "today's CNN-generated images share some common systematic flaws, preventing\n",
      "them from achieving realistic image synthesis. Code and pre-trained networks\n",
      "are available at https://peterwang512.github.io/CNNDetection/ . \n",
      "\n",
      "\n",
      "Recent advances in AI technology have made the forgery of digital images and\n",
      "videos easier, and it has become significantly more difficult to identify such\n",
      "forgeries. These forgeries, if disseminated with malicious intent, can\n",
      "negatively impact social and political stability, and pose significant ethical\n",
      "and legal challenges as well. Deepfake is a variant of auto-encoders that use\n",
      "deep learning techniques to identify and exchange images of a person's face in\n",
      "a picture or film. Deepfake can result in an erosion of public trust in digital\n",
      "images and videos, which has far-reaching effects on political and social\n",
      "stability. This study therefore proposes a solution for facial forgery\n",
      "detection to determine if a picture or film has ever been processed by\n",
      "Deepfake. The proposed solution reaches detection efficiency by using the\n",
      "recently proposed separable convolutional neural network (CNN) and image\n",
      "segmentation. In addition, this study also examined how different image\n",
      "segmentation methods affect detection results. Finally, the ensemble model is\n",
      "used to improve detection capabilities. Experiment results demonstrated the\n",
      "excellent performance of the proposed solution. \n",
      "\n",
      "\n",
      "Deep generative models have recently achieved impressive results for many\n",
      "real-world applications, successfully generating high-resolution and diverse\n",
      "samples from complex datasets. Due to this improvement, fake digital contents\n",
      "have proliferated growing concern and spreading distrust in image content,\n",
      "leading to an urgent need for automated ways to detect these AI-generated fake\n",
      "images.\n",
      "  Despite the fact that many face editing algorithms seem to produce realistic\n",
      "human faces, upon closer examination, they do exhibit artifacts in certain\n",
      "domains which are often hidden to the naked eye. In this work, we present a\n",
      "simple way to detect such fake face images - so-called DeepFakes. Our method is\n",
      "based on a classical frequency domain analysis followed by basic classifier.\n",
      "Compared to previous systems, which need to be fed with large amounts of\n",
      "labeled data, our approach showed very good results using only a few annotated\n",
      "training samples and even achieved good accuracies in fully unsupervised\n",
      "scenarios. For the evaluation on high resolution face images, we combined\n",
      "several public datasets of real and fake faces into a new benchmark: Faces-HQ.\n",
      "Given such high-resolution images, our approach reaches a perfect\n",
      "classification accuracy of 100% when it is trained on as little as 20 annotated\n",
      "samples. In a second experiment, in the evaluation of the medium-resolution\n",
      "images of the CelebA dataset, our method achieves 100% accuracy supervised and\n",
      "96% in an unsupervised setting. Finally, evaluating a low-resolution video\n",
      "sequences of the FaceForensics++ dataset, our method achieves 91% accuracy\n",
      "detecting manipulated videos.\n",
      "  Source Code: https://github.com/cc-hpc-itwm/DeepFakeDetection \n",
      "\n",
      "\n",
      "The revolution in computer hardware, especially in graphics processing units\n",
      "and tensor processing units, has enabled significant advances in computer\n",
      "graphics and artificial intelligence algorithms. In addition to their many\n",
      "beneficial applications in daily life and business,\n",
      "computer-generated/manipulated images and videos can be used for malicious\n",
      "purposes that violate security systems, privacy, and social trust. The deepfake\n",
      "phenomenon and its variations enable a normal user to use his or her personal\n",
      "computer to easily create fake videos of anybody from a short real online\n",
      "video. Several countermeasures have been introduced to deal with attacks using\n",
      "such videos. However, most of them are targeted at certain domains and are\n",
      "ineffective when applied to other domains or new attacks. In this paper, we\n",
      "introduce a capsule network that can detect various kinds of attacks, from\n",
      "presentation attacks using printed images and replayed videos to attacks using\n",
      "fake videos created using deep learning. It uses many fewer parameters than\n",
      "traditional convolutional neural networks with similar performance. Moreover,\n",
      "we explain, for the first time ever in the literature, the theory behind the\n",
      "application of capsule networks to the forensics problem through detailed\n",
      "analysis and visualization. \n",
      "\n",
      "\n",
      "In this paper, we introduce a preview of the Deepfakes Detection Challenge\n",
      "(DFDC) dataset consisting of 5K videos featuring two facial modification\n",
      "algorithms. A data collection campaign has been carried out where participating\n",
      "actors have entered into an agreement to the use and manipulation of their\n",
      "likenesses in our creation of the dataset. Diversity in several axes (gender,\n",
      "skin-tone, age, etc.) has been considered and actors recorded videos with\n",
      "arbitrary backgrounds thus bringing visual variability. Finally, a set of\n",
      "specific metrics to evaluate the performance have been defined and two existing\n",
      "models for detecting deepfakes have been tested to provide a reference\n",
      "performance baseline. The DFDC dataset preview can be downloaded at:\n",
      "deepfakedetectionchallenge.ai \n",
      "\n",
      "\n",
      "Nowadays, organizations collect vast quantities of accounting relevant\n",
      "transactions, referred to as 'journal entries', in 'Enterprise Resource\n",
      "Planning' (ERP) systems. The aggregation of those entries ultimately defines an\n",
      "organization's financial statement. To detect potential misstatements and\n",
      "fraud, international audit standards demand auditors to directly assess journal\n",
      "entries using 'Computer Assisted AuditTechniques' (CAATs). At the same time,\n",
      "discoveries in deep learning research revealed that machine learning models are\n",
      "vulnerable to 'adversarial attacks'. It also became evident that such attack\n",
      "techniques can be misused to generate 'Deepfakes' designed to directly attack\n",
      "the perception of humans by creating convincingly altered media content. The\n",
      "research of such developments and their potential impact on the finance and\n",
      "accounting domain is still in its early stage. We believe that it is of vital\n",
      "relevance to investigate how such techniques could be maliciously misused in\n",
      "this sphere. In this work, we show an adversarial attack against CAATs using\n",
      "deep neural networks. We first introduce a real-world 'thread model' designed\n",
      "to camouflage accounting anomalies such as fraudulent journal entries. Second,\n",
      "we show that adversarial autoencoder neural networks are capable of learning a\n",
      "human interpretable model of journal entries that disentangles the entries\n",
      "latent generative factors. Finally, we demonstrate how such a model can be\n",
      "maliciously misused by a perpetrator to generate robust 'adversarial' journal\n",
      "entries that mislead CAATs. \n",
      "\n",
      "\n",
      "It is increasingly easy to automatically swap faces in images and video or\n",
      "morph two faces into one using generative adversarial networks (GANs). The high\n",
      "quality of the resulted deep-morph raises the question of how vulnerable the\n",
      "current face recognition systems are to such fake images and videos. It also\n",
      "calls for automated ways to detect these GAN-generated faces. In this paper, we\n",
      "present the publicly available dataset of the Deepfake videos with faces\n",
      "morphed with a GAN-based algorithm. To generate these videos, we used open\n",
      "source software based on GANs, and we emphasize that training and blending\n",
      "parameters can significantly impact the quality of the resulted videos. We show\n",
      "that the state of the art face recognition systems based on VGG and Facenet\n",
      "neural networks are vulnerable to the deep morph videos, with 85.62 and 95.00\n",
      "false acceptance rates, respectively, which means methods for detecting these\n",
      "videos are necessary. We consider several baseline approaches for detecting\n",
      "deep morphs and find that the method based on visual quality metrics (often\n",
      "used in presentation attack detection domain) leads to the best performance\n",
      "with 8.97 equal error rate. Our experiments demonstrate that GAN-generated deep\n",
      "morph videos are challenging for both face recognition systems and existing\n",
      "detection methods, and the further development of deep morphing technologies\n",
      "will make it even more so. \n",
      "\n",
      "\n",
      "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an\n",
      "emerging problem threatening the trustworthiness of online information. The\n",
      "need to develop and evaluate DeepFake detection algorithms calls for\n",
      "large-scale datasets. However, current DeepFake datasets suffer from low visual\n",
      "quality and do not resemble DeepFake videos circulated on the Internet. We\n",
      "present a new large-scale challenging DeepFake video dataset, Celeb-DF, which\n",
      "contains 5,639 high-quality DeepFake videos of celebrities generated using\n",
      "improved synthesis process. We conduct a comprehensive evaluation of DeepFake\n",
      "detection methods and datasets to demonstrate the escalated level of challenges\n",
      "posed by Celeb-DF. \n",
      "\n",
      "\n",
      "Deep learning has been successfully applied to solve various complex problems\n",
      "ranging from big data analytics to computer vision and human-level control.\n",
      "Deep learning advances however have also been employed to create software that\n",
      "can cause threats to privacy, democracy and national security. One of those\n",
      "deep learning-powered applications recently emerged is deepfake. Deepfake\n",
      "algorithms can create fake images and videos that humans cannot distinguish\n",
      "them from authentic ones. The proposal of technologies that can automatically\n",
      "detect and assess the integrity of digital visual media is therefore\n",
      "indispensable. This paper presents a survey of algorithms used to create\n",
      "deepfakes and, more importantly, methods proposed to detect deepfakes in the\n",
      "literature to date. We present extensive discussions on challenges, research\n",
      "trends and directions related to deepfake technologies. By reviewing the\n",
      "background of deepfakes and state-of-the-art deepfake detection methods, this\n",
      "study provides a comprehensive overview of deepfake techniques and facilitates\n",
      "the development of new and more robust methods to deal with the increasingly\n",
      "challenging deepfakes. \n",
      "\n",
      "\n",
      "With advancements of deep learning techniques, it is now possible to generate\n",
      "super-realistic images and videos, i.e., deepfakes. These deepfakes could reach\n",
      "mass audience and result in adverse impacts on our society. Although lots of\n",
      "efforts have been devoted to detect deepfakes, their performance drops\n",
      "significantly on previously unseen but related manipulations and the detection\n",
      "generalization capability remains a problem. Motivated by the fine-grained\n",
      "nature and spatial locality characteristics of deepfakes, we propose\n",
      "Locality-Aware AutoEncoder (LAE) to bridge the generalization gap. In the\n",
      "training process, we use a pixel-wise mask to regularize local interpretation\n",
      "of LAE to enforce the model to learn intrinsic representation from the forgery\n",
      "region, instead of capturing artifacts in the training set and learning\n",
      "superficial correlations to perform detection. We further propose an active\n",
      "learning framework to select the challenging candidates for labeling, which\n",
      "requires human masks for less than 3% of the training data, dramatically\n",
      "reducing the annotation efforts to regularize interpretations. Experimental\n",
      "results on three deepfake detection tasks indicate that LAE could focus on the\n",
      "forgery regions to make decisions. The analysis further shows that LAE\n",
      "outperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on\n",
      "three deepfake detection tasks in terms of generalization accuracy on\n",
      "previously unseen manipulations. \n",
      "\n",
      "\n",
      "Deepfake detection is formulated as a hypothesis testing problem to classify\n",
      "an image as genuine or GAN-generated. A robust statistics view of GANs is\n",
      "considered to bound the error probability for various GAN implementations in\n",
      "terms of their performance. The bounds are further simplified using a Euclidean\n",
      "approximation for the low error regime. Lastly, relationships between error\n",
      "probability and epidemic thresholds for spreading processes in networks are\n",
      "established. \n",
      "\n",
      "\n",
      "The spread of misinformation through synthetically generated yet realistic\n",
      "images and videos has become a significant problem, calling for robust\n",
      "manipulation detection methods. Despite the predominant effort of detecting\n",
      "face manipulation in still images, less attention has been paid to the\n",
      "identification of tampered faces in videos by taking advantage of the temporal\n",
      "information present in the stream. Recurrent convolutional models are a class\n",
      "of deep learning models which have proven effective at exploiting the temporal\n",
      "information from image streams across domains. We thereby distill the best\n",
      "strategy for combining variations in these models along with domain specific\n",
      "face preprocessing techniques through extensive experimentation to obtain\n",
      "state-of-the-art performance on publicly available video-based facial\n",
      "manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face\n",
      "and FaceSwap tampered faces in video streams. Evaluation is performed on the\n",
      "recently introduced FaceForensics++ dataset, improving the previous\n",
      "state-of-the-art by up to 4.55% in accuracy. \n",
      "\n",
      "\n",
      "The rise of ubiquitous deepfakes, misinformation, disinformation, propaganda\n",
      "and post-truth, often referred to as fake news, raises concerns over the role\n",
      "of Internet and social media in modern democratic societies. Due to its rapid\n",
      "and widespread diffusion, digital deception has not only an individual or\n",
      "societal cost (e.g., to hamper the integrity of elections), but it can lead to\n",
      "significant economic losses (e.g., to affect stock market performance) or to\n",
      "risks to national security. Blockchain and other Distributed Ledger\n",
      "Technologies (DLTs) guarantee the provenance, authenticity and traceability of\n",
      "data by providing a transparent, immutable and verifiable record of\n",
      "transactions while creating a peer-to-peer secure platform for storing and\n",
      "exchanging information. This overview aims to explore the potential of DLTs and\n",
      "blockchain to combat digital deception, reviewing initiatives that are\n",
      "currently under development and identifying their main current challenges.\n",
      "Moreover, some recommendations are enumerated to guide future researchers on\n",
      "issues that will have to be tackled to face fake news, disinformation and\n",
      "deepfakes, as an integral part of strengthening the resilience against\n",
      "cyber-threats on today's online media. \n",
      "\n",
      "\n",
      "The advent of Generative Adversarial Networks (GANs) has brought about\n",
      "completely novel ways of transforming and manipulating pixels in digital\n",
      "images. GAN based techniques such as Image-to-Image translations, DeepFakes,\n",
      "and other automated methods have become increasingly popular in creating fake\n",
      "images. In this paper, we propose a novel approach to detect GAN generated fake\n",
      "images using a combination of co-occurrence matrices and deep learning. We\n",
      "extract co-occurrence matrices on three color channels in the pixel domain and\n",
      "train a model using a deep convolutional neural network (CNN) framework.\n",
      "Experimental results on two diverse and challenging GAN datasets comprising\n",
      "more than 56,000 images based on unpaired image-to-image translations (cycleGAN\n",
      "[1]) and facial attributes/expressions (StarGAN [2]) show that our approach is\n",
      "promising and achieves more than 99% classification accuracy in both datasets.\n",
      "Further, our approach also generalizes well and achieves good results when\n",
      "trained on one dataset and tested on the other. \n",
      "\n",
      "\n",
      "The rapid progress in synthetic image generation and manipulation has now\n",
      "come to a point where it raises significant concerns for the implications\n",
      "towards society. At best, this leads to a loss of trust in digital content, but\n",
      "could potentially cause further harm by spreading false information or fake\n",
      "news. This paper examines the realism of state-of-the-art image manipulations,\n",
      "and how difficult it is to detect them, either automatically or by humans. To\n",
      "standardize the evaluation of detection methods, we propose an automated\n",
      "benchmark for facial manipulation detection. In particular, the benchmark is\n",
      "based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent\n",
      "representatives for facial manipulations at random compression level and size.\n",
      "The benchmark is publicly available and contains a hidden test set as well as a\n",
      "database of over 1.8 million manipulated images. This dataset is over an order\n",
      "of magnitude larger than comparable, publicly available, forgery datasets.\n",
      "Based on this data, we performed a thorough analysis of data-driven forgery\n",
      "detectors. We show that the use of additional domainspecific knowledge improves\n",
      "forgery detection to unprecedented accuracy, even in the presence of strong\n",
      "compression, and clearly outperforms human observers. \n",
      "\n",
      "\n",
      "It is becoming increasingly easy to automatically replace a face of one\n",
      "person in a video with the face of another person by using a pre-trained\n",
      "generative adversarial network (GAN). Recent public scandals, e.g., the faces\n",
      "of celebrities being swapped onto pornographic videos, call for automated ways\n",
      "to detect these Deepfake videos. To help developing such methods, in this\n",
      "paper, we present the first publicly available set of Deepfake videos generated\n",
      "from videos of VidTIMIT database. We used open source software based on GANs to\n",
      "create the Deepfakes, and we emphasize that training and blending parameters\n",
      "can significantly impact the quality of the resulted videos. To demonstrate\n",
      "this impact, we generated videos with low and high visual quality (320 videos\n",
      "each) using differently tuned parameter sets. We showed that the state of the\n",
      "art face recognition systems based on VGG and Facenet neural networks are\n",
      "vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates\n",
      "respectively, which means methods for detecting Deepfake videos are necessary.\n",
      "By considering several baseline approaches, we found that audio-visual approach\n",
      "based on lip-sync inconsistency detection was not able to distinguish Deepfake\n",
      "videos. The best performing method, which is based on visual quality metrics\n",
      "and is often used in presentation attack detection domain, resulted in 8.97%\n",
      "equal error rate on high quality Deepfakes. Our experiments demonstrate that\n",
      "GAN-generated Deepfake videos are challenging for both face recognition systems\n",
      "and existing detection methods, and the further development of face swapping\n",
      "technology will make it even more so. \n",
      "\n",
      "\n",
      "Image forensics is an increasingly relevant problem, as it can potentially\n",
      "address online disinformation campaigns and mitigate problematic aspects of\n",
      "social media. Of particular interest, given its recent successes, is the\n",
      "detection of imagery produced by Generative Adversarial Networks (GANs), e.g.\n",
      "`deepfakes'. Leveraging large training sets and extensive computing resources,\n",
      "recent work has shown that GANs can be trained to generate synthetic imagery\n",
      "which is (in some ways) indistinguishable from real imagery. We analyze the\n",
      "structure of the generating network of a popular GAN implementation, and show\n",
      "that the network's treatment of color is markedly different from a real camera\n",
      "in two ways. We further show that these two cues can be used to distinguish\n",
      "GAN-generated imagery from camera imagery, demonstrating effective\n",
      "discrimination between GAN imagery and real camera images used to train the\n",
      "GAN. \n",
      "\n",
      "\n",
      "In this work, we describe a new deep learning based method that can\n",
      "effectively distinguish AI-generated fake videos (referred to as {\\em DeepFake}\n",
      "videos hereafter) from real videos. Our method is based on the observations\n",
      "that current DeepFake algorithm can only generate images of limited\n",
      "resolutions, which need to be further warped to match the original faces in the\n",
      "source video. Such transforms leave distinctive artifacts in the resulting\n",
      "DeepFake videos, and we show that they can be effectively captured by\n",
      "convolutional neural networks (CNNs). Compared to previous methods which use a\n",
      "large amount of real and DeepFake generated images to train CNN classifier, our\n",
      "method does not need DeepFake generated images as negative training examples\n",
      "since we target the artifacts in affine face warping as the distinctive feature\n",
      "to distinguish real and fake images. The advantages of our method are two-fold:\n",
      "(1) Such artifacts can be simulated directly using simple image processing\n",
      "operations on a image to make it as negative example. Since training a DeepFake\n",
      "model to generate negative examples is time-consuming and resource-demanding,\n",
      "our method saves a plenty of time and resources in training data collection;\n",
      "(2) Since such artifacts are general existed in DeepFake videos from different\n",
      "sources, our method is more robust compared to others. Our method is evaluated\n",
      "on two sets of DeepFake video datasets for its effectiveness in practice. \n",
      "\n",
      "\n",
      "This paper presents a method to automatically and efficiently detect face\n",
      "tampering in videos, and particularly focuses on two recent techniques used to\n",
      "generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional\n",
      "image forensics techniques are usually not well suited to videos due to the\n",
      "compression that strongly degrades the data. Thus, this paper follows a deep\n",
      "learning approach and presents two networks, both with a low number of layers\n",
      "to focus on the mesoscopic properties of images. We evaluate those fast\n",
      "networks on both an existing dataset and a dataset we have constituted from\n",
      "online videos. The tests demonstrate a very successful detection rate with more\n",
      "than 98% for Deepfake and 95% for Face2Face. \n",
      "\n",
      "\n",
      "The new developments in deep generative networks have significantly improve\n",
      "the quality and efficiency in generating realistically-looking fake face\n",
      "videos. In this work, we describe a new method to expose fake face videos\n",
      "generated with neural networks. Our method is based on detection of eye\n",
      "blinking in the videos, which is a physiological signal that is not well\n",
      "presented in the synthesized fake videos. Our method is tested over benchmarks\n",
      "of eye-blinking detection datasets and also show promising performance on\n",
      "detecting videos generated with DeepFake. \n",
      "\n",
      "\n",
      "Recent advances in artificial speech and audio technologies have improved the\n",
      "abilities of deep-fake operators to falsify media and spread malicious\n",
      "misinformation. Anyone with limited coding skills can use freely available\n",
      "speech synthesis tools to create convincing simulations of influential\n",
      "speakers' voices with the malicious intent to distort the original message.\n",
      "With the latest technology, malicious operators do not have to generate an\n",
      "entire audio clip; instead, they can insert a partial manipulation or a segment\n",
      "of synthetic speech into a genuine audio recording to change the entire context\n",
      "and meaning of the original message. Detecting these insertions is especially\n",
      "challenging because partially manipulated audio can more easily avoid synthetic\n",
      "speech detectors than entirely fake messages can. This paper describes a\n",
      "potential partial synthetic speech detection system based on the x-ResNet\n",
      "architecture with a probabilistic linear discriminant analysis (PLDA) backend\n",
      "and interleaved aware score processing. Experimental results suggest that the\n",
      "PLDA backend results in a 25% average error reduction among partially\n",
      "synthesized datasets over a non-PLDA baseline. \n",
      "\n",
      "\n",
      "Detection of fake news is crucial to ensure the authenticity of information\n",
      "and maintain the news ecosystems reliability. Recently, there has been an\n",
      "increase in fake news content due to the recent proliferation of social media\n",
      "and fake content generation techniques such as Deep Fake. The majority of the\n",
      "existing modalities of fake news detection focus on content based approaches.\n",
      "However, most of these techniques fail to deal with ultra realistic synthesized\n",
      "media produced by generative models. Our recent studies find that the\n",
      "propagation characteristics of authentic and fake news are distinguishable,\n",
      "irrespective of their modalities. In this regard, we have investigated the\n",
      "auxiliary information based on social context to detect fake news. This paper\n",
      "has analyzed the social context of fake news detection with a hybrid graph\n",
      "neural network based approach. This hybrid model is based on integrating a\n",
      "graph neural network on the propagation of news and bi directional encoder\n",
      "representations from the transformers model on news content to learn the text\n",
      "features. Thus this proposed approach learns the content as well as the context\n",
      "features and hence able to outperform the baseline models with an f1 score of\n",
      "0.91 on PolitiFact and 0.93 on the Gossipcop dataset, respectively \n",
      "\n",
      "\n",
      "After Big Data and Artificial Intelligence (AI), the subject of Digital Twins\n",
      "has emerged as another promising technology, advocated, built, and sold by\n",
      "various IT companies. The approach aims to produce highly realistic models of\n",
      "real systems. In the case of dynamically changing systems, such digital twins\n",
      "would have a life, i.e. they would change their behaviour over time and, in\n",
      "perspective, take decisions like their real counterparts \\textemdash so the\n",
      "vision. In contrast to animated avatars, however, which only imitate the\n",
      "behaviour of real systems, like deep fakes, digital twins aim to be accurate\n",
      "\"digital copies\", i.e. \"duplicates\" of reality, which may interact with reality\n",
      "and with their physical counterparts. This chapter explores, what are possible\n",
      "applications and implications, limitations, and threats. \n",
      "\n",
      "\n",
      "The 2022 Russian invasion of Ukraine is being fought on two fronts: a brutal\n",
      "ground war and a duplicitous disinformation campaign designed to conceal and\n",
      "justify Russia's actions. This campaign includes at least one example of a\n",
      "deep-fake video purportedly showing Ukrainian President Zelenskyy admitting\n",
      "defeat and surrendering. In anticipation of future attacks of this form, we\n",
      "describe a facial and gestural behavioral model that captures distinctive\n",
      "characteristics of Zelenskyy's speaking style. Trained on over eight hours of\n",
      "authentic video from four different settings, we show that this behavioral\n",
      "model can distinguish Zelenskyy from deep-fake imposters.This model can play an\n",
      "important role -- particularly during the fog of war -- in distinguishing the\n",
      "real from the fake. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) have made remarkable progress in\n",
      "synthesizing realistic-looking images that effectively outsmart even humans.\n",
      "Although several detection methods can recognize these deep fakes by checking\n",
      "for image artifacts from the generation process, multiple counterattacks have\n",
      "demonstrated their limitations. These attacks, however, still require certain\n",
      "conditions to hold, such as interacting with the detection method or adjusting\n",
      "the GAN directly. In this paper, we introduce a novel class of simple\n",
      "counterattacks that overcomes these limitations. In particular, we show that an\n",
      "adversary can remove indicative artifacts, the GAN fingerprint, directly from\n",
      "the frequency spectrum of a generated image. We explore different realizations\n",
      "of this removal, ranging from filtering high frequencies to more nuanced\n",
      "frequency-peak cleansing. We evaluate the performance of our attack with\n",
      "different detection methods, GAN architectures, and datasets. Our results show\n",
      "that an adversary can often remove GAN fingerprints and thus evade the\n",
      "detection of generated images. \n",
      "\n",
      "\n",
      "This paper describes our best system and methodology for ADD 2022: The First\n",
      "Audio Deep Synthesis Detection Challenge\\cite{Yi2022ADD}. The very same system\n",
      "was used for both two rounds of evaluation in Track 3.2 with a similar training\n",
      "methodology. The first round of Track 3.2 data is generated from\n",
      "Text-to-Speech(TTS) or voice conversion (VC) algorithms, while the second round\n",
      "of data consists of generated fake audio from other participants in Track 3.1,\n",
      "aiming to spoof our systems. Our systems use a standard 34-layer ResNet, with\n",
      "multi-head attention pooling \\cite{india2019self} to learn the discriminative\n",
      "embedding for fake audio and spoof detection. We further utilize neural\n",
      "stitching to boost the model's generalization capability in order to perform\n",
      "equally well in different tasks, and more details will be explained in the\n",
      "following sessions. The experiments show that our proposed method outperforms\n",
      "all other systems with a 10.1% equal error rate(EER) in Track 3.2. \n",
      "\n",
      "\n",
      "The recent advancements in generative artificial speech models have made\n",
      "possible the generation of highly realistic speech signals. At first, it seems\n",
      "exciting to obtain these artificially synthesized signals such as speech clones\n",
      "or deep fakes but if left unchecked, it may lead us to digital dystopia. One of\n",
      "the primary focus in audio forensics is validating the authenticity of a\n",
      "speech. Though some solutions are proposed for English speeches but the\n",
      "detection of synthetic Hindi speeches have not gained much attention. Here, we\n",
      "propose an approach for discrimination of AI synthesized Hindi speech from an\n",
      "actual human speech. We have exploited the Bicoherence Phase, Bicoherence\n",
      "Magnitude, Mel Frequency Cepstral Coefficient (MFCC), Delta Cepstral, and Delta\n",
      "Square Cepstral as the discriminating features for machine learning models.\n",
      "Also, we extend the study to using deep neural networks for extensive\n",
      "experiments, specifically VGG16 and homemade CNN as the architecture models. We\n",
      "obtained an accuracy of 99.83% with VGG16 and 99.99% with homemade CNN models. \n",
      "\n",
      "\n",
      "In many applications of forensic image analysis, state-of-the-art results are\n",
      "nowadays achieved with machine learning methods. However, concerns about their\n",
      "reliability and opaqueness raise the question whether such methods can be used\n",
      "in criminal investigations. So far, this question of legal compliance has\n",
      "hardly been discussed, also because legal regulations for machine learning\n",
      "methods were not defined explicitly. To this end, the European Commission\n",
      "recently proposed the artificial intelligence (AI) act, a regulatory framework\n",
      "for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for\n",
      "use in law enforcement are permitted but subject to compliance with mandatory\n",
      "requirements. In this paper, we review why the use of machine learning in\n",
      "forensic image analysis is classified as high-risk. We then summarize the\n",
      "mandatory requirements for high-risk AI systems and discuss these requirements\n",
      "in light of two forensic applications, license plate recognition and deep fake\n",
      "detection. The goal of this paper is to raise awareness of the upcoming legal\n",
      "requirements and to point out avenues for future research. \n",
      "\n",
      "\n",
      "Deep fake technology became a hot field of research in the last few years.\n",
      "Researchers investigate sophisticated Generative Adversarial Networks (GAN),\n",
      "autoencoders, and other approaches to establish precise and robust algorithms\n",
      "for face swapping. Achieved results show that the deep fake unsupervised\n",
      "synthesis task has problems in terms of the visual quality of generated data.\n",
      "These problems usually lead to high fake detection accuracy when an expert\n",
      "analyzes them. The first problem is that existing image-to-image approaches do\n",
      "not consider video domain specificity and frame-by-frame processing leads to\n",
      "face jittering and other clearly visible distortions. Another problem is the\n",
      "generated data resolution, which is low for many existing methods due to high\n",
      "computational complexity. The third problem appears when the source face has\n",
      "larger proportions (like bigger cheeks), and after replacement it becomes\n",
      "visible on the face border. Our main goal was to develop such an approach that\n",
      "could solve these problems and outperform existing solutions on a number of\n",
      "clue metrics. We introduce a new face swap pipeline that is based on\n",
      "FaceShifter architecture and fixes the problems stated above. With a new eye\n",
      "loss function, super-resolution block, and Gaussian-based face mask generation\n",
      "leads to improvements in quality which is confirmed during evaluation. \n",
      "\n",
      "\n",
      "Modern AI tools, such as generative adversarial networks, have transformed\n",
      "our ability to create and modify visual data with photorealistic results.\n",
      "However, one of the deleterious side-effects of these advances is the emergence\n",
      "of nefarious uses in manipulating information in visual data, such as through\n",
      "the use of deep fakes. We propose a novel architecture for preserving the\n",
      "provenance of semantic information in images to make them less susceptible to\n",
      "deep fake attacks. Our architecture includes semantic signing and verification\n",
      "steps. We apply this architecture to verifying two types of semantic\n",
      "information: individual identities (faces) and whether the photo was taken\n",
      "indoors or outdoors. Verification accounts for a collection of common image\n",
      "transformation, such as translation, scaling, cropping, and small rotations,\n",
      "and rejects adversarial transformations, such as adversarially perturbed or, in\n",
      "the case of face verification, swapped faces. Experiments demonstrate that in\n",
      "the case of provenance of faces in an image, our approach is robust to\n",
      "black-box adversarial transformations (which are rejected) as well as benign\n",
      "transformations (which are accepted), with few false negatives and false\n",
      "positives. Background verification, on the other hand, is susceptible to\n",
      "black-box adversarial examples, but becomes significantly more robust after\n",
      "adversarial training. \n",
      "\n",
      "\n",
      "In this paper, we perform an in-depth study of how data augmentation\n",
      "techniques improve synthetic or spoofed audio detection. Specifically, we\n",
      "propose methods to deal with channel variability, different audio compressions,\n",
      "different band-widths, and unseen spoofing attacks, which have all been shown\n",
      "to significantly degrade the performance of audio-based systems and\n",
      "Anti-Spoofing systems. Our results are based on the ASVspoof 2021 challenge, in\n",
      "the Logical Access (LA) and Deep Fake (DF) categories. Our study is\n",
      "Data-Centric, meaning that the models are fixed and we significantly improve\n",
      "the results by making changes in the data. We introduce two forms of data\n",
      "augmentation - compression augmentation for the DF part, compression & channel\n",
      "augmentation for the LA part. In addition, a new type of online data\n",
      "augmentation, SpecAverage, is introduced in which the audio features are masked\n",
      "with their average value in order to improve generalization. Furthermore, we\n",
      "introduce a Log spectrogram feature design that improved the results. Our best\n",
      "single system and fusion scheme both achieve state-of-the-art performance in\n",
      "the DF category, with an EER of 15.46% and 14.46% respectively. Our best system\n",
      "for the LA task reduced the best baseline EER by 50% and the min t-DCF by 16%.\n",
      "Our techniques to deal with spoofed data from a wide variety of distributions\n",
      "can be replicated and can help anti-spoofing and speech-based systems enhance\n",
      "their results. \n",
      "\n",
      "\n",
      "The recent developments in technology have re-warded us with amazing audio\n",
      "synthesis models like TACOTRON and WAVENETS. On the other side, it poses\n",
      "greater threats such as speech clones and deep fakes, that may go undetected.\n",
      "To tackle these alarming situations, there is an urgent need to propose models\n",
      "that can help discriminate a synthesized speech from an actual human speech and\n",
      "also identify the source of such a synthesis. Here, we propose a model based on\n",
      "Convolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n",
      "(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\n",
      "dependencies present in AI synthesized speech are exploited using Bidirectional\n",
      "RNN and CNN. The model outperforms the state-of-the-art approaches by\n",
      "classifying the AI synthesized audio from real human speech with an error rate\n",
      "of 1.9% and detecting the underlying architecture with an accuracy of 97%. \n",
      "\n",
      "\n",
      "Faces generated using generative adversarial networks (GANs) have reached\n",
      "unprecedented realism. These faces, also known as \"Deep Fakes\", appear as\n",
      "realistic photographs with very little pixel-level distortions. While some work\n",
      "has enabled the training of models that lead to the generation of specific\n",
      "properties of the subject, generating a facial image based on a natural\n",
      "language description has not been fully explored. For security and criminal\n",
      "identification, the ability to provide a GAN-based system that works like a\n",
      "sketch artist would be incredibly useful. In this paper, we present a novel\n",
      "approach to generate facial images from semantic text descriptions. The learned\n",
      "model is provided with a text description and an outline of the type of face,\n",
      "which the model uses to sketch the features. Our models are trained using an\n",
      "Affine Combination Module (ACM) mechanism to combine the text embedding from\n",
      "BERT and the GAN latent space using a self-attention matrix. This avoids the\n",
      "loss of features due to inadequate \"attention\", which may happen if text\n",
      "embedding and latent vector are simply concatenated. Our approach is capable of\n",
      "generating images that are very accurately aligned to the exhaustive textual\n",
      "descriptions of faces with many fine detail features of the face and helps in\n",
      "generating better images. The proposed method is also capable of making\n",
      "incremental changes to a previously generated image if it is provided with\n",
      "additional textual descriptions or sentences. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GAN) are a class of powerful machine\n",
      "learning techniques, where both a generative and discriminative model are\n",
      "trained simultaneously. GANs have been used, for example, to successfully\n",
      "generate \"deep fake\" images. A recent trend in malware research consists of\n",
      "treating executables as images and employing image-based analysis techniques.\n",
      "In this research, we generate fake malware images using auxiliary classifier\n",
      "GANs (AC-GAN), and we consider the effectiveness of various techniques for\n",
      "classifying the resulting images. Our results indicate that the resulting\n",
      "multiclass classification problem is challenging, yet we can obtain strong\n",
      "results when restricting the problem to distinguishing between real and fake\n",
      "samples. While the AC-GAN generated images often appear to be very similar to\n",
      "real malware images, we conclude that from a deep learning perspective, the\n",
      "AC-GAN generated samples do not rise to the level of deep fake malware images. \n",
      "\n",
      "\n",
      "Deep Learning as a field has been successfully used to solve a plethora of\n",
      "complex problems, the likes of which we could not have imagined a few decades\n",
      "back. But as many benefits as it brings, there are still ways in which it can\n",
      "be used to bring harm to our society. Deep fakes have been proven to be one\n",
      "such problem, and now more than ever, when any individual can create a fake\n",
      "image or video simply using an application on the smartphone, there need to be\n",
      "some countermeasures, with which we can detect if the image or video is a fake\n",
      "or real and dispose of the problem threatening the trustworthiness of online\n",
      "information. Although the Deep fakes created by neural networks, may seem to be\n",
      "as real as a real image or video, it still leaves behind spatial and temporal\n",
      "traces or signatures after moderation, these signatures while being invisible\n",
      "to a human eye can be detected with the help of a neural network trained to\n",
      "specialize in Deep fake detection. In this paper, we analyze several such\n",
      "states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\n",
      "Net) and compare them against each other, to find an optimal solution for\n",
      "various scenarios like real-time deep fake detection to be deployed in online\n",
      "social media platforms where the classification should be made as fast as\n",
      "possible or for a small news agency where the classification need not be in\n",
      "real-time but requires utmost accuracy. \n",
      "\n",
      "\n",
      "Generative adversarial networks or GANs are a type of generative modeling\n",
      "framework. GANs involve a pair of neural networks engaged in a competition in\n",
      "iteratively creating fake data, indistinguishable from the real data. One\n",
      "notable application of GANs is developing fake human faces, also known as \"deep\n",
      "fakes,\" due to the deep learning algorithms at the core of the GAN framework.\n",
      "Measuring the quality of the generated images is inherently subjective but\n",
      "attempts to objectify quality using standardized metrics have been made. One\n",
      "example of objective metrics is the Frechet Inception Distance (FID), which\n",
      "measures the difference between distributions of feature vectors for two\n",
      "separate datasets of images. There are situations that images with low\n",
      "perceptual qualities are not assigned appropriate FID scores. We propose to\n",
      "improve the robustness of the evaluation process by integrating lower-level\n",
      "features to cover a wider array of visual defects. Our proposed method\n",
      "integrates three levels of feature abstractions to evaluate the quality of\n",
      "generated images. Experimental evaluations show better performance of the\n",
      "proposed method for distorted images. \n",
      "\n",
      "\n",
      "Deep fakes became extremely popular in the last years, also thanks to their\n",
      "increasing realism. Therefore, there is the need to measures human's ability to\n",
      "distinguish between real and synthetic face images when confronted with\n",
      "cutting-edge creation technologies. We describe the design and results of a\n",
      "perceptual experiment we have conducted, where a wide and diverse group of\n",
      "volunteers has been exposed to synthetic face images produced by\n",
      "state-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\n",
      "StyleGAN2). The experiment outcomes reveal how strongly we should call into\n",
      "question our human ability to discriminate real faces from synthetic ones\n",
      "generated through modern AI. \n",
      "\n",
      "\n",
      "Maliciously-manipulated images or videos - so-called deep fakes - especially\n",
      "face-swap images and videos have attracted more and more malicious attackers to\n",
      "discredit some key figures. Previous pixel-level artifacts based detection\n",
      "techniques always focus on some unclear patterns but ignore some available\n",
      "semantic clues. Therefore, these approaches show weak interpretability and\n",
      "robustness. In this paper, we propose a biometric information based method to\n",
      "fully exploit the appearance and shape feature for face-swap detection of key\n",
      "figures. The key aspect of our method is obtaining the inconsistency of 3D\n",
      "facial shape and facial appearance, and the inconsistency based clue offers\n",
      "natural interpretability for the proposed face-swap detection method.\n",
      "Experimental results show the superiority of our method in robustness on\n",
      "various laundering and cross-domain data, which validates the effectiveness of\n",
      "the proposed method. \n",
      "\n",
      "\n",
      "Since the introduction of the GDPR and CCPA legislation, both public and\n",
      "private facial image datasets are increasingly scrutinized. Several datasets\n",
      "have been taken offline completely and some have been anonymized. However, it\n",
      "is unclear how anonymization impacts face detection performance. To our\n",
      "knowledge, this paper presents the first empirical study on the effect of image\n",
      "anonymization on supervised training of face detectors. We compare conventional\n",
      "face anonymizers with three state-of-the-art Generative Adversarial\n",
      "Network-based (GAN) methods, by training an off-the-shelf face detector on\n",
      "anonymized data. Our experiments investigate the suitability of anonymization\n",
      "methods for maintaining face detector performance, the effect of detectors\n",
      "overtraining on anonymization artefacts, dataset size for training an\n",
      "anonymizer, and the effect of training time of anonymization GANs. A final\n",
      "experiment investigates the correlation between common GAN evaluation metrics\n",
      "and the performance of a trained face detector. Although all tested\n",
      "anonymization methods lower the performance of trained face detectors, faces\n",
      "anonymized using GANs cause far smaller performance degradation than\n",
      "conventional methods. As the most important finding, the best-performing GAN,\n",
      "DeepPrivacy, removes identifiable faces for a face detector trained on\n",
      "anonymized data, resulting in a modest decrease from 91.0 to 88.3 mAP. In the\n",
      "last few years, there have been rapid improvements in realism of GAN-generated\n",
      "faces. We expect that further progression in GAN research will allow the use of\n",
      "Deep Fake technology for privacy-preserving Safe Fakes, without any performance\n",
      "degradation for training face detectors. \n",
      "\n",
      "\n",
      "Online misinformation is a prevalent societal issue, with adversaries relying\n",
      "on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated\n",
      "by the threat scenario where an image is used out of context to support a\n",
      "certain narrative. While some prior datasets for detecting image-text\n",
      "inconsistency generate samples via text manipulation, we propose a dataset\n",
      "where both image and text are unmanipulated but mismatched. We introduce\n",
      "several strategies for automatically retrieving convincing images for a given\n",
      "caption, capturing cases with inconsistent entities or semantic context. Our\n",
      "large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates\n",
      "that machine-driven image repurposing is now a realistic threat, and (2)\n",
      "provides samples that represent challenging instances of mismatch between text\n",
      "and image in news that are able to mislead humans. We benchmark several\n",
      "state-of-the-art multimodal models on our dataset and analyze their performance\n",
      "across different pretraining domains and visual backbones. \n",
      "\n",
      "\n",
      "We present a novel conditional Generative Adversarial Network (cGAN)\n",
      "architecture that is capable of generating 3D Computed Tomography scans in\n",
      "voxels from noisy and/or pixelated approximations and with the potential to\n",
      "generate full synthetic 3D scan volumes. We believe conditional cGAN to be a\n",
      "tractable approach to generate 3D CT volumes, even though the problem of\n",
      "generating full resolution deep fakes is presently impractical due to GPU\n",
      "memory limitations. We present results for autoencoder, denoising, and\n",
      "depixelating tasks which are trained and tested on two novel COVID19 CT\n",
      "datasets. Our evaluation metrics, Peak Signal to Noise ratio (PSNR) range from\n",
      "12.53 - 46.46 dB, and the Structural Similarity index ( SSIM) range from 0.89\n",
      "to 1. \n",
      "\n",
      "\n",
      "Deep generative models (DGM) are neural networks with many hidden layers\n",
      "trained to approximate complicated, high-dimensional probability distributions\n",
      "using a large number of samples. When trained successfully, we can use the DGMs\n",
      "to estimate the likelihood of each observation and to create new samples from\n",
      "the underlying distribution. Developing DGMs has become one of the most hotly\n",
      "researched fields in artificial intelligence in recent years. The literature on\n",
      "DGMs has become vast and is growing rapidly. Some advances have even reached\n",
      "the public sphere, for example, the recent successes in generating\n",
      "realistic-looking images, voices, or movies; so-called deep fakes. Despite\n",
      "these successes, several mathematical and practical issues limit the broader\n",
      "use of DGMs: given a specific dataset, it remains challenging to design and\n",
      "train a DGM and even more challenging to find out why a particular model is or\n",
      "is not effective. To help advance the theoretical understanding of DGMs, we\n",
      "introduce DGMs and provide a concise mathematical framework for modeling the\n",
      "three most popular approaches: normalizing flows (NF), variational autoencoders\n",
      "(VAE), and generative adversarial networks (GAN). We illustrate the advantages\n",
      "and disadvantages of these basic approaches using numerical experiments. Our\n",
      "goal is to enable and motivate the reader to contribute to this proliferating\n",
      "research area. Our presentation also emphasizes relations between generative\n",
      "modeling and optimal transport. \n",
      "\n",
      "\n",
      "Following the recent initiatives for the democratization of AI, deep fake\n",
      "generators have become increasingly popular and accessible, causing dystopian\n",
      "scenarios towards social erosion of trust. A particular domain, such as\n",
      "biological signals, attracted attention towards detection methods that are\n",
      "capable of exploiting authenticity signatures in real videos that are not yet\n",
      "faked by generative approaches. In this paper, we first propose several\n",
      "prominent eye and gaze features that deep fakes exhibit differently. Second, we\n",
      "compile those features into signatures and analyze and compare those of real\n",
      "and fake videos, formulating geometric, visual, metric, temporal, and spectral\n",
      "variations. Third, we generalize this formulation to the deep fake detection\n",
      "problem by a deep neural network, to classify any video in the wild as fake or\n",
      "real. We evaluate our approach on several deep fake datasets, achieving 92.48%\n",
      "accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on\n",
      "CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most\n",
      "deep and biological fake detectors with complex network architectures without\n",
      "the proposed gaze signatures. We conduct ablation studies involving different\n",
      "features, architectures, sequence durations, and post-processing artifacts. \n",
      "\n",
      "\n",
      "Over the past years, deep generative models have achieved a new level of\n",
      "performance. Generated data has become difficult, if not impossible, to be\n",
      "distinguished from real data. While there are plenty of use cases that benefit\n",
      "from this technology, there are also strong concerns on how this new technology\n",
      "can be misused to generate deep fakes and enable misinformation at scale.\n",
      "Unfortunately, current deep fake detection methods are not sustainable, as the\n",
      "gap between real and fake continues to close. In contrast, our work enables a\n",
      "responsible disclosure of such state-of-the-art generative models, that allows\n",
      "model inventors to fingerprint their models, so that the generated samples\n",
      "containing a fingerprint can be accurately detected and attributed to a source.\n",
      "Our technique achieves this by an efficient and scalable ad-hoc generation of a\n",
      "large population of models with distinct fingerprints. Our recommended\n",
      "operation point uses a 128-bit fingerprint which in principle results in more\n",
      "than $10^{38}$ identifiable models. Experiments show that our method fulfills\n",
      "key properties of a fingerprinting mechanism and achieves effectiveness in deep\n",
      "fake detection and attribution. Code and models are available at\n",
      "https://github.com/ningyu1991/ScalableGANFingerprints . \n",
      "\n",
      "\n",
      "This paper tackles the problem of verifying the authenticity of speech\n",
      "recordings from world leaders. Whereas previous work on detecting deep fake or\n",
      "tampered audio focus on scrutinizing an audio recording in isolation, we\n",
      "instead reframe the problem and focus on cross-verifying a questionable\n",
      "recording against trusted references. We present a method for cross-verifying a\n",
      "speech recording against a reference that consists of two steps: aligning the\n",
      "two recordings and then classifying each query frame as matching or\n",
      "non-matching. We propose a subsequence alignment method based on the\n",
      "Needleman-Wunsch algorithm and show that it significantly outperforms dynamic\n",
      "time warping in handling common tampering operations. We also explore several\n",
      "binary classification models based on LSTM and Transformer architectures to\n",
      "verify content at the frame level. Through extensive experiments on tampered\n",
      "speech recordings of Donald Trump, we show that our system can reliably detect\n",
      "audio tampering operations of different types and durations. Our best model\n",
      "achieves 99.7% accuracy for the alignment task at an error tolerance of 50 ms\n",
      "and a 0.43% equal error rate in classifying audio frames as matching or\n",
      "non-matching. \n",
      "\n",
      "\n",
      "Digital technology has made possible unimaginable applications come true. It\n",
      "seems exciting to have a handful of tools for easy editing and manipulation,\n",
      "but it raises alarming concerns that can propagate as speech clones,\n",
      "duplicates, or maybe deep fakes. Validating the authenticity of a speech is one\n",
      "of the primary problems of digital audio forensics. We propose an approach to\n",
      "distinguish human speech from AI synthesized speech exploiting the Bi-spectral\n",
      "and Cepstral analysis. Higher-order statistics have less correlation for human\n",
      "speech in comparison to a synthesized speech. Also, Cepstral analysis revealed\n",
      "a durable power component in human speech that is missing for a synthesized\n",
      "speech. We integrate both these analyses and propose a machine learning model\n",
      "to detect AI synthesized speech. \n",
      "\n",
      "\n",
      "Fake portrait video generation techniques have been posing a new threat to\n",
      "the society with photorealistic deep fakes for political propaganda, celebrity\n",
      "imitation, forged evidences, and other identity related manipulations.\n",
      "Following these generation techniques, some detection approaches have also been\n",
      "proved useful due to their high classification accuracy. Nevertheless, almost\n",
      "no effort was spent to track down the source of deep fakes. We propose an\n",
      "approach not only to separate deep fakes from real videos, but also to discover\n",
      "the specific generative model behind a deep fake. Some pure deep learning based\n",
      "approaches try to classify deep fakes using CNNs where they actually learn the\n",
      "residuals of the generator. We believe that these residuals contain more\n",
      "information and we can reveal these manipulation artifacts by disentangling\n",
      "them with biological signals. Our key observation yields that the\n",
      "spatiotemporal patterns in biological signals can be conceived as a\n",
      "representative projection of residuals. To justify this observation, we extract\n",
      "PPG cells from real and fake videos and feed these to a state-of-the-art\n",
      "classification network for detecting the generative model per video. Our\n",
      "results indicate that our approach can detect fake videos with 97.29% accuracy,\n",
      "and the source model with 93.39% accuracy. \n",
      "\n",
      "\n",
      "Synthetically-generated audios and videos -- so-called deep fakes -- continue\n",
      "to capture the imagination of the computer-graphics and computer-vision\n",
      "communities. At the same time, the democratization of access to technology that\n",
      "can create sophisticated manipulated video of anybody saying anything continues\n",
      "to be of concern because of its power to disrupt democratic elections, commit\n",
      "small to large-scale fraud, fuel dis-information campaigns, and create\n",
      "non-consensual pornography. We describe a biometric-based forensic technique\n",
      "for detecting face-swap deep fakes. This technique combines a static biometric\n",
      "based on facial recognition with a temporal, behavioral biometric based on\n",
      "facial expressions and head movements, where the behavioral embedding is\n",
      "learned using a CNN with a metric-learning objective function. We show the\n",
      "efficacy of this approach across several large-scale video datasets, as well as\n",
      "in-the-wild deep fakes. \n",
      "\n",
      "\n",
      "Deep neural networks can generate images that are astonishingly realistic, so\n",
      "much so that it is often hard for humans to distinguish them from actual\n",
      "photos. These achievements have been largely made possible by Generative\n",
      "Adversarial Networks (GANs). While deep fake images have been thoroughly\n",
      "investigated in the image domain - a classical approach from the area of image\n",
      "forensics - an analysis in the frequency domain has been missing so far. In\n",
      "this paper, we address this shortcoming and our results reveal that in\n",
      "frequency space, GAN-generated images exhibit severe artifacts that can be\n",
      "easily identified. We perform a comprehensive analysis, showing that these\n",
      "artifacts are consistent across different neural network architectures, data\n",
      "sets, and resolutions. In a further investigation, we demonstrate that these\n",
      "artifacts are caused by upsampling operations found in all current GAN\n",
      "architectures, indicating a structural and fundamental problem in the way\n",
      "images are generated via GANs. Based on this analysis, we demonstrate how the\n",
      "frequency representation can be used to identify deep fake images in an\n",
      "automated way, surpassing state-of-the-art methods. \n",
      "\n",
      "\n",
      "Generative AI is a class of machine learning technology that learns to\n",
      "generate new data from training data. While deep fakes and media-and\n",
      "art-related generative AI breakthroughs have recently caught people's attention\n",
      "and imagination, the overall area is in its infancy for business use. Further,\n",
      "little is known about generative AI's potential for malicious misuse at large\n",
      "scale. Using co-creation design fictions with AI engineers, we explore the\n",
      "plausibility and severity of business misuse cases. \n",
      "\n",
      "\n",
      "We introduce some new forensics based on differential imaging, where a novel\n",
      "category of visual evidence created via subtle interactions of light with a\n",
      "scene, such as dim reflections, can be computationally extracted and amplified\n",
      "from an image of interest through a comparative analysis with an additional\n",
      "reference baseline image acquired under similar conditions. This paradigm of\n",
      "differential imaging forensics (DIF) enables forensic examiners for the first\n",
      "time to retrieve the said visual evidence that is readily available in an image\n",
      "or video footage but would otherwise remain faint or even invisible to a human\n",
      "observer. We demonstrate the relevance and effectiveness of our approach\n",
      "through practical experiments. We also show that DIF provides a novel method\n",
      "for detecting forged images and video clips, including deep fakes. \n",
      "\n",
      "\n",
      "With advances in Generative Adversarial Networks (GANs) leading to\n",
      "dramatically-improved synthetic images and video, there is an increased need\n",
      "for algorithms which extend traditional forensics to this new category of\n",
      "imagery. While GANs have been shown to be helpful in a number of computer\n",
      "vision applications, there are other problematic uses such as `deep fakes'\n",
      "which necessitate such forensics. Source camera attribution algorithms using\n",
      "various cues have addressed this need for imagery captured by a camera, but\n",
      "there are fewer options for synthetic imagery. We address the problem of\n",
      "attributing a synthetic image to a specific generator in a white box setting,\n",
      "by inverting the process of generation. This enables us to simultaneously\n",
      "determine whether the generator produced the image and recover an input which\n",
      "produces a close match to the synthetic image. \n",
      "\n",
      "\n",
      "The recent proliferation of fake portrait videos poses direct threats on\n",
      "society, law, and privacy. Believing the fake video of a politician,\n",
      "distributing fake pornographic content of celebrities, fabricating impersonated\n",
      "fake videos as evidence in courts are just a few real world consequences of\n",
      "deep fakes. We present a novel approach to detect synthetic content in portrait\n",
      "videos, as a preventive solution for the emerging threat of deep fakes. In\n",
      "other words, we introduce a deep fake detector. We observe that detectors\n",
      "blindly utilizing deep learning are not effective in catching fake content, as\n",
      "generative models produce formidably realistic results. Our key assertion\n",
      "follows that biological signals hidden in portrait videos can be used as an\n",
      "implicit descriptor of authenticity, because they are neither spatially nor\n",
      "temporally preserved in fake content. To prove and exploit this assertion, we\n",
      "first engage several signal transformations for the pairwise separation\n",
      "problem, achieving 99.39% accuracy. Second, we utilize those findings to\n",
      "formulate a generalized classifier for fake content, by analyzing proposed\n",
      "signal transformations and corresponding feature sets. Third, we generate novel\n",
      "signal maps and employ a CNN to improve our traditional classifier for\n",
      "detecting synthetic content. Lastly, we release an \"in the wild\" dataset of\n",
      "fake portrait videos that we collected as a part of our evaluation process. We\n",
      "evaluate FakeCatcher on several datasets, resulting with 96%, 94.65%, 91.50%,\n",
      "and 91.07% accuracies, on Face Forensics, Face Forensics++, CelebDF, and on our\n",
      "new Deep Fakes Dataset respectively. We also analyze signals from various\n",
      "facial regions, under image distortions, with varying segment durations, from\n",
      "different generators, against unseen datasets, and under several dimensionality\n",
      "reduction techniques. \n",
      "\n",
      "\n",
      "In this paper, we propose a new method to expose AI-generated fake face\n",
      "images or videos (commonly known as the Deep Fakes). Our method is based on the\n",
      "observations that Deep Fakes are created by splicing synthesized face region\n",
      "into the original image, and in doing so, introducing errors that can be\n",
      "revealed when 3D head poses are estimated from the face images. We perform\n",
      "experiments to demonstrate this phenomenon and further develop a classification\n",
      "method based on this cue. Using features based on this cue, an SVM classifier\n",
      "is evaluated using a set of real face images and Deep Fakes. \n",
      "\n",
      "\n",
      "Training multilingual Neural Text-To-Speech (NTTS) models using only\n",
      "monolingual corpora has emerged as a popular way for building voice cloning\n",
      "based Polyglot NTTS systems. In order to train these models, it is essential to\n",
      "understand how the composition of the training corpora affects the quality of\n",
      "multilingual speech synthesis. In this context, it is common to hear questions\n",
      "such as \"Would including more Spanish data help my Italian synthesis, given the\n",
      "closeness of both languages?\". Unfortunately, we found existing literature on\n",
      "the topic lacking in completeness in this regard. In the present work, we\n",
      "conduct an extensive ablation study aimed at understanding how various factors\n",
      "of the training corpora, such as language family affiliation, gender\n",
      "composition, and the number of speakers, contribute to the quality of Polyglot\n",
      "synthesis. Our findings include the observation that female speaker data are\n",
      "preferred in most scenarios, and that it is not always beneficial to have more\n",
      "speakers from the target language variant in the training corpus. The findings\n",
      "herein are informative for the process of data procurement and corpora\n",
      "building. \n",
      "\n",
      "\n",
      "In this paper, we propose dictionary attacks against speaker verification - a\n",
      "novel attack vector that aims to match a large fraction of speaker population\n",
      "by chance. We introduce a generic formulation of the attack that can be used\n",
      "with various speech representations and threat models. The attacker uses\n",
      "adversarial optimization to maximize raw similarity of speaker embeddings\n",
      "between a seed speech sample and a proxy population. The resulting master voice\n",
      "successfully matches a non-trivial fraction of people in an unknown population.\n",
      "Adversarial waveforms obtained with our approach can match on average 69% of\n",
      "females and 38% of males enrolled in the target system at a strict decision\n",
      "threshold calibrated to yield false alarm rate of 1%. By using the attack with\n",
      "a black-box voice cloning system, we obtain master voices that are effective in\n",
      "the most challenging conditions and transferable between speaker encoders. We\n",
      "also show that, combined with multiple attempts, this attack opens even more to\n",
      "serious issues on the security of these systems. \n",
      "\n",
      "\n",
      "Voice cloning is a difficult task which requires robust and informative\n",
      "features incorporated in a high quality TTS system in order to effectively copy\n",
      "an unseen speaker's voice. In our work, we utilize features learned in a\n",
      "self-supervised framework via the Bootstrap Your Own Latent (BYOL) method,\n",
      "which is shown to produce high quality speech representations when specific\n",
      "audio augmentations are applied to the vanilla algorithm. We further extend the\n",
      "augmentations in the training procedure to aid the resulting features to\n",
      "capture the speaker identity and to make them robust to noise and acoustic\n",
      "conditions. The learned features are used as pre-trained utterance-level\n",
      "embeddings and as inputs to a Non-Attentive Tacotron based architecture, aiming\n",
      "to achieve multispeaker speech synthesis without utilizing additional speaker\n",
      "features. This method enables us to train our model in an unlabeled\n",
      "multispeaker dataset as well as use unseen speaker embeddings to copy a\n",
      "speaker's voice. Subjective and objective evaluations are used to validate the\n",
      "proposed model, as well as the robustness to the acoustic conditions of the\n",
      "target utterance. \n",
      "\n",
      "\n",
      "Recently, few-shot voice cloning has achieved a significant improvement.\n",
      "However, most models for few-shot voice cloning are single-modal, and\n",
      "multi-modal few-shot voice cloning has been understudied. In this paper, we\n",
      "propose to use multi-modal learning to improve the few-shot voice cloning\n",
      "performance. Inspired by the recent works on unsupervised speech\n",
      "representation, the proposed multi-modal system is built by extending Tacotron2\n",
      "with an unsupervised speech representation module. We evaluate our proposed\n",
      "system in two few-shot voice cloning scenarios, namely few-shot\n",
      "text-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate\n",
      "that the proposed multi-modal learning can significantly improve the few-shot\n",
      "voice cloning performance over their counterpart single-modal systems. \n",
      "\n",
      "\n",
      "With recent advancements in voice cloning, the performance of speech\n",
      "synthesis for a target speaker has been rendered similar to the human level.\n",
      "However, autoregressive voice cloning systems still suffer from text alignment\n",
      "failures, resulting in an inability to synthesize long sentences. In this work,\n",
      "we propose a variant of attention-based text-to-speech system that can\n",
      "reproduce a target voice from a few seconds of reference speech and generalize\n",
      "to very long utterances as well. The proposed system is based on three\n",
      "independently trained components: a speaker encoder, synthesizer and universal\n",
      "vocoder. Generalization to long utterances is realized using an energy-based\n",
      "attention mechanism known as Dynamic Convolution Attention, in combination with\n",
      "a set of modifications proposed for the synthesizer based on Tacotron 2.\n",
      "Moreover, effective zero-shot speaker adaptation is achieved by conditioning\n",
      "both the synthesizer and vocoder on a speaker encoder that has been pretrained\n",
      "on a large corpus of diverse data. We compare several implementations of voice\n",
      "cloning systems in terms of speech naturalness, speaker similarity, alignment\n",
      "consistency and ability to synthesize long utterances, and conclude that the\n",
      "proposed model can produce intelligible synthetic speech for extremely long\n",
      "utterances, while preserving a high extent of naturalness and similarity for\n",
      "short texts. \n",
      "\n",
      "\n",
      "Existing Voice Cloning (VC) tasks aim to convert a paragraph text to a speech\n",
      "with desired voice specified by a reference audio. This has significantly\n",
      "boosted the development of artificial speech applications. However, there also\n",
      "exist many scenarios that cannot be well reflected by these VC tasks, such as\n",
      "movie dubbing, which requires the speech to be with emotions consistent with\n",
      "the movie plots. To fill this gap, in this work we propose a new task named\n",
      "Visual Voice Cloning (V2C), which seeks to convert a paragraph of text to a\n",
      "speech with both desired voice specified by a reference audio and desired\n",
      "emotion specified by a reference video. To facilitate research in this field,\n",
      "we construct a dataset, V2C-Animation, and propose a strong baseline based on\n",
      "existing state-of-the-art (SoTA) VC techniques. Our dataset contains 10,217\n",
      "animated movie clips covering a large variety of genres (e.g., Comedy, Fantasy)\n",
      "and emotions (e.g., happy, sad). We further design a set of evaluation metrics,\n",
      "named MCD-DTW-SL, which help evaluate the similarity between ground-truth\n",
      "speeches and the synthesised ones. Extensive experimental results show that\n",
      "even SoTA VC methods cannot generate satisfying speeches for our V2C task. We\n",
      "hope the proposed new task together with the constructed dataset and evaluation\n",
      "metric will facilitate the research in the field of voice cloning and the\n",
      "broader vision-and-language community. \n",
      "\n",
      "\n",
      "The task of few-shot style transfer for voice cloning in text-to-speech (TTS)\n",
      "synthesis aims at transferring speaking styles of an arbitrary source speaker\n",
      "to a target speaker's voice using very limited amount of neutral data. This is\n",
      "a very challenging task since the learning algorithm needs to deal with\n",
      "few-shot voice cloning and speaker-prosody disentanglement at the same time.\n",
      "Accelerating the adaptation process for a new target speaker is of importance\n",
      "in real-world applications, but even more challenging. In this paper, we\n",
      "approach to the hard fast few-shot style transfer for voice cloning task using\n",
      "meta learning. We investigate the model-agnostic meta-learning (MAML) algorithm\n",
      "and meta-transfer a pre-trained multi-speaker and multi-prosody base TTS model\n",
      "to be highly sensitive for adaptation with few samples. Domain adversarial\n",
      "training mechanism and orthogonal constraint are adopted to disentangle speaker\n",
      "and prosody representations for effective cross-speaker style transfer.\n",
      "Experimental results show that the proposed approach is able to conduct fast\n",
      "voice cloning using only 5 samples (around 12 second speech data) from a target\n",
      "speaker, with only 100 adaptation steps. Audio samples are available online. \n",
      "\n",
      "\n",
      "Nowadays, as more and more systems achieve good performance in traditional\n",
      "voice conversion (VC) tasks, people's attention gradually turns to VC tasks\n",
      "under extreme conditions. In this paper, we propose a novel method for\n",
      "zero-shot voice conversion. We aim to obtain intermediate representations for\n",
      "speaker-content disentanglement of speech to better remove speaker information\n",
      "and get pure content information. Accordingly, our proposed framework contains\n",
      "a module that removes the speaker information from the acoustic feature of the\n",
      "source speaker. Moreover, speaker information control is added to our system to\n",
      "maintain the voice cloning performance. The proposed system is evaluated by\n",
      "subjective and objective metrics. Results show that our proposed system\n",
      "significantly reduces the trade-off problem in zero-shot voice conversion,\n",
      "while it also manages to have high spoofing power to the speaker verification\n",
      "system. \n",
      "\n",
      "\n",
      "Recently, sequence-to-sequence (seq-to-seq) models have been successfully\n",
      "applied in text-to-speech (TTS) to synthesize speech for single-language text.\n",
      "To synthesize speech for multiple languages usually requires multi-lingual\n",
      "speech from the target speaker. However, it is both laborious and expensive to\n",
      "collect high-quality multi-lingual TTS data for the target speakers. In this\n",
      "paper, we proposed to use low-quality code-switched found data from the\n",
      "non-target speakers to achieve cross-lingual voice cloning for the target\n",
      "speakers. Experiments show that our proposed method can generate high-quality\n",
      "code-switched speech in the target voices in terms of both naturalness and\n",
      "speaker consistency. More importantly, we find that our method can achieve a\n",
      "comparable result to the state-of-the-art (SOTA) performance in cross-lingual\n",
      "voice cloning. \n",
      "\n",
      "\n",
      "In this paper, we study the disentanglement of speaker and language\n",
      "representations in non-autoregressive cross-lingual TTS models from various\n",
      "aspects. We propose a phoneme length regulator that solves the length mismatch\n",
      "problem between IPA input sequence and monolingual alignment results. Using the\n",
      "phoneme length regulator, we present a FastPitch-based cross-lingual model with\n",
      "IPA symbols as input representations. Our experiments show that\n",
      "language-independent input representations (e.g. IPA symbols), an increasing\n",
      "number of training speakers, and explicit modeling of speech variance\n",
      "information all encourage non-autoregressive cross-lingual TTS model to\n",
      "disentangle speaker and language representations. The subjective evaluation\n",
      "shows that our proposed model can achieve decent naturalness and speaker\n",
      "similarity in cross-language voice cloning. \n",
      "\n",
      "\n",
      "International Phonetic Alphabet (IPA) has been widely used in cross-lingual\n",
      "text-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\n",
      "IPA itself has been understudied in cross-lingual TTS. In this paper, we report\n",
      "some empirical findings of building a cross-lingual TTS model using IPA as\n",
      "inputs. Experiments show that the way to process the IPA and suprasegmental\n",
      "sequence has a negligible impact on the CL VC performance. Furthermore, we find\n",
      "that using a dataset including one speaker per language to build an IPA-based\n",
      "TTS system would fail CL VC since the language-unique IPA and tone/stress\n",
      "symbols could leak the speaker information. In addition, we experiment with\n",
      "different combinations of speakers in the training dataset to further\n",
      "investigate the effect of the number of speakers on the CL VC performance. \n",
      "\n",
      "\n",
      "Latent variable discovery is a central problem in data analysis with a broad\n",
      "range of applications in applied science. In this work, we consider data given\n",
      "as an invertible mixture of two statistically independent components, and\n",
      "assume that one of the components is observed while the other is hidden. Our\n",
      "goal is to recover the hidden component. For this purpose, we propose an\n",
      "autoencoder equipped with a discriminator. Unlike the standard nonlinear ICA\n",
      "problem, which was shown to be non-identifiable, in the special case of ICA we\n",
      "consider here, we show that our approach can recover the component of interest\n",
      "up to entropy-preserving transformation. We demonstrate the performance of the\n",
      "proposed approach on several datasets, including image synthesis, voice\n",
      "cloning, and fetal ECG extraction. \n",
      "\n",
      "\n",
      "Training neural text-to-speech (TTS) models for a new speaker typically\n",
      "requires several hours of high quality speech data. Prior works on voice\n",
      "cloning attempt to address this challenge by adapting pre-trained multi-speaker\n",
      "TTS models for a new voice, using a few minutes of speech data of the new\n",
      "speaker. However, publicly available large multi-speaker datasets are often\n",
      "noisy, thereby resulting in TTS models that are not suitable for use in\n",
      "products. We address this challenge by proposing transfer-learning guidelines\n",
      "for adapting high quality single-speaker TTS models for a new speaker, using\n",
      "only a few minutes of speech data. We conduct an extensive study using\n",
      "different amounts of data for a new speaker and evaluate the synthesized speech\n",
      "in terms of naturalness and voice/style similarity to the target speaker. We\n",
      "find that fine-tuning a single-speaker TTS model on just 30 minutes of data,\n",
      "can yield comparable performance to a model trained from scratch on more than\n",
      "27 hours of data for both male and female target speakers. \n",
      "\n",
      "\n",
      "One-shot voice cloning aims to transform speaker voice and speaking style in\n",
      "speech synthesized from a text-to-speech (TTS) system, where only a shot\n",
      "recording from the target reference speech can be used. Out-of-domain transfer\n",
      "is still a challenging task, and one important aspect that impacts the accuracy\n",
      "and similarity of synthetic speech is the conditional representations carrying\n",
      "speaker or style cues extracted from the limited references. In this paper, we\n",
      "present a novel one-shot voice cloning algorithm called Unet-TTS that has good\n",
      "generalization ability for unseen speakers and styles. Based on a\n",
      "skip-connected U-net structure, the new model can efficiently discover\n",
      "speaker-level and utterance-level spectral feature details from the reference\n",
      "audio, enabling accurate inference of complex acoustic characteristics as well\n",
      "as imitation of speaking styles into the synthetic speech. According to both\n",
      "subjective and objective evaluations of similarity, the new model outperforms\n",
      "both speaker embedding and unsupervised style modeling (GST) approaches on an\n",
      "unseen emotional corpus. \n",
      "\n",
      "\n",
      "Speech synthesis, voice cloning, and voice conversion techniques present\n",
      "severe privacy and security threats to users of voice user interfaces (VUIs).\n",
      "These techniques transform one or more elements of a speech signal, e.g.,\n",
      "identity and emotion, while preserving linguistic information. Adversaries may\n",
      "use advanced transformation tools to trigger a spoofing attack using fraudulent\n",
      "biometrics for a legitimate speaker. Conversely, such techniques have been used\n",
      "to generate privacy-transformed speech by suppressing personally identifiable\n",
      "attributes in the voice signals, achieving anonymization. Prior works have\n",
      "studied the security and privacy vectors in parallel, and thus it raises alarm\n",
      "that if a benign user can achieve privacy by a transformation, it also means\n",
      "that a malicious user can break security by bypassing the anti-spoofing\n",
      "mechanism. In this paper, we take a step towards balancing two seemingly\n",
      "conflicting requirements: security and privacy. It remains unclear what the\n",
      "vulnerabilities in one domain imply for the other, and what dynamic\n",
      "interactions exist between them. A better understanding of these aspects is\n",
      "crucial for assessing and mitigating vulnerabilities inherent with VUIs and\n",
      "building effective defenses. In this paper,(i) we investigate the applicability\n",
      "of the current voice anonymization methods by deploying a tandem framework that\n",
      "jointly combines anti-spoofing and authentication models, and evaluate the\n",
      "performance of these methods;(ii) examining analytical and empirical evidence,\n",
      "we reveal a duality between the two mechanisms as they offer different ways to\n",
      "achieve the same objective, and we show that leveraging one vector\n",
      "significantly amplifies the effectiveness of the other;(iii) we demonstrate\n",
      "that to effectively defend from potential attacks against VUIs, it is necessary\n",
      "to investigate the attacks from multiple complementary perspectives(security\n",
      "and privacy). \n",
      "\n",
      "\n",
      "We present Translatotron 2, a neural direct speech-to-speech translation\n",
      "model that can be trained end-to-end. Translatotron 2 consists of a speech\n",
      "encoder, a linguistic decoder, an acoustic synthesizer, and a single attention\n",
      "module that connects them together. Experimental results on three datasets\n",
      "consistently show that Translatotron 2 outperforms the original Translatotron\n",
      "by a large margin on both translation quality (up to +15.5 BLEU) and speech\n",
      "generation quality, and approaches the same of cascade systems. In addition, we\n",
      "propose a simple method for preserving speakers' voices from the source speech\n",
      "to the translation speech in a different language. Unlike existing approaches,\n",
      "the proposed method is able to preserve each speaker's voice on speaker turns\n",
      "without requiring for speaker segmentation. Furthermore, compared to existing\n",
      "approaches, it better preserves speaker's privacy and mitigates potential\n",
      "misuse of voice cloning for creating spoofing audio artifacts. \n",
      "\n",
      "\n",
      "In this paper, we propose an architecture to solve a novel problem statement\n",
      "that has stemmed more so in recent times with an increase in demand for virtual\n",
      "content delivery due to the COVID-19 pandemic. All educational institutions,\n",
      "workplaces, research centers, etc. are trying to bridge the gap of\n",
      "communication during these socially distanced times with the use of online\n",
      "content delivery. The trend now is to create presentations, and then\n",
      "subsequently deliver the same using various virtual meeting platforms. The time\n",
      "being spent in such creation of presentations and delivering is what we try to\n",
      "reduce and eliminate through this paper which aims to use Machine Learning (ML)\n",
      "algorithms and Natural Language Processing (NLP) modules to automate the\n",
      "process of creating a slides-based presentation from a document, and then use\n",
      "state-of-the-art voice cloning models to deliver the content in the desired\n",
      "author's voice. We consider a structured document such as a research paper to\n",
      "be the content that has to be presented. The research paper is first summarized\n",
      "using BERT summarization techniques and condensed into bullet points that go\n",
      "into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\n",
      "a Generative Adversarial Network (GAN) based vocoder, is used to convey the\n",
      "contents of the slides in the author's voice (or any customized voice). Almost\n",
      "all learning has now been shifted to online mode, and professionals are now\n",
      "working from the comfort of their homes. Due to the current situation, teachers\n",
      "and professionals have shifted to presentations to help them in imparting\n",
      "information. In this paper, we aim to reduce the considerable amount of time\n",
      "that is taken in creating a presentation by automating this process and\n",
      "subsequently delivering this presentation in a customized voice, using a\n",
      "content delivery mechanism that can clone any voice using a short audio clip. \n",
      "\n",
      "\n",
      "Video represents the majority of internet traffic today, driving a continual\n",
      "race between the generation of higher quality content, transmission of larger\n",
      "file sizes, and the development of network infrastructure. In addition, the\n",
      "recent COVID-19 pandemic fueled a surge in the use of video conferencing tools.\n",
      "Since videos take up considerable bandwidth (~100 Kbps to a few Mbps), improved\n",
      "video compression can have a substantial impact on network performance for live\n",
      "and pre-recorded content, providing broader access to multimedia content\n",
      "worldwide. We present a novel video compression pipeline, called Txt2Vid, which\n",
      "dramatically reduces data transmission rates by compressing webcam videos\n",
      "(\"talking-head videos\") to a text transcript. The text is transmitted and\n",
      "decoded into a realistic reconstruction of the original video using recent\n",
      "advances in deep learning based voice cloning and lip syncing models. Our\n",
      "generative pipeline achieves two to three orders of magnitude reduction in the\n",
      "bitrate as compared to the standard audio-video codecs (encoders-decoders),\n",
      "while maintaining equivalent Quality-of-Experience based on a subjective\n",
      "evaluation by users (n = 242) in an online study. The Txt2Vid framework opens\n",
      "up the potential for creating novel applications such as enabling audio-video\n",
      "communication during poor internet connectivity, or in remote terrains with\n",
      "limited bandwidth. The code for this work is available at\n",
      "https://github.com/tpulkit/txt2vid.git. \n",
      "\n",
      "\n",
      "Generally speaking, the main objective when training a neural speech\n",
      "synthesis system is to synthesize natural and expressive speech from the output\n",
      "layer of the neural network without much attention given to the hidden layers.\n",
      "However, by learning useful latent representation, the system can be used for\n",
      "many more practical scenarios. In this paper, we investigate the use of\n",
      "quantized vectors to model the latent linguistic embedding and compare it with\n",
      "the continuous counterpart. By enforcing different policies over the latent\n",
      "spaces in the training, we are able to obtain a latent linguistic embedding\n",
      "that takes on different properties while having a similar performance in terms\n",
      "of quality and speaker similarity. Our experiments show that the voice cloning\n",
      "system built with vector quantization has only a small degradation in terms of\n",
      "perceptive evaluations, but has a discrete latent space that is useful for\n",
      "reducing the representation bit-rate, which is desirable for data transferring,\n",
      "or limiting the information leaking, which is important for speaker\n",
      "anonymization and other tasks of that nature. \n",
      "\n",
      "\n",
      "Building cross-lingual voice conversion (VC) systems for multiple speakers\n",
      "and multiple languages has been a challenging task for a long time. This paper\n",
      "describes a parallel non-autoregressive network to achieve bilingual and\n",
      "code-switched voice conversion for multiple speakers when there are only\n",
      "mono-lingual corpora for each language. We achieve cross-lingual VC between\n",
      "Mandarin speech with multiple speakers and English speech with multiple\n",
      "speakers by applying bilingual bottleneck features. To boost voice cloning\n",
      "performance, we use an adversarial speaker classifier with a gradient reversal\n",
      "layer to reduce the source speaker's information from the output of encoder.\n",
      "Furthermore, in order to improve speaker similarity between reference speech\n",
      "and converted speech, we adopt an embedding consistency loss between the\n",
      "synthesized speech and its natural reference speech in our network.\n",
      "Experimental results show that our proposed method can achieve high quality\n",
      "converted speech with mean opinion score (MOS) around 4. The conversion system\n",
      "performs well in terms of speaker similarity for both in-set speaker conversion\n",
      "and out-set-of one-shot conversion. \n",
      "\n",
      "\n",
      "This paper describes the AS-NU systems for two tracks in MultiSpeaker\n",
      "Multi-Style Voice Cloning Challenge (M2VoC). The first track focuses on using a\n",
      "small number of 100 target utterances for voice cloning, while the second track\n",
      "focuses on using only 5 target utterances for voice cloning. Due to the serious\n",
      "lack of data in the second track, we selected the speaker most similar to the\n",
      "target speaker from the training data of the TTS system, and used the speaker's\n",
      "utterances and the given 5 target utterances to fine-tune our model. The\n",
      "evaluation results show that our systems on the two tracks perform similarly in\n",
      "terms of quality, but there is still a clear gap between the similarity score\n",
      "of the second track and the similarity score of the first track. \n",
      "\n",
      "\n",
      "The Multi-speaker Multi-style Voice Cloning Challenge (M2VoC) aims to provide\n",
      "a common sizable dataset as well as a fair testbed for the benchmarking of the\n",
      "popular voice cloning task. Specifically, we formulate the challenge to adapt\n",
      "an average TTS model to the stylistic target voice with limited data from\n",
      "target speaker, evaluated by speaker identity and style similarity. The\n",
      "challenge consists of two tracks, namely few-shot track and one-shot track,\n",
      "where the participants are required to clone multiple target voices with 100\n",
      "and 5 samples respectively. There are also two sub-tracks in each track. For\n",
      "sub-track a, to fairly compare different strategies, the participants are\n",
      "allowed to use only the training data provided by the organizer strictly. For\n",
      "sub-track b, the participants are allowed to use any data publicly available.\n",
      "In this paper, we present a detailed explanation on the tasks and data used in\n",
      "the challenge, followed by a summary of submitted systems and evaluation\n",
      "results. \n",
      "\n",
      "\n",
      "This paper presents the CUHK-EE voice cloning system for ICASSP 2021 M2VoC\n",
      "challenge. The challenge provides two Mandarin speech corpora: the AIShell-3\n",
      "corpus of 218 speakers with noise and reverberation and the MST corpus\n",
      "including high-quality speech of one male and one female speakers. 100 and 5\n",
      "utterances of 3 target speakers in different voice and style are provided in\n",
      "track 1 and 2 respectively, and the participants are required to synthesize\n",
      "speech in target speaker's voice and style. We take part in the track 1 and\n",
      "carry out voice cloning based on 100 utterances of target speakers. An\n",
      "end-to-end voicing cloning system is developed to accomplish the task, which\n",
      "includes: 1. a text and speech front-end module with the help of forced\n",
      "alignment, 2. an acoustic model combining Tacotron2 and DurIAN to predict\n",
      "melspectrogram, 3. a Hifigan vocoder for waveform generation. Our system\n",
      "comprises three stages: multi-speaker training stage, target speaker adaption\n",
      "stage and target speaker synthesis stage. Our team is identified as T17. The\n",
      "subjective evaluation results provided by the challenge organizer demonstrate\n",
      "the effectiveness of our system. Audio samples are available at our demo page:\n",
      "https://daxintan-cuhk.github.io/CUHK-EE-system-M2VoC-challenge/ . \n",
      "\n",
      "\n",
      "The few-shot multi-speaker multi-style voice cloning task is to synthesize\n",
      "utterances with voice and speaking style similar to a reference speaker given\n",
      "only a few reference samples. In this work, we investigate different speaker\n",
      "representations and proposed to integrate pretrained and learnable speaker\n",
      "representations. Among different types of embeddings, the embedding pretrained\n",
      "by voice conversion achieves the best performance. The FastSpeech 2 model\n",
      "combined with both pretrained and learnable speaker representations shows great\n",
      "generalization ability on few-shot speakers and achieved 2nd place in the\n",
      "one-shot track of the ICASSP 2021 M2VoC challenge. \n",
      "\n",
      "\n",
      "Deep learning models are becoming predominant in many fields of machine\n",
      "learning. Text-to-Speech (TTS), the process of synthesizing artificial speech\n",
      "from text, is no exception. To this end, a deep neural network is usually\n",
      "trained using a corpus of several hours of recorded speech from a single\n",
      "speaker. Trying to produce the voice of a speaker other than the one learned is\n",
      "expensive and requires large effort since it is necessary to record a new\n",
      "dataset and retrain the model. This is the main reason why the TTS models are\n",
      "usually single speaker. The proposed approach has the goal to overcome these\n",
      "limitations trying to obtain a system which is able to model a multi-speaker\n",
      "acoustic space. This allows the generation of speech audio similar to the voice\n",
      "of different target speakers, even if they were not observed during the\n",
      "training phase. \n",
      "\n",
      "\n",
      "Voice cloning is the task of learning to synthesize the voice of an unseen\n",
      "speaker from a few samples. While current voice cloning methods achieve\n",
      "promising results in Text-to-Speech (TTS) synthesis for a new voice, these\n",
      "approaches lack the ability to control the expressiveness of synthesized audio.\n",
      "In this work, we propose a controllable voice cloning method that allows\n",
      "fine-grained control over various style aspects of the synthesized speech for\n",
      "an unseen speaker. We achieve this by explicitly conditioning the speech\n",
      "synthesis model on a speaker encoding, pitch contour and latent style tokens\n",
      "during training. Through both quantitative and qualitative evaluations, we show\n",
      "that our framework can be used for various expressive voice cloning tasks using\n",
      "only a few transcribed or untranscribed speech samples for a new speaker. These\n",
      "cloning tasks include style transfer from a reference speech, synthesizing\n",
      "speech directly from text, and fine-grained style control by manipulating the\n",
      "style conditioning variables during inference. \n",
      "\n",
      "\n",
      "In this paper, we present AISHELL-3, a large-scale and high-fidelity\n",
      "multi-speaker Mandarin speech corpus which could be used to train multi-speaker\n",
      "Text-to-Speech (TTS) systems. The corpus contains roughly 85 hours of\n",
      "emotion-neutral recordings spoken by 218 native Chinese mandarin speakers.\n",
      "Their auxiliary attributes such as gender, age group and native accents are\n",
      "explicitly marked and provided in the corpus. Accordingly, transcripts in\n",
      "Chinese character-level and pinyin-level are provided along with the\n",
      "recordings. We present a baseline system that uses AISHELL-3 for multi-speaker\n",
      "Madarin speech synthesis. The multi-speaker speech synthesis system is an\n",
      "extension on Tacotron-2 where a speaker verification model and a corresponding\n",
      "loss regarding voice similarity are incorporated as the feedback constraint. We\n",
      "aim to use the presented corpus to build a robust synthesis model that is able\n",
      "to achieve zero-shot voice cloning. The system trained on this dataset also\n",
      "generalizes well on speakers that are never seen in the training process.\n",
      "Objective evaluation results from our experiments show that the proposed\n",
      "multi-speaker synthesis system achieves high voice similarity concerning both\n",
      "speaker embedding similarity and equal error rate measurement. The dataset,\n",
      "baseline system code and generated samples are available online. \n",
      "\n",
      "\n",
      "As the recently proposed voice cloning system, NAUTILUS, is capable of\n",
      "cloning unseen voices using untranscribed speech, we investigate the\n",
      "feasibility of using it to develop a unified cross-lingual TTS/VC system.\n",
      "Cross-lingual speech generation is the scenario in which speech utterances are\n",
      "generated with the voices of target speakers in a language not spoken by them\n",
      "originally. This type of system is not simply cloning the voice of the target\n",
      "speaker, but essentially creating a new voice that can be considered better\n",
      "than the original under a specific framing. By using a well-trained English\n",
      "latent linguistic embedding to create a cross-lingual TTS and VC system for\n",
      "several German, Finnish, and Mandarin speakers included in the Voice Conversion\n",
      "Challenge 2020, we show that our method not only creates cross-lingual VC with\n",
      "high speaker similarity but also can be seamlessly used for cross-lingual TTS\n",
      "without having to perform any extra steps. However, the subjective evaluations\n",
      "of perceived naturalness seemed to vary between target speakers, which is one\n",
      "aspect for future improvement. \n",
      "\n",
      "\n",
      "Data efficient voice cloning aims at synthesizing target speaker's voice with\n",
      "only a few enrollment samples at hand. To this end, speaker adaptation and\n",
      "speaker encoding are two typical methods based on base model trained from\n",
      "multiple speakers. The former uses a small set of target speaker data to\n",
      "transfer the multi-speaker model to target speaker's voice through direct model\n",
      "update, while in the latter, only a few seconds of target speaker's audio\n",
      "directly goes through an extra speaker encoding model along with the\n",
      "multi-speaker model to synthesize target speaker's voice without model update.\n",
      "Nevertheless, the two methods need clean target speaker data. However, the\n",
      "samples provided by user may inevitably contain acoustic noise in real\n",
      "applications. It's still challenging to generating target voice with noisy\n",
      "data. In this paper, we study the data efficient voice cloning problem from\n",
      "noisy samples under the sequence-to-sequence based TTS paradigm. Specifically,\n",
      "we introduce domain adversarial training (DAT) to speaker adaptation and\n",
      "speaker encoding, which aims to disentangle noise from speech-noise mixture.\n",
      "Experiments show that for both speaker adaptation and encoding, the proposed\n",
      "approaches can consistently synthesize clean speech from noisy speaker samples,\n",
      "apparently outperforming the method adopting state-of-the-art speech\n",
      "enhancement module. \n",
      "\n",
      "\n",
      "We introduce an approach to multilingual speech synthesis which uses the\n",
      "meta-learning concept of contextual parameter generation and produces\n",
      "natural-sounding multilingual speech using more languages and less training\n",
      "data than previous approaches. Our model is based on Tacotron 2 with a fully\n",
      "convolutional input text encoder whose weights are predicted by a separate\n",
      "parameter generator network. To boost voice cloning, the model uses an\n",
      "adversarial speaker classifier with a gradient reversal layer that removes\n",
      "speaker-specific information from the encoder.\n",
      "  We arranged two experiments to compare our model with baselines using various\n",
      "levels of cross-lingual parameter sharing, in order to evaluate: (1) stability\n",
      "and performance when training on low amounts of data, (2) pronunciation\n",
      "accuracy and voice quality of code-switching synthesis. For training, we used\n",
      "the CSS10 dataset and our new small dataset based on Common Voice recordings in\n",
      "five languages. Our model is shown to effectively share information across\n",
      "languages and according to a subjective evaluation test, it produces more\n",
      "natural and accurate code-switching speech than the baselines. \n",
      "\n",
      "\n",
      "This paper proposes the building of Xiaomingbot, an intelligent, multilingual\n",
      "and multimodal software robot equipped with four integral capabilities: news\n",
      "generation, news translation, news reading and avatar animation. Its system\n",
      "summarizes Chinese news that it automatically generates from data tables. Next,\n",
      "it translates the summary or the full article into multiple languages, and\n",
      "reads the multilingual rendition through synthesized speech. Notably,\n",
      "Xiaomingbot utilizes a voice cloning technology to synthesize the speech\n",
      "trained from a real person's voice data in one input language. The proposed\n",
      "system enjoys several merits: it has an animated avatar, and is able to\n",
      "generate and read multilingual news. Since it was put into practice,\n",
      "Xiaomingbot has written over 600,000 articles, and gained over 150,000\n",
      "followers on social media platforms. \n",
      "\n",
      "\n",
      "In this paper, we explore the possibility of speech synthesis from low\n",
      "quality found data using only limited number of samples of target speaker. We\n",
      "try to extract only the speaker embedding from found data of target speaker\n",
      "unlike previous works which tries to train the entire text-to-speech system on\n",
      "found data. Also, the two speaker mimicking approaches which are adaptation and\n",
      "speaker-encoder-based are applied on newly released LibriTTS dataset and\n",
      "previously released VCTK corpus to examine the impact of speaker variety on\n",
      "clarity and target-speaker-similarity . \n",
      "\n",
      "\n",
      "We introduce a novel speech synthesis system, called NAUTILUS, that can\n",
      "generate speech with a target voice either from a text input or a reference\n",
      "utterance of an arbitrary source speaker. By using a multi-speaker speech\n",
      "corpus to train all requisite encoders and decoders in the initial training\n",
      "stage, our system can clone unseen voices using untranscribed speech of target\n",
      "speakers on the basis of the backpropagation algorithm. Moreover, depending on\n",
      "the data circumstance of the target speaker, the cloning strategy can be\n",
      "adjusted to take advantage of additional data and modify the behaviors of\n",
      "text-to-speech (TTS) and/or voice conversion (VC) systems to accommodate the\n",
      "situation. We test the performance of the proposed framework by using deep\n",
      "convolution layers to model the encoders, decoders and WaveNet vocoder.\n",
      "Evaluations show that it achieves comparable quality with state-of-the-art TTS\n",
      "and VC systems when cloning with just five minutes of untranscribed speech.\n",
      "Moreover, it is demonstrated that the proposed framework has the ability to\n",
      "switch between TTS and VC with high speaker consistency, which will be useful\n",
      "for many applications. \n",
      "\n",
      "\n",
      "We investigate a novel cross-lingual multi-speaker text-to-speech synthesis\n",
      "approach for generating high-quality native or accented speech for\n",
      "native/foreign seen/unseen speakers in English and Mandarin. The system\n",
      "consists of three separately trained components: an x-vector speaker encoder, a\n",
      "Tacotron-based synthesizer and a WaveNet vocoder. It is conditioned on 3 kinds\n",
      "of embeddings: (1) speaker embedding so that the system can be trained with\n",
      "speech from many speakers will little data from each speaker; (2) language\n",
      "embedding with shared phoneme inputs; (3) stress and tone embedding which\n",
      "improves naturalness of synthesized speech, especially for a tonal language\n",
      "like Mandarin. By adjusting the various embeddings, MOS results show that our\n",
      "method can generate high-quality natural and intelligible native speech for\n",
      "native/foreign seen/unseen speakers. Intelligibility and naturalness of\n",
      "accented speech is low as expected. Speaker similarity is good for native\n",
      "speech from native speakers. Interestingly, speaker similarity is also good for\n",
      "accented speech from foreign speakers. We also find that normalizing speaker\n",
      "embedding x-vectors by L2-norm normalization or whitening improves output\n",
      "quality a lot in many cases, and the WaveNet performance seems to be\n",
      "language-independent: our WaveNet is trained with Cantonese speech and can be\n",
      "used to generate Mandarin and English speech very well. \n",
      "\n",
      "\n",
      "Speech data conveys sensitive speaker attributes like identity or accent.\n",
      "With a small amount of found data, such attributes can be inferred and\n",
      "exploited for malicious purposes: voice cloning, spoofing, etc. Anonymization\n",
      "aims to make the data unlinkable, i.e., ensure that no utterance can be linked\n",
      "to its original speaker. In this paper, we investigate anonymization methods\n",
      "based on voice conversion. In contrast to prior work, we argue that various\n",
      "linkage attacks can be designed depending on the attackers' knowledge about the\n",
      "anonymization scheme. We compare two frequency warping-based conversion methods\n",
      "and a deep learning based method in three attack scenarios. The utility of\n",
      "converted speech is measured via the word error rate achieved by automatic\n",
      "speech recognition, while privacy protection is assessed by the increase in\n",
      "equal error rate achieved by state-of-the-art i-vector or x-vector based\n",
      "speaker verification. Our results show that voice conversion schemes are unable\n",
      "to effectively protect against an attacker that has extensive knowledge of the\n",
      "type of conversion and how it has been applied, but may provide some protection\n",
      "against less knowledgeable attackers. \n",
      "\n",
      "\n",
      "In this paper, we present a cross-lingual voice cloning approach. BN features\n",
      "obtained by SI-ASR model are used as a bridge across speakers and language\n",
      "boundaries. The relationships between text and BN features are modeled by the\n",
      "latent prosody model. The acoustic model learns the translation from BN\n",
      "features to acoustic features. The acoustic model is fine-tuned with a few\n",
      "samples of the target speaker to realize voice cloning. This system can\n",
      "generate speech of arbitrary utterance of target language in cross-lingual\n",
      "speakers' voice. We verify that with small amount of audio data, our proposed\n",
      "approach can well handle cross-lingual tasks. And in intra-lingual tasks, our\n",
      "proposed approach also performs better than baseline approach in naturalness\n",
      "and similarity. \n",
      "\n",
      "\n",
      "We present a multispeaker, multilingual text-to-speech (TTS) synthesis model\n",
      "based on Tacotron that is able to produce high quality speech in multiple\n",
      "languages. Moreover, the model is able to transfer voices across languages,\n",
      "e.g. synthesize fluent Spanish speech using an English speaker's voice, without\n",
      "training on any bilingual or parallel examples. Such transfer works across\n",
      "distantly related languages, e.g. English and Mandarin.\n",
      "  Critical to achieving this result are: 1. using a phonemic input\n",
      "representation to encourage sharing of model capacity across languages, and 2.\n",
      "incorporating an adversarial loss term to encourage the model to disentangle\n",
      "its representation of speaker identity (which is perfectly correlated with\n",
      "language in the training data) from the speech content. Further scaling up the\n",
      "model by training on multiple speakers of each language, and incorporating an\n",
      "autoencoding input to help stabilize attention during training, results in a\n",
      "model which can be used to consistently synthesize intelligible speech for\n",
      "training speakers in all languages seen during training, and in native or\n",
      "foreign accents. \n",
      "\n",
      "\n",
      "There are many use cases in singing synthesis where creating voices from\n",
      "small amounts of data is desirable. In text-to-speech there have been several\n",
      "promising results that apply voice cloning techniques to modern deep learning\n",
      "based models. In this work, we adapt one such technique to the case of singing\n",
      "synthesis. By leveraging data from many speakers to first create a multispeaker\n",
      "model, small amounts of target data can then efficiently adapt the model to new\n",
      "unseen voices. We evaluate the system using listening tests across a number of\n",
      "different use cases, languages and kinds of data. \n",
      "\n",
      "\n",
      "Voice cloning technologies have found applications in a variety of areas\n",
      "ranging from personalized speech interfaces to advertisement, robotics, and so\n",
      "on. Existing voice cloning systems are capable of learning speaker\n",
      "characteristics and use trained models to synthesize a person's voice from only\n",
      "a few audio samples. Advances in cloned speech generation technologies are\n",
      "capable of generating perceptually indistinguishable speech from a bona-fide\n",
      "speech. These advances pose new security and privacy threats to voice-driven\n",
      "interfaces and speech-based access control systems. The state-of-the-art speech\n",
      "synthesis technologies use trained or tuned generative models for cloned speech\n",
      "generation. Trained generative models rely on linear operations, learned\n",
      "weights, and excitation source for cloned speech synthesis. These systems leave\n",
      "characteristic artifacts in the synthesized speech. Higher-order spectral\n",
      "analysis is used to capture differentiating attributes between bona-fide and\n",
      "cloned audios. Specifically, quadrature phase coupling (QPC) in the estimated\n",
      "bicoherence, Gaussianity test statistics, and linearity test statistics are\n",
      "used to capture generative model artifacts. Performance of the proposed method\n",
      "is evaluated on cloned audios generated using speaker adaptation- and speaker\n",
      "encoding-based approaches. Experimental results for a dataset consisting of 126\n",
      "cloned speech and 8 bona-fide speech samples indicate that the proposed method\n",
      "is capable of detecting bona-fide and cloned audios with close to a perfect\n",
      "detection rate. \n",
      "\n",
      "\n",
      "Voice cloning is a highly desired feature for personalized speech interfaces.\n",
      "Neural network based speech synthesis has been shown to generate high quality\n",
      "speech for a large number of speakers. In this paper, we introduce a neural\n",
      "voice cloning system that takes a few audio samples as input. We study two\n",
      "approaches: speaker adaptation and speaker encoding. Speaker adaptation is\n",
      "based on fine-tuning a multi-speaker generative model with a few cloning\n",
      "samples. Speaker encoding is based on training a separate model to directly\n",
      "infer a new speaker embedding from cloning audios and to be used with a\n",
      "multi-speaker generative model. In terms of naturalness of the speech and its\n",
      "similarity to original speaker, both approaches can achieve good performance,\n",
      "even with very few cloning audios. While speaker adaptation can achieve better\n",
      "naturalness and similarity, the cloning time or required memory for the speaker\n",
      "encoding approach is significantly less, making it favorable for low-resource\n",
      "deployment. \n",
      "\n",
      "\n",
      "Malicious actors may seek to use different voice-spoofing attacks to fool ASV\n",
      "systems and even use them for spreading misinformation. Various countermeasures\n",
      "have been proposed to detect these spoofing attacks. Due to the extensive work\n",
      "done on spoofing detection in automated speaker verification (ASV) systems in\n",
      "the last 6-7 years, there is a need to classify the research and perform\n",
      "qualitative and quantitative comparisons on state-of-the-art countermeasures.\n",
      "Additionally, no existing survey paper has reviewed integrated solutions to\n",
      "voice spoofing evaluation and speaker verification, adversarial/antiforensics\n",
      "attacks on spoofing countermeasures, and ASV itself, or unified solutions to\n",
      "detect multiple attacks using a single model. Further, no work has been done to\n",
      "provide an apples-to-apples comparison of published countermeasures in order to\n",
      "assess their generalizability by evaluating them across corpora. In this work,\n",
      "we conduct a review of the literature on spoofing detection using hand-crafted\n",
      "features, deep learning, end-to-end, and universal spoofing countermeasure\n",
      "solutions to detect speech synthesis (SS), voice conversion (VC), and replay\n",
      "attacks. Additionally, we also review integrated solutions to voice spoofing\n",
      "evaluation and speaker verification, adversarial and anti-forensics attacks on\n",
      "voice countermeasures, and ASV. The limitations and challenges of the existing\n",
      "spoofing countermeasures are also presented. We report the performance of these\n",
      "countermeasures on several datasets and evaluate them across corpora. For the\n",
      "experiments, we employ the ASVspoof2019 and VSDC datasets along with GMM, SVM,\n",
      "CNN, and CNN-GRU classifiers. (For reproduceability of the results, the code of\n",
      "the test bed can be found in our GitHub Repository. \n",
      "\n",
      "\n",
      "Recent developments in neural speech synthesis and vocoding have sparked a\n",
      "renewed interest in voice conversion (VC). Beyond timbre transfer, achieving\n",
      "controllability on para-linguistic parameters such as pitch and rhythm is\n",
      "critical in deploying VC systems in many application scenarios. Existing\n",
      "studies, however, either only provide utterance-level global control or lack\n",
      "interpretability on the controls. In this paper, we propose ControlVC, the\n",
      "first neural voice conversion system that achieves time-varying controls on\n",
      "pitch and rhythm. ControlVC uses pre-trained encoders to compute pitch\n",
      "embeddings and linguistic embeddings from the source utterance and speaker\n",
      "embeddings from the target utterance. These embeddings are then concatenated\n",
      "and converted to speech using a vocoder. It achieves rhythm control through\n",
      "TD-PSOLA pre-processing on the source utterance, and achieves pitch control by\n",
      "manipulating the pitch contour before feeding it to the pitch encoder.\n",
      "Systematic subjective and objective evaluations are conducted to assess the\n",
      "speech quality and controllability. Results show that, on non-parallel and\n",
      "zero-shot conversion tasks, ControlVC significantly outperforms two other\n",
      "self-constructed baselines on speech quality, and it can successfully achieve\n",
      "time-varying pitch control. \n",
      "\n",
      "\n",
      "Nonparallel multi-domain voice conversion methods such as the StarGAN-VCs\n",
      "have been widely applied in many scenarios. However, the training of these\n",
      "models usually poses a challenge due to their complicated adversarial network\n",
      "architectures. To address this, in this work we leverage the state-of-the-art\n",
      "contrastive learning techniques and incorporate an efficient Siamese network\n",
      "structure into the StarGAN discriminator. Our method is called\n",
      "SimSiam-StarGAN-VC and it boosts the training stability and effectively\n",
      "prevents the discriminator overfitting issue in the training process. We\n",
      "conduct experiments on the Voice Conversion Challenge (VCC 2018) dataset, plus\n",
      "a user study to validate the performance of our framework. Our experimental\n",
      "results show that SimSiam-StarGAN-VC significantly outperforms existing\n",
      "StarGAN-VC methods in terms of both the objective and subjective metrics. \n",
      "\n",
      "\n",
      "Automatic speech recognition (ASR) needs to be robust to speaker differences.\n",
      "Voice Conversion (VC) modifies speaker characteristics of input speech. This is\n",
      "an attractive feature for ASR data augmentation. In this paper, we demonstrate\n",
      "that voice conversion can be used as a data augmentation technique to improve\n",
      "ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR\n",
      "augmentation, it is necessary that the VC model be robust to a wide range of\n",
      "input speech. This motivates the use of a non-autoregressive, non-parallel VC\n",
      "model, and the use of a pretrained ASR encoder within the VC model. This work\n",
      "suggests that despite including many speakers, speaker diversity may remain a\n",
      "limitation to ASR quality. Finally, interrogation of our VC performance has\n",
      "provided useful metrics for objective evaluation of VC quality. \n",
      "\n",
      "\n",
      "Non-reference speech quality models are important for a growing number of\n",
      "applications. The VoiceMOS 2022 challenge provided a dataset of synthetic voice\n",
      "conversion and text-to-speech samples with subjective labels. This study looks\n",
      "at the amount of variance that can be explained in subjective ratings of speech\n",
      "quality from metadata and the distribution imbalances of the dataset. Speech\n",
      "quality models were constructed using wav2vec 2.0 with additional metadata\n",
      "features that included rater groups and system identifiers and obtained\n",
      "competitive metrics including a Spearman rank correlation coefficient (SRCC) of\n",
      "0.934 and MSE of 0.088 at the system-level, and 0.877 and 0.198 at the\n",
      "utterance-level. Using data and metadata that the test restricted or blinded\n",
      "further improved the metrics. A metadata analysis showed that the system-level\n",
      "metrics do not represent the model's system-level prediction as a result of the\n",
      "wide variation in the number of utterances used for each system on the\n",
      "validation and test datasets. We conclude that, in general, conditions should\n",
      "have enough utterances in the test set to bound the sample mean error, and be\n",
      "relatively balanced in utterance count between systems, otherwise the\n",
      "utterance-level metrics may be more reliable and interpretable. \n",
      "\n",
      "\n",
      "The widespread adoption of speech-based online services raises security and\n",
      "privacy concerns regarding the data that they use and share. If the data were\n",
      "compromised, attackers could exploit user speech to bypass speaker verification\n",
      "systems or even impersonate users. To mitigate this, we propose DeID-VC, a\n",
      "speaker de-identification system that converts a real speaker to pseudo\n",
      "speakers, thus removing or obfuscating the speaker-dependent attributes from a\n",
      "spoken voice. The key components of DeID-VC include a Variational Autoencoder\n",
      "(VAE) based Pseudo Speaker Generator (PSG) and a voice conversion Autoencoder\n",
      "(AE) under zero-shot settings. With the help of PSG, DeID-VC can assign unique\n",
      "pseudo speakers at speaker level or even at utterance level. Also, two novel\n",
      "learning objectives are added to bridge the gap between training and inference\n",
      "of zero-shot voice conversion. We present our experimental results with word\n",
      "error rate (WER) and equal error rate (EER), along with three subjective\n",
      "metrics to evaluate the generated output of DeID-VC. The result shows that our\n",
      "method substantially improved intelligibility (WER 10% lower) and\n",
      "de-identification effectiveness (EER 5% higher) compared to our baseline. Code\n",
      "and listening demo: https://github.com/a43992899/DeID-VC \n",
      "\n",
      "\n",
      "Disentangling speaker and content attributes of a speech signal into separate\n",
      "latent representations followed by decoding the content with an exchanged\n",
      "speaker representation is a popular approach for voice conversion, which can be\n",
      "trained with non-parallel and unlabeled speech data. However, previous\n",
      "approaches perform disentanglement only implicitly via some sort of information\n",
      "bottleneck or normalization, where it is usually hard to find a good trade-off\n",
      "between voice conversion and content reconstruction. Further, previous works\n",
      "usually do not consider an adaptation of the speaking rate to the target\n",
      "speaker or they put some major restrictions to the data or use case. Therefore,\n",
      "the contribution of this work is two-fold. First, we employ an explicit and\n",
      "fully unsupervised disentanglement approach, which has previously only been\n",
      "used for representation learning, and show that it allows to obtain both\n",
      "superior voice conversion and content reconstruction. Second, we investigate\n",
      "simple and generic approaches to linearly adapt the length of a speech signal,\n",
      "and hence the speaking rate, to a target speaker and show that the proposed\n",
      "adaptation allows to increase the speaking rate similarity with respect to the\n",
      "target speaker. \n",
      "\n",
      "\n",
      "Speech signals contain a lot of sensitive information, such as the speaker's\n",
      "identity, which raises privacy concerns when speech data get collected. Speaker\n",
      "anonymization aims to transform a speech signal to remove the source speaker's\n",
      "identity while leaving the spoken content unchanged. Current methods perform\n",
      "the transformation by relying on content/speaker disentanglement and voice\n",
      "conversion. Usually, an acoustic model from an automatic speech recognition\n",
      "system extracts the content representation while an x-vector system extracts\n",
      "the speaker representation. Prior work has shown that the extracted features\n",
      "are not perfectly disentangled. This paper tackles how to improve features\n",
      "disentanglement, and thus the converted anonymized speech. We propose enhancing\n",
      "the disentanglement by removing speaker information from the acoustic model\n",
      "using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit\n",
      "showed that vector quantization helps conceal the original speaker identity\n",
      "while maintaining utility for speech recognition. \n",
      "\n",
      "\n",
      "One-shot voice conversion (VC) with only a single target speaker's speech for\n",
      "reference has become a hot research topic. Existing works generally disentangle\n",
      "timbre, while information about pitch, rhythm and content is still mixed\n",
      "together. To perform one-shot VC effectively with further disentangling these\n",
      "speech components, we employ random resampling for pitch and content encoder\n",
      "and use the variational contrastive log-ratio upper bound of mutual information\n",
      "and gradient reversal layer based adversarial mutual information learning to\n",
      "ensure the different parts of the latent space containing only the desired\n",
      "disentangled representation during training. Experiments on the VCTK dataset\n",
      "show the model achieves state-of-the-art performance for one-shot VC in terms\n",
      "of naturalness and intellgibility. In addition, we can transfer characteristics\n",
      "of one-shot VC on timbre, pitch and rhythm separately by speech representation\n",
      "disentanglement. Our code, pre-trained models and demo are available at\n",
      "https://im1eon.github.io/IS2022-SRDVC/. \n",
      "\n",
      "\n",
      "In this paper, we propose a differentiable WORLD synthesizer and demonstrate\n",
      "its use in end-to-end audio style transfer tasks such as (singing) voice\n",
      "conversion and the DDSP timbre transfer task. Accordingly, our baseline\n",
      "differentiable synthesizer has no model parameters, yet it yields adequate\n",
      "synthesis quality. We can extend the baseline synthesizer by appending\n",
      "lightweight black-box postnets which apply further processing to the baseline\n",
      "output in order to improve fidelity. An alternative differentiable approach\n",
      "considers extraction of the source excitation spectrum directly, which can\n",
      "improve naturalness albeit for a narrower class of style transfer applications.\n",
      "The acoustic feature parameterization used by our approaches has the added\n",
      "benefit that it naturally disentangles pitch and timbral information so that\n",
      "they can be modeled separately. Moreover, as there exists a robust means of\n",
      "estimating these acoustic features from monophonic audio sources, it allows for\n",
      "parameter loss terms to be added to an end-to-end objective function, which can\n",
      "help convergence and/or further stabilize (adversarial) training. \n",
      "\n",
      "\n",
      "Chatbot is a machine with the ability to answer automatically through a\n",
      "conversational interface. A chatbot is considered as one of the most\n",
      "exceptional and promising expressions of human computer interaction.\n",
      "Voice-based chatbots or artificial intelligence devices transform\n",
      "human-computer bidirectional interactions that allow users to navigate an\n",
      "interactive voice response system with their voice generally using natural\n",
      "language. In this paper, we focus on voice based chatbots for mediating\n",
      "interactions between hotels and guests from both the hospitality technology\n",
      "providers' and guests' perspectives. We developed a hotel web application with\n",
      "the capability to receive a voice input. The application was developed with\n",
      "Speech recognition and deep synthesis API for voice to text and text to voice\n",
      "conversion, a closed domain question answering NLP solution was used for query\n",
      "the answer. \n",
      "\n",
      "\n",
      "Non-parallel many-to-many voice conversion remains an interesting but\n",
      "challenging speech processing task. Recently, AutoVC, a conditional autoencoder\n",
      "based method, achieved excellent conversion results by disentangling the\n",
      "speaker identity and the speech content using information-constraining\n",
      "bottlenecks. However, due to the pure autoencoder training method, it is\n",
      "difficult to evaluate the separation effect of content and speaker identity. In\n",
      "this paper, a novel voice conversion framework, named $\\boldsymbol T$ext\n",
      "$\\boldsymbol G$uided $\\boldsymbol A$utoVC(TGAVC), is proposed to more\n",
      "effectively separate content and timbre from speech, where an expected content\n",
      "embedding produced based on the text transcriptions is designed to guide the\n",
      "extraction of voice content. In addition, the adversarial training is applied\n",
      "to eliminate the speaker identity information in the estimated content\n",
      "embedding extracted from speech. Under the guidance of the expected content\n",
      "embedding and the adversarial training, the content encoder is trained to\n",
      "extract speaker-independent content embedding from speech. Experiments on\n",
      "AIShell-3 dataset show that the proposed model outperforms AutoVC in terms of\n",
      "naturalness and similarity of converted speech. \n",
      "\n",
      "\n",
      "The availability of data in expressive styles across languages is limited,\n",
      "and recording sessions are costly and time consuming. To overcome these issues,\n",
      "we demonstrate how to build low-resource, neural text-to-speech (TTS) voices\n",
      "with only 1 hour of conversational speech, when no other conversational data\n",
      "are available in the same language. Assuming the availability of non-expressive\n",
      "speech data in that language, we propose a 3-step technology: 1) we train an\n",
      "F0-conditioned voice conversion (VC) model as data augmentation technique; 2)\n",
      "we train an F0 predictor to control the conversational flavour of the\n",
      "voice-converted synthetic data; 3) we train a TTS system that consumes the\n",
      "augmented data. We prove that our technology enables F0 controllability, is\n",
      "scalable across speakers and languages and is competitive in terms of\n",
      "naturalness over a state-of-the-art baseline model, another augmented method\n",
      "which does not make use of F0 information. \n",
      "\n",
      "\n",
      "Sequence-to-Sequence Text-to-Speech architectures that directly generate low\n",
      "level acoustic features from phonetic sequences are known to produce natural\n",
      "and expressive speech when provided with adequate amounts of training data.\n",
      "Such systems can learn and transfer desired speaking styles from one seen\n",
      "speaker to another (in multi-style multi-speaker settings), which is highly\n",
      "desirable for creating scalable and customizable Human-Computer Interaction\n",
      "systems. In this work we explore one-to-many style transfer from a dedicated\n",
      "single-speaker conversational corpus with style nuances and interjections. We\n",
      "elaborate on the corpus design and explore the feasibility of such style\n",
      "transfer when assisted with Voice-Conversion-based data augmentation. In a set\n",
      "of subjective listening experiments, this approach resulted in high-fidelity\n",
      "style transfer with no quality degradation. However, a certain voice persona\n",
      "shift was observed, requiring further improvements in voice conversion. \n",
      "\n",
      "\n",
      "Voice conversion is to generate a new speech with the source content and a\n",
      "target voice style. In this paper, we focus on one general setting, i.e.,\n",
      "non-parallel many-to-many voice conversion, which is close to the real-world\n",
      "scenario. As the name implies, non-parallel many-to-many voice conversion does\n",
      "not require the paired source and reference speeches and can be applied to\n",
      "arbitrary voice transfer. In recent years, Generative Adversarial Networks\n",
      "(GANs) and other techniques such as Conditional Variational Autoencoders\n",
      "(CVAEs) have made considerable progress in this field. However, due to the\n",
      "sophistication of voice conversion, the style similarity of the converted\n",
      "speech is still unsatisfactory. Inspired by the inherent structure of\n",
      "mel-spectrogram, we propose a new voice conversion framework, i.e.,\n",
      "Subband-based Generative Adversarial Network for Voice Conversion (SGAN-VC).\n",
      "SGAN-VC converts each subband content of the source speech separately by\n",
      "explicitly utilizing the spatial characteristics between different subbands.\n",
      "SGAN-VC contains one style encoder, one content encoder, and one decoder. In\n",
      "particular, the style encoder network is designed to learn style codes for\n",
      "different subbands of the target speaker. The content encoder network can\n",
      "capture the content information on the source speech. Finally, the decoder\n",
      "generates particular subband content. In addition, we propose a pitch-shift\n",
      "module to fine-tune the pitch of the source speaker, making the converted tone\n",
      "more accurate and explainable. Extensive experiments demonstrate that the\n",
      "proposed approach achieves state-of-the-art performance on VCTK Corpus and\n",
      "AISHELL3 datasets both qualitatively and quantitatively, whether on seen or\n",
      "unseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data\n",
      "even exceeds that of StarGANv2-VC with ASR network assistance. \n",
      "\n",
      "\n",
      "We present a large-scale comparative study of self-supervised speech\n",
      "representation (S3R)-based voice conversion (VC). In the context of\n",
      "recognition-synthesis VC, S3Rs are attractive owing to their potential to\n",
      "replace expensive supervised representations such as phonetic posteriorgrams\n",
      "(PPGs), which are commonly adopted by state-of-the-art VC systems. Using\n",
      "S3PRL-VC, an open-source VC software we previously developed, we provide a\n",
      "series of in-depth objective and subjective analyses under three VC settings:\n",
      "intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice\n",
      "conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in\n",
      "various aspects, including model type, multilinguality, and supervision. We\n",
      "also studied the effect of a post-discretization process with k-means\n",
      "clustering and showed how it improves in the A2A setting. Finally, the\n",
      "comparison with state-of-the-art VC systems demonstrates the competitiveness of\n",
      "S3R-based VC and also sheds light on the possible improving directions. \n",
      "\n",
      "\n",
      "The zero-shot scenario for speech generation aims at synthesizing a novel\n",
      "unseen voice with only one utterance of the target speaker. Although the\n",
      "challenges of adapting new voices in zero-shot scenario exist in both stages --\n",
      "acoustic modeling and vocoder, previous works usually consider the problem from\n",
      "only one stage. In this paper, we extend our previous Glow-WaveGAN to\n",
      "Glow-WaveGAN 2, aiming to solve the problem from both stages for high-quality\n",
      "zero-shot text-to-speech and any-to-any voice conversion. We first build a\n",
      "universal WaveGAN model for extracting latent distribution $p(z)$ of speech and\n",
      "reconstructing waveform from it. Then a flow-based acoustic model only needs to\n",
      "learn the same $p(z)$ from texts, which naturally avoids the mismatch between\n",
      "the acoustic model and the vocoder, resulting in high-quality generated speech\n",
      "without model fine-tuning. Based on a continuous speaker space and the\n",
      "reversible property of flows, the conditional distribution can be obtained for\n",
      "any speaker, and thus we can further conduct high-quality zero-shot speech\n",
      "generation for new speakers. We particularly investigate two methods to\n",
      "construct the speaker space, namely pre-trained speaker encoder and\n",
      "jointly-trained speaker encoder. The superiority of Glow-WaveGAN 2 has been\n",
      "proved through TTS and VC experiments conducted on LibriTTS corpus and VTCK\n",
      "corpus. \n",
      "\n",
      "\n",
      "In this paper, we propose GlowVC: a multilingual multi-speaker flow-based\n",
      "model for language-independent text-free voice conversion. We build on\n",
      "Glow-TTS, which provides an architecture that enables use of linguistic\n",
      "features during training without the necessity of using them for VC inference.\n",
      "We consider two versions of our model: GlowVC-conditional and GlowVC-explicit.\n",
      "GlowVC-conditional models the distribution of mel-spectrograms with\n",
      "speaker-conditioned flow and disentangles the mel-spectrogram space into\n",
      "content- and pitch-relevant dimensions, while GlowVC-explicit models the\n",
      "explicit distribution with unconditioned flow and disentangles said space into\n",
      "content-, pitch- and speaker-relevant dimensions. We evaluate our models in\n",
      "terms of intelligibility, speaker similarity and naturalness for intra- and\n",
      "cross-lingual conversion in seen and unseen languages. GlowVC models greatly\n",
      "outperform AutoVC baseline in terms of intelligibility, while achieving just as\n",
      "high speaker similarity in intra-lingual VC, and slightly worse in the\n",
      "cross-lingual setting. Moreover, we demonstrate that GlowVC-explicit surpasses\n",
      "both GlowVC-conditional and AutoVC in terms of naturalness. \n",
      "\n",
      "\n",
      "Building a voice conversion system for noisy target speakers, such as users\n",
      "providing noisy samples or Internet found data, is a challenging task since the\n",
      "use of contaminated speech in model training will apparently degrade the\n",
      "conversion performance. In this paper, we leverage the advances of our recently\n",
      "proposed Glow-WaveGAN and propose a noise-independent speech representation\n",
      "learning approach for high-quality voice conversion for noisy target speakers.\n",
      "Specifically, we learn a latent feature space where we ensure that the target\n",
      "distribution modeled by the conversion model is exactly from the modeled\n",
      "distribution of the waveform generator. With this premise, we further manage to\n",
      "make the latent feature to be noise-invariant. Specifically, we introduce a\n",
      "noise-controllable WaveGAN, which directly learns the noise-independent\n",
      "acoustic representation from waveform by the encoder and conducts noise control\n",
      "in the hidden space through a FiLM module in the decoder. As for the conversion\n",
      "model, importantly, we use a flow-based model to learn the distribution of\n",
      "noise-independent but speaker-related latent features from phoneme\n",
      "posteriorgrams. Experimental results demonstrate that the proposed model\n",
      "achieves high speech quality and speaker similarity in the voice conversion for\n",
      "noisy target speakers. \n",
      "\n",
      "\n",
      "This paper presents a new voice conversion (VC) framework capable of dealing\n",
      "with both additive noise and reverberation, and its performance evaluation.\n",
      "There have been studied some VC researches focusing on real-world circumstances\n",
      "where speech data are interfered with background noise and reverberation. To\n",
      "deal with more practical conditions where no clean target dataset is available,\n",
      "one possible approach is zero-shot VC, but its performance tends to degrade\n",
      "compared with VC using sufficient amount of target speech data. To leverage\n",
      "large amount of noisy-reverberant target speech data, we propose a three-stage\n",
      "VC framework based on denoising process using a pretrained denoising model,\n",
      "dereverberation process using a dereverberation model, and VC process using a\n",
      "nonparallel VC model based on a variational autoencoder. The experimental\n",
      "results show that 1) noise and reverberation additively cause significant VC\n",
      "performance degradation, 2) the proposed method alleviates the adverse effects\n",
      "caused by both noise and reverberation, and significantly outperforms the\n",
      "baseline directly trained on the noisy-reverberant speech data, and 3) the\n",
      "potential degradation introduced by the denoising and dereverberation still\n",
      "causes noticeable adverse effects on VC performance. \n",
      "\n",
      "\n",
      "Automatic methods to predict Mean Opinion Score (MOS) of listeners have been\n",
      "researched to assure the quality of Text-to-Speech systems. Many previous\n",
      "studies focus on architectural advances (e.g. MBNet, LDNet, etc.) to capture\n",
      "relations between spectral features and MOS in a more effective way and\n",
      "achieved high accuracy. However, the optimal representation in terms of\n",
      "generalization capability still largely remains unknown. To this end, we\n",
      "compare the performance of Self-Supervised Learning (SSL) features obtained by\n",
      "the wav2vec framework to that of spectral features such as magnitude of\n",
      "spectrogram and melspectrogram. Moreover, we propose to combine the SSL\n",
      "features and features which we believe to retain essential information to the\n",
      "automatic MOS to compensate each other for their drawbacks. We conduct\n",
      "comprehensive experiments on a large-scale listening test corpus collected from\n",
      "past Blizzard and Voice Conversion Challenges. We found that the wav2vec\n",
      "feature set showed the best generalization even though the given ground-truth\n",
      "was not always reliable. Furthermore, we found that the combinations performed\n",
      "the best and analyzed how they bridged the gap between spectral and the wav2vec\n",
      "feature sets. \n",
      "\n",
      "\n",
      "Typically, singing voice conversion (SVC) depends on an embedding vector,\n",
      "extracted from either a speaker lookup table (LUT) or a speaker recognition\n",
      "network (SRN), to model speaker identity. However, singing contains more\n",
      "expressive speaker characteristics than conversational speech. It is suspected\n",
      "that a single embedding vector may only capture averaged and coarse-grained\n",
      "speaker characteristics, which is insufficient for the SVC task. To this end,\n",
      "this work proposes a novel hierarchical speaker representation framework for\n",
      "SVC, which can capture fine-grained speaker characteristics at different\n",
      "granularity. It consists of an up-sampling stream and three down-sampling\n",
      "streams. The up-sampling stream transforms the linguistic features into audio\n",
      "samples, while one down-sampling stream of the three operates in the reverse\n",
      "direction. It is expected that the temporal statistics of each down-sampling\n",
      "block can represent speaker characteristics at different granularity, which\n",
      "will be engaged in the up-sampling blocks to enhance the speaker modeling.\n",
      "Experiment results verify that the proposed method outperforms both the LUT and\n",
      "SRN based SVC systems. Moreover, the proposed system supports the one-shot SVC\n",
      "with only a few seconds of reference audio. \n",
      "\n",
      "\n",
      "In most of practical scenarios, the announcement system must deliver speech\n",
      "messages in a noisy environment, in which the background noise cannot be\n",
      "cancelled out. The local noise reduces speech intelligibility and increases\n",
      "listening effort of the listener, hence hamper the effectiveness of\n",
      "announcement system. There has been reported that voices of professional\n",
      "announcers are clearer and more comprehensive than that of non-expert speakers\n",
      "in noisy environment. This finding suggests that the speech intelligibility\n",
      "might be related to the speaking style of professional announcer, which can be\n",
      "adapted using voice conversion method. Motivated by this idea, this paper\n",
      "proposes a speech intelligibility enhancement in noisy environment by applying\n",
      "voice conversion method on non-professional voice. We discovered that the\n",
      "professional announcers and non-professional speakers are clusterized into\n",
      "different clusters on the speaker embedding plane. This implies that the speech\n",
      "intelligibility can be controlled as an independent feature of speaker\n",
      "individuality. To examine the advantage of converted voice in noisy\n",
      "environment, we experimented using test words masked in pink noise at different\n",
      "SNR levels. The results of objective and subjective evaluations confirm that\n",
      "the speech intelligibility of converted voice is higher than that of original\n",
      "voice in low SNR conditions. \n",
      "\n",
      "\n",
      "An automatic speaker verification system aims to verify the speaker identity\n",
      "of a speech signal. However, a voice conversion system manipulates the original\n",
      "person's speech signal to make it sound like the target speaker's voice and\n",
      "deceive the speaker verification system. Most countermeasures for voice\n",
      "conversion-based spoofing attacks are designed to discriminate bona fide speech\n",
      "from spoofed speech for speaker verification systems. In this paper, we\n",
      "investigate the problem of source speaker identification -- inferring the\n",
      "identity of the source speaker given the voice converted speech. To perform\n",
      "source speaker identification, we simply add voice-converted speech data with\n",
      "the label of source speaker identity to the genuine speech dataset during\n",
      "speaker embedding network training. Experimental results show the feasibility\n",
      "of source speaker identification when training and testing with converted\n",
      "speeches from the same voice conversion model(s). When testing on converted\n",
      "speeches from an unseen voice conversion algorithm, the performance of source\n",
      "speaker identification improves when more voice conversion models are used\n",
      "during training. \n",
      "\n",
      "\n",
      "The ideal goal of voice conversion is to convert the source speaker's speech\n",
      "to sound naturally like the target speaker while maintaining the linguistic\n",
      "content and the prosody of the source speech. However, current approaches are\n",
      "insufficient to achieve comprehensive source prosody transfer and target\n",
      "speaker timbre preservation in the converted speech, and the quality of the\n",
      "converted speech is also unsatisfied due to the mismatch between the acoustic\n",
      "model and the vocoder. In this paper, we leverage the recent advances in\n",
      "information perturbation and propose a fully end-to-end approach to conduct\n",
      "high-quality voice conversion. We first adopt information perturbation to\n",
      "remove speaker-related information in the source speech to disentangle speaker\n",
      "timbre and linguistic content and thus the linguistic information is\n",
      "subsequently modeled by a content encoder. To better transfer the prosody of\n",
      "the source speech to the target, we particularly introduce a speaker-related\n",
      "pitch encoder which can maintain the general pitch pattern of the source\n",
      "speaker while flexibly modifying the pitch intensity of the generated speech.\n",
      "Finally, one-shot voice conversion is set up through continuous speaker space\n",
      "modeling. Experimental results indicate that the proposed end-to-end approach\n",
      "significantly outperforms the state-of-the-art models in terms of\n",
      "intelligibility, naturalness, and speaker similarity. \n",
      "\n",
      "\n",
      "Voice conversion models have developed for decades, and current mainstream\n",
      "research focuses on non-streaming voice conversion. However, streaming voice\n",
      "conversion is more suitable for practical application scenarios than\n",
      "non-streaming voice conversion. In this paper, we propose a streaming\n",
      "any-to-many voice conversion based on fully non-autoregressive model, which\n",
      "includes a streaming transformer based acoustic model and a streaming vocoder.\n",
      "Streaming transformer based acoustic model is composed of a pre-trained encoder\n",
      "from streaming end-to-end based automatic speech recognition model and a\n",
      "decoder modified on FastSpeech blocks. Streaming vocoder is designed for\n",
      "streaming task with pseudo quadrature mirror filter bank and causal\n",
      "convolution. Experimental results show that the proposed method achieves\n",
      "significant performance both in latency and conversion quality and can be\n",
      "real-time on CPU and GPU. \n",
      "\n",
      "\n",
      "Emotion classification of speech and assessment of the emotion strength are\n",
      "required in applications such as emotional text-to-speech and voice conversion.\n",
      "The emotion attribute ranking function based on Support Vector Machine (SVM)\n",
      "was proposed to predict emotion strength for emotional speech corpus. However,\n",
      "the trained ranking function doesn't generalize to new domains, which limits\n",
      "the scope of applications, especially for out-of-domain or unseen speech. In\n",
      "this paper, we propose a data-driven deep learning model, i.e. StrengthNet, to\n",
      "improve the generalization of emotion strength assessment for seen and unseen\n",
      "speech. This is achieved by the fusion of emotional data from various domains.\n",
      "We follow a multi-task learning network architecture that includes an acoustic\n",
      "encoder, a strength predictor, and an auxiliary emotion predictor. Experiments\n",
      "show that the predicted emotion strength of the proposed StrengthNet is highly\n",
      "correlated with ground truth scores for both seen and unseen speech. We release\n",
      "the source codes at: https://github.com/ttslr/StrengthNet. \n",
      "\n",
      "\n",
      "This paper proposes a new voice conversion (VC) task from human speech to\n",
      "dog-like speech while preserving linguistic information as an example of human\n",
      "to non-human creature voice conversion (H2NH-VC) tasks. Although most VC\n",
      "studies deal with human to human VC, H2NH-VC aims to convert human speech into\n",
      "non-human creature-like speech. Non-parallel VC allows us to develop H2NH-VC,\n",
      "because we cannot collect a parallel dataset that non-human creatures speak\n",
      "human language. In this study, we propose to use dogs as an example of a\n",
      "non-human creature target domain and define the \"speak like a dog\" task. To\n",
      "clarify the possibilities and characteristics of the \"speak like a dog\" task,\n",
      "we conducted a comparative experiment using existing representative\n",
      "non-parallel VC methods in acoustic features (Mel-cepstral coefficients and\n",
      "Mel-spectrograms), network architectures (five different kernel-size settings),\n",
      "and training criteria (variational autoencoder (VAE)- based and generative\n",
      "adversarial network-based). Finally, the converted voices were evaluated using\n",
      "mean opinion scores: dog-likeness, sound quality and intelligibility, and\n",
      "character error rate (CER). The experiment showed that the employment of the\n",
      "Mel-spectrogram improved the dog-likeness of the converted speech, while it is\n",
      "challenging to preserve linguistic information. Challenges and limitations of\n",
      "the current VC methods for H2NH-VC are highlighted. \n",
      "\n",
      "\n",
      "In this paper, we propose a neural end-to-end system for voice preserving,\n",
      "lip-synchronous translation of videos. The system is designed to combine\n",
      "multiple component models and produces a video of the original speaker speaking\n",
      "in the target language that is lip-synchronous with the target speech, yet\n",
      "maintains emphases in speech, voice characteristics, face video of the original\n",
      "speaker. The pipeline starts with automatic speech recognition including\n",
      "emphasis detection, followed by a translation model. The translated text is\n",
      "then synthesized by a Text-to-Speech model that recreates the original emphases\n",
      "mapped from the original sentence. The resulting synthetic voice is then mapped\n",
      "back to the original speakers' voice using a voice conversion model. Finally,\n",
      "to synchronize the lips of the speaker with the translated audio, a conditional\n",
      "generative adversarial network-based model generates frames of adapted lip\n",
      "movements with respect to the input face image as well as the output of the\n",
      "voice conversion model. In the end, the system combines the generated video\n",
      "with the converted audio to produce the final output. The result is a video of\n",
      "a speaker speaking in another language without actually knowing it. To evaluate\n",
      "our design, we present a user study of the complete system as well as separate\n",
      "evaluations of the single components. Since there is no available dataset to\n",
      "evaluate our whole system, we collect a test set and evaluate our system on\n",
      "this test set. The results indicate that our system is able to generate\n",
      "convincing videos of the original speaker speaking the target language while\n",
      "preserving the original speaker's characteristics. The collected dataset will\n",
      "be shared. \n",
      "\n",
      "\n",
      "Zero-shot voice conversion is becoming an increasingly popular research\n",
      "direction, as it promises the ability to transform speech to match the voice\n",
      "style of any speaker. However, little work has been done on end-to-end methods\n",
      "for this task, which are appealing because they remove the need for a separate\n",
      "vocoder to generate audio from intermediate features. In this work, we propose\n",
      "Location-Variable Convolution-based Voice Conversion (LVC-VC), a model for\n",
      "performing end-to-end zero-shot voice conversion that is based on a neural\n",
      "vocoder. LVC-VC utilizes carefully designed input features that have\n",
      "disentangled content and speaker style information, and the vocoder-like\n",
      "architecture learns to combine them to simultaneously perform voice conversion\n",
      "while synthesizing audio. To the best of our knowledge, LVC-VC is one of the\n",
      "first models to be proposed that can perform zero-shot voice conversion in an\n",
      "end-to-end manner, and it is the first to do so using a vocoder-like neural\n",
      "framework. Experiments show that our model achieves competitive or better voice\n",
      "style transfer performance compared to several baselines while maintaining the\n",
      "intelligibility of transformed speech much better. \n",
      "\n",
      "\n",
      "Disentangling content and speaking style information is essential for\n",
      "zero-shot non-parallel voice conversion (VC). Our previous study investigated a\n",
      "novel framework with disentangled sequential variational autoencoder (DSVAE) as\n",
      "the backbone for information decomposition. We have demonstrated that\n",
      "simultaneous disentangling content embedding and speaker embedding from one\n",
      "utterance is feasible for zero-shot VC. In this study, we continue the\n",
      "direction by raising one concern about the prior distribution of content branch\n",
      "in the DSVAE baseline. We find the random initialized prior distribution will\n",
      "force the content embedding to reduce the phonetic-structure information during\n",
      "the learning process, which is not a desired property. Here, we seek to achieve\n",
      "a better content embedding with more phonetic information preserved. We propose\n",
      "conditional DSVAE, a new model that enables content bias as a condition to the\n",
      "prior modeling and reshapes the content embedding sampled from the posterior\n",
      "distribution. In our experiment on the VCTK dataset, we demonstrate that\n",
      "content embeddings derived from the conditional DSVAE overcome the randomness\n",
      "and achieve a much better phoneme classification accuracy, a stabilized\n",
      "vocalization and a better zero-shot VC performance compared with the\n",
      "competitive DSVAE baseline. \n",
      "\n",
      "\n",
      "Adapting one's voice to different ambient environments and social\n",
      "interactions is required for human social interaction. In robotics, the ability\n",
      "to recognize speech in noisy and quiet environments has received significant\n",
      "attention, but considering ambient cues in the production of social speech\n",
      "features has been little explored. Our research aims to modify a robot's speech\n",
      "to maximize acceptability in various social and acoustic contexts, starting\n",
      "with a use case for service robots in varying restaurants. We created an\n",
      "original dataset collected over Zoom with participants conversing in scripted\n",
      "and unscripted tasks given 7 different ambient sounds and background images.\n",
      "Voice conversion methods, in addition to altered Text-to-Speech that matched\n",
      "ambient specific data, were used for speech synthesis tasks. We conducted a\n",
      "subjective perception study that showed humans prefer synthetic speech that\n",
      "matches ambience and social context, ultimately preferring more human-like\n",
      "voices. This work provides three solutions to ambient and socially appropriate\n",
      "synthetic voices: (1) a novel protocol to collect real contextual audio voice\n",
      "data, (2) tools and directions to manipulate robot speech for appropriate\n",
      "social and ambient specific interactions, and (3) insight into voice\n",
      "conversion's role in flexibly altering robot speech to match different ambient\n",
      "environments. \n",
      "\n",
      "\n",
      "The main objective of the spoofing countermeasure system is to detect the\n",
      "artifacts within the input speech caused by the speech synthesis or voice\n",
      "conversion process. In order to achieve this, we propose to adopt an attentive\n",
      "activation function, more specifically attention rectified linear unit (AReLU)\n",
      "to the end-to-end spoofing countermeasure system. Since the AReLU employs the\n",
      "attention mechanism to boost the contribution of relevant input features while\n",
      "suppressing the irrelevant ones, introducing AReLU can help the countermeasure\n",
      "system to focus on the features related to the artifacts. The proposed\n",
      "framework was experimented on the logical access (LA) task of ASVSpoof2019\n",
      "dataset, and outperformed the systems using the standard non-learnable\n",
      "activation functions. \n",
      "\n",
      "\n",
      "Data augmentation via voice conversion (VC) has been successfully applied to\n",
      "low-resource expressive text-to-speech (TTS) when only neutral data for the\n",
      "target speaker are available. Although the quality of VC is crucial for this\n",
      "approach, it is challenging to learn a stable VC model because the amount of\n",
      "data is limited in low-resource scenarios, and highly expressive speech has\n",
      "large acoustic variety. To address this issue, we propose a novel data\n",
      "augmentation method that combines pitch-shifting and VC techniques. Because\n",
      "pitch-shift data augmentation enables the coverage of a variety of pitch\n",
      "dynamics, it greatly stabilizes training for both VC and TTS models, even when\n",
      "only 1,000 utterances of the target speaker's neutral data are available.\n",
      "Subjective test results showed that a FastSpeech 2-based emotional TTS system\n",
      "with the proposed method improved naturalness and emotional similarity compared\n",
      "with conventional methods. \n",
      "\n",
      "\n",
      "In this paper, we describe our speech generation system for the first Audio\n",
      "Deep Synthesis Detection Challenge (ADD 2022). Firstly, we build an any-to-many\n",
      "voice conversion (VC) system to convert source speech with arbitrary language\n",
      "content into the target speaker%u2019s fake speech. Then the converted speech\n",
      "generated from VC is post-processed in the time domain to improve the deception\n",
      "ability. The experimental results show that our system has adversarial ability\n",
      "against anti-spoofing detectors with a little compromise in audio quality and\n",
      "speaker similarity. This system ranks top in Track 3.1 in the ADD 2022, showing\n",
      "that our method could also gain good generalization ability against different\n",
      "detectors. \n",
      "\n",
      "\n",
      "Automatic speaker verification is susceptible to various manipulations and\n",
      "spoofing, such as text-to-speech (TTS) synthesis, voice conversion (VC),\n",
      "replay, tampering, and so on. In this paper, we consider a new spoofing\n",
      "scenario called \"Partial Spoof\" (PS) in which synthesized or transformed audio\n",
      "segments are embedded into a bona fide speech utterance. While existing\n",
      "countermeasures (CMs) can detect fully spoofed utterances, there is a need for\n",
      "their adaptation or extension to the PS scenario to detect utterances in which\n",
      "only a part of the audio signal is generated and hence only a fraction of an\n",
      "utterance is spoofed. For improved explainability, such new CMs should ideally\n",
      "also be able to detect such short spoofed segments. Our previous study\n",
      "introduced the first version of a speech database suitable for training CMs for\n",
      "the PS scenario and showed that, although it is possible to train CMs to\n",
      "execute the two types of detection described above, there is much room for\n",
      "improvement. In this paper we propose various improvements to construct a\n",
      "significantly more accurate CM that can detect short generated spoofed audio\n",
      "segments at finer temporal resolutions. First, we introduce newly proposed\n",
      "self-supervised pre-trained models as enhanced feature extractors. Second, we\n",
      "extend the PartialSpoof database by adding segment labels for various temporal\n",
      "resolutions, ranging from 20 ms to 640 ms. Third, we propose a new CM and\n",
      "training strategies that enable the simultaneous use of the utterance-level and\n",
      "segment-level labels at different temporal resolutions. We also show that the\n",
      "proposed CM is capable of detecting spoofing at the utterance level with low\n",
      "error rates, not only in the PS scenario but also in a related logical access\n",
      "(LA) scenario. The equal error rates of utterance-level detection on the\n",
      "PartialSpoof and the ASVspoof 2019 LA database were 0.47% and 0.59%,\n",
      "respectively. \n",
      "\n",
      "\n",
      "Recent research showed that an autoencoder trained with speech of a single\n",
      "speaker, called exemplar autoencoder (eAE), can be used for any-to-one voice\n",
      "conversion (VC). Compared to large-scale many-to-many models such as AutoVC,\n",
      "the eAE model is easy and fast in training, and may recover more details of the\n",
      "target speaker.\n",
      "  To ensure VC quality, the latent code should represent and only represent\n",
      "content information. However, this is not easy to attain for eAE as it is\n",
      "unaware of any speaker variation in model training. To tackle the problem, we\n",
      "propose a simple yet effective approach based on a cycle consistency loss.\n",
      "Specifically, we train eAEs of multiple speakers with a shared encoder, and\n",
      "meanwhile encourage the speech reconstructed from any speaker-specific decoder\n",
      "to get a consistent latent code as the original speech when cycled back and\n",
      "encoded again. Experiments conducted on the AISHELL-3 corpus showed that this\n",
      "new approach improved the baseline eAE consistently. The source code and\n",
      "examples are available at the project page: http://project.cslt.org/. \n",
      "\n",
      "\n",
      "Text-to-speech and voice conversion studies are constantly improving to the\n",
      "extent where they can produce synthetic speech almost indistinguishable from\n",
      "bona fide human speech. In this regard, the importance of countermeasures (CM)\n",
      "against synthetic voice attacks of the automatic speaker verification (ASV)\n",
      "systems emerges. Nonetheless, most end-to-end spoofing detection networks are\n",
      "black-box systems, and the answer to what is an effective representation for\n",
      "finding artifacts remains veiled. In this paper, we examine which feature space\n",
      "can effectively represent synthetic artifacts using wav2vec 2.0, and study\n",
      "which architecture can effectively utilize the space. Our study allows us to\n",
      "analyze which attribute of speech signals is advantageous for the CM systems.\n",
      "The proposed CM system achieved 0.31% equal error rate (EER) on ASVspoof 2019\n",
      "LA evaluation set for the spoof detection task. We further propose a simple yet\n",
      "effective spoofing aware speaker verification (SASV) method, which takes\n",
      "advantage of the disentangled representations from our countermeasure system.\n",
      "Evaluation performed with the SASV Challenge 2022 database show 1.08% of SASV\n",
      "EER. Quantitative analysis shows that using the explored feature space of\n",
      "wav2vec 2.0 advantages both spoofing CM and SASV. \n",
      "\n",
      "\n",
      "Disentangled representation learning aims to extract explanatory features or\n",
      "factors and retain salient information. Factorized hierarchical variational\n",
      "autoencoder (FHVAE) presents a way to disentangle a speech signal into\n",
      "sequential-level and segmental-level features, which represent speaker identity\n",
      "and speech content information, respectively. As a self-supervised objective,\n",
      "autoregressive predictive coding (APC), on the other hand, has been used in\n",
      "extracting meaningful and transferable speech features for multiple downstream\n",
      "tasks. Inspired by the success of these two representation learning methods,\n",
      "this paper proposes to integrate the APC objective into the FHVAE framework\n",
      "aiming at benefiting from the additional self-supervision target. The main\n",
      "proposed method requires neither more training data nor more computational cost\n",
      "at test time, but obtains improved meaningful representations while maintaining\n",
      "disentanglement. The experiments were conducted on the TIMIT dataset. Results\n",
      "demonstrate that FHVAE equipped with the additional self-supervised objective\n",
      "is able to learn features providing superior performance for tasks including\n",
      "speech recognition and speaker recognition. Furthermore, voice conversion, as\n",
      "one application of disentangled representation learning, has been applied and\n",
      "evaluated. The results show performance similar to baseline of the new\n",
      "framework on voice conversion. \n",
      "\n",
      "\n",
      "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system\n",
      "submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS\n",
      "values of speech samples collected from previous Blizzard Challenges and Voice\n",
      "Conversion Challenges for two tracks: a main track for in-domain prediction and\n",
      "an out-of-domain (OOD) track for which there is less labeled data from\n",
      "different listening tests. Our system is based on ensemble learning of strong\n",
      "and weak learners. Strong learners incorporate several improvements to the\n",
      "previous fine-tuning models of self-supervised learning (SSL) models, while\n",
      "weak learners use basic machine-learning methods to predict scores from SSL\n",
      "features. In the Challenge, our system had the highest score on several metrics\n",
      "for both the main and OOD tracks. In addition, we conducted ablation studies to\n",
      "investigate the effectiveness of our proposed methods. \n",
      "\n",
      "\n",
      "Collecting speech data is an important step in training speech recognition\n",
      "systems and other speech-based machine learning models. However, the issue of\n",
      "privacy protection is an increasing concern that must be addressed. The current\n",
      "study investigates the use of voice conversion as a method for anonymizing\n",
      "voices. In particular, we train several voice conversion models using\n",
      "self-supervised speech representations including Wav2Vec2.0, Hubert and\n",
      "UniSpeech. Converted voices retain a low word error rate within 1% of the\n",
      "original voice. Equal error rate increases from 1.52% to 46.24% on the\n",
      "LibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus\n",
      "which signifies degraded performance on speaker verification. Lastly, we\n",
      "conduct experiments on dysarthric speech data to show that speech features\n",
      "relevant to articulation, prosody, phonation and phonology can be extracted\n",
      "from anonymized voices for discriminating between healthy and pathological\n",
      "speech. \n",
      "\n",
      "\n",
      "Recent advances in sophisticated synthetic speech generated from\n",
      "text-to-speech (TTS) or voice conversion (VC) systems cause threats to the\n",
      "existing automatic speaker verification (ASV) systems. Since such synthetic\n",
      "speech is generated from diverse algorithms, generalization ability with using\n",
      "limited training data is indispensable for a robust anti-spoofing system. In\n",
      "this work, we propose a transfer learning scheme based on the wav2vec 2.0\n",
      "pretrained model with variational information bottleneck (VIB) for speech\n",
      "anti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA)\n",
      "database shows that our method improves the performance of distinguishing\n",
      "unseen spoofed and genuine speech, outperforming current state-of-the-art\n",
      "anti-spoofing systems. Furthermore, we show that the proposed system improves\n",
      "performance in low-resource and cross-dataset settings of anti-spoofing task\n",
      "significantly, demonstrating that our system is also robust in terms of data\n",
      "size and data distribution. \n",
      "\n",
      "\n",
      "It was shown recently that a combination of ASR and TTS models yield highly\n",
      "competitive performance on standard voice conversion tasks such as the Voice\n",
      "Conversion Challenge 2020 (VCC2020). To obtain good performance both models\n",
      "require pretraining on large amounts of data, thereby obtaining large models\n",
      "that are potentially inefficient in use. In this work we present a model that\n",
      "is significantly smaller and thereby faster in processing while obtaining\n",
      "equivalent performance. To achieve this the proposed model, Dynamic-GAN-VC\n",
      "(DYGAN-VC), uses a non-autoregressive structure and makes use of vector\n",
      "quantised embeddings obtained from a VQWav2vec model. Furthermore dynamic\n",
      "convolution is introduced to improve speech content modeling while requiring a\n",
      "small number of parameters. Objective and subjective evaluation was performed\n",
      "using the VCC2020 task, yielding MOS scores of up to 3.86, and character error\n",
      "rates as low as 4.3\\%. This was achieved with approximately half the number of\n",
      "model parameters, and up to 8 times faster decoding speed. \n",
      "\n",
      "\n",
      "The goal of voice conversion (VC) is to convert input voice to match the\n",
      "target speaker's voice while keeping text and prosody intact. VC is usually\n",
      "used in entertainment and speaking-aid systems, as well as applied for speech\n",
      "data generation and augmentation. The development of any-to-any VC systems,\n",
      "which are capable of generating voices unseen during model training, is of\n",
      "particular interest to both researchers and the industry. Despite recent\n",
      "progress, any-to-any conversion quality is still inferior to natural speech.\n",
      "  In this work, we propose a new any-to-any voice conversion pipeline. Our\n",
      "approach uses automated speech recognition (ASR) features, pitch tracking, and\n",
      "a state-of-the-art waveform prediction model. According to multiple subjective\n",
      "and objective evaluations, our method outperforms modern baselines in terms of\n",
      "voice quality, similarity and consistency. \n",
      "\n",
      "\n",
      "Recent advances in neural text-to-speech research have been dominated by\n",
      "two-stage pipelines utilizing low-level intermediate speech representation such\n",
      "as mel-spectrograms. However, such predetermined features are fundamentally\n",
      "limited, because they do not allow to exploit the full potential of a\n",
      "data-driven approach through learning hidden representations. For this reason,\n",
      "several end-to-end methods have been proposed. However, such models are harder\n",
      "to train and require a large number of high-quality recordings with\n",
      "transcriptions. Here, we propose WavThruVec - a two-stage architecture that\n",
      "resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as\n",
      "intermediate speech representation. Since these hidden activations provide\n",
      "high-level linguistic features, they are more robust to noise. That allows us\n",
      "to utilize annotated speech datasets of a lower quality to train the\n",
      "first-stage module. At the same time, the second-stage component can be trained\n",
      "on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are\n",
      "already time-aligned. This results in an increased generalization capability to\n",
      "out-of-vocabulary words, as well as to a better generalization to unseen\n",
      "speakers. We show that the proposed model not only matches the quality of\n",
      "state-of-the-art neural models, but also presents useful properties enabling\n",
      "tasks like voice conversion or zero-shot synthesis. \n",
      "\n",
      "\n",
      "Traditional studies on voice conversion (VC) have made progress with parallel\n",
      "training data and known speakers. Good voice conversion quality is obtained by\n",
      "exploring better alignment modules or expressive mapping functions. In this\n",
      "study, we investigate zero-shot VC from a novel perspective of self-supervised\n",
      "disentangled speech representation learning. Specifically, we achieve the\n",
      "disentanglement by balancing the information flow between global speaker\n",
      "representation and time-varying content representation in a sequential\n",
      "variational autoencoder (VAE). A zero-shot voice conversion is performed by\n",
      "feeding an arbitrary speaker embedding and content embeddings to the VAE\n",
      "decoder. Besides that, an on-the-fly data augmentation training strategy is\n",
      "applied to make the learned representation noise invariant. On TIMIT and VCTK\n",
      "datasets, we achieve state-of-the-art performance on both objective evaluation,\n",
      "i.e., speaker verification (SV) on speaker embedding and content embedding, and\n",
      "subjective evaluation, i.e., voice naturalness and similarity, and remains to\n",
      "be robust even with noisy source/target utterances. \n",
      "\n",
      "\n",
      "Variational auto-encoder (VAE) is an effective neural network architecture to\n",
      "disentangle a speech utterance into speaker identity and linguistic content\n",
      "latent embeddings, then generate an utterance for a target speaker from that of\n",
      "a source speaker. This is possible by concatenating the identity embedding of\n",
      "the target speaker and the content embedding of the source speaker uttering a\n",
      "desired sentence. In this work, we propose to improve VAE models with\n",
      "self-attention and structural regularization (RGSM). Specifically, we found a\n",
      "suitable location of VAE's decoder to add a self-attention layer for\n",
      "incorporating non-local information in generating a converted utterance and\n",
      "hiding the source speaker's identity. We applied relaxed group-wise splitting\n",
      "method (RGSM) to regularize network weights and remarkably enhance\n",
      "generalization performance.\n",
      "  In experiments of zero-shot many-to-many voice conversion task on VCTK data\n",
      "set, with the self-attention layer and relaxed group-wise splitting method, our\n",
      "model achieves a gain of speaker classification accuracy on unseen speakers by\n",
      "28.3\\% while slightly improved conversion voice quality in terms of MOSNet\n",
      "scores. Our encouraging findings point to future research on integrating more\n",
      "variety of attention structures in VAE framework while controlling model size\n",
      "and overfitting for advancing zero-shot many-to-many voice conversions. \n",
      "\n",
      "\n",
      "Emotional voice conversion (EVC) focuses on converting a speech utterance\n",
      "from a source to a target emotion; it can thus be a key enabling technology for\n",
      "human-computer interaction applications and beyond. However, EVC remains an\n",
      "unsolved research problem with several challenges. In particular, as speech\n",
      "rate and rhythm are two key factors of emotional conversion, models have to\n",
      "generate output sequences of differing length. Sequence-to-sequence modelling\n",
      "is recently emerging as a competitive paradigm for models that can overcome\n",
      "those challenges. In an attempt to stimulate further research in this promising\n",
      "new direction, recent sequence-to-sequence EVC papers were systematically\n",
      "investigated and reviewed from six perspectives: their motivation, training\n",
      "strategies, model architectures, datasets, model inputs, and evaluation\n",
      "methods. This information is organised to provide the research community with\n",
      "an easily digestible overview of the current state-of-the-art. Finally, we\n",
      "discuss existing challenges of sequence-to-sequence EVC. \n",
      "\n",
      "\n",
      "We explore cross-lingual multi-speaker speech synthesis and cross-lingual\n",
      "voice conversion applied to data augmentation for automatic speech recognition\n",
      "(ASR) systems. Through extensive experiments, we show that our approach permits\n",
      "the application of speech synthesis and voice conversion to improve ASR systems\n",
      "on a target language using only one target-language speaker during model\n",
      "training. We managed to close the gap between ASR models trained with\n",
      "synthesized versus human speech compared to other works that use many speakers.\n",
      "Finally, we show that it is possible to obtain promising ASR training results\n",
      "with our data augmentation method using only a single real speaker in a target\n",
      "language. \n",
      "\n",
      "\n",
      "This paper presents an analysis of speech synthesis quality achieved by\n",
      "simultaneously performing voice conversion and language code-switching using\n",
      "multilingual VQ-VAE speech synthesis in German, French, English and Italian. In\n",
      "this paper, we utilize VQ code indices representing phone information from\n",
      "VQ-VAE to perform code-switching and a VQ speaker code to perform voice\n",
      "conversion in a single system with a neural vocoder. Our analysis examines\n",
      "several aspects of code-switching including the number of language switches and\n",
      "the number of words involved in each switch. We found that speech synthesis\n",
      "quality degrades after increasing the number of language switches within an\n",
      "utterance and decreasing the number of words. We also found some evidence of\n",
      "accent transfer when performing voice conversion across languages as observed\n",
      "when a speaker's original language differs from the language of a synthetic\n",
      "target utterance. We present results from our listening tests and discuss the\n",
      "inherent difficulties of assessing accent transfer in speech synthesis. Our\n",
      "work highlights some of the limitations and strengths of using a\n",
      "semi-supervised end-to-end system like VQ-VAE for handling multilingual\n",
      "synthesis. Our work provides insight into why multilingual speech synthesis is\n",
      "challenging and we suggest some directions for expanding work in this area. \n",
      "\n",
      "\n",
      "Privacy and security are major concerns when sharing and collecting speech\n",
      "data for cloud services such as automatic speech recognition (ASR) and speech\n",
      "emotion recognition (SER). Existing solutions for client-side privacy mainly\n",
      "focus on voice conversion or voice modification to convert a raw utterance into\n",
      "another one with similar content but different, or no, identity-related\n",
      "information. However, an alternative approach to share speech data under the\n",
      "form of privacy-preserving representations has been largely under-explored. To\n",
      "fill this gap, we propose a speech anonymization framework that provides formal\n",
      "privacy guarantees via noise perturbation to a selected subset of the\n",
      "high-utility representations extracted using a pre-trained speech encoder. The\n",
      "subset is chosen with a Transformer-based privacy-risk saliency estimator. We\n",
      "validate our framework on four tasks, namely, Automatic Speaker Verification\n",
      "(ASV), ASR, SER and Intent Classification (IC) for privacy and utility\n",
      "assessment. Experimental results show that our approach is able to achieve a\n",
      "competitive, or even better, utility compared to the baselines that use voice\n",
      "conversion and voice modification, providing the same level of privacy.\n",
      "Moreover, the easily-controlled amount of perturbation allows our framework to\n",
      "have a flexible range of privacy-utility trade-offs without re-training any\n",
      "components. \n",
      "\n",
      "\n",
      "SpeechSplit can perform aspect-specific voice conversion by disentangling\n",
      "speech into content, rhythm, pitch, and timbre using multiple autoencoders in\n",
      "an unsupervised manner. However, SpeechSplit requires careful tuning of the\n",
      "autoencoder bottlenecks, which can be time-consuming and less robust. This\n",
      "paper proposes SpeechSplit 2.0, which constrains the information flow of the\n",
      "speech component to be disentangled on the autoencoder input using efficient\n",
      "signal processing methods instead of bottleneck tuning. Evaluation results show\n",
      "that SpeechSplit 2.0 achieves comparable performance to SpeechSplit in speech\n",
      "disentanglement and superior robustness to the bottleneck size variations. Our\n",
      "code is available at https://github.com/biggytruck/SpeechSplit2. \n",
      "\n",
      "\n",
      "Non-parallel data voice conversion (VC) have achieved considerable\n",
      "breakthroughs recently through introducing bottleneck features (BNFs) extracted\n",
      "by the automatic speech recognition(ASR) model. However, selection of BNFs have\n",
      "a significant impact on VC result. For example, when extracting BNFs from ASR\n",
      "trained with Cross Entropy loss (CE-BNFs) and feeding into neural network to\n",
      "train a VC system, the timbre similarity of converted speech is significantly\n",
      "degraded. If BNFs are extracted from ASR trained using Connectionist Temporal\n",
      "Classification loss (CTC-BNFs), the naturalness of the converted speech may\n",
      "decrease. This phenomenon is caused by the difference of information contained\n",
      "in BNFs. In this paper, we proposed an any-to-one VC method using hybrid\n",
      "bottleneck features extracted from CTC-BNFs and CE-BNFs to complement each\n",
      "other advantages. Gradient reversal layer and instance normalization were used\n",
      "to extract prosody information from CE-BNFs and content information from\n",
      "CTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate\n",
      "high-quality waveform. Experimental results show that our proposed method\n",
      "achieves higher similarity, naturalness, quality than baseline method and\n",
      "reveals the differences between the information contained in CE-BNFs and\n",
      "CTC-BNFs as well as the influence they have on the converted speech. \n",
      "\n",
      "\n",
      "We present the first edition of the VoiceMOS Challenge, a scientific event\n",
      "that aims to promote the study of automatic prediction of the mean opinion\n",
      "score (MOS) of synthetic speech. This challenge drew 22 participating teams\n",
      "from academia and industry who tried a variety of approaches to tackle the\n",
      "problem of predicting human ratings of synthesized speech. The listening test\n",
      "data for the main track of the challenge consisted of samples from 187\n",
      "different text-to-speech and voice conversion systems spanning over a decade of\n",
      "research, and the out-of-domain track consisted of data from more recent\n",
      "systems rated in a separate listening test. Results of the challenge show the\n",
      "effectiveness of fine-tuning self-supervised speech models for the MOS\n",
      "prediction task, as well as the difficulty of predicting MOS ratings for unseen\n",
      "speakers and listeners, and for unseen systems in the out-of-domain setting. \n",
      "\n",
      "\n",
      "Deep speaker embeddings have been shown effective for assessing cognitive\n",
      "impairments aside from their original purpose of speaker verification. However,\n",
      "the research found that speaker embeddings encode speaker identity and an array\n",
      "of information, including speaker demographics, such as sex and age, and speech\n",
      "contents to an extent, which are known confounders in the assessment of\n",
      "cognitive impairments. In this paper, we hypothesize that content information\n",
      "separated from speaker identity using a framework for voice conversion is more\n",
      "effective for assessing cognitive impairments and train simple classifiers for\n",
      "the comparative analysis on the DementiaBank Pitt Corpus. Our results show that\n",
      "while content embeddings have an advantage over speaker embeddings for the\n",
      "defined problem, further experiments show their effectiveness depends on\n",
      "information encoded in speaker embeddings due to the inherent design of the\n",
      "architecture used for extracting contents. \n",
      "\n",
      "\n",
      "Recently, more and more zero-shot voice conversion algorithms have been\n",
      "proposed. As a fundamental part of zero-shot voice conversion, speaker\n",
      "embeddings are the key to improving the converted speech's speaker similarity.\n",
      "In this paper, we study the impact of speaker embeddings on zero-shot voice\n",
      "conversion performance. To better represent the characteristics of the target\n",
      "speaker and improve the speaker similarity in zero-shot voice conversion, we\n",
      "propose a novel speaker representation method in this paper. Our method\n",
      "combines the advantages of D-vector, global style token (GST) based speaker\n",
      "representation and auxiliary supervision. Objective and subjective evaluations\n",
      "show that the proposed method achieves a decent performance on zero-shot voice\n",
      "conversion and significantly improves speaker similarity over D-vector and\n",
      "GST-based speaker embedding. \n",
      "\n",
      "\n",
      "Non-parallel voice conversion (VC) is typically achieved using lossy\n",
      "representations of the source speech. However, ensuring only speaker identity\n",
      "information is dropped whilst all other information from the source speech is\n",
      "retained is a large challenge. This is particularly challenging in the scenario\n",
      "where at inference-time we have no knowledge of the text being read, i.e.,\n",
      "text-free VC. To mitigate this, we investigate information-preserving VC\n",
      "approaches.\n",
      "  Normalising flows have gained attention for text-to-speech synthesis, however\n",
      "have been under-explored for VC. Flows utilize invertible functions to learn\n",
      "the likelihood of the data, thus provide a lossless encoding of speech. We\n",
      "investigate normalising flows for VC in both text-conditioned and text-free\n",
      "scenarios. Furthermore, for text-free VC we compare pre-trained and\n",
      "jointly-learnt priors. Flow-based VC evaluations show no degradation between\n",
      "text-free and text-conditioned VC, resulting in improvements over the\n",
      "state-of-the-art. Also, joint-training of the prior is found to negatively\n",
      "impact text-free VC quality. \n",
      "\n",
      "\n",
      "In recent text-to-speech synthesis and voice conversion systems, a\n",
      "mel-spectrogram is commonly applied as an intermediate representation, and the\n",
      "necessity for a mel-spectrogram vocoder is increasing. A mel-spectrogram\n",
      "vocoder must solve three inverse problems: recovery of the original-scale\n",
      "magnitude spectrogram, phase reconstruction, and frequency-to-time conversion.\n",
      "A typical convolutional mel-spectrogram vocoder solves these problems jointly\n",
      "and implicitly using a convolutional neural network, including temporal\n",
      "upsampling layers, when directly calculating a raw waveform. Such an approach\n",
      "allows skipping redundant processes during waveform synthesis (e.g., the direct\n",
      "reconstruction of high-dimensional original-scale spectrograms). By contrast,\n",
      "the approach solves all problems in a black box and cannot effectively employ\n",
      "the time-frequency structures existing in a mel-spectrogram. We thus propose\n",
      "iSTFTNet, which replaces some output-side layers of the mel-spectrogram vocoder\n",
      "with the inverse short-time Fourier transform (iSTFT) after sufficiently\n",
      "reducing the frequency dimension using upsampling layers, reducing the\n",
      "computational cost from black-box modeling and avoiding redundant estimations\n",
      "of high-dimensional spectrograms. During our experiments, we applied our ideas\n",
      "to three HiFi-GAN variants and made the models faster and more lightweight with\n",
      "a reasonable speech quality. Audio samples are available at\n",
      "https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet/. \n",
      "\n",
      "\n",
      "This paper addresses the unsupervised learning of content-style decomposed\n",
      "representation. We first give a definition of style and then model the\n",
      "content-style representation as a token-level bipartite graph. An unsupervised\n",
      "framework, named Retriever, is proposed to learn such representations. First, a\n",
      "cross-attention module is employed to retrieve permutation invariant (P.I.)\n",
      "information, defined as style, from the input data. Second, a vector\n",
      "quantization (VQ) module is used, together with man-induced constraints, to\n",
      "produce interpretable content tokens. Last, an innovative link attention module\n",
      "serves as the decoder to reconstruct data from the decomposed content and\n",
      "style, with the help of the linking keys. Being modal-agnostic, the proposed\n",
      "Retriever is evaluated in both speech and image domains. The state-of-the-art\n",
      "zero-shot voice conversion performance confirms the disentangling ability of\n",
      "our framework. Top performance is also achieved in the part discovery task for\n",
      "images, verifying the interpretability of our representation. In addition, the\n",
      "vivid part-based style transfer quality demonstrates the potential of Retriever\n",
      "to support various fascinating generative tasks. Project page at\n",
      "https://ydcustc.github.io/retriever-demo/. \n",
      "\n",
      "\n",
      "Any-to-any voice conversion problem aims to convert voices for source and\n",
      "target speakers, which are out of the training data. Previous works wildly\n",
      "utilize the disentangle-based models. The disentangle-based model assumes the\n",
      "speech consists of content and speaker style information and aims to untangle\n",
      "them to change the style information for conversion. Previous works focus on\n",
      "reducing the dimension of speech to get the content information. But the size\n",
      "is hard to determine to lead to the untangle overlapping problem. We propose\n",
      "the Disentangled Representation Voice Conversion (DRVC) model to address the\n",
      "issue. DRVC model is an end-to-end self-supervised model consisting of the\n",
      "content encoder, timbre encoder, and generator. Instead of the previous work\n",
      "for reducing speech size to get content, we propose a cycle for restricting the\n",
      "disentanglement by the Cycle Reconstruct Loss and Same Loss. The experiments\n",
      "show there is an improvement for converted speech on quality and voice\n",
      "similarity. \n",
      "\n",
      "\n",
      "Intent classifiers are vital to the successful operation of virtual agent\n",
      "systems. This is especially so in voice activated systems where the data can be\n",
      "noisy with many ambiguous directions for user intents. Before operation begins,\n",
      "these classifiers are generally lacking in real-world training data. Active\n",
      "learning is a common approach used to help label large amounts of collected\n",
      "user input. However, this approach requires many hours of manual labeling work.\n",
      "We present the Nearest Neighbors Scores Improvement (NNSI) algorithm for\n",
      "automatic data selection and labeling. The NNSI reduces the need for manual\n",
      "labeling by automatically selecting highly-ambiguous samples and labeling them\n",
      "with high accuracy. This is done by integrating the classifier's output from a\n",
      "semantically similar group of text samples. The labeled samples can then be\n",
      "added to the training set to improve the accuracy of the classifier. We\n",
      "demonstrated the use of NNSI on two large-scale, real-life voice conversation\n",
      "systems. Evaluation of our results showed that our method was able to select\n",
      "and label useful samples with high accuracy. Adding these new samples to the\n",
      "training data significantly improved the classifiers and reduced error rates by\n",
      "up to 10%. \n",
      "\n",
      "\n",
      "Voice Conversion(VC) refers to changing the timbre of a speech while\n",
      "retaining the discourse content. Recently, many works have focused on\n",
      "disentangle-based learning techniques to separate the timbre and the linguistic\n",
      "content information from a speech signal. Once successful, voice conversion\n",
      "will be feasible and straightforward. This paper proposed a novel one-shot\n",
      "voice conversion framework based on vector quantization voice conversion (VQVC)\n",
      "and AutoVC, called AVQVC. A new training method is applied to VQVC to separate\n",
      "content and timbre information from speech more effectively. The result shows\n",
      "that this approach has better performance than VQVC in separating content and\n",
      "timbre to improve the sound quality of generated speech. \n",
      "\n",
      "\n",
      "Though significant progress has been made for speaker-dependent\n",
      "Video-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker\n",
      "VTS that can map silent video to speech, while allowing flexible control of\n",
      "speaker identity, all in a single system. This paper proposes a novel\n",
      "multi-speaker VTS system based on cross-modal knowledge transfer from voice\n",
      "conversion (VC), where vector quantization with contrastive predictive coding\n",
      "(VQCPC) is used for the content encoder of VC to derive discrete phoneme-like\n",
      "acoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to\n",
      "infer the index sequence of acoustic units. The Lip2Ind network can then\n",
      "substitute the content encoder of VC to form a multi-speaker VTS system to\n",
      "convert silent video to acoustic units for reconstructing accurate spoken\n",
      "content. The VTS system also inherits the advantages of VC by using a speaker\n",
      "encoder to produce speaker representations to effectively control the speaker\n",
      "identity of generated speech. Extensive evaluations verify the effectiveness of\n",
      "proposed approach, which can be applied in both constrained vocabulary and open\n",
      "vocabulary conditions, achieving state-of-the-art performance in generating\n",
      "high-quality speech with high naturalness, intelligibility and speaker\n",
      "similarity. Our demo page is released here:\n",
      "https://wendison.github.io/VCVTS-demo/ \n",
      "\n",
      "\n",
      "State-of-the-art text-to-speech (TTS) systems require several hours of\n",
      "recorded speech data to generate high-quality synthetic speech. When using\n",
      "reduced amounts of training data, standard TTS models suffer from speech\n",
      "quality and intelligibility degradations, making training low-resource TTS\n",
      "systems problematic. In this paper, we propose a novel extremely low-resource\n",
      "TTS method called Voice Filter that uses as little as one minute of speech from\n",
      "a target speaker. It uses voice conversion (VC) as a post-processing module\n",
      "appended to a pre-existing high-quality TTS system and marks a conceptual shift\n",
      "in the existing TTS paradigm, framing the few-shot TTS problem as a VC task.\n",
      "Furthermore, we propose to use a duration-controllable TTS system to create a\n",
      "parallel speech corpus to facilitate the VC task. Results show that the Voice\n",
      "Filter outperforms state-of-the-art few-shot speech synthesis techniques in\n",
      "terms of objective and subjective metrics on one minute of speech on a diverse\n",
      "set of voices, while being competitive against a TTS model built on 30 times\n",
      "more data. \n",
      "\n",
      "\n",
      "The past few years have witnessed the significant advances of speech\n",
      "synthesis and voice conversion technologies. However, such technologies can\n",
      "undermine the robustness of broadly implemented biometric identification models\n",
      "and can be harnessed by in-the-wild attackers for illegal uses. The ASVspoof\n",
      "challenge mainly focuses on synthesized audios by advanced speech synthesis and\n",
      "voice conversion models, and replay attacks. Recently, the first Audio Deep\n",
      "Synthesis Detection challenge (ADD 2022) extends the attack scenarios into more\n",
      "aspects. Also ADD 2022 is the first challenge to propose the partially fake\n",
      "audio detection task. Such brand new attacks are dangerous and how to tackle\n",
      "such attacks remains an open question. Thus, we propose a novel framework by\n",
      "introducing the question-answering (fake span discovery) strategy with the\n",
      "self-attention mechanism to detect partially fake audios. The proposed fake\n",
      "span detection module tasks the anti-spoofing model to predict the start and\n",
      "end positions of the fake clip within the partially fake audio, address the\n",
      "model's attention into discovering the fake spans rather than other shortcuts\n",
      "with less generalization, and finally equips the model with the discrimination\n",
      "capacity between real and partially fake audios. Our submission ranked second\n",
      "in the partially fake audio detection track of ADD 2022. \n",
      "\n",
      "\n",
      "We address the problem of cross-speaker style transfer for text-to-speech\n",
      "(TTS) using data augmentation via voice conversion. We assume to have a corpus\n",
      "of neutral non-expressive data from a target speaker and supporting\n",
      "conversational expressive data from different speakers. Our goal is to build a\n",
      "TTS system that is expressive, while retaining the target speaker's identity.\n",
      "The proposed approach relies on voice conversion to first generate high-quality\n",
      "data from the set of supporting expressive speakers. The voice converted data\n",
      "is then pooled with natural data from the target speaker and used to train a\n",
      "single-speaker multi-style TTS system. We provide evidence that this approach\n",
      "is efficient, flexible, and scalable. The method is evaluated using one or more\n",
      "supporting speakers, as well as a variable amount of supporting data. We\n",
      "further provide evidence that this approach allows some controllability of\n",
      "speaking style, when using multiple supporting speakers. We conclude by scaling\n",
      "our proposed technology to a set of 14 speakers across 7 languages. Results\n",
      "indicate that our technology consistently improves synthetic samples in terms\n",
      "of style similarity, while retaining the target speaker's identity. \n",
      "\n",
      "\n",
      "The voice conversion task is to modify the speaker identity of continuous\n",
      "speech while preserving the linguistic content. Generally, the naturalness and\n",
      "similarity are two main metrics for evaluating the conversion quality, which\n",
      "has been improved significantly in recent years. This paper presents the\n",
      "HCCL-DKU entry for the fake audio generation task of the 2022 ICASSP ADD\n",
      "challenge. We propose a novel ppg-based voice conversion model that adopts a\n",
      "fully end-to-end structure. Experimental results show that the proposed method\n",
      "outperforms other conversion models, including Tacotron-based and\n",
      "Fastspeech-based models, on conversion quality and spoofing performance against\n",
      "anti-spoofing systems. In addition, we investigate several post-processing\n",
      "methods for better spoofing power. Finally, we achieve second place with a\n",
      "deception success rate of 0.916 in the ADD challenge. \n",
      "\n",
      "\n",
      "Voice conversion has made great progress in the past few years under the\n",
      "studio-quality test scenario in terms of speech quality and speaker similarity.\n",
      "However, in real applications, test speech from source speaker or target\n",
      "speaker can be corrupted by various environment noises, which seriously degrade\n",
      "the speech quality and speaker similarity. In this paper, we propose a novel\n",
      "encoder-decoder based noise-robust voice conversion framework, which consists\n",
      "of a speaker encoder, a content encoder, a decoder, and two domain adversarial\n",
      "neural networks. Specifically, we integrate disentangling speaker and content\n",
      "representation technique with domain adversarial training technique. Domain\n",
      "adversarial training makes speaker representations and content representations\n",
      "extracted by speaker encoder and content encoder from clean speech and noisy\n",
      "speech in the same space, respectively. In this way, the learned speaker and\n",
      "content representations are noise-invariant. Therefore, the two noise-invariant\n",
      "representations can be taken as input by the decoder to predict the clean\n",
      "converted spectrum. The experimental results demonstrate that our proposed\n",
      "method can synthesize clean converted speech under noisy test scenarios, where\n",
      "the source speech and target speech can be corrupted by seen or unseen noise\n",
      "types during the training process. Additionally, both speech quality and\n",
      "speaker similarity are improved. \n",
      "\n",
      "\n",
      "In this paper, we propose an invertible deep learning framework called INVVC\n",
      "for voice conversion. It is designed against the possible threats that\n",
      "inherently come along with voice conversion systems. Specifically, we develop\n",
      "an invertible framework that makes the source identity traceable. The framework\n",
      "is built on a series of invertible $1\\times1$ convolutions and flows consisting\n",
      "of affine coupling layers. We apply the proposed framework to one-to-one voice\n",
      "conversion and many-to-one conversion using parallel training data.\n",
      "Experimental results show that this approach yields impressive performance on\n",
      "voice conversion and, moreover, the converted results can be reversed back to\n",
      "the source inputs utilizing the same parameters as in forwarding. \n",
      "\n",
      "\n",
      "Adversarial waveform generation has been a popular approach as the backend of\n",
      "singing voice conversion (SVC) to generate high-quality singing audio. However,\n",
      "the instability of GAN also leads to other problems, such as pitch jitters and\n",
      "U/V errors. It affects the smoothness and continuity of harmonics, hence\n",
      "degrades the conversion quality seriously. This paper proposes to feed harmonic\n",
      "signals to the SVC model in advance to enhance audio generation. We extract the\n",
      "sine excitation from the pitch, and filter it with a linear time-varying (LTV)\n",
      "filter estimated by a neural network. Both these two harmonic signals are\n",
      "adopted as the inputs to generate the singing waveform. In our experiments, two\n",
      "mainstream models, MelGAN and ParallelWaveGAN, are investigated to validate the\n",
      "effectiveness of the proposed approach. We conduct a MOS test on clean and\n",
      "noisy test sets. The result shows that both signals significantly improve SVC\n",
      "in fidelity and timbre similarity. Besides, the case analysis further validates\n",
      "that this method enhances the smoothness and continuity of harmonics in the\n",
      "generated audio, and the filtered excitation better matches the target audio. \n",
      "\n",
      "\n",
      "In this paper, we investigate several existing and a new state-of-the-art\n",
      "generative adversarial network-based (GAN) voice conversion method for\n",
      "enhancing dysarthric speech for improved dysarthric speech recognition. We\n",
      "compare key components of existing methods as part of a rigorous ablation study\n",
      "to find the most effective solution to improve dysarthric speech recognition.\n",
      "We find that straightforward signal processing methods such as stationary noise\n",
      "removal and vocoder-based time stretching lead to dysarthric speech recognition\n",
      "results comparable to those obtained when using state-of-the-art GAN-based\n",
      "voice conversion methods as measured using a phoneme recognition task.\n",
      "Additionally, our proposed solution of a combination of MaskCycleGAN-VC and\n",
      "time stretched enhancement is able to improve the phoneme recognition results\n",
      "for certain dysarthric speakers compared to our time stretched baseline. \n",
      "\n",
      "\n",
      "Voice-based human-machine interfaces with an automatic speaker verification\n",
      "(ASV) component are commonly used in the market. However, the threat from\n",
      "presentation attacks is also growing since attackers can use recent speech\n",
      "synthesis technology to produce a natural-sounding voice of a victim.\n",
      "Presentation attack detection (PAD) for ASV, or speech anti-spoofing, is\n",
      "therefore indispensable. Research on voice PAD has seen significant progress\n",
      "since the early 2010s, including the advancement in PAD models, benchmark\n",
      "datasets, and evaluation campaigns. This chapter presents a practical guide to\n",
      "the field of voice PAD, with a focus on logical access attacks using\n",
      "text-to-speech and voice conversion algorithms and spoofing countermeasures\n",
      "based on artifact detection. It introduces the basic concept of voice PAD,\n",
      "explains the common techniques, and provides an experimental study using recent\n",
      "methods on a benchmark dataset. Code for the experiments is open-sourced. \n",
      "\n",
      "\n",
      "Emotional voice conversion (EVC) seeks to convert the emotional state of an\n",
      "utterance while preserving the linguistic content and speaker identity. In EVC,\n",
      "emotions are usually treated as discrete categories overlooking the fact that\n",
      "speech also conveys emotions with various intensity levels that the listener\n",
      "can perceive. In this paper, we aim to explicitly characterize and control the\n",
      "intensity of emotion. We propose to disentangle the speaker style from\n",
      "linguistic content and encode the speaker style into a style embedding in a\n",
      "continuous space that forms the prototype of emotion embedding. We further\n",
      "learn the actual emotion encoder from an emotion-labelled database and study\n",
      "the use of relative attributes to represent fine-grained emotion intensity. To\n",
      "ensure emotional intelligibility, we incorporate emotion classification loss\n",
      "and emotion embedding similarity loss into the training of the EVC network. As\n",
      "desired, the proposed network controls the fine-grained emotion intensity in\n",
      "the output speech. Through both objective and subjective evaluations, we\n",
      "validate the effectiveness of the proposed network for emotional expressiveness\n",
      "and emotion intensity control. \n",
      "\n",
      "\n",
      "Voice biometric systems based on automatic speaker verification (ASV) are\n",
      "exposed to \\textit{spoofing} attacks which may compromise their security. To\n",
      "increase the robustness against such attacks, anti-spoofing or presentation\n",
      "attack detection (PAD) systems have been proposed for the detection of replay,\n",
      "synthesis and voice conversion based attacks. Recently, the scientific\n",
      "community has shown that PAD systems are also vulnerable to adversarial\n",
      "attacks. However, to the best of our knowledge, no previous work have studied\n",
      "the robustness of full voice biometrics systems (ASV + PAD) to these new types\n",
      "of adversarial \\textit{spoofing} attacks. In this work, we develop a new\n",
      "adversarial biometrics transformation network (ABTN) which jointly processes\n",
      "the loss of the PAD and ASV systems in order to generate white-box and\n",
      "black-box adversarial \\textit{spoofing} attacks. The core idea of this system\n",
      "is to generate adversarial \\textit{spoofing} attacks which are able to fool the\n",
      "PAD system without being detected by the ASV system. The experiments were\n",
      "carried out on the ASVspoof 2019 corpus, including both logical access (LA) and\n",
      "physical access (PA) scenarios. The experimental results show that the proposed\n",
      "ABTN clearly outperforms some well-known adversarial techniques in both\n",
      "white-box and black-box attack scenarios. \n",
      "\n",
      "\n",
      "Prosody modeling is important, but still challenging in expressive voice\n",
      "conversion. As prosody is difficult to model, and other factors, e.g., speaker,\n",
      "environment and content, which are entangled with prosody in speech, should be\n",
      "removed in prosody modeling. In this paper, we present IQDubbing to solve this\n",
      "problem for expressive voice conversion. To model prosody, we leverage the\n",
      "recent advances in discrete self-supervised speech representation (DSSR).\n",
      "Specifically, prosody vector is first extracted from pre-trained VQ-Wav2Vec\n",
      "model, where rich prosody information is embedded while most speaker and\n",
      "environment information are removed effectively by quantization. To further\n",
      "filter out the redundant information except prosody, such as content and\n",
      "partial speaker information, we propose two kinds of prosody filters to sample\n",
      "prosody from the prosody vector. Experiments show that IQDubbing is superior to\n",
      "baseline and comparison systems in terms of speech quality while maintaining\n",
      "prosody consistency and speaker similarity. \n",
      "\n",
      "\n",
      "Due to improvements in artificial intelligence, speaker identification (SI)\n",
      "technologies have brought a great direction and are now widely used in a\n",
      "variety of sectors. One of the most important components of SI is feature\n",
      "extraction, which has a substantial impact on the SI process and performance.\n",
      "As a result, numerous feature extraction strategies are thoroughly\n",
      "investigated, contrasted, and analyzed. This article exploits five distinct\n",
      "feature extraction methods for speaker identification in disguised voices under\n",
      "emotional environments. To evaluate this work significantly, three effects are\n",
      "used: high-pitched, low-pitched, and Electronic Voice Conversion (EVC).\n",
      "Experimental results reported that the concatenated Mel-Frequency Cepstral\n",
      "Coefficients (MFCCs), MFCCs-delta, and MFCCs-delta-delta is the best feature\n",
      "extraction method. \n",
      "\n",
      "\n",
      "Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker\n",
      "characteristic of an utterance to match an unseen target speaker without\n",
      "relying on parallel training data. Recently, self-supervised learning of speech\n",
      "representation has been shown to produce useful linguistic units without using\n",
      "transcripts, which can be directly passed to a VC model. In this paper, we\n",
      "showed that high-quality audio samples can be achieved by using a length\n",
      "resampling decoder, which enables the VC model to work in conjunction with\n",
      "different linguistic feature extractors and vocoders without requiring them to\n",
      "operate on the same sequence length. We showed that our method can outperform\n",
      "many baselines on the VCTK dataset. Without modifying the architecture, we\n",
      "further demonstrated that a) using pairs of different audio segments from the\n",
      "same speaker, b) adding a cycle consistency loss, and c) adding a speaker\n",
      "classification loss can help to learn a better speaker embedding. Our model\n",
      "trained on LibriTTS using these techniques achieves the best performance,\n",
      "producing audio samples transferred well to the target speaker's voice, while\n",
      "preserving the linguistic content that is comparable with actual human\n",
      "utterances in terms of Character Error Rate. \n",
      "\n",
      "\n",
      "Variational autoencoder-based voice conversion (VAE-VC) has the advantage of\n",
      "requiring only pairs of speeches and speaker labels for training. Unlike the\n",
      "majority of the research in VAE-VC which focuses on utilizing auxiliary losses\n",
      "or discretizing latent variables, this paper investigates how an increasing\n",
      "model expressiveness has benefits and impacts on the VAE-VC. Specifically, we\n",
      "first analyze VAE-VC from a rate-distortion perspective, and point out that\n",
      "model expressiveness is significant for VAE-VC because rate and distortion\n",
      "reflect similarity and naturalness of converted speeches. Based on the\n",
      "analysis, we propose a novel VC method using a deep hierarchical VAE, which has\n",
      "high model expressiveness as well as having fast conversion speed thanks to its\n",
      "non-autoregressive decoder. Also, our analysis reveals another problem that\n",
      "similarity can be degraded when the latent variable of VAEs has redundant\n",
      "information. We address the problem by controlling the information contained in\n",
      "the latent variable using $\\beta$-VAE objective. In the experiment using VCTK\n",
      "corpus, the proposed method achieved mean opinion scores higher than 3.5 on\n",
      "both naturalness and similarity in inter-gender settings, which are higher than\n",
      "the scores of existing autoencoder-based VC methods. \n",
      "\n",
      "\n",
      "YourTTS brings the power of a multilingual approach to the task of zero-shot\n",
      "multi-speaker TTS. Our method builds upon the VITS model and adds several novel\n",
      "modifications for zero-shot multi-speaker and multilingual training. We\n",
      "achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and\n",
      "results comparable to SOTA in zero-shot voice conversion on the VCTK dataset.\n",
      "Additionally, our approach achieves promising results in a target language with\n",
      "a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS\n",
      "and zero-shot voice conversion systems in low-resource languages. Finally, it\n",
      "is possible to fine-tune the YourTTS model with less than 1 minute of speech\n",
      "and achieve state-of-the-art results in voice similarity and with reasonable\n",
      "quality. This is important to allow synthesis for speakers with a very\n",
      "different voice or recording characteristics from those seen during training. \n",
      "\n",
      "\n",
      "In this study, we explore the transformer's ability to capture\n",
      "intra-relations among frames by augmenting the receptive field of models.\n",
      "Concretely, we propose a CycleGAN-based model with the transformer and\n",
      "investigate its ability in the emotional voice conversion task. In the training\n",
      "procedure, we adopt curriculum learning to gradually increase the frame length\n",
      "so that the model can see from the short segment till the entire speech. The\n",
      "proposed method was evaluated on the Japanese emotional speech dataset and\n",
      "compared to several baselines (ACVAE, CycleGAN) with objective and subjective\n",
      "evaluations. The results show that our proposed model is able to convert\n",
      "emotion with higher strength and quality. \n",
      "\n",
      "\n",
      "One-shot style transfer is a challenging task, since training on one\n",
      "utterance makes model extremely easy to over-fit to training data and causes\n",
      "low speaker similarity and lack of expressiveness. In this paper, we build on\n",
      "the recognition-synthesis framework and propose a one-shot voice conversion\n",
      "approach for style transfer based on speaker adaptation. First, a speaker\n",
      "normalization module is adopted to remove speaker-related information in\n",
      "bottleneck features extracted by ASR. Second, we adopt weight regularization in\n",
      "the adaptation process to prevent over-fitting caused by using only one\n",
      "utterance from target speaker as training data. Finally, to comprehensively\n",
      "decouple the speech factors, i.e., content, speaker, style, and transfer source\n",
      "style to the target, a prosody module is used to extract prosody\n",
      "representation. Experiments show that our approach is superior to the\n",
      "state-of-the-art one-shot VC systems in terms of style and speaker similarity;\n",
      "additionally, our approach also maintains good speech quality. \n",
      "\n",
      "\n",
      "Beyond the conventional voice conversion (VC) where the speaker information\n",
      "is converted without altering the linguistic content, the background sounds are\n",
      "informative and need to be retained in some real-world scenarios, such as VC in\n",
      "movie/video and VC in music where the voice is entangled with background\n",
      "sounds. As a new VC framework, we have developed a noisy-to-noisy (N2N) VC\n",
      "framework to convert the speaker's identity while preserving the background\n",
      "sounds. Although our framework consisting of a denoising module and a VC module\n",
      "well handles the background sounds, the VC module is sensitive to the\n",
      "distortion caused by the denoising module. To address this distortion issue, in\n",
      "this paper we propose the improved VC module to directly model the noisy speech\n",
      "waveform while controlling the background sounds. The experimental results have\n",
      "demonstrated that our improved framework significantly outperforms the previous\n",
      "one and achieves an acceptable score in terms of naturalness, while reaching\n",
      "comparable similarity performance to the upper bound of our framework. \n",
      "\n",
      "\n",
      "This paper presents AC-VC (Almost Causal Voice Conversion), a phonetic\n",
      "posteriorgrams based voice conversion system that can perform any-to-many voice\n",
      "conversion while having only 57.5 ms future look-ahead. The complete system is\n",
      "composed of three neural networks trained separately with non-parallel data.\n",
      "While most of the current voice conversion systems focus primarily on quality\n",
      "irrespective of algorithmic latency, this work elaborates on designing a method\n",
      "using a minimal amount of future context thus allowing a future real-time\n",
      "implementation. According to a subjective listening test organized in this\n",
      "work, the proposed AC-VC system achieves parity with the non-causal ASR-TTS\n",
      "baseline of the Voice Conversion Challenge 2020 in naturalness with a MOS of\n",
      "3.5. In contrast, the results indicate that missing future context impacts\n",
      "speaker similarity. Obtained similarity percentage of 65% is lower than the\n",
      "similarity of current best voice conversion systems. \n",
      "\n",
      "\n",
      "Voice conversion (VC) could be used to improve speech recognition systems in\n",
      "low-resource languages by using it to augment limited training data. However,\n",
      "VC has not been widely used for this purpose because of practical issues such\n",
      "as compute speed and limitations when converting to and from unseen speakers.\n",
      "Moreover, it is still unclear whether a VC model trained on one well-resourced\n",
      "language can be applied to speech from another low-resource language for the\n",
      "aim of data augmentation. In this work we assess whether a VC system can be\n",
      "used cross-lingually to improve low-resource speech recognition. We combine\n",
      "several recent techniques to design and train a practical VC system in English,\n",
      "and then use this system to augment data for training speech recognition models\n",
      "in several low-resource languages. When using a sensible amount of VC augmented\n",
      "data, speech recognition performance is improved in all four low-resource\n",
      "languages considered. We also show that VC-based augmentation is superior to\n",
      "SpecAugment (a widely used signal processing augmentation method) in the\n",
      "low-resource languages considered. \n",
      "\n",
      "\n",
      "The goal of voice conversion is to transform source speech into a target\n",
      "voice, keeping the content unchanged. In this paper, we focus on\n",
      "self-supervised representation learning for voice conversion. Specifically, we\n",
      "compare discrete and soft speech units as input features. We find that discrete\n",
      "representations effectively remove speaker information but discard some\n",
      "linguistic content - leading to mispronunciations. As a solution, we propose\n",
      "soft speech units. To learn soft units, we predict a distribution over discrete\n",
      "speech units. By modeling uncertainty, soft units capture more content\n",
      "information, improving the intelligibility and naturalness of converted speech.\n",
      "Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code\n",
      "available at https://github.com/bshall/soft-vc/. \n",
      "\n",
      "\n",
      "We present a neural analysis and synthesis (NANSY) framework that can\n",
      "manipulate voice, pitch, and speed of an arbitrary speech signal. Most of the\n",
      "previous works have focused on using information bottleneck to disentangle\n",
      "analysis features for controllable synthesis, which usually results in poor\n",
      "reconstruction quality. We address this issue by proposing a novel training\n",
      "strategy based on information perturbation. The idea is to perturb information\n",
      "in the original input signal (e.g., formant, pitch, and frequency response),\n",
      "thereby letting synthesis networks selectively take essential attributes to\n",
      "reconstruct the input signal. Because NANSY does not need any bottleneck\n",
      "structures, it enjoys both high reconstruction quality and controllability.\n",
      "Furthermore, NANSY does not require any labels associated with speech data such\n",
      "as text and speaker information, but rather uses a new set of analysis\n",
      "features, i.e., wav2vec feature and newly proposed pitch feature, Yingram,\n",
      "which allows for fully self-supervised training. Taking advantage of fully\n",
      "self-supervised training, NANSY can be easily extended to a multilingual\n",
      "setting by simply training it with a multilingual dataset. The experiments show\n",
      "that NANSY can achieve significant improvement in performance in several\n",
      "applications such as zero-shot voice conversion, pitch shift, and time-scale\n",
      "modification. \n",
      "\n",
      "\n",
      "Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an\n",
      "attractive research topic as it enables a range of applications like voice\n",
      "customizing, animation production, and others. Recent work in this area made\n",
      "progress with disentanglement methods that separate utterance content and\n",
      "speaker characteristics from speech audio recordings. However, many of these\n",
      "methods are subject to the leakage of prosody (e.g., pitch, volume), causing\n",
      "the speaker voice in the synthesized speech to be different from the desired\n",
      "target speakers. To prevent this issue, we propose a novel self-supervised\n",
      "approach that effectively learns disentangled pitch and volume representations\n",
      "that can represent the prosody styles of different speakers. We then use the\n",
      "learned prosodic representations as conditional information to train and\n",
      "enhance our VC model for zero-shot conversion. In our experiments, we show that\n",
      "our prosody representations are disentangled and rich in prosody information.\n",
      "Moreover, we demonstrate that the addition of our prosody representations\n",
      "improves our VC performance and surpasses state-of-the-art zero-shot VC\n",
      "performances. \n",
      "\n",
      "\n",
      "Expressive voice conversion performs identity conversion for emotional\n",
      "speakers by jointly converting speaker identity and emotional style. Due to the\n",
      "hierarchical structure of speech emotion, it is challenging to disentangle the\n",
      "emotional style for different speakers. Inspired by the recent success of\n",
      "speaker disentanglement with variational autoencoder (VAE), we propose an\n",
      "any-to-any expressive voice conversion framework, that is called StyleVC.\n",
      "StyleVC is designed to disentangle linguistic content, speaker identity, pitch,\n",
      "and emotional style information. We study the use of style encoder to model\n",
      "emotional style explicitly. At run-time, StyleVC converts both speaker identity\n",
      "and emotional style for arbitrary speakers. Experiments validate the\n",
      "effectiveness of our proposed framework in both objective and subjective\n",
      "evaluations. \n",
      "\n",
      "\n",
      "Numerous voice conversion (VC) techniques have been proposed for the\n",
      "conversion of voices among different speakers. Although the decent quality of\n",
      "converted speech can be observed when VC is applied in a clean environment, the\n",
      "quality will drop sharply when the system is running under noisy conditions. In\n",
      "order to address this issue, we propose a novel enhancement-based StarGAN\n",
      "(E-StarGAN) VC system, which leverages a speech enhancement (SE) technique for\n",
      "signal pre-processing. SE systems are generally used to reduce noise components\n",
      "in noisy speech and to generate enhanced speech for downstream application\n",
      "tasks. Therefore, we investigated the effectiveness of E-StarGAN, which\n",
      "combines VC and SE, and demonstrated the robustness of the proposed approach in\n",
      "various noisy environments. The results of VC experiments conducted on a\n",
      "Mandarin dataset show that when combined with SE, the proposed E-StarGAN VC\n",
      "model is robust to unseen noises. In addition, the subjective listening test\n",
      "results show that the proposed E-StarGAN model can improve the sound quality of\n",
      "speech signals converted from noise-corrupted source utterances. \n",
      "\n",
      "\n",
      "As increasing development of text-to-speech (TTS) and voice conversion (VC)\n",
      "technologies, the detection of synthetic speech has been suffered dramatically.\n",
      "In order to promote the development of synthetic speech detection model against\n",
      "Mandarin TTS and VC technologies, we have constructed a challenging Mandarin\n",
      "dataset and organized the accompanying audio track of the first fake media\n",
      "forensic challenge of China Society of Image and Graphics (FMFCC-A). The\n",
      "FMFCC-A dataset is by far the largest publicly-available Mandarin dataset for\n",
      "synthetic speech detection, which contains 40,000 synthesized Mandarin\n",
      "utterances that generated by 11 Mandarin TTS systems and two Mandarin VC\n",
      "systems, and 10,000 genuine Mandarin utterances collected from 58 speakers. The\n",
      "FMFCC-A dataset is divided into the training, development and evaluation sets,\n",
      "which are used for the research of detection of synthesized Mandarin speech\n",
      "under various previously unknown speech synthesis systems or audio\n",
      "post-processing operations. In addition to describing the construction of the\n",
      "FMFCC-A dataset, we provide a detailed analysis of two baseline methods and the\n",
      "top-performing submissions from the FMFCC-A, which illustrates the usefulness\n",
      "and challenge of FMFCC-A dataset. We hope that the FMFCC-A dataset can fill the\n",
      "gap of lack of Mandarin datasets for synthetic speech detection. \n",
      "\n",
      "\n",
      "SpeechFlow is a powerful factorization model based on information bottleneck\n",
      "(IB), and its effectiveness has been reported by several studies. A potential\n",
      "problem of SpeechFlow, however, is that if the IB channels are not well\n",
      "designed, the resultant factors cannot be well disentangled. In this study, we\n",
      "propose a CycleFlow model that combines random factor substitution and cycle\n",
      "loss to solve this problem. Experiments on voice conversion tasks demonstrate\n",
      "that this simple technique can effectively reduce mutual information among\n",
      "individual factors, and produce clearly better conversion than the IB-based\n",
      "SpeechFlow. CycleFlow can also be used as a powerful tool for speech editing.\n",
      "We demonstrate this usage by an emotion perception experiment. \n",
      "\n",
      "\n",
      "An effective approach to automatically predict the subjective rating for\n",
      "synthetic speech is to train on a listening test dataset with human-annotated\n",
      "scores. Although each speech sample in the dataset is rated by several\n",
      "listeners, most previous works only used the mean score as the training target.\n",
      "In this work, we present LDNet, a unified framework for mean opinion score\n",
      "(MOS) prediction that predicts the listener-wise perceived quality given the\n",
      "input speech and the listener identity. We reflect recent advances in LD\n",
      "modeling, including design choices of the model architecture, and propose two\n",
      "inference methods that provide more stable results and efficient computation.\n",
      "We conduct systematic experiments on the voice conversion challenge (VCC) 2018\n",
      "benchmark and a newly collected large-scale MOS dataset, providing an in-depth\n",
      "analysis of the proposed framework. Results show that the mean listener\n",
      "inference method is a better way to utilize the mean scores, whose\n",
      "effectiveness is more obvious when having more ratings per sample. \n",
      "\n",
      "\n",
      "We present a voice conversion framework that converts normal speech into\n",
      "dysarthric speech while preserving the speaker identity. Such a framework is\n",
      "essential for (1) clinical decision making processes and alleviation of patient\n",
      "stress, (2) data augmentation for dysarthric speech recognition. This is an\n",
      "especially challenging task since the converted samples should capture the\n",
      "severity of dysarthric speech while being highly natural and possessing the\n",
      "speaker identity of the normal speaker. To this end, we adopted a two-stage\n",
      "framework, which consists of a sequence-to-sequence model and a nonparallel\n",
      "frame-wise model. Objective and subjective evaluations were conducted on the\n",
      "UASpeech dataset, and results showed that the method was able to yield\n",
      "reasonable naturalness and capture severity aspects of the pathological speech.\n",
      "On the other hand, the similarity to the normal source speaker's voice was\n",
      "limited and requires further improvements. \n",
      "\n",
      "\n",
      "Any-to-any voice conversion technologies convert the vocal timbre of an\n",
      "utterance to any speaker even unseen during training. Although there have been\n",
      "several state-of-the-art any-to-any voice conversion models, they were all\n",
      "based on clean utterances to convert successfully. However, in real-world\n",
      "scenarios, it is difficult to collect clean utterances of a speaker, and they\n",
      "are usually degraded by noises or reverberations. It thus becomes highly\n",
      "desired to understand how these degradations affect voice conversion and build\n",
      "a degradation-robust model. We report in this paper the first comprehensive\n",
      "study on the degradation robustness of any-to-any voice conversion. We show\n",
      "that the performance of state-of-the-art models nowadays was severely hampered\n",
      "given degraded utterances. To this end, we then propose speech enhancement\n",
      "concatenation and denoising training to improve the robustness. In addition to\n",
      "common degradations, we also consider adversarial noises, which alter the model\n",
      "output significantly yet are human-imperceptible. It was shown that both\n",
      "concatenations with off-the-shelf speech enhancement models and denoising\n",
      "training on voice conversion models could improve the robustness, while each of\n",
      "them had pros and cons. \n",
      "\n",
      "\n",
      "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\n",
      "pre-trained natural language processing models, we propose a unified-modal\n",
      "SpeechT5 framework that explores the encoder-decoder pre-training for\n",
      "self-supervised speech/text representation learning. The SpeechT5 framework\n",
      "consists of a shared encoder-decoder network and six modal-specific\n",
      "(speech/text) pre/post-nets. After preprocessing the input speech/text through\n",
      "the pre-nets, the shared encoder-decoder network models the\n",
      "sequence-to-sequence transformation, and then the post-nets generate the output\n",
      "in the speech/text modality based on the output of the decoder. Leveraging\n",
      "large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a\n",
      "unified-modal representation, hoping to improve the modeling capability for\n",
      "both speech and text. To align the textual and speech information into this\n",
      "unified semantic space, we propose a cross-modal vector quantization approach\n",
      "that randomly mixes up speech/text states with latent units as the interface\n",
      "between encoder and decoder. Extensive evaluations show the superiority of the\n",
      "proposed SpeechT5 framework on a wide variety of spoken language processing\n",
      "tasks, including automatic speech recognition, speech synthesis, speech\n",
      "translation, voice conversion, speech enhancement, and speaker identification.\n",
      "We release our code and model at https://github.com/microsoft/SpeechT5. \n",
      "\n",
      "\n",
      "Voice conversion for speaker anonymization is an emerging field in speech\n",
      "processing research. Many state-of-the-art approaches are based on the\n",
      "resynthesis of the phoneme posteriorgrams (PPG), the fundamental frequency (F0)\n",
      "of the input signal together with modified X-vectors. Our research focuses on\n",
      "the role of F0 for speaker anonymization, which is an understudied area.\n",
      "Utilizing the VoicePrivacy Challenge 2020 framework and its datasets we\n",
      "developed and evaluated eight low-complexity F0 modifications prior\n",
      "resynthesis. We found that modifying the F0 can improve speaker anonymization\n",
      "by as much as 8% with minor word-error rate degradation. \n",
      "\n",
      "\n",
      "Conventional vocoders are commonly used as analysis tools to provide\n",
      "interpretable features for downstream tasks such as speech synthesis and voice\n",
      "conversion. They are built under certain assumptions about the signals\n",
      "following signal processing principle, therefore, not easily generalizable to\n",
      "different audio, for example, from speech to singing. In this paper, we propose\n",
      "a deep neural analyzer, denoted as DeepA - a neural vocoder that extracts F0\n",
      "and timbre/aperiodicity encoding from the input speech that emulate those\n",
      "defined in conventional vocoders. Therefore, the resulting parameters are more\n",
      "interpretable than other latent neural representations. At the same time, as\n",
      "the deep neural analyzer is learnable, it is expected to be more accurate for\n",
      "signal reconstruction and manipulation, and generalizable from speech to\n",
      "singing. The proposed neural analyzer is built based on a variational\n",
      "autoencoder (VAE) architecture. We show that DeepA improves F0 estimation over\n",
      "the conventional vocoder (WORLD). To our best knowledge, this is the first\n",
      "study dedicated to the development of a neural framework for extracting\n",
      "learnable vocoder-like parameters. \n",
      "\n",
      "\n",
      "This paper introduces S3PRL-VC, an open-source voice conversion (VC)\n",
      "framework based on the S3PRL toolkit. In the context of recognition-synthesis\n",
      "VC, self-supervised speech representation (S3R) is valuable in its potential to\n",
      "replace the expensive supervised representation adopted by state-of-the-art VC\n",
      "systems. Moreover, we claim that VC is a good probing task for S3R analysis. In\n",
      "this work, we provide a series of in-depth analyses by benchmarking on the two\n",
      "tasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as\n",
      "an any-to-any (A2A) setting. We also provide comparisons between not only\n",
      "different S3Rs but also top systems in VCC2020 with supervised representations.\n",
      "Systematic objective and subjective evaluation were conducted, and we show that\n",
      "S3R is comparable with VCC2020 top systems in the A2O setting in terms of\n",
      "similarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the\n",
      "extensive analysis, as well as the toolkit itself, contribute to not only the\n",
      "S3R community but also the VC community. The codebase is now open-sourced. \n",
      "\n",
      "\n",
      "Recently, phonetic posteriorgrams (PPGs) based methods have been quite\n",
      "popular in non-parallel singing voice conversion systems. However, due to the\n",
      "lack of acoustic information in PPGs, style and naturalness of the converted\n",
      "singing voices are still limited. To solve these problems, in this paper, we\n",
      "utilize an acoustic reference encoder to implicitly model singing\n",
      "characteristics. We experiment with different auxiliary features, including mel\n",
      "spectrograms, HuBERT, and the middle hidden feature (PPG-Mid) of pretrained\n",
      "automatic speech recognition (ASR) model, as the input of the reference\n",
      "encoder, and finally find the HuBERT feature is the best choice. In addition,\n",
      "we use contrastive predictive coding (CPC) module to further smooth the voices\n",
      "by predicting future observations in latent space. Experiments show that,\n",
      "compared with the baseline models, our proposed model can significantly improve\n",
      "the naturalness of converted singing voices and the similarity with the target\n",
      "singer. Moreover, our proposed model can also make the speakers with just\n",
      "speech data sing. \n",
      "\n",
      "\n",
      "This paper introduces voice reenactement as the task of voice conversion (VC)\n",
      "in which the expressivity of the source speaker is preserved during conversion\n",
      "while the identity of a target speaker is transferred. To do so, an original\n",
      "neural- VC architecture is proposed based on sequence-to-sequence voice\n",
      "conversion (S2S-VC) in which the speech prosody of the source speaker is\n",
      "preserved during conversion. First, the S2S-VC architecture is modified so as\n",
      "to synchronize the converted speech with the source speech by mean of phonetic\n",
      "duration encoding; second, the decoder is conditioned on the desired sequence\n",
      "of F0- values and an explicit F0-loss is formulated between the F0 of the\n",
      "source speaker and the one of the converted speech. Besides, an adversarial\n",
      "learning of conversions is integrated within the S2S-VC architecture so as to\n",
      "exploit both advantages of reconstruction of original speech and converted\n",
      "speech with manipulated attributes during training and then reducing the\n",
      "inconsistency between training and conversion. An experimental evaluation on\n",
      "the VCTK speech database shows that the speech prosody can be efficiently\n",
      "preserved during conversion, and that the proposed adversarial learning\n",
      "consistently improves the conversion and the naturalness of the reenacted\n",
      "speech. \n",
      "\n",
      "\n",
      "To realize any-to-any (A2A) voice conversion (VC), most methods are to\n",
      "perform symmetric self-supervised reconstruction tasks (Xi to Xi), which\n",
      "usually results in inefficient performances due to inadequate feature\n",
      "decoupling, especially for unseen speakers. We propose a two-stage\n",
      "reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches\n",
      "as intermedium features, where A2A VC is divided into two stages: any-to-one\n",
      "(A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method:\n",
      "SingleVC, by employing a noval data augment strategy(pitch-shifted and\n",
      "duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is\n",
      "proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such\n",
      "asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in\n",
      "MediumVC), the models are to capture robust disentangled features purposefully.\n",
      "Experiments indicate MediumVC can enhance the similarity of converted speeches\n",
      "while maintaining a high degree of naturalness. \n",
      "\n",
      "\n",
      "Emotional voice conversion (VC) aims to convert a neutral voice to an\n",
      "emotional (e.g. happy) one while retaining the linguistic information and\n",
      "speaker identity. We note that the decoupling of emotional features from other\n",
      "speech information (such as speaker, content, etc.) is the key to achieving\n",
      "remarkable performance. Some recent attempts about speech representation\n",
      "decoupling on the neutral speech can not work well on the emotional speech, due\n",
      "to the more complex acoustic properties involved in the latter. To address this\n",
      "problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC)\n",
      "to achieve proper filtering of speaker-independent emotion features from both\n",
      "the timbre and pitch features. Our SFEVC model consists of multi-channel\n",
      "encoders, emotion separate encoders, and one decoder. Note that all encoder\n",
      "modules adopt a designed information bottlenecks auto-encoder. Additionally, to\n",
      "further improve the conversion quality for various emotions, a novel two-stage\n",
      "training strategy based on the 2D Valence-Arousal (VA) space was proposed.\n",
      "Experimental results show that the proposed SFEVC along with a two-stage\n",
      "training strategy outperforms all baselines and achieves the state-of-the-art\n",
      "performance in speaker-independent emotional VC with nonparallel data. \n",
      "\n",
      "\n",
      "Voice conversion is a common speech synthesis task which can be solved in\n",
      "different ways depending on a particular real-world scenario. The most\n",
      "challenging one often referred to as one-shot many-to-many voice conversion\n",
      "consists in copying the target voice from only one reference utterance in the\n",
      "most general case when both source and target speakers do not belong to the\n",
      "training dataset. We present a scalable high-quality solution based on\n",
      "diffusion probabilistic modeling and demonstrate its superior quality compared\n",
      "to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on\n",
      "real-time applications, we investigate general principles which can make\n",
      "diffusion models faster while keeping synthesis quality at a high level. As a\n",
      "result, we develop a novel Stochastic Differential Equations solver suitable\n",
      "for various diffusion model types and generative tasks as shown through\n",
      "empirical studies and justify it by theoretical analysis. \n",
      "\n",
      "\n",
      "In a conventional voice conversion (VC) framework, a VC model is often\n",
      "trained with a clean dataset consisting of speech data carefully recorded and\n",
      "selected by minimizing background interference. However, collecting such a\n",
      "high-quality dataset is expensive and time-consuming. Leveraging crowd-sourced\n",
      "speech data in training is more economical. Moreover, for some real-world VC\n",
      "scenarios such as VC in video and VC-based data augmentation for speech\n",
      "recognition systems, the background sounds themselves are also informative and\n",
      "need to be maintained. In this paper, to explore VC with the flexibility of\n",
      "handling background sounds, we propose a noisy-to-noisy (N2N) VC framework\n",
      "composed of a denoising module and a VC module. With the proposed framework, we\n",
      "can convert the speaker's identity while preserving the background sounds. Both\n",
      "objective and subjective evaluations are conducted, and the results reveal the\n",
      "effectiveness of the proposed framework. \n",
      "\n",
      "\n",
      "Given a piece of speech and its transcript text, text-based speech editing\n",
      "aims to generate speech that can be seamlessly inserted into the given speech\n",
      "by editing the transcript. Existing methods adopt a two-stage approach:\n",
      "synthesize the input text using a generic text-to-speech (TTS) engine and then\n",
      "transform the voice to the desired voice using voice conversion (VC). A major\n",
      "problem of this framework is that VC is a challenging problem which usually\n",
      "needs a moderate amount of parallel training data to work satisfactorily. In\n",
      "this paper, we propose a one-stage context-aware framework to generate natural\n",
      "and coherent target speech without any training data of the target speaker. In\n",
      "particular, we manage to perform accurate zero-shot duration prediction for the\n",
      "inserted text. The predicted duration is used to regulate both text embedding\n",
      "and speech embedding. Then, based on the aligned cross-modality input, we\n",
      "directly generate the mel-spectrogram of the edited speech with a\n",
      "transformer-based decoder. Subjective listening tests show that despite the\n",
      "lack of training data for the speaker, our method has achieved satisfactory\n",
      "results. It outperforms a recent zero-shot TTS engine by a large margin. \n",
      "\n",
      "\n",
      "Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\n",
      "speech enhancement, a task that aims to improve the quality of the artificial\n",
      "voice from an electrolarynx device. In frame-based VC methods, time alignment\n",
      "needs to be performed prior to model training, and the dynamic time warping\n",
      "(DTW) algorithm is widely adopted to compute the best time alignment between\n",
      "each utterance pair. The validity is based on the assumption that the same\n",
      "phonemes of the speakers have similar features and can be mapped by measuring a\n",
      "pre-defined distance between speech frames of the source and the target.\n",
      "However, the special characteristics of the EL speech can break the assumption,\n",
      "resulting in a sub-optimal DTW alignment. In this work, we propose to use lip\n",
      "images for time alignment, as we assume that the lip movements of laryngectomee\n",
      "remain normal compared to healthy people. We investigate two naive lip\n",
      "representations and distance metrics, and experimental results demonstrate that\n",
      "the proposed method can significantly outperform the audio-only alignment in\n",
      "terms of objective and subjective evaluations. \n",
      "\n",
      "\n",
      "Speaker verification systems have been used in many production scenarios in\n",
      "recent years. Unfortunately, they are still highly prone to different kinds of\n",
      "spoofing attacks such as voice conversion and speech synthesis, etc. In this\n",
      "paper, we propose a new method base on physiological-physical feature fusion to\n",
      "deal with voice spoofing attacks. This method involves feature extraction, a\n",
      "densely connected convolutional neural network with squeeze and excitation\n",
      "block (SE-DenseNet), multi-scale residual neural network with squeeze and\n",
      "excitation block (SE-Res2Net) and feature fusion strategies. We first\n",
      "pre-trained a convolutional neural network using the speaker's voice and face\n",
      "in the video as surveillance signals. It can extract physiological features\n",
      "from speech. Then we use SE-DenseNet and SE-Res2Net to extract physical\n",
      "features. Such a densely connection pattern has high parameter efficiency and\n",
      "squeeze and excitation block can enhance the transmission of the feature.\n",
      "Finally, we integrate the two features into the SE-Densenet to identify the\n",
      "spoofing attacks. Experimental results on the ASVspoof 2019 data set show that\n",
      "our model is effective for voice spoofing detection. In the logical access\n",
      "scenario, our model improves the tandem decision cost function (t-DCF) and\n",
      "equal error rate (EER) scores by 4% and 7%, respectively, compared with other\n",
      "methods. In the physical access scenario, our model improved t-DCF and EER\n",
      "scores by 8% and 10%, respectively. \n",
      "\n",
      "\n",
      "In recent years, synthetic speech generated by advanced text-to-speech (TTS)\n",
      "and voice conversion (VC) systems has caused great harms to automatic speaker\n",
      "verification (ASV) systems, urging us to design a synthetic speech detection\n",
      "system to protect ASV systems. In this paper, we propose a new speech\n",
      "anti-spoofing model named ResWavegram-Resnet (RW-Resnet). The model contains\n",
      "two parts, Conv1D Resblocks and backbone Resnet34. The Conv1D Resblock is based\n",
      "on the Conv1D block with a residual connection. For the first part, we use the\n",
      "raw waveform as input and feed it to the stacked Conv1D Resblocks to get the\n",
      "ResWavegram. Compared with traditional methods, ResWavegram keeps all the\n",
      "information from the audio signal and has a stronger ability in extracting\n",
      "features. For the second part, the extracted features are fed to the backbone\n",
      "Resnet34 for the spoofed or bonafide decision. The ASVspoof2019 logical access\n",
      "(LA) corpus is used to evaluate our proposed RW-Resnet. Experimental results\n",
      "show that the RW-Resnet achieves better performance than other state-of-the-art\n",
      "anti-spoofing models, which illustrates its effectiveness in detecting\n",
      "synthetic speech attacks. \n",
      "\n",
      "\n",
      "Preserving the linguistic content of input speech is essential during voice\n",
      "conversion (VC). The star generative adversarial network-based VC method\n",
      "(StarGAN-VC) is a recently developed method that allows non-parallel\n",
      "many-to-many VC. Although this method is powerful, it can fail to preserve the\n",
      "linguistic content of input speech when the number of available training\n",
      "samples is extremely small. To overcome this problem, we propose the use of\n",
      "automatic speech recognition to assist model training, to improve StarGAN-VC,\n",
      "especially in low-resource scenarios.\n",
      "  Experimental results show that using our proposed method, StarGAN-VC can\n",
      "retain more linguistic information than vanilla StarGAN-VC. \n",
      "\n",
      "\n",
      "Voice conversion (VC) consists of digitally altering the voice of an\n",
      "individual to manipulate part of its content, primarily its identity, while\n",
      "maintaining the rest unchanged. Research in neural VC has accomplished\n",
      "considerable breakthroughs with the capacity to falsify a voice identity using\n",
      "a small amount of data with a highly realistic rendering. This paper goes\n",
      "beyond voice identity and presents a neural architecture that allows the\n",
      "manipulation of voice attributes (e.g., gender and age). Leveraging the latest\n",
      "advances on adversarial learning of structured speech representation, a novel\n",
      "structured neural network is proposed in which multiple auto-encoders are used\n",
      "to encode speech as a set of idealistically independent linguistic and\n",
      "extra-linguistic representations, which are learned adversariarly and can be\n",
      "manipulated during VC. Moreover, the proposed architecture is time-synchronized\n",
      "so that the original voice timing is preserved during conversion which allows\n",
      "lip-sync applications. Applied to voice gender conversion on the real-world\n",
      "VCTK dataset, our proposed architecture can learn successfully\n",
      "gender-independent representation and convert the voice gender with a very high\n",
      "efficiency and naturalness. \n",
      "\n",
      "\n",
      "We present an unsupervised non-parallel many-to-many voice conversion (VC)\n",
      "method using a generative adversarial network (GAN) called StarGAN v2. Using a\n",
      "combination of adversarial source classifier loss and perceptual loss, our\n",
      "model significantly outperforms previous VC models. Although our model is\n",
      "trained only with 20 English speakers, it generalizes to a variety of voice\n",
      "conversion tasks, such as any-to-many, cross-lingual, and singing conversion.\n",
      "Using a style encoder, our framework can also convert plain reading speech into\n",
      "stylistic speech, such as emotional and falsetto speech. Subjective and\n",
      "objective evaluation experiments on a non-parallel many-to-many voice\n",
      "conversion task revealed that our model produces natural sounding voices, close\n",
      "to the sound quality of state-of-the-art text-to-speech (TTS) based voice\n",
      "conversion methods without the need for text labels. Moreover, our model is\n",
      "completely convolutional and with a faster-than-real-time vocoder such as\n",
      "Parallel WaveGAN can perform real-time voice conversion. \n",
      "\n",
      "\n",
      "In voice conversion (VC), an approach showing promising results in the latest\n",
      "voice conversion challenge (VCC) 2020 is to first use an automatic speech\n",
      "recognition (ASR) model to transcribe the source speech into the underlying\n",
      "linguistic contents; these are then used as input by a text-to-speech (TTS)\n",
      "system to generate the converted speech. Such a paradigm, referred to as\n",
      "ASR+TTS, overlooks the modeling of prosody, which plays an important role in\n",
      "speech naturalness and conversion similarity. Although some researchers have\n",
      "considered transferring prosodic clues from the source speech, there arises a\n",
      "speaker mismatch during training and conversion. To address this issue, in this\n",
      "work, we propose to directly predict prosody from the linguistic representation\n",
      "in a target-speaker-dependent manner, referred to as target text prediction\n",
      "(TTP). We evaluate both methods on the VCC2020 benchmark and consider different\n",
      "linguistic representations. The results demonstrate the effectiveness of TTP in\n",
      "both objective and subjective evaluations. \n",
      "\n",
      "\n",
      "Neural evaluation metrics derived for numerous speech generation tasks have\n",
      "recently attracted great attention. In this paper, we propose SVSNet, the first\n",
      "end-to-end neural network model to assess the speaker voice similarity between\n",
      "converted speech and natural speech for voice conversion tasks. Unlike most\n",
      "neural evaluation metrics that use hand-crafted features, SVSNet directly takes\n",
      "the raw waveform as input to more completely utilize speech information for\n",
      "prediction. SVSNet consists of encoder, co-attention, distance calculation, and\n",
      "prediction modules and is trained in an end-to-end manner. The experimental\n",
      "results on the Voice Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020)\n",
      "datasets show that SVSNet outperforms well-known baseline systems in the\n",
      "assessment of speaker similarity at the utterance and system levels. \n",
      "\n",
      "\n",
      "Emotional Voice Conversion (EVC) aims to convert the emotional style of a\n",
      "source speech signal to a target style while preserving its content and speaker\n",
      "identity information. Previous emotional conversion studies do not disentangle\n",
      "emotional information from emotion-independent information that should be\n",
      "preserved, thus transforming it all in a monolithic manner and generating audio\n",
      "of low quality, with linguistic distortions. To address this distortion\n",
      "problem, we propose a novel StarGAN framework along with a two-stage training\n",
      "process that separates emotional features from those independent of emotion by\n",
      "using an autoencoder with two encoders as the generator of the Generative\n",
      "Adversarial Network (GAN). The proposed model achieves favourable results in\n",
      "both the objective evaluation and the subjective evaluation in terms of\n",
      "distortion, which reveals that the proposed model can effectively reduce\n",
      "distortion. Furthermore, in data augmentation experiments for end-to-end speech\n",
      "emotion recognition, the proposed StarGAN model achieves an increase of 2% in\n",
      "Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which\n",
      "indicates that the proposed model is more valuable for data augmentation. \n",
      "\n",
      "\n",
      "Voice conversion is a challenging task which transforms the voice\n",
      "characteristics of a source speaker to a target speaker without changing\n",
      "linguistic content. Recently, there have been many works on many-to-many Voice\n",
      "Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results,\n",
      "however, these methods lack the ability to disentangle speaker identity and\n",
      "linguistic content to achieve good performance on unseen speaker scenarios. In\n",
      "this paper, we propose a new method based on feature disentanglement to tackle\n",
      "many to many voice conversion. The method has the capability to disentangle\n",
      "speaker identity and linguistic content from utterances, it can convert from\n",
      "many source speakers to many target speakers with a single autoencoder network.\n",
      "Moreover, it naturally deals with the unseen target speaker scenarios. We\n",
      "perform both objective and subjective evaluations to show the competitive\n",
      "performance of our proposed method compared with other state-of-the-art models\n",
      "in terms of naturalness and target speaker similarity. \n",
      "\n",
      "\n",
      "We propose the first method to adaptively modify the duration of a given\n",
      "speech signal. Our approach uses a Bayesian framework to define a latent\n",
      "attention map that links frames of the input and target utterances. We train a\n",
      "masked convolutional encoder-decoder network to produce this attention map via\n",
      "a stochastic version of the mean absolute error loss function; our model also\n",
      "predicts the length of the target speech signal using the encoder embeddings.\n",
      "The predicted length determines the number of steps for the decoder operation.\n",
      "During inference, we generate the attention map as a proxy for the similarity\n",
      "matrix between the given input speech and an unknown target speech signal.\n",
      "Using this similarity matrix, we compute a warping path of alignment between\n",
      "the two signals. Our experiments demonstrate that this adaptive framework\n",
      "produces similar results to dynamic time warping, which relies on a known\n",
      "target signal, on both voice conversion and emotion conversion tasks. We also\n",
      "show that our technique results in a high quality of generated speech that is\n",
      "on par with state-of-the-art vocoders. \n",
      "\n",
      "\n",
      "Traditional voice conversion(VC) has been focused on speaker identity\n",
      "conversion for speech with a neutral expression. We note that emotional\n",
      "expression plays an essential role in daily communication, and the emotional\n",
      "style of speech can be speaker-dependent. In this paper, we study the technique\n",
      "to jointly convert the speaker identity and speaker-dependent emotional style,\n",
      "that is called expressive voice conversion. We propose a StarGAN-based\n",
      "framework to learn a many-to-many mapping across different speakers, that takes\n",
      "into account speaker-dependent emotional style without the need for parallel\n",
      "data. To achieve this, we condition the generator on emotional style encoding\n",
      "derived from a pre-trained speech emotion recognition(SER) model. The\n",
      "experiments validate the effectiveness of our proposed framework in both\n",
      "objective and subjective evaluations. To our best knowledge, this is the first\n",
      "study on expressive voice conversion. \n",
      "\n",
      "\n",
      "The development of pathological speech systems is currently hindered by the\n",
      "lack of a standardised objective evaluation framework. In this work, (1) we\n",
      "utilise existing detection and analysis techniques to propose a general\n",
      "framework for the consistent evaluation of synthetic pathological speech. This\n",
      "framework evaluates the voice quality and the intelligibility aspects of speech\n",
      "and is shown to be complementary using our experiments. (2) Using our proposed\n",
      "evaluation framework, we develop and test a dysarthric voice conversion system\n",
      "(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\n",
      "show that the developed system is able to synthesise dysarthric speech with\n",
      "different levels of speech intelligibility. \n",
      "\n",
      "\n",
      "One-shot voice conversion has received significant attention since only one\n",
      "utterance from source speaker and target speaker respectively is required.\n",
      "Moreover, source speaker and target speaker do not need to be seen during\n",
      "training. However, available one-shot voice conversion approaches are not\n",
      "stable for unseen speakers as the speaker embedding extracted from one\n",
      "utterance of an unseen speaker is not reliable. In this paper, we propose a\n",
      "deep discriminative speaker encoder to extract speaker embedding from one\n",
      "utterance more effectively. Specifically, the speaker encoder first integrates\n",
      "residual network and squeeze-and-excitation network to extract discriminative\n",
      "speaker information in frame level by modeling frame-wise and channel-wise\n",
      "interdependence in features. Then attention mechanism is introduced to further\n",
      "emphasize speaker related information via assigning different weights to frame\n",
      "level speaker information. Finally a statistic pooling layer is used to\n",
      "aggregate weighted frame level speaker information to form utterance level\n",
      "speaker embedding. The experimental results demonstrate that our proposed\n",
      "speaker encoder can improve the robustness of one-shot voice conversion for\n",
      "unseen speakers and outperforms baseline systems in terms of speech quality and\n",
      "speaker similarity. \n",
      "\n",
      "\n",
      "One-shot voice conversion (VC), which performs conversion across arbitrary\n",
      "speakers with only a single target-speaker utterance for reference, can be\n",
      "effectively achieved by speech representation disentanglement. Existing work\n",
      "generally ignores the correlation between different speech representations\n",
      "during training, which causes leakage of content information into the speaker\n",
      "representation and thus degrades VC performance. To alleviate this issue, we\n",
      "employ vector quantization (VQ) for content encoding and introduce mutual\n",
      "information (MI) as the correlation metric during training, to achieve proper\n",
      "disentanglement of content, speaker and pitch representations, by reducing\n",
      "their inter-dependencies in an unsupervised manner. Experimental results\n",
      "reflect the superiority of the proposed method in learning effective\n",
      "disentangled speech representations for retaining source linguistic content and\n",
      "intonation variations, while capturing target speaker characteristics. In doing\n",
      "so, the proposed approach achieves higher speech naturalness and speaker\n",
      "similarity than current state-of-the-art one-shot VC systems. Our code,\n",
      "pre-trained models and demo are available at\n",
      "https://github.com/Wendison/VQMIVC. \n",
      "\n",
      "\n",
      "Voice Conversion (VC) is a technique that aims to transform the\n",
      "non-linguistic information of a source utterance to change the perceived\n",
      "identity of the speaker. While there is a rich literature on VC, most proposed\n",
      "methods are trained and evaluated on clean speech recordings. However, many\n",
      "acoustic environments are noisy and reverberant, severely restricting the\n",
      "applicability of popular VC methods to such scenarios. To address this\n",
      "limitation, we propose Voicy, a new VC framework particularly tailored for\n",
      "noisy speech. Our method, which is inspired by the de-noising auto-encoders\n",
      "framework, is comprised of four encoders (speaker, content, phonetic and\n",
      "acoustic-ASR) and one decoder. Importantly, Voicy is capable of performing\n",
      "non-parallel zero-shot VC, an important requirement for any VC system that\n",
      "needs to work on speakers not seen during training. We have validated our\n",
      "approach using a noisy reverberant version of the LibriSpeech dataset.\n",
      "Experimental results show that Voicy outperforms other tested VC techniques in\n",
      "terms of naturalness and target speaker similarity in noisy reverberant\n",
      "environments. \n",
      "\n",
      "\n",
      "Current voice conversion (VC) methods can successfully convert timbre of the\n",
      "audio. As modeling source audio's prosody effectively is a challenging task,\n",
      "there are still limitations of transferring source style to the converted\n",
      "speech. This study proposes a source style transfer method based on\n",
      "recognition-synthesis framework. Previously in speech generation task, prosody\n",
      "can be modeled explicitly with prosodic features or implicitly with a latent\n",
      "prosody extractor. In this paper, taking advantages of both, we model the\n",
      "prosody in a hybrid manner, which effectively combines explicit and implicit\n",
      "methods in a proposed prosody module. Specifically, prosodic features are used\n",
      "to explicit model prosody, while VAE and reference encoder are used to\n",
      "implicitly model prosody, which take Mel spectrum and bottleneck feature as\n",
      "input respectively. Furthermore, adversarial training is introduced to remove\n",
      "speaker-related information from the VAE outputs, avoiding leaking source\n",
      "speaker information while transferring style. Finally, we use a modified\n",
      "self-attention based encoder to extract sentential context from bottleneck\n",
      "features, which also implicitly aggregates the prosodic aspects of source\n",
      "speech from the layered representations. Experiments show that our approach is\n",
      "superior to the baseline and a competitive system in terms of style transfer;\n",
      "meanwhile, the speech quality and speaker similarity are well maintained. \n",
      "\n",
      "\n",
      "In this paper, we propose a new approach to pathological speech synthesis.\n",
      "Instead of using healthy speech as a source, we customise an existing\n",
      "pathological speech sample to a new speaker's voice characteristics. This\n",
      "approach alleviates the evaluation problem one normally has when converting\n",
      "typical speech to pathological speech, as in our approach, the voice conversion\n",
      "(VC) model does not need to be optimised for speech degradation but only for\n",
      "the speaker change. This change in the optimisation ensures that any\n",
      "degradation found in naturalness is due to the conversion process and not due\n",
      "to the model exaggerating characteristics of a speech pathology. To show a\n",
      "proof of concept of this method, we convert dysarthric speech using the\n",
      "UASpeech database and an autoencoder-based VC technique. Subjective evaluation\n",
      "results show reasonable naturalness for high intelligibility dysarthric\n",
      "speakers, though lower intelligibility seems to introduce a marginal\n",
      "degradation in naturalness scores for mid and low intelligibility speakers\n",
      "compared to ground truth. Conversion of speaker characteristics for low and\n",
      "high intelligibility speakers is successful, but not for mid. Whether the\n",
      "differences in the results for the different intelligibility levels is due to\n",
      "the intelligibility levels or due to the speakers needs to be further\n",
      "investigated. \n",
      "\n",
      "\n",
      "We propose a new paradigm for maintaining speaker identity in dysarthric\n",
      "voice conversion (DVC). The poor quality of dysarthric speech can be greatly\n",
      "improved by statistical VC, but as the normal speech utterances of a dysarthria\n",
      "patient are nearly impossible to collect, previous work failed to recover the\n",
      "individuality of the patient. In light of this, we suggest a novel, two-stage\n",
      "approach for DVC, which is highly flexible in that no normal speech of the\n",
      "patient is required. First, a powerful parallel sequence-to-sequence model\n",
      "converts the input dysarthric speech into a normal speech of a reference\n",
      "speaker as an intermediate product, and a nonparallel, frame-wise VC model\n",
      "realized with a variational autoencoder then converts the speaker identity of\n",
      "the reference speech back to that of the patient while assumed to be capable of\n",
      "preserving the enhanced quality. We investigate several design options.\n",
      "Experimental evaluation results demonstrate the potential of our approach to\n",
      "improving the quality of the dysarthric speech while maintaining the speaker\n",
      "identity. \n",
      "\n",
      "\n",
      "Voice conversion has gained increasing popularity in many applications of\n",
      "speech synthesis. The idea is to change the voice identity from one speaker\n",
      "into another while keeping the linguistic content unchanged. Many voice\n",
      "conversion approaches rely on the use of a vocoder to reconstruct the speech\n",
      "from acoustic features, and as a consequence, the speech quality heavily\n",
      "depends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end\n",
      "adversarial network, which performs voice conversion directly on the raw audio\n",
      "waveform of arbitrary length. By disentangling the speaker identity from the\n",
      "speech content, NVC-Net is able to perform non-parallel traditional\n",
      "many-to-many voice conversion as well as zero-shot voice conversion from a\n",
      "short utterance of an unseen target speaker. Importantly, NVC-Net is\n",
      "non-autoregressive and fully convolutional, achieving fast inference. Our model\n",
      "is capable of producing samples at a rate of more than 3600 kHz on an NVIDIA\n",
      "V100 GPU, being orders of magnitude faster than state-of-the-art methods under\n",
      "the same hardware configurations. Objective and subjective evaluations on\n",
      "non-parallel many-to-many voice conversion tasks show that NVC-Net obtains\n",
      "competitive results with significantly fewer parameters. \n",
      "\n",
      "\n",
      "Voice conversion is the task of converting a spoken utterance from a source\n",
      "speaker so that it appears to be said by a different target speaker while\n",
      "retaining the linguistic content of the utterance. Recent advances have led to\n",
      "major improvements in the quality of voice conversion systems. However, to be\n",
      "useful in a wider range of contexts, voice conversion systems would need to be\n",
      "(i) trainable without access to parallel data, (ii) work in a zero-shot setting\n",
      "where both the source and target speakers are unseen during training, and (iii)\n",
      "run in real time or faster. Recent techniques fulfil one or two of these\n",
      "requirements, but not all three. This paper extends recent voice conversion\n",
      "models based on generative adversarial networks (GANs), to satisfy all three of\n",
      "these conditions. We specifically extend the recent StarGAN-VC model by\n",
      "conditioning it on a speaker embedding (from a potentially unseen speaker).\n",
      "This allows the model to be used in a zero-shot setting, and we therefore call\n",
      "it StarGAN-ZSVC. We compare StarGAN-ZSVC against other voice conversion\n",
      "techniques in a low-resource setting using a small 9-minute training set.\n",
      "Compared to AutoVC -- another recent neural zero-shot approach -- we observe\n",
      "that StarGAN-ZSVC gives small improvements in the zero-shot setting, showing\n",
      "that real-time zero-shot voice conversion is possible even for a model trained\n",
      "on very little data. Further work is required to see whether scaling up\n",
      "StarGAN-ZSVC will also improve zero-shot voice conversion quality in\n",
      "high-resource contexts. \n",
      "\n",
      "\n",
      "In this paper, we first provide a review of the state-of-the-art emotional\n",
      "voice conversion research, and the existing emotional speech databases. We then\n",
      "motivate the development of a novel emotional speech database (ESD) that\n",
      "addresses the increasing research need. With this paper, the ESD database is\n",
      "now made available to the research community. The ESD database consists of 350\n",
      "parallel utterances spoken by 10 native English and 10 native Chinese speakers\n",
      "and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More\n",
      "than 29 hours of speech data were recorded in a controlled acoustic\n",
      "environment. The database is suitable for multi-speaker and cross-lingual\n",
      "emotional voice conversion studies. As case studies, we implement several\n",
      "state-of-the-art emotional voice conversion systems on the ESD database. This\n",
      "paper provides a reference study on ESD in conjunction with its release. \n",
      "\n",
      "\n",
      "Singing voice conversion (SVC) is one promising technique which can enrich\n",
      "the way of human-computer interaction by endowing a computer the ability to\n",
      "produce high-fidelity and expressive singing voice. In this paper, we propose\n",
      "DiffSVC, an SVC system based on denoising diffusion probabilistic model.\n",
      "DiffSVC uses phonetic posteriorgrams (PPGs) as content features. A denoising\n",
      "module is trained in DiffSVC, which takes destroyed mel spectrogram produced by\n",
      "the diffusion/forward process and its corresponding step information as input\n",
      "to predict the added Gaussian noise. We use PPGs, fundamental frequency\n",
      "features and loudness features as auxiliary input to assist the denoising\n",
      "process. Experiments show that DiffSVC can achieve superior conversion\n",
      "performance in terms of naturalness and voice similarity to current\n",
      "state-of-the-art SVC approaches. \n",
      "\n",
      "\n",
      "This paper presents a low-latency real-time (LLRT) non-parallel voice\n",
      "conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\n",
      "and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\n",
      "robust non-parallel multispeaker spectral model, which utilizes a\n",
      "speaker-independent latent space and a speaker-dependent code to generate\n",
      "reconstructed/converted spectral features given the spectral features of an\n",
      "input speaker. On the other hand, MWDLP is an efficient and a high-quality\n",
      "neural vocoder that can handle multispeaker data and generate speech waveform\n",
      "for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\n",
      "propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\n",
      "features and is built with a sparse network architecture. Further, to improve\n",
      "the modeling performance, we also propose a novel fine-tuning procedure that\n",
      "refines the frame-rate CycleVAE network by utilizing the waveform loss from the\n",
      "MWDLP network. The experimental results demonstrate that the proposed framework\n",
      "achieves high-performance VC, while allowing for LLRT usage with a single-core\n",
      "of $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\n",
      "input/output, feature extraction, on a frame shift of $10$ ms, a window length\n",
      "of $27.5$ ms, and $2$ lookup frames. \n",
      "\n",
      "\n",
      "Text-to-Speech (TTS) synthesis plays an important role in human-computer\n",
      "interaction. Currently, most TTS technologies focus on the naturalness of\n",
      "speech, namely,making the speeches sound like humans. However, the key tasks of\n",
      "the expression of emotion and the speaker identity are ignored, which limits\n",
      "the application scenarios of TTS synthesis technology. To make the synthesized\n",
      "speech more realistic and expand the application scenarios, we propose a\n",
      "multi-task anthropomorphic speech synthesis framework (MASS), which can\n",
      "synthesize speeches from text with specified emotion and speaker identity. The\n",
      "MASS framework consists of a base TTS module and two novel voice conversion\n",
      "modules: the emotional voice conversion module and the speaker voice conversion\n",
      "module. We propose deep emotion voice conversion model (DEVC) and deep speaker\n",
      "voice conversion model (DSVC) based on convolution residual networks. It solves\n",
      "the problem of feature loss during voice conversion. The model trainings are\n",
      "independent of parallel datasets, and are capable of many-to-many voice\n",
      "conversion. In the emotional voice conversion, speaker voice conversion\n",
      "experiments, as well as the multi-task speech synthesis experiments,\n",
      "experimental results show DEVC and DSVC convert speech effectively. The\n",
      "quantitative and qualitative evaluation results of multi-task speech synthesis\n",
      "experiments show MASS can effectively synthesis speech with specified text,\n",
      "emotion and speaker identity. \n",
      "\n",
      "\n",
      "Shared challenges provide a venue for comparing systems trained on common\n",
      "data using a standardized evaluation, and they also provide an invaluable\n",
      "resource for researchers when the data and evaluation results are publicly\n",
      "released. The Blizzard Challenge and Voice Conversion Challenge are two such\n",
      "challenges for text-to-speech synthesis and for speaker conversion,\n",
      "respectively, and their publicly-available system samples and listening test\n",
      "results comprise a historical record of state-of-the-art synthesis methods over\n",
      "the years. In this paper, we revisit these past challenges and conduct a\n",
      "large-scale listening test with samples from many challenges combined. Our aims\n",
      "are to analyze and compare opinions of a large number of systems together, to\n",
      "determine whether and how opinions change over time, and to collect a\n",
      "large-scale dataset of a diverse variety of synthetic samples and their ratings\n",
      "for further research. We found strong correlations challenge by challenge at\n",
      "the system level between the original results and our new listening test. We\n",
      "also observed the importance of the choice of speaker on synthesis quality. \n",
      "\n",
      "\n",
      "Discovering speaker independent acoustic units purely from spoken input is\n",
      "known to be a hard problem. In this work we propose an unsupervised speaker\n",
      "normalization technique prior to unit discovery. It is based on separating\n",
      "speaker related from content induced variations in a speech signal with an\n",
      "adversarial contrastive predictive coding approach. This technique does neither\n",
      "require transcribed speech nor speaker labels, and, furthermore, can be trained\n",
      "in a multilingual fashion, thus achieving speaker normalization even if only\n",
      "few unlabeled data is available from the target language. The speaker\n",
      "normalization is done by mapping all utterances to a medoid style which is\n",
      "representative for the whole database. We demonstrate the effectiveness of the\n",
      "approach by conducting acoustic unit discovery with a hidden Markov model\n",
      "variational autoencoder noting, however, that the proposed speaker\n",
      "normalization can serve as a front end to any unit discovery system.\n",
      "Experiments on English, Yoruba and Mboshi show improvements compared to using\n",
      "non-normalized input. \n",
      "\n",
      "\n",
      "Voice Conversion (VC) emerged as a significant domain of research in the\n",
      "field of speech synthesis in recent years due to its emerging application in\n",
      "voice-assisting technology, automated movie dubbing, and speech-to-singing\n",
      "conversion to name a few. VC basically deals with the conversion of vocal style\n",
      "of one speaker to another speaker while keeping the linguistic contents\n",
      "unchanged. VC task is performed through a three-stage pipeline consisting of\n",
      "speech analysis, speech feature mapping, and speech reconstruction. Nowadays\n",
      "the Generative Adversarial Network (GAN) models are widely in use for speech\n",
      "feature mapping from source to target speaker. In this paper, we propose an\n",
      "adaptive learning-based GAN model called ALGAN-VC for an efficient one-to-one\n",
      "VC of speakers. Our ALGAN-VC framework consists of some approaches to improve\n",
      "the speech quality and voice similarity between source and target speakers. The\n",
      "model incorporates a Dense Residual Network (DRN) like architecture to the\n",
      "generator network for efficient speech feature learning, for source to target\n",
      "speech feature conversion. We also integrate an adaptive learning mechanism to\n",
      "compute the loss function for the proposed model. Moreover, we use a boosted\n",
      "learning rate approach to enhance the learning capability of the proposed\n",
      "model. The model is trained by using both forward and inverse mapping\n",
      "simultaneously for a one-to-one VC. The proposed model is tested on Voice\n",
      "Conversion Challenge (VCC) 2016, 2018, and 2020 datasets as well as on our\n",
      "self-prepared speech dataset, which has been recorded in Indian regional\n",
      "languages and in English. A subjective and objective evaluation of the\n",
      "generated speech samples indicated that the proposed model elegantly performed\n",
      "the voice conversion task by achieving high speaker similarity and adequate\n",
      "speech quality. \n",
      "\n",
      "\n",
      "In this paper, we present a new objective prediction model for synthetic\n",
      "speech naturalness. It can be used to evaluate Text-To-Speech or Voice\n",
      "Conversion systems and works language independently. The model is trained\n",
      "end-to-end and based on a CNN-LSTM network that previously showed to give good\n",
      "results for speech quality estimation. We trained and tested the model on 16\n",
      "different datasets, such as from the Blizzard Challenge and the Voice\n",
      "Conversion Challenge. Further, we show that the reliability of deep\n",
      "learning-based naturalness prediction can be improved by transfer learning from\n",
      "speech quality prediction models that are trained on objective POLQA scores.\n",
      "The proposed model is made publicly available and can, for example, be used to\n",
      "evaluate different TTS system configurations. \n",
      "\n",
      "\n",
      "This paper presents a end-to-end framework for the F0 transformation in the\n",
      "context of expressive voice conversion. A single neural network is proposed, in\n",
      "which a first module is used to learn F0 representation over different temporal\n",
      "scales and a second adversarial module is used to learn the transformation from\n",
      "one emotion to another. The first module is composed of a convolution layer\n",
      "with wavelet kernels so that the various temporal scales of F0 variations can\n",
      "be efficiently encoded. The single decomposition/transformation network allows\n",
      "to learn in a end-to-end manner the F0 decomposition that are optimal with\n",
      "respect to the transformation, directly from the raw F0 signal. \n",
      "\n",
      "\n",
      "This paper proposes a non-autoregressive extension of our previously proposed\n",
      "sequence-to-sequence (S2S) model-based voice conversion (VC) methods. S2S\n",
      "model-based VC methods have attracted particular attention in recent years for\n",
      "their flexibility in converting not only the voice identity but also the pitch\n",
      "contour and local duration of input speech, thanks to the ability of the\n",
      "encoder-decoder architecture with the attention mechanism. However, one of the\n",
      "obstacles to making these methods work in real-time is the autoregressive (AR)\n",
      "structure. To overcome this obstacle, we develop a method to obtain a model\n",
      "that is free from an AR structure and behaves similarly to the original S2S\n",
      "models, based on a teacher-student learning framework. In our method, called\n",
      "\"FastS2S-VC\", the student model consists of encoder, decoder, and attention\n",
      "predictor. The attention predictor learns to predict attention distributions\n",
      "solely from source speech along with a target class index with the guidance of\n",
      "those predicted by the teacher model from both source and target speech. Thanks\n",
      "to this structure, the model is freed from an AR structure and allows for\n",
      "parallelization. Furthermore, we show that FastS2S-VC is suitable for real-time\n",
      "implementation based on a sliding-window approach, and describe how to make it\n",
      "run in real-time. Through speaker-identity and emotional-expression conversion\n",
      "experiments, we confirmed that FastS2S-VC was able to speed up the conversion\n",
      "process by 70 to 100 times compared to the original AR-type S2S-VC methods,\n",
      "without significantly degrading the audio quality and similarity to target\n",
      "speech. We also confirmed that the real-time version of FastS2S-VC can be run\n",
      "with a latency of 32 ms when run on a GPU. \n",
      "\n",
      "\n",
      "This paper proposes a novel voice conversion (VC) method based on\n",
      "non-autoregressive sequence-to-sequence (NAR-S2S) models. Inspired by the great\n",
      "success of NAR-S2S models such as FastSpeech in text-to-speech (TTS), we extend\n",
      "the FastSpeech2 model for the VC problem. We introduce the\n",
      "convolution-augmented Transformer (Conformer) instead of the Transformer,\n",
      "making it possible to capture both local and global context information from\n",
      "the input sequence. Furthermore, we extend variance predictors to variance\n",
      "converters to explicitly convert the source speaker's prosody components such\n",
      "as pitch and energy into the target speaker. The experimental evaluation with\n",
      "the Japanese speaker dataset, which consists of male and female speakers of\n",
      "1,000 utterances, demonstrates that the proposed model enables us to perform\n",
      "more stable, faster, and better conversion than autoregressive S2S (AR-S2S)\n",
      "models such as Tacotron2 and Transformer. \n",
      "\n",
      "\n",
      "Voice conversion (VC) is a task that transforms voice from target audio to\n",
      "source without losing linguistic contents, it is challenging especially when\n",
      "source and target speakers are unseen during training (zero-shot VC). Previous\n",
      "approaches require a pre-trained model or linguistic data to do the zero-shot\n",
      "conversion. Meanwhile, VC models with Vector Quantization (VQ) or Instance\n",
      "Normalization (IN) are able to disentangle contents from audios and achieve\n",
      "successful conversions. However, disentanglement in these models highly relies\n",
      "on heavily constrained bottleneck layers, thus, the sound quality is\n",
      "drastically sacrificed. In this paper, we propose NoiseVC, an approach that can\n",
      "disentangle contents based on VQ and Contrastive Predictive Coding (CPC).\n",
      "Additionally, Noise Augmentation is performed to further enhance\n",
      "disentanglement capability. We conduct several experiments and demonstrate that\n",
      "NoiseVC has a strong disentanglement ability with a small sacrifice of quality. \n",
      "\n",
      "\n",
      "Speech quality assessment has been a critical issue in speech processing for\n",
      "decades. Existing automatic evaluations usually require clean references or\n",
      "parallel ground truth data, which is infeasible when the amount of data soars.\n",
      "Subjective tests, on the other hand, do not need any additional clean or\n",
      "parallel data and correlates better to human perception. However, such a test\n",
      "is expensive and time-consuming because crowd work is necessary. It thus\n",
      "becomes highly desired to develop an automatic evaluation approach that\n",
      "correlates well with human perception while not requiring ground truth data. In\n",
      "this paper, we use self-supervised pre-trained models for MOS prediction. We\n",
      "show their representations can distinguish between clean and noisy audios.\n",
      "Then, we fine-tune these pre-trained models followed by simple linear layers in\n",
      "an end-to-end manner. The experiment results showed that our framework\n",
      "outperforms the two previous state-of-the-art models by a significant\n",
      "improvement on Voice Conversion Challenge 2018 and achieves comparable or\n",
      "superior performance on Voice Conversion Challenge 2016. We also conducted an\n",
      "ablation study to further investigate how each module benefits the task. The\n",
      "experiment results are implemented and reproducible with publicly available\n",
      "toolkits. \n",
      "\n",
      "\n",
      "Any-to-any voice conversion (VC) aims to convert the timbre of utterances\n",
      "from and to any speakers seen or unseen during training. Various any-to-any VC\n",
      "approaches have been proposed like AUTOVC, AdaINVC, and FragmentVC. AUTOVC, and\n",
      "AdaINVC utilize source and target encoders to disentangle the content and\n",
      "speaker information of the features. FragmentVC utilizes two encoders to encode\n",
      "source and target information and adopts cross attention to align the source\n",
      "and target features with similar phonetic content. Moreover, pre-trained\n",
      "features are adopted. AUTOVC used dvector to extract speaker information, and\n",
      "self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC\n",
      "to extract the phonetic content information. Different from previous works, we\n",
      "proposed S2VC that utilizes Self-Supervised features as both source and target\n",
      "features for VC model. Supervised phoneme posteriororgram (PPG), which is\n",
      "believed to be speaker-independent and widely used in VC to extract content\n",
      "information, is chosen as a strong baseline for SSL features. The objective\n",
      "evaluation and subjective evaluation both show models taking SSL feature CPC as\n",
      "both source and target features outperforms that taking PPG as source feature,\n",
      "suggesting that SSL features have great potential in improving VC. \n",
      "\n",
      "\n",
      "This paper shows that StarGAN-VC, a spectral envelope transformation method\n",
      "for non-parallel many-to-many voice conversion (VC), is capable of emotional VC\n",
      "(EVC). Although StarGAN-VC has been shown to enable speaker identity\n",
      "conversion, its capability for EVC for Japanese phrases has not been clarified.\n",
      "In this paper, we describe the direct application of StarGAN-VC to an EVC task\n",
      "with minimal fundamental frequency and aperiodicity processing. Through\n",
      "subjective evaluation experiments, we evaluated the performance of our\n",
      "StarGAN-EVC system in terms of its ability to achieve EVC for Japanese phrases.\n",
      "The subjective evaluation is conducted in terms of subjective classification\n",
      "and mean opinion score of neutrality and similarity. In addition, the\n",
      "interdependence between the source and target emotional domains was\n",
      "investigated from the perspective of the quality of EVC. \n",
      "\n",
      "\n",
      "Recent works on voice conversion (VC) focus on preserving the rhythm and the\n",
      "intonation as well as the linguistic content. To preserve these features from\n",
      "the source, we decompose current non-parallel VC systems into two encoders and\n",
      "one decoder. We analyze each module with several experiments and reassemble the\n",
      "best components to propose Assem-VC, a new state-of-the-art any-to-many\n",
      "non-parallel VC system. We also examine that PPG and Cotatron features are\n",
      "speaker-dependent, and attempt to remove speaker identity with adversarial\n",
      "training. Code and audio samples are available at\n",
      "https://github.com/mindslab-ai/assem-vc. \n",
      "\n",
      "\n",
      "We propose using self-supervised discrete representations for the task of\n",
      "speech resynthesis. To generate disentangled representation, we separately\n",
      "extract low-bitrate representations for speech content, prosodic information,\n",
      "and speaker identity. This allows to synthesize speech in a controllable\n",
      "manner. We analyze various state-of-the-art, self-supervised representation\n",
      "learning methods and shed light on the advantages of each method while\n",
      "considering reconstruction quality and disentanglement properties.\n",
      "Specifically, we evaluate the F0 reconstruction, speaker identification\n",
      "performance (for both resynthesis and voice conversion), recordings'\n",
      "intelligibility, and overall quality using subjective human evaluation. Lastly,\n",
      "we demonstrate how these representations can be used for an ultra-lightweight\n",
      "speech codec. Using the obtained representations, we can get to a rate of 365\n",
      "bits per second while providing better speech quality than the baseline\n",
      "methods. Audio samples can be found under the following link:\n",
      "speechbot.github.io/resynthesis. \n",
      "\n",
      "\n",
      "Emotional voice conversion (EVC) aims to change the emotional state of an\n",
      "utterance while preserving the linguistic content and speaker identity. In this\n",
      "paper, we propose a novel 2-stage training strategy for sequence-to-sequence\n",
      "emotional voice conversion with a limited amount of emotional speech data. We\n",
      "note that the proposed EVC framework leverages text-to-speech (TTS) as they\n",
      "share a common goal that is to generate high-quality expressive voice. In stage\n",
      "1, we perform style initialization with a multi-speaker TTS corpus, to\n",
      "disentangle speaking style and linguistic content. In stage 2, we perform\n",
      "emotion training with a limited amount of emotional speech data, to learn how\n",
      "to disentangle emotional style and linguistic information from the speech. The\n",
      "proposed framework can perform both spectrum and prosody conversion and\n",
      "achieves significant improvement over the state-of-the-art baselines in both\n",
      "objective and subjective evaluation. \n",
      "\n",
      "\n",
      "Voice style transfer, also called voice conversion, seeks to modify one\n",
      "speaker's voice to generate speech as if it came from another (target) speaker.\n",
      "Previous works have made progress on voice conversion with parallel training\n",
      "data and pre-known speakers. However, zero-shot voice style transfer, which\n",
      "learns from non-parallel data and generates voices for previously unseen\n",
      "speakers, remains a challenging problem. We propose a novel zero-shot voice\n",
      "transfer method via disentangled representation learning. The proposed method\n",
      "first encodes speaker-related style and voice content of each input voice into\n",
      "separated low-dimensional embedding spaces, and then transfers to a new voice\n",
      "by combining the source content embedding and target style embedding through a\n",
      "decoder. With information-theoretic guidance, the style and content embedding\n",
      "spaces are representative and (ideally) independent of each other. On\n",
      "real-world VCTK datasets, our method outperforms other baselines and obtains\n",
      "state-of-the-art results in terms of transfer accuracy and voice naturalness\n",
      "for voice style transfer experiments under both many-to-many and zero-shot\n",
      "setups. \n",
      "\n",
      "\n",
      "In this paper, we present an open-source software for developing a\n",
      "nonparallel voice conversion (VC) system named crank. Although we have released\n",
      "an open-source VC software based on the Gaussian mixture model named sprocket\n",
      "in the last VC Challenge, it is not straightforward to apply any speech corpus\n",
      "because it is necessary to prepare parallel utterances of source and target\n",
      "speakers to model a statistical conversion function. To address this issue, in\n",
      "this study, we developed a new open-source VC software that enables users to\n",
      "model the conversion function by using only a nonparallel speech corpus. For\n",
      "implementing the VC software, we used a vector-quantized variational\n",
      "autoencoder (VQVAE). To rapidly examine the effectiveness of recent\n",
      "technologies developed in this research field, crank also supports several\n",
      "representative works for autoencoder-based VC methods such as the use of\n",
      "hierarchical architectures, cyclic architectures, generative adversarial\n",
      "networks, speaker adversarial training, and neural vocoders. Moreover, it is\n",
      "possible to automatically estimate objective measures such as mel-cepstrum\n",
      "distortion and pseudo mean opinion score based on MOSNet. In this paper, we\n",
      "describe representative functions developed in crank and make brief comparisons\n",
      "by objective evaluations. \n",
      "\n",
      "\n",
      "Non-parallel voice conversion (VC) is a technique for training voice\n",
      "converters without a parallel corpus. Cycle-consistent adversarial\n",
      "network-based VCs (CycleGAN-VC and CycleGAN-VC2) are widely accepted as\n",
      "benchmark methods. However, owing to their insufficient ability to grasp\n",
      "time-frequency structures, their application is limited to mel-cepstrum\n",
      "conversion and not mel-spectrogram conversion despite recent advances in\n",
      "mel-spectrogram vocoders. To overcome this, CycleGAN-VC3, an improved variant\n",
      "of CycleGAN-VC2 that incorporates an additional module called time-frequency\n",
      "adaptive normalization (TFAN), has been proposed. However, an increase in the\n",
      "number of learned parameters is imposed. As an alternative, we propose\n",
      "MaskCycleGAN-VC, which is another extension of CycleGAN-VC2 and is trained\n",
      "using a novel auxiliary task called filling in frames (FIF). With FIF, we apply\n",
      "a temporal mask to the input mel-spectrogram and encourage the converter to\n",
      "fill in missing frames based on surrounding frames. This task allows the\n",
      "converter to learn time-frequency structures in a self-supervised manner and\n",
      "eliminates the need for an additional module such as TFAN. A subjective\n",
      "evaluation of the naturalness and speaker similarity showed that\n",
      "MaskCycleGAN-VC outperformed both CycleGAN-VC2 and CycleGAN-VC3 with a model\n",
      "size similar to that of CycleGAN-VC2. Audio samples are available at\n",
      "http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are machine learning networks based\n",
      "around creating synthetic data. Voice Conversion (VC) is a subset of voice\n",
      "translation that involves translating the paralinguistic features of a source\n",
      "speaker to a target speaker while preserving the linguistic information. The\n",
      "aim of non-parallel conditional GANs for VC is to translate an acoustic speech\n",
      "feature sequence from one domain to another without the use of paired data. In\n",
      "the study reported here, we investigated the interpretability of\n",
      "state-of-the-art implementations of non-parallel GANs in the domain of VC. We\n",
      "show that the learned representations in the repeating layers of a particular\n",
      "GAN architecture remain close to their original random initialised parameters,\n",
      "demonstrating that it is the number of repeating layers that is more\n",
      "responsible for the quality of the output. We also analysed the learned\n",
      "representations of a model trained on one particular dataset when used during\n",
      "transfer learning on another dataset. This showed extremely high levels of\n",
      "similarity across the entire network. Together, these results provide new\n",
      "insight into how the learned representations of deep generative networks change\n",
      "during learning and the importance in the number of layers. \n",
      "\n",
      "\n",
      "We propose a novel architecture and improved training objectives for\n",
      "non-parallel voice conversion. Our proposed CycleGAN-based model performs a\n",
      "shape-preserving transformation directly on a high frequency-resolution\n",
      "magnitude spectrogram, converting its style (i.e. speaker identity) while\n",
      "preserving the speech content. Throughout the entire conversion process, the\n",
      "model does not resort to compressed intermediate representations of any sort\n",
      "(e.g. mel spectrogram, low resolution spectrogram, decomposed network feature).\n",
      "We propose an efficient axial residual block architecture to support this\n",
      "expensive procedure and various modifications to the CycleGAN losses to\n",
      "stabilize the training process. We demonstrate via experiments that our\n",
      "proposed model outperforms Scyclone and shows a comparable or better\n",
      "performance to that of CycleGAN-VC2 even without employing a neural vocoder. \n",
      "\n",
      "\n",
      "The recent advances in voice conversion (VC) and text-to-speech (TTS) make it\n",
      "possible to produce natural sounding speech that poses threat to automatic\n",
      "speaker verification (ASV) systems. To this end, research on spoofing\n",
      "countermeasures has gained attention to protect ASV systems from such attacks.\n",
      "While the advanced spoofing countermeasures are able to detect known nature of\n",
      "spoofing attacks, they are not that effective under unknown attacks. In this\n",
      "work, we propose a novel data augmentation technique using a-law and mu-law\n",
      "based signal companding. We believe that the proposed method has an edge over\n",
      "traditional data augmentation by adding small perturbation or quantization\n",
      "noise. The studies are conducted on ASVspoof 2019 logical access corpus using\n",
      "light convolutional neural network based system. We find that the proposed data\n",
      "augmentation technique based on signal companding outperforms the\n",
      "state-of-the-art spoofing countermeasures showing ability to handle unknown\n",
      "nature of attacks. \n",
      "\n",
      "\n",
      "The ASVspoof initiative was conceived to spearhead research in anti-spoofing\n",
      "for automatic speaker verification (ASV). This paper describes the third in a\n",
      "series of bi-annual challenges: ASVspoof 2019. With the challenge database and\n",
      "protocols being described elsewhere, the focus of this paper is on results and\n",
      "the top performing single and ensemble system submissions from 62 teams, all of\n",
      "which out-perform the two baseline systems, often by a substantial margin.\n",
      "Deeper analyses shows that performance is dominated by specific conditions\n",
      "involving either specific spoofing attacks or specific acoustic environments.\n",
      "While fusion is shown to be particularly effective for the logical access\n",
      "scenario involving speech synthesis and voice conversion attacks, participants\n",
      "largely struggled to apply fusion successfully for the physical access scenario\n",
      "involving simulated replay attacks. This is likely the result of a lack of\n",
      "system complementarity, while oracle fusion experiments show clear potential to\n",
      "improve performance. Furthermore, while results for simulated data are\n",
      "promising, experiments with real replay data show a substantial gap, most\n",
      "likely due to the presence of additive noise in the latter. This finding, among\n",
      "others, leads to a number of ideas for further research and directions for\n",
      "future editions of the ASVspoof challenge. \n",
      "\n",
      "\n",
      "Cross-lingual voice conversion (VC) is an important and challenging problem\n",
      "due to significant mismatches of the phonetic set and the speech prosody of\n",
      "different languages. In this paper, we build upon the neural text-to-speech\n",
      "(TTS) model, i.e., FastSpeech, and LPCNet neural vocoder to design a new\n",
      "cross-lingual VC framework named FastSpeech-VC. We address the mismatches of\n",
      "the phonetic set and the speech prosody by applying Phonetic PosteriorGrams\n",
      "(PPGs), which have been proved to bridge across speaker and language\n",
      "boundaries. Moreover, we add normalized logarithm-scale fundamental frequency\n",
      "(Log-F0) to further compensate for the prosodic mismatches and significantly\n",
      "improve naturalness. Our experiments on English and Mandarin languages\n",
      "demonstrate that with only mono-lingual corpus, the proposed FastSpeech-VC can\n",
      "achieve high quality converted speech with mean opinion score (MOS) close to\n",
      "the professional records while maintaining good speaker similarity. Compared to\n",
      "the baselines using Tacotron2 and Transformer TTS models, the FastSpeech-VC can\n",
      "achieve controllable converted speech rate and much faster inference speed.\n",
      "More importantly, the FastSpeech-VC can easily be adapted to a speaker with\n",
      "limited training utterances. \n",
      "\n",
      "\n",
      "Speech enhancement has seen great improvement in recent years mainly through\n",
      "contributions in denoising, speaker separation, and dereverberation methods\n",
      "that mostly deal with environmental effects on vocal audio. To enhance speech\n",
      "beyond the limitations of the original signal, we take a regeneration approach,\n",
      "in which we recreate the speech from its essence, including the semi-recognized\n",
      "speech, prosody features, and identity. We propose a wav-to-wav generative\n",
      "model for speech that can generate 24khz speech in a real-time manner and which\n",
      "utilizes a compact speech representation, composed of ASR and identity\n",
      "features, to achieve a higher level of intelligibility. Inspired by voice\n",
      "conversion methods, we train to augment the speech characteristics while\n",
      "preserving the identity of the source using an auxiliary identity network.\n",
      "Perceptual acoustic metrics and subjective tests show that the method obtains\n",
      "valuable improvements over recent baselines. \n",
      "\n",
      "\n",
      "Factorizing speech as disentangled speech representations is vital to achieve\n",
      "highly controllable style transfer in voice conversion (VC). Conventional\n",
      "speech representation learning methods in VC only factorize speech as speaker\n",
      "and content, lacking controllability on other prosody-related factors.\n",
      "State-of-the-art speech representation learning methods for more speechfactors\n",
      "are using primary disentangle algorithms such as random resampling and ad-hoc\n",
      "bottleneck layer size adjustment,which however is hard to ensure robust speech\n",
      "representationdisentanglement. To increase the robustness of highly\n",
      "controllable style transfer on multiple factors in VC, we propose a\n",
      "disentangled speech representation learning framework based on adversarial\n",
      "learning. Four speech representations characterizing content, timbre, rhythm\n",
      "and pitch are extracted, and further disentangled by an adversarial\n",
      "Mask-And-Predict (MAP)network inspired by BERT. The adversarial network is used\n",
      "tominimize the correlations between the speech representations,by randomly\n",
      "masking and predicting one of the representationsfrom the others. Experimental\n",
      "results show that the proposedframework significantly improves the robustness\n",
      "of VC on multiple factors by increasing the speech quality MOS from 2.79 to3.30\n",
      "and decreasing the MCD from 3.89 to 3.58. \n",
      "\n",
      "\n",
      "This Ph.D. thesis focuses on developing a system for high-quality speech\n",
      "synthesis and voice conversion. Vocoder-based speech analysis, manipulation,\n",
      "and synthesis plays a crucial role in various kinds of statistical parametric\n",
      "speech research. Although there are vocoding methods which yield close to\n",
      "natural synthesized speech, they are typically computationally expensive, and\n",
      "are thus not suitable for real-time implementation, especially in embedded\n",
      "environments. Therefore, there is a need for simple and computationally\n",
      "feasible digital signal processing algorithms for generating high-quality and\n",
      "natural-sounding synthesized speech. In this dissertation, I propose a solution\n",
      "to extract optimal acoustic features and a new waveform generator to achieve\n",
      "higher sound quality and conversion accuracy by applying advances in deep\n",
      "learning. The approach remains computationally efficient. This challenge\n",
      "resulted in five thesis groups, which are briefly summarized below. \n",
      "\n",
      "\n",
      "Conventional singing voice conversion (SVC) methods often suffer from\n",
      "operating in high-resolution audio owing to a high dimensionality of data. In\n",
      "this paper, we propose a hierarchical representation learning that enables the\n",
      "learning of disentangled representations with multiple resolutions\n",
      "independently. With the learned disentangled representations, the proposed\n",
      "method progressively performs SVC from low to high resolutions. Experimental\n",
      "results show that the proposed method outperforms baselines that operate with a\n",
      "single resolution in terms of mean opinion score (MOS), similarity score, and\n",
      "pitch accuracy. \n",
      "\n",
      "\n",
      "Emotional voice conversion models adapt the emotion in speech without\n",
      "changing the speaker identity or linguistic content. They are less data hungry\n",
      "than text-to-speech models and allow to generate large amounts of emotional\n",
      "data for downstream tasks. In this work we propose EmoCat, a language-agnostic\n",
      "emotional voice conversion model. It achieves high-quality emotion conversion\n",
      "in German with less than 45 minutes of German emotional recordings by\n",
      "exploiting large amounts of emotional data in US English. EmoCat is an\n",
      "encoder-decoder model based on CopyCat, a voice conversion system which\n",
      "transfers prosody. We use adversarial training to remove emotion leakage from\n",
      "the encoder to the decoder. The adversarial training is improved by a novel\n",
      "contribution to gradient reversal to truly reverse gradients. This allows to\n",
      "remove only the leaking information and to converge to better optima with\n",
      "higher conversion performance. Evaluations show that Emocat can convert to\n",
      "different emotions but misses on emotion intensity compared to the recordings,\n",
      "especially for very expressive emotions. EmoCat is able to achieve audio\n",
      "quality on par with the recordings for five out of six tested emotion\n",
      "intensities. \n",
      "\n",
      "\n",
      "In this paper we propose a new cross-lingual Voice Conversion (VC) approach\n",
      "which can generate all speech parameters (MCEP, LF0, BAP) from one DNN model\n",
      "using PPGs (Phonetic PosteriorGrams) extracted from inputted speech using\n",
      "several ASR acoustic models. Using the proposed VC method, we tried three\n",
      "different approaches to build a multilingual TTS system without recording a\n",
      "multilingual speech corpus. A listening test was carried out to evaluate both\n",
      "speech quality (naturalness) and voice similarity between converted speech and\n",
      "target speech. The results show that Approach 1 achieved the highest level of\n",
      "naturalness (3.28 MOS on a 5-point scale) and similarity (2.77 MOS). \n",
      "\n",
      "\n",
      "This paper describes the recent development of ESPnet\n",
      "(https://github.com/espnet/espnet), an end-to-end speech processing toolkit.\n",
      "This project was initiated in December 2017 to mainly deal with end-to-end\n",
      "speech recognition experiments based on sequence-to-sequence modeling. The\n",
      "project has grown rapidly and now covers a wide range of speech processing\n",
      "applications. Now ESPnet also includes text to speech (TTS), voice conversation\n",
      "(VC), speech translation (ST), and speech enhancement (SE) with support for\n",
      "beamforming, speech separation, denoising, and dereverberation. All\n",
      "applications are trained in an end-to-end manner, thanks to the generic\n",
      "sequence to sequence modeling properties, and they can be further integrated\n",
      "and jointly optimized. Also, ESPnet provides reproducible all-in-one recipes\n",
      "for these applications with state-of-the-art performance in various benchmarks\n",
      "by incorporating transformer, advanced data augmentation, and conformer. This\n",
      "project aims to provide up-to-date speech processing experience to the\n",
      "community so that researchers in academia and various industry scales can\n",
      "develop their technologies collaboratively. \n",
      "\n",
      "\n",
      "In speech technologies, speaker's voice representation is used in many\n",
      "applications such as speech recognition, voice conversion, speech synthesis\n",
      "and, obviously, user authentication. Modern vocal representations of the\n",
      "speaker are based on neural embeddings. In addition to the targeted\n",
      "information, these representations usually contain sensitive information about\n",
      "the speaker, like the age, sex, physical state, education level or ethnicity.\n",
      "In order to allow the user to choose which information to protect, we introduce\n",
      "in this paper the concept of attribute-driven privacy preservation in speaker\n",
      "voice representation. It allows a person to hide one or more personal aspects\n",
      "to a potential malicious interceptor and to the application provider. As a\n",
      "first solution to this concept, we propose to use an adversarial autoencoding\n",
      "method that disentangles in the voice representation a given speaker attribute\n",
      "thus allowing its concealment. We focus here on the sex attribute for an\n",
      "Automatic Speaker Verification (ASV) task. Experiments carried out using the\n",
      "VoxCeleb datasets have shown that the proposed method enables the concealment\n",
      "of this attribute while preserving ASV ability. \n",
      "\n",
      "\n",
      "This paper describes an end-to-end adversarial singing voice conversion\n",
      "(EA-SVC) approach. It can directly generate arbitrary singing waveform by given\n",
      "phonetic posteriorgram (PPG) representing content, F0 representing pitch, and\n",
      "speaker embedding representing timbre, respectively. Proposed system is\n",
      "composed of three modules: generator $G$, the audio generation discriminator\n",
      "$D_{A}$, and the feature disentanglement discriminator $D_F$. The generator $G$\n",
      "encodes the features in parallel and inversely transforms them into the target\n",
      "waveform. In order to make timbre conversion more stable and controllable,\n",
      "speaker embedding is further decomposed to the weighted sum of a group of\n",
      "trainable vectors representing different timbre clusters. Further, to realize\n",
      "more robust and accurate singing conversion, disentanglement discriminator\n",
      "$D_F$ is proposed to remove pitch and timbre related information that remains\n",
      "in the encoded PPG. Finally, a two-stage training is conducted to keep a stable\n",
      "and effective adversarial training process. Subjective evaluation results\n",
      "demonstrate the effectiveness of our proposed methods. Proposed system\n",
      "outperforms conventional cascade approach and the WaveNet based end-to-end\n",
      "approach in terms of both singing quality and singer similarity. Further\n",
      "objective analysis reveals that the model trained with the proposed two-stage\n",
      "training strategy can produce a smoother and sharper formant which leads to\n",
      "higher audio quality. \n",
      "\n",
      "\n",
      "Voice conversion technologies have been greatly improved in recent years with\n",
      "the help of deep learning, but their capabilities of producing natural sounding\n",
      "utterances in different conditions remain unclear. In this paper, we gave a\n",
      "thorough study of the robustness of known VC models. We also modified these\n",
      "models, such as the replacement of speaker embeddings, to further improve their\n",
      "performances. We found that the sampling rate and audio duration greatly\n",
      "influence voice conversion. All the VC models suffer from unseen data, but\n",
      "AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained\n",
      "is more suitable for voice conversion than those trained on speaker\n",
      "identification. \n",
      "\n",
      "\n",
      "This thesis work presents an architectural design of a system to bring\n",
      "non-repudiation concept into the IP based digital voice conversations (VoIP) in\n",
      "LTE and UMTS networks, using electronic signatures, by considering a\n",
      "centralized approach. Moreover, functionalities and technical methods to\n",
      "support such a system are researched. Last but not least, ways to introduce\n",
      "this system as a public and commercial service are discussed. Non-repudiation\n",
      "concept provided by electronic signatures and related cryptographic functions,\n",
      "as introduced in this study, allow using digital records of these voice\n",
      "conversations as legally binding statements or proofs likewise and even instead\n",
      "of traditional wet signatures. The system is designed as a subsystem to IMS\n",
      "based 3G and 4G networks and maximum compatibility with current configurations,\n",
      "components and interfaces of these networks is intended. On the other hand\n",
      "non-repudiation is achieved by special signature, storage and verification\n",
      "units located in the IMS core network. Voice data is proposed to be processed\n",
      "in MRF unit of the IMS. Additionally, a USSD/USSI based special solution to\n",
      "initiate these signed calls is developed. According to the proposed scheme;\n",
      "during a signed call, two unidirectional voice streams originating from two\n",
      "parties of the call, which are transferred in IP and UDP encapsulated RTP\n",
      "packages, are received by the signature unit and interweaved using their\n",
      "arrival times, so that they become a unified stream. Signature unit generates\n",
      "hashes of groups of received packages and signs them using PKI algorithms and\n",
      "applying hash/signature chaining to increase integrity protection and to\n",
      "empower non-repudiation. Then, it forwards packages and signature information\n",
      "to the storage unit. Storage unit keeps all the call records, signature data\n",
      "and metadata of these calls. Verification unit later gathers relevant data from\n",
      "the storage unit... \n",
      "\n",
      "\n",
      "In this paper, we focus on improving the performance of the text-dependent\n",
      "speaker verification system in the scenario of limited training data. The\n",
      "speaker verification system deep learning based text-dependent generally needs\n",
      "a large scale text-dependent training data set which could be labor and cost\n",
      "expensive, especially for customized new wake-up words. In recent studies,\n",
      "voice conversion systems that can generate high quality synthesized speech of\n",
      "seen and unseen speakers have been proposed. Inspired by those works, we adopt\n",
      "two different voice conversion methods as well as the very simple re-sampling\n",
      "approach to generate new text-dependent speech samples for data augmentation\n",
      "purposes. Experimental results show that the proposed method significantly\n",
      "improves the Equal Error Rare performance from 6.51% to 4.51% in the scenario\n",
      "of limited training data. \n",
      "\n",
      "\n",
      "This paper proposes an interesting voice and accent joint conversion\n",
      "approach, which can convert an arbitrary source speaker's voice to a target\n",
      "speaker with non-native accent. This problem is challenging as each target\n",
      "speaker only has training data in native accent and we need to disentangle\n",
      "accent and speaker information in the conversion model training and re-combine\n",
      "them in the conversion stage. In our recognition-synthesis conversion\n",
      "framework, we manage to solve this problem by two proposed tricks. First, we\n",
      "use accent-dependent speech recognizers to obtain bottleneck features for\n",
      "different accented speakers. This aims to wipe out other factors beyond the\n",
      "linguistic information in the BN features for conversion model training.\n",
      "Second, we propose to use adversarial training to better disentangle the\n",
      "speaker and accent information in our encoder-decoder based conversion model.\n",
      "Specifically, we plug an auxiliary speaker classifier to the encoder, trained\n",
      "with an adversarial loss to wipe out speaker information from the encoder\n",
      "output. Experiments show that our approach is superior to the baseline. The\n",
      "proposed tricks are quite effective in improving accentedness and audio quality\n",
      "and speaker similarity are well maintained. \n",
      "\n",
      "\n",
      "We propose a novel training scheme to optimize voice conversion network with\n",
      "a speaker identity loss function. The training scheme not only minimizes\n",
      "frame-level spectral loss, but also speaker identity loss. We introduce a cycle\n",
      "consistency loss that constrains the converted speech to maintain the same\n",
      "speaker identity as reference speech at utterance level. While the proposed\n",
      "training scheme is applicable to any voice conversion networks, we formulate\n",
      "the study under the average model voice conversion framework in this paper.\n",
      "Experiments conducted on CMU-ARCTIC and CSTR-VCTK corpus confirm that the\n",
      "proposed method outperforms baseline methods in terms of speaker similarity. \n",
      "\n",
      "\n",
      "This paper presents FastSVC, a light-weight cross-domain singing voice\n",
      "conversion (SVC) system, which can achieve high conversion performance, with\n",
      "inference speed 4x faster than real-time on CPUs. FastSVC uses Conformer-based\n",
      "phoneme recognizer to extract singer-agnostic linguistic features from singing\n",
      "signals. A feature-wise linear modulation based generator is used to synthesize\n",
      "waveform directly from linguistic features, leveraging information from\n",
      "sine-excitation signals and loudness features. The waveform generator can be\n",
      "trained conveniently using a multi-resolution spectral loss and an adversarial\n",
      "loss. Experimental results show that the proposed FastSVC system, compared with\n",
      "a computationally heavy baseline system, can achieve comparable conversion\n",
      "performance in some scenarios and significantly better conversion performance\n",
      "in other scenarios. Moreover, the proposed FastSVC system achieves desirable\n",
      "cross-lingual singing conversion performance. The inference speed of the\n",
      "FastSVC system is 3x and 70x faster than the baseline system on GPUs and CPUs,\n",
      "respectively. \n",
      "\n",
      "\n",
      "While recent neural text-to-speech (TTS) systems perform remarkably well,\n",
      "they typically require a substantial amount of recordings from the target\n",
      "speaker reading in the desired speaking style. In this work, we present a novel\n",
      "3-step methodology to circumvent the costly operation of recording large\n",
      "amounts of target data in order to build expressive style voices with as little\n",
      "as 15 minutes of such recordings. First, we augment data via voice conversion\n",
      "by leveraging recordings in the desired speaking style from other speakers.\n",
      "Next, we use that synthetic data on top of the available recordings to train a\n",
      "TTS model. Finally, we fine-tune that model to further increase quality. Our\n",
      "evaluations show that the proposed changes bring significant improvements over\n",
      "non-augmented models across many perceived aspects of synthesised speech. We\n",
      "demonstrate the proposed approach on 2 styles (newscaster and conversational),\n",
      "on various speakers, and on both single and multi-speaker models, illustrating\n",
      "the robustness of our approach. \n",
      "\n",
      "\n",
      "In this paper, we propose a novel voice conversion strategy to resolve the\n",
      "mismatch between the training and conversion scenarios when parallel speech\n",
      "corpus is unavailable for training. Based on auto-encoder and disentanglement\n",
      "frameworks, we design the proposed model to extract identity and content\n",
      "representations while reconstructing the input speech signal itself. Since we\n",
      "use other speaker's identity information in the training process, the training\n",
      "philosophy is naturally matched with the objective of voice conversion process.\n",
      "In addition, we effectively design the disentanglement framework to reliably\n",
      "preserve linguistic information and to enhance the quality of converted speech\n",
      "signals. The superiority of the proposed method is shown in subjective\n",
      "listening tests as well as objective measures. \n",
      "\n",
      "\n",
      "Though significant progress has been made for the voice conversion (VC) of\n",
      "typical speech, VC for atypical speech, e.g., dysarthric and second-language\n",
      "(L2) speech, remains a challenge, since it involves correcting for atypical\n",
      "prosody while maintaining speaker identity. To address this issue, we propose a\n",
      "VC system with explicit prosodic modelling and deep speaker embedding (DSE)\n",
      "learning. First, a speech-encoder strives to extract robust phoneme embeddings\n",
      "from atypical speech. Second, a prosody corrector takes in phoneme embeddings\n",
      "to infer typical phoneme duration and pitch values. Third, a conversion model\n",
      "takes phoneme embeddings and typical prosody features as inputs to generate the\n",
      "converted speech, conditioned on the target DSE that is learned via speaker\n",
      "encoder or speaker adaptation. Extensive experiments demonstrate that speaker\n",
      "adaptation can achieve higher speaker similarity, and the speaker encoder based\n",
      "conversion model can greatly reduce dysarthric and non-native pronunciation\n",
      "patterns with improved speech intelligibility. A comparison of speech\n",
      "recognition results between the original dysarthric speech and converted speech\n",
      "show that absolute reduction of 47.6% character error rate (CER) and 29.3% word\n",
      "error rate (WER) can be achieved. \n",
      "\n",
      "\n",
      "Emotional voice conversion (EVC) aims to convert the emotion of speech from\n",
      "one state to another while preserving the linguistic content and speaker\n",
      "identity. In this paper, we study the disentanglement and recomposition of\n",
      "emotional elements in speech through variational autoencoding Wasserstein\n",
      "generative adversarial network (VAW-GAN). We propose a speaker-dependent EVC\n",
      "framework based on VAW-GAN, that includes two VAW-GAN pipelines, one for\n",
      "spectrum conversion, and another for prosody conversion. We train a spectral\n",
      "encoder that disentangles emotion and prosody (F0) information from spectral\n",
      "features; we also train a prosodic encoder that disentangles emotion modulation\n",
      "of prosody (affective prosody) from linguistic prosody. At run-time, the\n",
      "decoder of spectral VAW-GAN is conditioned on the output of prosodic VAW-GAN.\n",
      "The vocoder takes the converted spectral and prosodic features to generate the\n",
      "target emotional speech. Experiments validate the effectiveness of our proposed\n",
      "method in both objective and subjective evaluations. \n",
      "\n",
      "\n",
      "Cycle consistent generative adversarial network (CycleGAN) and variational\n",
      "autoencoder (VAE) based models have gained popularity in non-parallel voice\n",
      "conversion recently. However, they often suffer from difficult training process\n",
      "and unsatisfactory results. In this paper, we propose CVC, a contrastive\n",
      "learning-based adversarial approach for voice conversion. Compared to previous\n",
      "CycleGAN-based methods, CVC only requires an efficient one-way GAN training by\n",
      "taking the advantage of contrastive learning. When it comes to non-parallel\n",
      "one-to-one voice conversion, CVC is on par or better than CycleGAN and VAE\n",
      "while effectively reducing training time. CVC further demonstrates superior\n",
      "performance in many-to-one voice conversion, enabling the conversion from\n",
      "unseen speakers. \n",
      "\n",
      "\n",
      "Recently, voice conversion (VC) has been widely studied. Many VC systems use\n",
      "disentangle-based learning techniques to separate the speaker and the\n",
      "linguistic content information from a speech signal. Subsequently, they convert\n",
      "the voice by changing the speaker information to that of the target speaker. To\n",
      "prevent the speaker information from leaking into the content embeddings,\n",
      "previous works either reduce the dimension or quantize the content embedding as\n",
      "a strong information bottleneck. These mechanisms somehow hurt the synthesis\n",
      "quality. In this work, we propose AGAIN-VC, an innovative VC system using\n",
      "Activation Guidance and Adaptive Instance Normalization. AGAIN-VC is an\n",
      "auto-encoder-based model, comprising of a single encoder and a decoder. With a\n",
      "proper activation as an information bottleneck on content embeddings, the\n",
      "trade-off between the synthesis quality and the speaker similarity of the\n",
      "converted speech is improved drastically. This one-shot VC system obtains the\n",
      "best performance regardless of the subjective or objective evaluations. \n",
      "\n",
      "\n",
      "This paper presents the IQIYI voice conversion system (T24) for Voice\n",
      "Conversion 2020. In the competition, each target speaker has 70 sentences. We\n",
      "have built an end-to-end voice conversion system based on PPG. First, the ASR\n",
      "acoustic model calculates the BN feature, which represents the content-related\n",
      "information in the speech. Then the Mel feature is calculated through an\n",
      "improved prosody tacotron model. Finally, the Mel spectrum is converted to wav\n",
      "through an improved LPCNet. The evaluation results show that this system can\n",
      "achieve better voice conversion effects. In the case of using 16k rather than\n",
      "24k sampling rate audio, the conversion result is relatively good in\n",
      "naturalness and similarity. Among them, our best results are in the similarity\n",
      "evaluation of the Task 2, the 2nd in the ASV-based objective evaluation and the\n",
      "5th in the subjective evaluation. \n",
      "\n",
      "\n",
      "Singing voice conversion (SVC) aims to convert the voice of one singer to\n",
      "that of other singers while keeping the singing content and melody. On top of\n",
      "recent voice conversion works, we propose a novel model to steadily convert\n",
      "songs while keeping their naturalness and intonation. We build an end-to-end\n",
      "architecture, taking phonetic posteriorgrams (PPGs) as inputs and generating\n",
      "mel spectrograms. Specifically, we implement two separate encoders: one encodes\n",
      "PPGs as content, and the other compresses mel spectrograms to supply acoustic\n",
      "and musical information. To improve the performance on timbre and melody, an\n",
      "adversarial singer confusion module and a mel-regressive representation\n",
      "learning module are designed for the model. Objective and subjective\n",
      "experiments are conducted on our private Chinese singing corpus. Comparing with\n",
      "the baselines, our methods can significantly improve the conversion performance\n",
      "in terms of naturalness, melody, and voice similarity. Moreover, our PPG-based\n",
      "method is proved to be robust for noisy sources. \n",
      "\n",
      "\n",
      "Emotional voice conversion aims to transform emotional prosody in speech\n",
      "while preserving the linguistic content and speaker identity. Prior studies\n",
      "show that it is possible to disentangle emotional prosody using an\n",
      "encoder-decoder network conditioned on discrete representation, such as one-hot\n",
      "emotion labels. Such networks learn to remember a fixed set of emotional\n",
      "styles. In this paper, we propose a novel framework based on variational\n",
      "auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes\n",
      "use of a pre-trained speech emotion recognition (SER) model to transfer\n",
      "emotional style during training and at run-time inference. In this way, the\n",
      "network is able to transfer both seen and unseen emotional style to a new\n",
      "utterance. We show that the proposed framework achieves remarkable performance\n",
      "by consistently outperforming the baseline framework. This paper also marks the\n",
      "release of an emotional speech dataset (ESD) for voice conversion, which has\n",
      "multiple speakers and languages. \n",
      "\n",
      "\n",
      "Any-to-any voice conversion aims to convert the voice from and to any\n",
      "speakers even unseen during training, which is much more challenging compared\n",
      "to one-to-one or many-to-many tasks, but much more attractive in real-world\n",
      "scenarios. In this paper we proposed FragmentVC, in which the latent phonetic\n",
      "structure of the utterance from the source speaker is obtained from Wav2Vec\n",
      "2.0, while the spectral features of the utterance(s) from the target speaker\n",
      "are obtained from log mel-spectrograms. By aligning the hidden structures of\n",
      "the two different feature spaces with a two-stage training process, FragmentVC\n",
      "is able to extract fine-grained voice fragments from the target speaker\n",
      "utterance(s) and fuse them into the desired utterance, all based on the\n",
      "attention mechanism of Transformer as verified with analysis on attention maps,\n",
      "and is accomplished end-to-end. This approach is trained with reconstruction\n",
      "loss only without any disentanglement considerations between content and\n",
      "speaker information and doesn't require parallel data. Objective evaluation\n",
      "based on speaker verification and subjective evaluation with MOS both showed\n",
      "that this approach outperformed SOTA approaches, such as AdaIN-VC and AutoVC. \n",
      "\n",
      "\n",
      "Human voices can be used to authenticate the identity of the speaker, but the\n",
      "automatic speaker verification (ASV) systems are vulnerable to voice spoofing\n",
      "attacks, such as impersonation, replay, text-to-speech, and voice conversion.\n",
      "Recently, researchers developed anti-spoofing techniques to improve the\n",
      "reliability of ASV systems against spoofing attacks. However, most methods\n",
      "encounter difficulties in detecting unknown attacks in practical use, which\n",
      "often have different statistical distributions from known attacks. Especially,\n",
      "the fast development of synthetic voice spoofing algorithms is generating\n",
      "increasingly powerful attacks, putting the ASV systems at risk of unseen\n",
      "attacks. In this work, we propose an anti-spoofing system to detect unknown\n",
      "synthetic voice spoofing attacks (i.e., text-to-speech or voice conversion)\n",
      "using one-class learning. The key idea is to compact the bona fide speech\n",
      "representation and inject an angular margin to separate the spoofing attacks in\n",
      "the embedding space. Without resorting to any data augmentation methods, our\n",
      "proposed system achieves an equal error rate (EER) of 2.19% on the evaluation\n",
      "set of ASVspoof 2019 Challenge logical access scenario, outperforming all\n",
      "existing single systems (i.e., those without model ensemble). \n",
      "\n",
      "\n",
      "Non-parallel many-to-many voice conversion is recently attract-ing huge\n",
      "research efforts in the speech processing community. A voice conversion system\n",
      "transforms an utterance of a source speaker to another utterance of a target\n",
      "speaker by keeping the content in the original utterance and replacing by the\n",
      "vocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2,\n",
      "present promising results, only when speech corpus of the engaged speakers is\n",
      "available during model training. AUTOVCis able to perform voice conversion on\n",
      "unseen speakers, but it needs an external pretrained speaker verification\n",
      "model. In this paper, we present our new GAN-based zero-shot voice conversion\n",
      "solution, called GAZEV, which targets to support unseen speakers on both source\n",
      "and target utterances. Our key technical contribution is the adoption of\n",
      "speaker embedding loss on top of the GAN framework, as well as adaptive\n",
      "instance normalization strategy, in order to address the limitations of speaker\n",
      "identity transfer in existing solutions. Our empirical evaluations demonstrate\n",
      "significant performance improvement on output speech quality and comparable\n",
      "speaker similarity to AUTOVC. \n",
      "\n",
      "\n",
      "We present a novel approach to any-to-one (A2O) voice conversion (VC) in a\n",
      "sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker,\n",
      "including those unseen during training, to a fixed target speaker. We utilize\n",
      "vq-wav2vec (VQW2V), a discretized self-supervised speech representation that\n",
      "was learned from massive unlabeled data, which is assumed to be\n",
      "speaker-independent and well corresponds to underlying linguistic contents.\n",
      "Given a training dataset of the target speaker, we extract VQW2V and acoustic\n",
      "features to estimate a seq2seq mapping function from the former to the latter.\n",
      "With the help of a pretraining method and a newly designed postprocessing\n",
      "technique, our model can be generalized to only 5 min of data, even\n",
      "outperforming the same model trained with parallel data. \n",
      "\n",
      "\n",
      "Non-parallel voice conversion (VC) is a technique for learning mappings\n",
      "between source and target speeches without using a parallel corpus. Recently,\n",
      "cycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown\n",
      "promising results regarding this problem and have been widely used as benchmark\n",
      "methods. However, owing to the ambiguity of the effectiveness of\n",
      "CycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for\n",
      "mel-cepstrum conversion even when comparative methods employ mel-spectrogram as\n",
      "a conversion target. To address this, we examined the applicability of\n",
      "CycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we\n",
      "discovered that their direct applications compromised the time-frequency\n",
      "structure that should be preserved during conversion. To remedy this, we\n",
      "propose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates\n",
      "time-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the\n",
      "scale and bias of the converted features while reflecting the time-frequency\n",
      "structure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on\n",
      "inter-gender and intra-gender non-parallel VC. A subjective evaluation of\n",
      "naturalness and similarity showed that for every VC pair, CycleGAN-VC3\n",
      "outperforms or is competitive with the two types of CycleGAN-VC2, one of which\n",
      "was applied to mel-cepstrum and the other to mel-spectrogram. Audio samples are\n",
      "available at\n",
      "http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html. \n",
      "\n",
      "\n",
      "Many-to-many voice conversion with non-parallel training data has seen\n",
      "significant progress in recent years. StarGAN-based models have been interests\n",
      "of voice conversion. However, most of the StarGAN-based methods only focused on\n",
      "voice conversion experiments for the situations where the number of speakers\n",
      "was small, and the amount of training data was large. In this work, we aim at\n",
      "improving the data efficiency of the model and achieving a many-to-many\n",
      "non-parallel StarGAN-based voice conversion for a relatively large number of\n",
      "speakers with limited training samples. In order to improve data efficiency,\n",
      "the proposed model uses a speaker encoder for extracting speaker embeddings and\n",
      "conducts adaptive instance normalization (AdaIN) on convolutional weights.\n",
      "Experiments are conducted with 109 speakers under two low-resource situations,\n",
      "where the number of training samples is 20 and 5 per speaker. An objective\n",
      "evaluation shows the proposed model is better than the baseline methods.\n",
      "Furthermore, a subjective evaluation shows that, for both naturalness and\n",
      "similarity, the proposed model outperforms the baseline method. \n",
      "\n",
      "\n",
      "This paper introduces a new open-sourced Mandarin speech corpus, called\n",
      "DiDiSpeech. It consists of about 800 hours of speech data at 48kHz sampling\n",
      "rate from 6000 speakers and the corresponding texts. All speech data in the\n",
      "corpus is recorded in quiet environment and is suitable for various speech\n",
      "processing tasks, such as voice conversion, multi-speaker text-to-speech and\n",
      "automatic speech recognition. We conduct experiments with multiple speech tasks\n",
      "and evaluate the performance, showing that it is promising to use the corpus\n",
      "for both academic research and practical application. The corpus is available\n",
      "at https://outreach.didichuxing.com/research/opendata/. \n",
      "\n",
      "\n",
      "Recent state-of-the-art neural text-to-speech (TTS) synthesis models have\n",
      "dramatically improved intelligibility and naturalness of generated speech from\n",
      "text. However, building a good bilingual or code-switched TTS for a particular\n",
      "voice is still a challenge. The main reason is that it is not easy to obtain a\n",
      "bilingual corpus from a speaker who achieves native-level fluency in both\n",
      "languages. In this paper, we explore the use of Mandarin speech recordings from\n",
      "a Mandarin speaker, and English speech recordings from another English speaker\n",
      "to build high-quality bilingual and code-switched TTS for both speakers. A\n",
      "Tacotron2-based cross-lingual voice conversion system is employed to generate\n",
      "the Mandarin speaker's English speech and the English speaker's Mandarin\n",
      "speech, which show good naturalness and speaker similarity. The obtained\n",
      "bilingual data are then augmented with code-switched utterances synthesized\n",
      "using a Transformer model. With these data, three neural TTS models --\n",
      "Tacotron2, Transformer and FastSpeech are applied for building bilingual and\n",
      "code-switched TTS. Subjective evaluation results show that all the three\n",
      "systems can produce (near-)native-level speech in both languages for each of\n",
      "the speaker. \n",
      "\n",
      "\n",
      "This paper presents the description of our submitted system for Voice\n",
      "Conversion Challenge (VCC) 2020 with vector-quantization variational\n",
      "autoencoder (VQ-VAE) with WaveNet as the decoder, i.e., VQ-VAE-WaveNet.\n",
      "VQ-VAE-WaveNet is a nonparallel VAE-based voice conversion that reconstructs\n",
      "the acoustic features along with separating the linguistic information with\n",
      "speaker identity. The model is further improved with the WaveNet cycle as the\n",
      "decoder to generate the high-quality speech waveform, since WaveNet, as an\n",
      "autoregressive neural vocoder, has achieved the SoTA result of waveform\n",
      "generation. In practice, our system can be developed with VCC 2020 dataset for\n",
      "both Task 1 (intra-lingual) and Task 2 (cross-lingual). However, we only submit\n",
      "our system for the intra-lingual voice conversion task. The results of VCC 2020\n",
      "demonstrate that our system VQ-VAE-WaveNet achieves: 3.04 mean opinion score\n",
      "(MOS) in naturalness and a 3.28 average score in similarity ( the speaker\n",
      "similarity percentage (Sim) of 75.99%) for Task 1. The subjective evaluations\n",
      "also reveal that our system gives top performance when no supervised learning\n",
      "is involved. What's more, our system performs well in some objective\n",
      "evaluations. Specifically, our system achieves an average score of 3.95 in\n",
      "naturalness in automatic naturalness prediction and ranked the 6th and 8th,\n",
      "respectively in ASV-based speaker similarity and spoofing countermeasures. \n",
      "\n",
      "\n",
      "In this paper, we present the voice conversion (VC) systems developed at\n",
      "Nagoya University (NU) for the Voice Conversion Challenge 2020 (VCC2020). We\n",
      "aim to determine the effectiveness of two recent significant technologies in\n",
      "VC: sequence-to-sequence (seq2seq) models and autoregressive (AR) neural\n",
      "vocoders. Two respective systems were developed for the two tasks in the\n",
      "challenge: for task 1, we adopted the Voice Transformer Network, a\n",
      "Transformer-based seq2seq VC model, and extended it with synthetic parallel\n",
      "data to tackle nonparallel data; for task 2, we used the frame-based cyclic\n",
      "variational autoencoder (CycleVAE) to model the spectral features of a speech\n",
      "waveform and the AR WaveNet vocoder with additional fine-tuning. By comparing\n",
      "with the baseline systems, we confirmed that the seq2seq modeling can improve\n",
      "the conversion similarity and that the use of AR vocoders can improve the\n",
      "naturalness of the converted speech. \n",
      "\n",
      "\n",
      "In this paper, we present a description of the baseline system of Voice\n",
      "Conversion Challenge (VCC) 2020 with a cyclic variational autoencoder\n",
      "(CycleVAE) and Parallel WaveGAN (PWG), i.e., CycleVAEPWG. CycleVAE is a\n",
      "nonparallel VAE-based voice conversion that utilizes converted acoustic\n",
      "features to consider cyclically reconstructed spectra during optimization. On\n",
      "the other hand, PWG is a non-autoregressive neural vocoder that is based on a\n",
      "generative adversarial network for a high-quality and fast waveform generator.\n",
      "In practice, the CycleVAEPWG system can be straightforwardly developed with the\n",
      "VCC 2020 dataset using a unified model for both Task 1 (intralingual) and Task\n",
      "2 (cross-lingual), where our open-source implementation is available at\n",
      "https://github.com/bigpon/vcc20_baseline_cyclevae. The results of VCC 2020 have\n",
      "demonstrated that the CycleVAEPWG baseline achieves the following: 1) a mean\n",
      "opinion score (MOS) of 2.87 in naturalness and a speaker similarity percentage\n",
      "(Sim) of 75.37% for Task 1, and 2) a MOS of 2.56 and a Sim of 56.46% for Task\n",
      "2, showing an approximately or nearly average score for naturalness and an\n",
      "above average score for speaker similarity. \n",
      "\n",
      "\n",
      "This paper introduces FastVC, an end-to-end model for fast Voice Conversion\n",
      "(VC). The proposed model can convert speech of arbitrary length from multiple\n",
      "source speakers to multiple target speakers. FastVC is based on a conditional\n",
      "AutoEncoder (AE) trained on non-parallel data and requires no annotations at\n",
      "all. This model's latent representation is shown to be speaker-independent and\n",
      "similar to phonemes, which is a desirable feature for VC systems. While the\n",
      "current VC systems primarily focus on achieving the highest overall speech\n",
      "quality, this paper tries to balance the development concerning resources\n",
      "needed to run the systems. Despite the simple structure of the proposed model,\n",
      "it outperforms the VC Challenge 2020 baselines on the cross-lingual task in\n",
      "terms of naturalness. \n",
      "\n",
      "\n",
      "In this paper, we propose a non-parallel any-to-many voice conversion (VC)\n",
      "method termed VoiceGrad. Inspired by WaveGrad, a recently introduced novel\n",
      "waveform generation method, VoiceGrad is based upon the concepts of score\n",
      "matching and Langevin dynamics. It uses weighted denoising score matching to\n",
      "train a score approximator, a fully convolutional network with a U-Net\n",
      "structure designed to predict the gradient of the log density of the speech\n",
      "feature sequences of multiple speakers, and performs VC by using annealed\n",
      "Langevin dynamics to iteratively update an input feature sequence towards the\n",
      "nearest stationary point of the target distribution based on the trained score\n",
      "approximator network. Thanks to the nature of this concept, VoiceGrad enables\n",
      "any-to-many VC, a VC scenario in which the speaker of input speech can be\n",
      "arbitrary, and allows for non-parallel training, which requires no parallel\n",
      "utterances or transcriptions. \n",
      "\n",
      "\n",
      "This paper describes the Academia Sinica systems for the two tasks of Voice\n",
      "Conversion Challenge 2020, namely voice conversion within the same language\n",
      "(Task 1) and cross-lingual voice conversion (Task 2). For both tasks, we\n",
      "followed the cascaded ASR+TTS structure, using phonetic tokens as the TTS input\n",
      "instead of the text or characters. For Task 1, we used the international\n",
      "phonetic alphabet (IPA) as the input of the TTS model. For Task 2, we used\n",
      "unsupervised phonetic symbols extracted by the vector-quantized variational\n",
      "autoencoder (VQVAE). In the evaluation, the listening test showed that our\n",
      "systems performed well in the VCC2020 challenge. \n",
      "\n",
      "\n",
      "This paper presents the sequence-to-sequence (seq2seq) baseline system for\n",
      "the voice conversion challenge (VCC) 2020. We consider a naive approach for\n",
      "voice conversion (VC), which is to first transcribe the input speech with an\n",
      "automatic speech recognition (ASR) model, followed using the transcriptions to\n",
      "generate the voice of the target with a text-to-speech (TTS) model. We revisit\n",
      "this method under a sequence-to-sequence (seq2seq) framework by utilizing\n",
      "ESPnet, an open-source end-to-end speech processing toolkit, and the many\n",
      "well-configured pretrained models provided by the community. Official\n",
      "evaluation results show that our system comes out top among the participating\n",
      "systems in terms of conversion similarity, demonstrating the promising ability\n",
      "of seq2seq models to convert speaker identity. The implementation is made\n",
      "open-source at: https://github.com/espnet/espnet/tree/master/egs/vcc20. \n",
      "\n",
      "\n",
      "Cross-lingual voice conversion (VC) is a task that aims to synthesize target\n",
      "voices with the same content while source and target speakers speak in\n",
      "different languages. Its challenge lies in the fact that the source and target\n",
      "data are naturally non-parallel, and it is even difficult to bridge the gaps\n",
      "between languages with no transcriptions provided. In this paper, we focus on\n",
      "knowledge transfer from monolin-gual ASR to cross-lingual VC, in order to\n",
      "address the con-tent mismatch problem. To achieve this, we first train a\n",
      "monolingual acoustic model for the source language, use it to extract phonetic\n",
      "features for all the speech in the VC dataset, and then train a Seq2Seq\n",
      "conversion model to pre-dict the mel-spectrograms. We successfully address\n",
      "cross-lingual VC without any transcription or language-specific knowledge for\n",
      "foreign speech. We experiment this on Voice Conversion Challenge 2020 datasets\n",
      "and show that our speaker-dependent conversion model outperforms the zero-shot\n",
      "baseline, achieving MOS of 3.83 and 3.54 in speech quality and speaker\n",
      "similarity for cross-lingual conversion. When compared to Cascade ASR-TTS\n",
      "method, our proposed one significantly reduces the MOS drop be-tween intra- and\n",
      "cross-lingual conversion. \n",
      "\n",
      "\n",
      "This paper presents a novel framework to build a voice conversion (VC) system\n",
      "by learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC\n",
      "transfer learning. We first develop a multi-speaker speech synthesis system\n",
      "with sequence-to-sequence encoder-decoder architecture, where the encoder\n",
      "extracts robust linguistic representations of text, and the decoder,\n",
      "conditioned on target speaker embedding, takes the context vectors and the\n",
      "attention recurrent network cell output to generate target acoustic features.\n",
      "We take advantage of the fact that TTS system maps input text to speaker\n",
      "independent context vectors, and reuse such a mapping to supervise the training\n",
      "of latent representations of an encoder-decoder voice conversion system. In the\n",
      "voice conversion system, the encoder takes speech instead of text as input,\n",
      "while the decoder is functionally similar to TTS decoder. As we condition the\n",
      "decoder on speaker embedding, the system can be trained on non-parallel data\n",
      "for any-to-any voice conversion. During voice conversion training, we present\n",
      "both text and speech to speech synthesis and voice conversion networks\n",
      "respectively. At run-time, the voice conversion network uses its own\n",
      "encoder-decoder architecture. Experiments show that the proposed approach\n",
      "outperforms two competitive voice conversion baselines consistently, namely\n",
      "phonetic posteriorgram and variational autoencoder methods, in terms of speech\n",
      "quality, naturalness, and speaker similarity. \n",
      "\n",
      "\n",
      "Modern text-to-speech (TTS) and voice conversion (VC) systems produce natural\n",
      "sounding speech that questions the security of automatic speaker verification\n",
      "(ASV). This makes detection of such synthetic speech very important to\n",
      "safeguard ASV systems from unauthorized access. Most of the existing spoofing\n",
      "countermeasures perform well when the nature of the attacks is made known to\n",
      "the system during training. However, their performance degrades in face of\n",
      "unseen nature of attacks. In comparison to the synthetic speech created by a\n",
      "wide range of TTS and VC methods, genuine speech has a more consistent\n",
      "distribution. We believe that the difference between the distribution of\n",
      "synthetic and genuine speech is an important discriminative feature between the\n",
      "two classes. In this regard, we propose a novel method referred to as feature\n",
      "genuinization that learns a transformer with convolutional neural network (CNN)\n",
      "using the characteristics of only genuine speech. We then use this\n",
      "genuinization transformer with a light CNN classifier. The ASVspoof 2019\n",
      "logical access corpus is used to evaluate the proposed method. The studies show\n",
      "that the proposed feature genuinization based LCNN system outperforms other\n",
      "state-of-the-art spoofing countermeasures, depicting its effectiveness for\n",
      "detection of synthetic speech attacks. \n",
      "\n",
      "\n",
      "The technique of transforming voices in order to hide the real identity of a\n",
      "speaker is called voice disguise, among which automatic voice disguise (AVD) by\n",
      "modifying the spectral and temporal characteristics of voices with\n",
      "miscellaneous algorithms are easily conducted with softwares accessible to the\n",
      "public. AVD has posed great threat to both human listening and automatic\n",
      "speaker verification (ASV). In this paper, we have found that ASV is not only a\n",
      "victim of AVD but could be a tool to beat some simple types of AVD. Firstly,\n",
      "three types of AVD, pitch scaling, vocal tract length normalization (VTLN) and\n",
      "voice conversion (VC), are introduced as representative methods.\n",
      "State-of-the-art ASV methods are subsequently utilized to objectively evaluate\n",
      "the impact of AVD on ASV by equal error rates (EER). Moreover, an approach to\n",
      "restore disguised voice to its original version is proposed by minimizing a\n",
      "function of ASV scores w.r.t. restoration parameters. Experiments are then\n",
      "conducted on disguised voices from Voxceleb, a dataset recorded in real-world\n",
      "noisy scenario. The results have shown that, for the voice disguise by pitch\n",
      "scaling, the proposed approach obtains an EER around 7% comparing to the 30%\n",
      "EER of a recently proposed baseline using the ratio of fundamental frequencies.\n",
      "The proposed approach generalizes well to restore the disguise with nonlinear\n",
      "frequency warping in VTLN by reducing its EER from 34.3% to 18.5%. However, it\n",
      "is difficult to restore the source speakers in VC by our approach, where more\n",
      "complex forms of restoration functions or other paralinguistic cues might be\n",
      "necessary to restore the nonlinear transform in VC. Finally, contrastive\n",
      "visualization on ASV features with and without restoration illustrate the role\n",
      "of the proposed approach in an intuitive way. \n",
      "\n",
      "\n",
      "The Voice Conversion Challenge 2020 is the third edition under its flagship\n",
      "that promotes intra-lingual semiparallel and cross-lingual voice conversion\n",
      "(VC). While the primary evaluation of the challenge submissions was done\n",
      "through crowd-sourced listening tests, we also performed an objective\n",
      "assessment of the submitted systems. The aim of the objective assessment is to\n",
      "provide complementary performance analysis that may be more beneficial than the\n",
      "time-consuming listening tests. In this study, we examined five types of\n",
      "objective assessments using automatic speaker verification (ASV), neural\n",
      "speaker embeddings, spoofing countermeasures, predicted mean opinion scores\n",
      "(MOS), and automatic speech recognition (ASR). Each of these objective measures\n",
      "assesses the VC output along different aspects. We observed that the\n",
      "correlations of these objective assessments with the subjective results were\n",
      "high for ASV, neural speaker embedding, and ASR, which makes them more\n",
      "influential for predicting subjective test results. In addition, we performed\n",
      "spoofing assessments on the submitted systems and identified some of the VC\n",
      "methods showing a potentially high security risk. \n",
      "\n",
      "\n",
      "This paper proposes an any-to-many location-relative, sequence-to-sequence\n",
      "(seq2seq), non-parallel voice conversion approach, which utilizes text\n",
      "supervision during training. In this approach, we combine a bottle-neck feature\n",
      "extractor (BNE) with a seq2seq synthesis module. During the training stage, an\n",
      "encoder-decoder-based hybrid connectionist-temporal-classification-attention\n",
      "(CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck\n",
      "layer. A BNE is obtained from the phoneme recognizer and is utilized to extract\n",
      "speaker-independent, dense and rich spoken content representations from\n",
      "spectral features. Then a multi-speaker location-relative attention based\n",
      "seq2seq synthesis model is trained to reconstruct spectral features from the\n",
      "bottle-neck features, conditioning on speaker representations for speaker\n",
      "identity control in the generated speech. To mitigate the difficulties of using\n",
      "seq2seq models to align long sequences, we down-sample the input spectral\n",
      "feature along the temporal dimension and equip the synthesis model with a\n",
      "discretized mixture of logistic (MoL) attention mechanism. Since the phoneme\n",
      "recognizer is trained with large speech recognition data corpus, the proposed\n",
      "approach can conduct any-to-many voice conversion. Objective and subjective\n",
      "evaluations show that the proposed any-to-many approach has superior voice\n",
      "conversion performance in terms of both naturalness and speaker similarity.\n",
      "Ablation studies are conducted to confirm the effectiveness of feature\n",
      "selection and model design strategies in the proposed approach. The proposed VC\n",
      "approach can readily be extended to support any-to-any VC (also known as\n",
      "one/few-shot VC), and achieve high performance according to objective and\n",
      "subjective evaluations. \n",
      "\n",
      "\n",
      "With the development of automatic speech recognition (ASR) and text-to-speech\n",
      "synthesis (TTS) technique, it's intuitive to construct a voice conversion\n",
      "system by cascading an ASR and TTS system. In this paper, we present a ASR-TTS\n",
      "method for voice conversion, which used iFLYTEK ASR engine to transcribe the\n",
      "source speech into text and a Transformer TTS model with WaveNet vocoder to\n",
      "synthesize the converted speech from the decoded text. For the TTS model, we\n",
      "proposed to use a prosody code to describe the prosody information other than\n",
      "text and speaker information contained in speech. A prosody encoder is used to\n",
      "extract the prosody code. During conversion, the source prosody is transferred\n",
      "to converted speech by conditioning the Transformer TTS model with its code.\n",
      "Experiments were conducted to demonstrate the effectiveness of our proposed\n",
      "method. Our system also obtained the best naturalness and similarity in the\n",
      "mono-lingual task of Voice Conversion Challenge 2020. \n",
      "\n",
      "\n",
      "The voice conversion challenge is a bi-annual scientific event held to\n",
      "compare and understand different voice conversion (VC) systems built on a\n",
      "common dataset. In 2020, we organized the third edition of the challenge and\n",
      "constructed and distributed a new database for two tasks, intra-lingual\n",
      "semi-parallel and cross-lingual VC. After a two-month challenge period, we\n",
      "received 33 submissions, including 3 baselines built on the database. From the\n",
      "results of crowd-sourced listening tests, we observed that VC methods have\n",
      "progressed rapidly thanks to advanced deep learning methods. In particular,\n",
      "speaker similarity scores of several systems turned out to be as high as target\n",
      "speakers in the intra-lingual semi-parallel VC task. However, we confirmed that\n",
      "none of them have achieved human-level naturalness yet for the same task. The\n",
      "cross-lingual conversion task is, as expected, a more difficult task, and the\n",
      "overall naturalness and similarity scores were lower than those for the\n",
      "intra-lingual conversion task. However, we observed encouraging results, and\n",
      "the MOS scores of the best systems were higher than 4.0. We also show a few\n",
      "additional analysis results to aid in understanding cross-lingual VC better. \n",
      "\n",
      "\n",
      "We previously proposed a method that allows for nonparallel voice conversion\n",
      "(VC) by using a variant of generative adversarial networks (GANs) called\n",
      "StarGAN. The main features of our method, called StarGAN-VC, are as follows:\n",
      "First, it requires no parallel utterances, transcriptions, or time alignment\n",
      "procedures for speech generator training. Second, it can simultaneously learn\n",
      "mappings across multiple domains using a single generator network and thus\n",
      "fully exploit available training data collected from multiple domains to\n",
      "capture latent features that are common to all the domains. Third, it can\n",
      "generate converted speech signals quickly enough to allow real-time\n",
      "implementations and requires only several minutes of training examples to\n",
      "generate reasonably realistic-sounding speech. In this paper, we describe three\n",
      "formulations of StarGAN, including a newly introduced novel StarGAN variant\n",
      "called \"Augmented classifier StarGAN (A-StarGAN)\", and compare them in a\n",
      "nonparallel VC task. We also compare them with several baseline methods. \n",
      "\n",
      "\n",
      "Recently, Generative Adversarial Networks (GAN)-based methods have shown\n",
      "remarkable performance for the Voice Conversion and WHiSPer-to-normal SPeeCH\n",
      "(WHSP2SPCH) conversion. One of the key challenges in WHSP2SPCH conversion is\n",
      "the prediction of fundamental frequency (F0). Recently, authors have proposed\n",
      "state-of-the-art method Cycle-Consistent Generative Adversarial Networks\n",
      "(CycleGAN) for WHSP2SPCH conversion. The CycleGAN-based method uses two\n",
      "different models, one for Mel Cepstral Coefficients (MCC) mapping, and another\n",
      "for F0 prediction, where F0 is highly dependent on the pre-trained model of MCC\n",
      "mapping. This leads to additional non-linear noise in predicted F0. To suppress\n",
      "this noise, we propose Cycle-in-Cycle GAN (i.e., CinC-GAN). It is specially\n",
      "designed to increase the effectiveness in F0 prediction without losing the\n",
      "accuracy of MCC mapping. We evaluated the proposed method on a non-parallel\n",
      "setting and analyzed on speaker-specific, and gender-specific tasks. The\n",
      "objective and subjective tests show that CinC-GAN significantly outperforms the\n",
      "CycleGAN. In addition, we analyze the CycleGAN and CinC-GAN for unseen speakers\n",
      "and the results show the clear superiority of CinC-GAN. \n",
      "\n",
      "\n",
      "Unsupervised representation learning of speech has been of keen interest in\n",
      "recent years, which is for example evident in the wide interest of the\n",
      "ZeroSpeech challenges. This work presents a new method for learning frame level\n",
      "representations based on WaveNet auto-encoders. Of particular interest in the\n",
      "ZeroSpeech Challenge 2019 were models with discrete latent variable such as the\n",
      "Vector Quantized Variational Auto-Encoder (VQVAE). However these models\n",
      "generate speech with relatively poor quality. In this work we aim to address\n",
      "this with two approaches: first WaveNet is used as the decoder and to generate\n",
      "waveform data directly from the latent representation; second, the low\n",
      "complexity of latent representations is improved with two alternative\n",
      "disentanglement learning methods, namely instance normalization and sliced\n",
      "vector quantization. The method was developed and tested in the context of the\n",
      "recent ZeroSpeech challenge 2020. The system output submitted to the challenge\n",
      "obtained the top position for naturalness (Mean Opinion Score 4.06), top\n",
      "position for intelligibility (Character Error Rate 0.15), and third position\n",
      "for the quality of the representation (ABX test score 12.5). These and further\n",
      "analysis in this paper illustrates that quality of the converted speech and the\n",
      "acoustic units representation can be well balanced. \n",
      "\n",
      "\n",
      "Cross-lingual voice conversion aims to change source speaker's voice to sound\n",
      "like that of target speaker, when source and target speakers speak different\n",
      "languages. It relies on non-parallel training data from two different\n",
      "languages, hence, is more challenging than mono-lingual voice conversion.\n",
      "Previous studies on cross-lingual voice conversion mainly focus on spectral\n",
      "conversion with a linear transformation for F0 transfer. However, as an\n",
      "important prosodic factor, F0 is inherently hierarchical, thus it is\n",
      "insufficient to just use a linear method for conversion. We propose the use of\n",
      "continuous wavelet transform (CWT) decomposition for F0 modeling. CWT provides\n",
      "a way to decompose a signal into different temporal scales that explain prosody\n",
      "in different time resolutions. We also propose to train two CycleGAN pipelines\n",
      "for spectrum and prosody mapping respectively. In this way, we eliminate the\n",
      "need for parallel data of any two languages and any alignment techniques.\n",
      "Experimental results show that our proposed Spectrum-Prosody-CycleGAN framework\n",
      "outperforms the Spectrum-CycleGAN baseline in subjective evaluation. To our\n",
      "best knowledge, this is the first study of prosody in cross-lingual voice\n",
      "conversion. \n",
      "\n",
      "\n",
      "Singing voice conversion aims to convert singer's voice from source to target\n",
      "without changing singing content. Parallel training data is typically required\n",
      "for the training of singing voice conversion system, that is however not\n",
      "practical in real-life applications. Recent encoder-decoder structures, such as\n",
      "variational autoencoding Wasserstein generative adversarial network (VAW-GAN),\n",
      "provide an effective way to learn a mapping through non-parallel training data.\n",
      "In this paper, we propose a singing voice conversion framework that is based on\n",
      "VAW-GAN. We train an encoder to disentangle singer identity and singing prosody\n",
      "(F0 contour) from phonetic content. By conditioning on singer identity and F0,\n",
      "the decoder generates output spectral features with unseen target singer\n",
      "identity, and improves the F0 rendering. Experimental results show that the\n",
      "proposed framework achieves better performance than the baseline frameworks. \n",
      "\n",
      "\n",
      "While deep learning has made impressive progress in speech synthesis and\n",
      "voice conversion, the assessment of the synthesized speech is still carried out\n",
      "by human participants. Several recent papers have proposed deep-learning-based\n",
      "assessment models and shown the potential to automate the speech quality\n",
      "assessment. To improve the previously proposed assessment model, MOSNet, we\n",
      "propose three models using cluster-based modeling methods: using a global\n",
      "quality token (GQT) layer, using an Encoding Layer, and using both of them. We\n",
      "perform experiments using the evaluation results of the Voice Conversion\n",
      "Challenge 2018 to predict the mean opinion score of synthesized speech and\n",
      "similarity score between synthesized speech and reference speech. The results\n",
      "show that the GQT layer helps to predict human assessment better by\n",
      "automatically learning the useful quality tokens for the task and that the\n",
      "Encoding Layer helps to utilize frame-level scores more precisely. \n",
      "\n",
      "\n",
      "Speaker identity is one of the important characteristics of human speech. In\n",
      "voice conversion, we change the speaker identity from one to another, while\n",
      "keeping the linguistic content unchanged. Voice conversion involves multiple\n",
      "speech processing techniques, such as speech analysis, spectral conversion,\n",
      "prosody conversion, speaker characterization, and vocoding. With the recent\n",
      "advances in theory and practice, we are now able to produce human-like voice\n",
      "quality with high speaker similarity. In this paper, we provide a comprehensive\n",
      "overview of the state-of-the-art of voice conversion techniques and their\n",
      "performance evaluation methods from the statistical approaches to deep\n",
      "learning, and discuss their promise and limitations. We will also report the\n",
      "recent Voice Conversion Challenges (VCC), the performance of the current state\n",
      "of technology, and provide a summary of the available resources for voice\n",
      "conversion research. \n",
      "\n",
      "\n",
      "Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive\n",
      "owing to their ability to convert prosody. Nonetheless, without sufficient\n",
      "data, seq2seq VC models can suffer from unstable training and mispronunciation\n",
      "problems in the converted speech, thus far from practical. To tackle these\n",
      "shortcomings, we propose to transfer knowledge from other speech processing\n",
      "tasks where large-scale corpora are easily available, typically text-to-speech\n",
      "(TTS) and automatic speech recognition (ASR). We argue that VC models\n",
      "initialized with such pretrained ASR or TTS model parameters can generate\n",
      "effective hidden representations for high-fidelity, highly intelligible\n",
      "converted speech. We apply such techniques to recurrent neural network\n",
      "(RNN)-based and Transformer based models, and through systematical experiments,\n",
      "we demonstrate the effectiveness of the pretraining scheme and the superiority\n",
      "of Transformer based models over RNN-based models in terms of intelligibility,\n",
      "naturalness, and similarity. \n",
      "\n",
      "\n",
      "Singing voice conversion is converting the timbre in the source singing to\n",
      "the target speaker's voice while keeping singing content the same. However,\n",
      "singing data for target speaker is much more difficult to collect compared with\n",
      "normal speech data.In this paper, we introduce a singing voice conversion\n",
      "algorithm that is capable of generating high quality target speaker's singing\n",
      "using only his/her normal speech data. First, we manage to integrate the\n",
      "training and conversion process of speech and singing into one framework by\n",
      "unifying the features used in standard speech synthesis system and singing\n",
      "synthesis system. In this way, normal speech data can also contribute to\n",
      "singing voice conversion training, making the singing voice conversion system\n",
      "more robust especially when the singing database is small.Moreover, in order to\n",
      "achieve one-shot singing voice conversion, a speaker embedding module is\n",
      "developed using both speech and singing data, which provides target speaker\n",
      "identify information during conversion. Experiments indicate proposed sing\n",
      "conversion system can convert source singing to target speaker's high-quality\n",
      "singing with only 20 seconds of target speaker's enrollment speech data. \n",
      "\n",
      "\n",
      "We present a wav-to-wav generative model for the task of singing voice\n",
      "conversion from any identity. Our method utilizes both an acoustic model,\n",
      "trained for the task of automatic speech recognition, together with melody\n",
      "extracted features to drive a waveform-based generator. The proposed generative\n",
      "architecture is invariant to the speaker's identity and can be trained to\n",
      "generate target singers from unlabeled training data, using either speech or\n",
      "singing sources. The model is optimized in an end-to-end fashion without any\n",
      "manual supervision, such as lyrics, musical notes or parallel samples. The\n",
      "proposed approach is fully-convolutional and can generate audio in real-time.\n",
      "Experiments show that our method significantly outperforms the baseline methods\n",
      "while generating convincingly better audio samples than alternative attempts. \n",
      "\n",
      "\n",
      "This paper presents an adversarial learning method for recognition-synthesis\n",
      "based non-parallel voice conversion. A recognizer is used to transform acoustic\n",
      "features into linguistic representations while a synthesizer recovers output\n",
      "features from the recognizer outputs together with the speaker identity. By\n",
      "separating the speaker characteristics from the linguistic representations,\n",
      "voice conversion can be achieved by replacing the speaker identity with the\n",
      "target one. In our proposed method, a speaker adversarial loss is adopted in\n",
      "order to obtain speaker-independent linguistic representations using the\n",
      "recognizer. Furthermore, discriminators are introduced and a generative\n",
      "adversarial network (GAN) loss is used to prevent the predicted features from\n",
      "being over-smoothed. For training model parameters, a strategy of pre-training\n",
      "on a multi-speaker dataset and then fine-tuning on the source-target speaker\n",
      "pair is designed. Our method achieved higher similarity than the baseline model\n",
      "that obtained the best performance in Voice Conversion Challenge 2018. \n",
      "\n",
      "\n",
      "Several studies have proposed deep-learning-based models to predict the mean\n",
      "opinion score (MOS) of synthesized speech, showing the possibility of replacing\n",
      "human raters. However, inter- and intra-rater variability in MOSs makes it hard\n",
      "to ensure the high performance of the models. In this paper, we propose a\n",
      "multi-task learning (MTL) method to improve the performance of a MOS prediction\n",
      "model using the following two auxiliary tasks: spoofing detection (SD) and\n",
      "spoofing type classification (STC). Besides, we use the focal loss to maximize\n",
      "the synergy between SD and STC for MOS prediction. Experiments using the MOS\n",
      "evaluation results of the Voice Conversion Challenge 2018 show that proposed\n",
      "MTL with two auxiliary tasks improves MOS prediction. Our proposed model\n",
      "achieves up to 11.6% relative improvement in performance over the baseline\n",
      "model. \n",
      "\n",
      "\n",
      "Recent works of utilizing phonetic posteriograms (PPGs) for non-parallel\n",
      "voice conversion have significantly increased the usability of voice conversion\n",
      "since the source and target DBs are no longer required for matching contents.\n",
      "In this approach, the PPGs are used as the linguistic bridge between source and\n",
      "target speaker features. However, this PPG-based non-parallel voice conversion\n",
      "has some limitation that it needs two cascading networks at conversion time,\n",
      "making it less suitable for real-time applications and vulnerable to source\n",
      "speaker intelligibility at conversion stage. To address this limitation, we\n",
      "propose a new non-parallel voice conversion technique that employs a single\n",
      "neural network for direct source-to-target voice parameter mapping. With this\n",
      "single network structure, the proposed approach can reduce both conversion time\n",
      "and number of network parameters, which can be especially important factors in\n",
      "embedded or real-time environments. Additionally, it improves the quality of\n",
      "voice conversion by skipping the phone recognizer at conversion stage. It can\n",
      "effectively prevent possible loss of phonetic information the PPG-based\n",
      "indirect method suffers. Experiments show that our approach reduces number of\n",
      "network parameters and conversion time by 41.9% and 44.5%, respectively, with\n",
      "improved voice similarity over the original PPG-based method. \n",
      "\n",
      "\n",
      "Voice conversion (VC) is a task that transforms the source speaker's timbre,\n",
      "accent, and tones in audio into another one's while preserving the linguistic\n",
      "content. It is still a challenging work, especially in a one-shot setting.\n",
      "Auto-encoder-based VC methods disentangle the speaker and the content in input\n",
      "speech without given the speaker's identity, so these methods can further\n",
      "generalize to unseen speakers. The disentangle capability is achieved by vector\n",
      "quantization (VQ), adversarial training, or instance normalization (IN).\n",
      "However, the imperfect disentanglement may harm the quality of output speech.\n",
      "In this work, to further improve audio quality, we use the U-Net architecture\n",
      "within an auto-encoder-based VC system. We find that to leverage the U-Net\n",
      "architecture, a strong information bottleneck is necessary. The VQ-based\n",
      "method, which quantizes the latent vectors, can serve the purpose. The\n",
      "objective and the subjective evaluations show that the proposed method performs\n",
      "well in both audio naturalness and speaker similarity. \n",
      "\n",
      "\n",
      "High-performance anti-spoofing models for automatic speaker verification\n",
      "(ASV), have been widely used to protect ASV by identifying and filtering\n",
      "spoofing audio that is deliberately generated by text-to-speech, voice\n",
      "conversion, audio replay, etc. However, it has been shown that high-performance\n",
      "anti-spoofing models are vulnerable to adversarial attacks. Adversarial\n",
      "attacks, that are indistinguishable from original data but result in the\n",
      "incorrect predictions, are dangerous for anti-spoofing models and not in\n",
      "dispute we should detect them at any cost. To explore this issue, we proposed\n",
      "to employ Mockingjay, a self-supervised learning based model, to protect\n",
      "anti-spoofing models against adversarial attacks in the black-box scenario.\n",
      "Self-supervised learning models are effective in improving downstream task\n",
      "performance like phone classification or ASR. However, their effect in defense\n",
      "for adversarial attacks has not been explored yet. In this work, we explore the\n",
      "robustness of self-supervised learned high-level representations by using them\n",
      "in the defense against adversarial attacks. A layerwise noise to signal ratio\n",
      "(LNSR) is proposed to quantize and measure the effectiveness of deep models in\n",
      "countering adversarial noise. Experimental results on the ASVspoof 2019 dataset\n",
      "demonstrate that high-level representations extracted by Mockingjay can prevent\n",
      "the transferability of adversarial examples, and successfully counter black-box\n",
      "attacks. \n",
      "\n",
      "\n",
      "In this paper, we explore vector quantization for acoustic unit discovery.\n",
      "Leveraging unlabelled data, we aim to learn discrete representations of speech\n",
      "that separate phonetic content from speaker-specific details. We propose two\n",
      "neural models to tackle this challenge - both use vector quantization to map\n",
      "continuous features to a finite set of codes. The first model is a type of\n",
      "vector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech\n",
      "into a sequence of discrete units before reconstructing the audio waveform. Our\n",
      "second model combines vector quantization with contrastive predictive coding\n",
      "(VQ-CPC). The idea is to learn a representation of speech by predicting future\n",
      "acoustic units. We evaluate the models on English and Indonesian data for the\n",
      "ZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models\n",
      "outperform all submissions to the 2019 and 2020 challenges, with a relative\n",
      "improvement of more than 30%. The models also perform competitively on a\n",
      "downstream voice conversion task. Of the two, VQ-CPC performs slightly better\n",
      "in general and is simpler and faster to train. Finally, probing experiments\n",
      "show that vector quantization is an effective bottleneck, forcing the models to\n",
      "discard speaker information. \n",
      "\n",
      "\n",
      "It is important to transcribe and archive speech data of endangered languages\n",
      "for preserving heritages of verbal culture and automatic speech recognition\n",
      "(ASR) is a powerful tool to facilitate this process. However, since endangered\n",
      "languages do not generally have large corpora with many speakers, the\n",
      "performance of ASR models trained on them are considerably poor in general.\n",
      "Nevertheless, we are often left with a lot of recordings of spontaneous speech\n",
      "data that have to be transcribed. In this work, for mitigating this speaker\n",
      "sparsity problem, we propose to convert the whole training speech data and make\n",
      "it sound like the test speaker in order to develop a highly accurate ASR system\n",
      "for this speaker. For this purpose, we utilize a CycleGAN-based non-parallel\n",
      "voice conversion technology to forge a labeled training data that is close to\n",
      "the test speaker's speech. We evaluated this speaker adaptation approach on two\n",
      "low-resource corpora, namely, Ainu and Mboshi. We obtained 35-60% relative\n",
      "improvement in phone error rate on the Ainu corpus, and 40% relative\n",
      "improvement was attained on the Mboshi corpus. This approach outperformed two\n",
      "conventional methods namely unsupervised adaptation and multilingual training\n",
      "with these two corpora. \n",
      "\n",
      "\n",
      "Voice conversion (VC) techniques aim to modify speaker identity of an\n",
      "utterance while preserving the underlying linguistic information. Most VC\n",
      "approaches ignore modeling of the speaking style (e.g. emotion and emphasis),\n",
      "which may contain the factors intentionally added by the speaker and should be\n",
      "retained during conversion. This study proposes a sequence-to-sequence based\n",
      "non-parallel VC approach, which has the capability of transferring the speaking\n",
      "style from the source speech to the converted speech by explicitly modeling.\n",
      "Objective evaluation and subjective listening tests show superiority of the\n",
      "proposed VC approach in terms of speech naturalness and speaker similarity of\n",
      "the converted speech. Experiments are also conducted to show the source-style\n",
      "transferability of the proposed approach. \n",
      "\n",
      "\n",
      "Substantial improvements have been achieved in recent years in voice\n",
      "conversion, which converts the speaker characteristics of an utterance into\n",
      "those of another speaker without changing the linguistic content of the\n",
      "utterance. Nonetheless, the improved conversion technologies also led to\n",
      "concerns about privacy and authentication. It thus becomes highly desired to be\n",
      "able to prevent one's voice from being improperly utilized with such voice\n",
      "conversion technologies. This is why we report in this paper the first known\n",
      "attempt to perform adversarial attack on voice conversion. We introduce human\n",
      "imperceptible noise into the utterances of a speaker whose voice is to be\n",
      "defended. Given these adversarial examples, voice conversion models cannot\n",
      "convert other utterances so as to sound like being produced by the defended\n",
      "speaker. Preliminary experiments were conducted on two currently\n",
      "state-of-the-art zero-shot voice conversion models. Objective and subjective\n",
      "evaluation results in both white-box and black-box scenarios are reported. It\n",
      "was shown that the speaker characteristics of the converted utterances were\n",
      "made obviously different from those of the defended speaker, while the\n",
      "adversarial examples of the defended speaker are not distinguishable from the\n",
      "authentic utterances. \n",
      "\n",
      "\n",
      "Recently, the effectiveness of text-to-speech (TTS) systems combined with\n",
      "neural vocoders to generate high-fidelity speech has been shown. However,\n",
      "collecting the required training data and building these advanced systems from\n",
      "scratch are time and resource consuming. An economical approach is to develop a\n",
      "neural vocoder to enhance the speech generated by existing or low-cost TTS\n",
      "systems. Nonetheless, this approach usually suffers from two issues: 1)\n",
      "temporal mismatches between TTS and natural waveforms and 2) acoustic\n",
      "mismatches between training and testing data. To address these issues, we adopt\n",
      "a cyclic voice conversion (VC) model to generate temporally matched pseudo-VC\n",
      "data for training and acoustically matched enhanced data for testing the neural\n",
      "vocoders. Because of the generality, this framework can be applied to arbitrary\n",
      "TTS systems and neural vocoders. In this paper, we apply the proposed method\n",
      "with a state-of-the-art WaveNet vocoder for two different basic TTS systems,\n",
      "and both objective and subjective experimental results confirm the\n",
      "effectiveness of the proposed framework. \n",
      "\n",
      "\n",
      "In this paper we demonstrate methods for reliable and efficient training of\n",
      "discrete representation using Vector-Quantized Variational Auto-Encoder models\n",
      "(VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial\n",
      "representations of speech, applicable to unsupervised voice conversion and\n",
      "reaching state-of-the-art performance on unit discovery tasks. For unsupervised\n",
      "representation learning, they became viable alternatives to continuous latent\n",
      "variable models such as the Variational Auto-Encoder (VAE). However, training\n",
      "deep discrete variable models is challenging, due to the inherent\n",
      "non-differentiability of the discretization operation. In this paper we focus\n",
      "on VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par\n",
      "with its continuous counterparts. It quantizes encoder outputs with on-line\n",
      "$k$-means clustering. We show that the codebook learning can suffer from poor\n",
      "initialization and non-stationarity of clustered encoder outputs. We\n",
      "demonstrate that these can be successfully overcome by increasing the learning\n",
      "rate for the codebook and periodic date-dependent codeword re-initialization.\n",
      "As a result, we achieve more robust training across different tasks, and\n",
      "significantly increase the usage of latent codewords even for large codebooks.\n",
      "This has practical benefit, for instance, in unsupervised representation\n",
      "learning, where large codebooks may lead to disentanglement of latent\n",
      "representations. \n",
      "\n",
      "\n",
      "This paper proposes a voice conversion (VC) method based on a\n",
      "sequence-to-sequence (S2S) learning framework, which enables simultaneous\n",
      "conversion of the voice characteristics, pitch contour, and duration of input\n",
      "speech. We previously proposed an S2S-based VC method using a transformer\n",
      "network architecture called the voice transformer network (VTN). The original\n",
      "VTN was designed to learn only a mapping of speech feature sequences from one\n",
      "speaker to another. The main idea we propose is an extension of the original\n",
      "VTN that can simultaneously learn mappings among multiple speakers. This\n",
      "extension called the many-to-many VTN makes it able to fully use available\n",
      "training data collected from multiple speakers by capturing common latent\n",
      "features that can be shared across different speakers. It also allows us to\n",
      "introduce a training loss called the identity mapping loss to ensure that the\n",
      "input feature sequence will remain unchanged when the source and target speaker\n",
      "indices are the same. Using this particular loss for model training has been\n",
      "found to be extremely effective in improving the performance of the model at\n",
      "test time. We conducted speaker identity conversion experiments and found that\n",
      "our model obtained higher sound quality and speaker similarity than baseline\n",
      "methods. We also found that our model, with a slight modification to its\n",
      "architecture, could handle any-to-many conversion tasks reasonably well. \n",
      "\n",
      "\n",
      "We propose a neural network for zero-shot voice conversion (VC) without any\n",
      "parallel or transcribed data. Our approach uses pre-trained models for\n",
      "automatic speech recognition (ASR) and speaker embedding, obtained from a\n",
      "speaker verification task. Our model is fully convolutional and\n",
      "non-autoregressive except for a small pre-trained recurrent neural network for\n",
      "speaker encoding. ConVoice can convert speech of any length without\n",
      "compromising quality due to its convolutional architecture. Our model has\n",
      "comparable quality to similar state-of-the-art models while being extremely\n",
      "fast. \n",
      "\n",
      "\n",
      "Emotional voice conversion aims to convert the emotion of speech from one\n",
      "state to another while preserving the linguistic content and speaker identity.\n",
      "The prior studies on emotional voice conversion are mostly carried out under\n",
      "the assumption that emotion is speaker-dependent. We consider that there is a\n",
      "common code between speakers for emotional expression in a spoken language,\n",
      "therefore, a speaker-independent mapping between emotional states is possible.\n",
      "In this paper, we propose a speaker-independent emotional voice conversion\n",
      "framework, that can convert anyone's emotion without the need for parallel\n",
      "data. We propose a VAW-GAN based encoder-decoder structure to learn the\n",
      "spectrum and prosody mapping. We perform prosody conversion by using continuous\n",
      "wavelet transform (CWT) to model the temporal dependencies. We also investigate\n",
      "the use of F0 as an additional input to the decoder to improve emotion\n",
      "conversion performance. Experiments show that the proposed speaker-independent\n",
      "framework achieves competitive results for both seen and unseen speakers. \n",
      "\n",
      "\n",
      "This paper proposes Scyclone, a high-quality voice conversion (VC) technique\n",
      "without parallel data training. Scyclone improves speech naturalness and\n",
      "speaker similarity of the converted speech by introducing CycleGAN-based\n",
      "spectrogram conversion with a simplified WaveRNN-based vocoder. In Scyclone, a\n",
      "linear spectrogram is used as the conversion features instead of vocoder\n",
      "parameters, which avoids quality degradation due to extraction errors in\n",
      "fundamental frequency and voiced/unvoiced parameters. The spectrogram of source\n",
      "and target speakers are modeled by modified CycleGAN networks, and the waveform\n",
      "is reconstructed using the simplified WaveRNN with a single Gaussian\n",
      "probability density function. The subjective experiments with completely\n",
      "unpaired training data show that Scyclone is significantly better than\n",
      "CycleGAN-VC2, one of the existing state-of-the-art parallel-data-free VC\n",
      "techniques. \n",
      "\n",
      "\n",
      "We propose Cotatron, a transcription-guided speech encoder for\n",
      "speaker-independent linguistic representation. Cotatron is based on the\n",
      "multispeaker TTS architecture and can be trained with conventional TTS\n",
      "datasets. We train a voice conversion system to reconstruct speech with\n",
      "Cotatron features, which is similar to the previous methods based on Phonetic\n",
      "Posteriorgram (PPG). By training and evaluating our system with 108 speakers\n",
      "from the VCTK dataset, we outperform the previous method in terms of both\n",
      "naturalness and speaker similarity. Our system can also convert speech from\n",
      "speakers that are unseen during training, and utilize ASR to automate the\n",
      "transcription with minimal reduction of the performance. Audio samples are\n",
      "available at https://mindslab-ai.github.io/cotatron, and the code with a\n",
      "pre-trained model will be made available soon. \n",
      "\n",
      "\n",
      "Speech information can be roughly decomposed into four components: language\n",
      "content, timbre, pitch, and rhythm. Obtaining disentangled representations of\n",
      "these components is useful in many speech analysis and generation applications.\n",
      "Recently, state-of-the-art voice conversion systems have led to speech\n",
      "representations that can disentangle speaker-dependent and independent\n",
      "information. However, these systems can only disentangle timbre, while\n",
      "information about pitch, rhythm and content is still mixed together. Further\n",
      "disentangling the remaining speech components is an under-determined problem in\n",
      "the absence of explicit annotations for each component, which are difficult and\n",
      "expensive to obtain. In this paper, we propose SpeechSplit, which can blindly\n",
      "decompose speech into its four components by introducing three carefully\n",
      "designed information bottlenecks. SpeechSplit is among the first algorithms\n",
      "that can separately perform style transfer on timbre, pitch and rhythm without\n",
      "text labels. Our code is publicly available at\n",
      "https://github.com/auspicious3000/SpeechSplit. \n",
      "\n",
      "\n",
      "Non-parallel many-to-many voice conversion remains an interesting but\n",
      "challenging speech processing task. Many style-transfer-inspired methods such\n",
      "as generative adversarial networks (GANs) and variational autoencoders (VAEs)\n",
      "have been proposed. Recently, AutoVC, a conditional autoencoders (CAEs) based\n",
      "method achieved state-of-the-art results by disentangling the speaker identity\n",
      "and speech content using information-constraining bottlenecks, and it achieves\n",
      "zero-shot conversion by swapping in a different speaker's identity embedding to\n",
      "synthesize a new voice. However, we found that while speaker identity is\n",
      "disentangled from speech content, a significant amount of prosodic information,\n",
      "such as source F0, leaks through the bottleneck, causing target F0 to fluctuate\n",
      "unnaturally. Furthermore, AutoVC has no control of the converted F0 and thus\n",
      "unsuitable for many applications. In the paper, we modified and improved\n",
      "autoencoder-based voice conversion to disentangle content, F0, and speaker\n",
      "identity at the same time. Therefore, we can control the F0 contour, generate\n",
      "speech with F0 consistent with the target speaker, and significantly improve\n",
      "quality and similarity. We support our improvement through quantitative and\n",
      "qualitative analysis. \n",
      "\n",
      "\n",
      "Emotional voice conversion (EVC) is one way to generate expressive synthetic\n",
      "speech. Previous approaches mainly focused on modeling one-to-one mapping,\n",
      "i.e., conversion from one emotional state to another emotional state, with\n",
      "Mel-cepstral vocoders. In this paper, we investigate building a multi-target\n",
      "EVC (MTEVC) architecture, which combines a deep bidirectional long-short term\n",
      "memory (DBLSTM)-based conversion model and a neural vocoder. Phonetic\n",
      "posteriorgrams (PPGs) containing rich linguistic information are incorporated\n",
      "into the conversion model as auxiliary input features, which boost the\n",
      "conversion performance. To leverage the advantages of the newly emerged neural\n",
      "vocoders, we investigate the conditional WaveNet and flow-based WaveNet\n",
      "(FloWaveNet) as speech generators. The vocoders take in additional speaker\n",
      "information and emotion information as auxiliary features and are trained with\n",
      "a multi-speaker and multi-emotion speech corpus. Objective metrics and\n",
      "subjective evaluation of the experimental results verify the efficacy of the\n",
      "proposed MTEVC architecture for EVC. \n",
      "\n",
      "\n",
      "Emotional Voice Conversion, or emotional VC, is a technique of converting\n",
      "speech from one emotion state into another one, keeping the basic linguistic\n",
      "information and speaker identity. Previous approaches for emotional VC need\n",
      "parallel data and use dynamic time warping (DTW) method to temporally align the\n",
      "source-target speech parameters. These approaches often define a minimum\n",
      "generation loss as the objective function, such as L1 or L2 loss, to learn\n",
      "model parameters. Recently, cycle-consistent generative adversarial networks\n",
      "(CycleGAN) have been used successfully for non-parallel VC. This paper\n",
      "investigates the efficacy of using CycleGAN for emotional VC tasks. Rather than\n",
      "attempting to learn a mapping between parallel training data using a\n",
      "frame-to-frame minimum generation loss, the CycleGAN uses two discriminators\n",
      "and one classifier to guide the learning process, where the discriminators aim\n",
      "to differentiate between the natural and converted speech and the classifier\n",
      "aims to classify the underlying emotion from the natural and converted speech.\n",
      "The training process of the CycleGAN models randomly pairs source-target speech\n",
      "parameters, without any temporal alignment operation. The objective and\n",
      "subjective evaluation results confirm the effectiveness of using CycleGAN\n",
      "models for emotional VC. The non-parallel training for a CycleGAN indicates its\n",
      "potential for non-parallel emotional VC. \n",
      "\n",
      "\n",
      "In this paper, we integrate a simple non-parallel voice conversion (VC)\n",
      "system with a WaveNet (WN) vocoder and a proposed collapsed speech suppression\n",
      "technique. The effectiveness of WN as a vocoder for generating high-fidelity\n",
      "speech waveforms on the basis of acoustic features has been confirmed in recent\n",
      "works. However, when combining the WN vocoder with a VC system, the distorted\n",
      "acoustic features, acoustic and temporal mismatches, and exposure bias usually\n",
      "lead to significant speech quality degradation, making WN generate some very\n",
      "noisy speech segments called collapsed speech. To tackle the problem, we take\n",
      "conventional-vocoder-generated speech as the reference speech to derive a\n",
      "linear predictive coding distribution constraint (LPCDC) to avoid the collapsed\n",
      "speech problem. Furthermore, to mitigate the negative effects introduced by the\n",
      "LPCDC, we propose a collapsed speech segment detector (CSSD) to ensure that the\n",
      "LPCDC is only applied to the problematic segments to limit the loss of quality\n",
      "to short periods. Objective and subjective evaluations are conducted, and the\n",
      "experimental results confirm the effectiveness of the proposed method, which\n",
      "further improves the speech quality of our previous non-parallel VC system\n",
      "submitted to Voice Conversion Challenge 2018. \n",
      "\n",
      "\n",
      "The research presents a voice conversion model using coefficient mapping and\n",
      "neural network. Most previous works on parametric speech synthesis did not\n",
      "account for losses in spectral details causing over smoothing and invariably,\n",
      "an appreciable deviation of the converted speech from the targeted speaker. An\n",
      "improved model that uses both linear predictive coding (LPC) and line spectral\n",
      "frequency (LSF) coefficients to parametrize the source speech signal was\n",
      "developed in this work to reveal the effect of over-smoothing. Non-linear\n",
      "mapping ability of neural network was employed in mapping the source speech\n",
      "vectors into the acoustic vector space of the target. Training LPC coefficients\n",
      "with neural network yielded a poor result due to the instability of the LPC\n",
      "filter poles. The LPC coefficients were converted to line spectral frequency\n",
      "coefficients before been trained with a 3-layer neural network. The algorithm\n",
      "was tested with noisy data with the result evaluated using Mel-Cepstral\n",
      "Distance measurement. Cepstral distance evaluation shows a 35.7 percent\n",
      "reduction in the spectral distance between the target and the converted speech. \n",
      "\n",
      "\n",
      "This research presents a neural network based voice conversion (VC) model.\n",
      "While it is a known fact that voiced sounds and prosody are the most important\n",
      "component of the voice conversion framework, what is not known is their\n",
      "objective contributions particularly in a noisy and uncontrolled environment.\n",
      "This model uses a 2-layer feedforward neural network to map the Linear\n",
      "prediction analysis coefficients of a source speaker to the acoustic vector\n",
      "space of the target speaker with a view to objectively determine the\n",
      "contributions of the voiced, unvoiced and supra-segmental components of sounds\n",
      "to the voice conversion model. Results showed that vowels 'a', 'i', 'o' have\n",
      "the most significant contribution in the conversion success. The voiceless\n",
      "sounds were also found to be most affected by the noisy training data. An\n",
      "average noise level of 40 dB above the noise floor were found to degrade the\n",
      "voice conversion success by 55.14 percent relative to the voiced sounds. The\n",
      "result also shows that for cross-gender voice conversion, prosody conversion is\n",
      "more significant in scenarios where a female is the target speaker. \n",
      "\n",
      "\n",
      "We aim to characterize how different speakers contribute to the perceived\n",
      "output quality of multi-speaker Text-to-Speech (TTS) synthesis. We\n",
      "automatically rate the quality of TTS using a neural network (NN) trained on\n",
      "human mean opinion score (MOS) ratings. First, we train and evaluate our NN\n",
      "model on 13 different TTS and voice conversion (VC) systems from the ASVSpoof\n",
      "2019 Logical Access (LA) Dataset. Since it is not known how best to represent\n",
      "speech for this task, we compare 8 different representations alongside MOSNet\n",
      "frame-based features. Our representations include image-based spectrogram\n",
      "features and x-vector embeddings that explicitly model different types of noise\n",
      "such as T60 reverberation time. Our NN predicts MOS with a high correlation to\n",
      "human judgments. We report prediction correlation and error. A key finding is\n",
      "the quality achieved for certain speakers seems consistent, regardless of the\n",
      "TTS or VC system. It is widely accepted that some speakers give higher quality\n",
      "than others for building a TTS system: our method provides an automatic way to\n",
      "identify such speakers. Finally, to see if our quality prediction models\n",
      "generalize, we predict quality scores for synthetic speech using a separate\n",
      "multi-speaker TTS system that was trained on LibriTTS data, and conduct our own\n",
      "MOS listening test to compare human ratings with our NN predictions. \n",
      "\n",
      "\n",
      "In this paper, we propose computationally efficient and high-quality methods\n",
      "for statistical voice conversion (VC) with direct waveform modification based\n",
      "on spectral differentials. The conventional method with a minimum-phase filter\n",
      "achieves high-quality conversion but requires heavy computation in filtering.\n",
      "This is because the minimum phase using a fixed lifter of the Hilbert transform\n",
      "often results in a long-tap filter. One of our methods is a data-driven method\n",
      "for lifter training. Since this method takes filter truncation into account in\n",
      "training, it can shorten the tap length of the filter while preserving\n",
      "conversion accuracy. Our other method is sub-band processing for extending the\n",
      "conventional method from narrow-band (16 kHz) to full-band (48 kHz) VC, which\n",
      "can convert a full-band waveform with higher converted-speech quality.\n",
      "Experimental results indicate that 1) the proposed lifter-training method for\n",
      "narrow-band VC can shorten the tap length to 1/16 without degrading the\n",
      "converted-speech quality and 2) the proposed sub-band-processing method for\n",
      "full-band VC can improve the converted-speech quality than the conventional\n",
      "method. \n",
      "\n",
      "\n",
      "Voice conversion (VC) refers to transforming the speaker characteristics of\n",
      "an utterance without altering its linguistic contents. Many works on voice\n",
      "conversion require to have parallel training data that is highly expensive to\n",
      "acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which\n",
      "does not require parallel training data, has been applied to voice conversion,\n",
      "showing the state-of-the-art performance. The CycleGAN based voice conversion,\n",
      "however, can be used only for a pair of speakers, i.e., one-to-one voice\n",
      "conversion between two speakers. In this paper, we extend the CycleGAN by\n",
      "conditioning the network on speakers. As a result, the proposed method can\n",
      "perform many-to-many voice conversion among multiple speakers using a single\n",
      "generative adversarial network (GAN). Compared to building multiple CycleGANs\n",
      "for each pair of speakers, the proposed method reduces the computational and\n",
      "spatial cost significantly without compromising the sound quality of the\n",
      "converted voice. Experimental results using the VCC2018 corpus confirm the\n",
      "efficiency of the proposed method. \n",
      "\n",
      "\n",
      "Mel-frequency filter bank (MFB) based approaches have the advantage of\n",
      "learning speech compared to raw spectrum since MFB has less feature size.\n",
      "However, speech generator with MFB approaches require additional vocoder that\n",
      "needs a huge amount of computation expense for training process. The additional\n",
      "pre/post processing such as MFB and vocoder is not essential to convert real\n",
      "human speech to others. It is possible to only use the raw spectrum along with\n",
      "the phase to generate different style of voices with clear pronunciation. In\n",
      "this regard, we propose a fast and effective approach to convert realistic\n",
      "voices using raw spectrum in a parallel manner. Our transformer-based model\n",
      "architecture which does not have any CNN or RNN layers has shown the advantage\n",
      "of learning fast and solved the limitation of sequential computation of\n",
      "conventional RNN. In this paper, we introduce a vocoder-free end-to-end voice\n",
      "conversion method using transformer network. The presented conversion model can\n",
      "also be used in speaker adaptation for speech recognition. Our approach can\n",
      "convert the source voice to a target voice without using MFB and vocoder. We\n",
      "can get an adapted MFB for speech recognition by multiplying the converted\n",
      "magnitude with phase. We perform our voice conversion experiments on TIDIGITS\n",
      "dataset using the metrics such as naturalness, similarity, and clarity with\n",
      "mean opinion score, respectively. \n",
      "\n",
      "\n",
      "Emotional voice conversion aims to convert the spectrum and prosody to change\n",
      "the emotional patterns of speech, while preserving the speaker identity and\n",
      "linguistic content. Many studies require parallel speech data between different\n",
      "emotional patterns, which is not practical in real life. Moreover, they often\n",
      "model the conversion of fundamental frequency (F0) with a simple linear\n",
      "transform. As F0 is a key aspect of intonation that is hierarchical in nature,\n",
      "we believe that it is more adequate to model F0 in different temporal scales by\n",
      "using wavelet transform. We propose a CycleGAN network to find an optimal\n",
      "pseudo pair from non-parallel training data by learning forward and inverse\n",
      "mappings simultaneously using adversarial and cycle-consistency losses. We also\n",
      "study the use of continuous wavelet transform (CWT) to decompose F0 into ten\n",
      "temporal scales, that describes speech prosody at different time resolution,\n",
      "for effective F0 conversion. Experimental results show that our proposed\n",
      "framework outperforms the baselines both in objective and subjective\n",
      "evaluations. \n",
      "\n",
      "\n",
      "An effective approach for voice conversion (VC) is to disentangle linguistic\n",
      "content from other components in the speech signal. The effectiveness of\n",
      "variational autoencoder (VAE) based VC (VAE-VC), for instance, strongly relies\n",
      "on this principle. In our prior work, we proposed a cross-domain VAE-VC\n",
      "(CDVAE-VC) framework, which utilized acoustic features of different properties,\n",
      "to improve the performance of VAE-VC. We believed that the success came from\n",
      "more disentangled latent representations. In this paper, we extend the CDVAE-VC\n",
      "framework by incorporating the concept of adversarial learning, in order to\n",
      "further increase the degree of disentanglement, thereby improving the quality\n",
      "and similarity of converted speech. More specifically, we first investigate the\n",
      "effectiveness of incorporating the generative adversarial networks (GANs) with\n",
      "CDVAE-VC. Then, we consider the concept of domain adversarial training and add\n",
      "an explicit constraint to the latent representation, realized by a speaker\n",
      "classifier, to explicitly eliminate the speaker information that resides in the\n",
      "latent code. Experimental results confirm that the degree of disentanglement of\n",
      "the learned latent representation can be enhanced by both GANs and the speaker\n",
      "classifier. Meanwhile, subjective evaluation results in terms of quality and\n",
      "similarity scores demonstrate the effectiveness of our proposed methods. \n",
      "\n",
      "\n",
      "AI is increasingly being offered 'as a service' (AIaaS). This entails service\n",
      "providers offering customers access to pre-built AI models and services, for\n",
      "tasks such as object recognition, text translation, text-to-voice conversion,\n",
      "and facial recognition, to name a few. The offerings enable customers to easily\n",
      "integrate a range of powerful AI-driven capabilities into their applications.\n",
      "Customers access these models through the provider's APIs, sending particular\n",
      "data to which models are applied, the results of which returned. However, there\n",
      "are many situations in which the use of AI can be problematic. AIaaS services\n",
      "typically represent generic functionality, available 'at a click'. Providers\n",
      "may therefore, for reasons of reputation or responsibility, seek to ensure that\n",
      "the AIaaS services they offer are being used by customers for 'appropriate'\n",
      "purposes. This paper introduces and explores the concept whereby AIaaS\n",
      "providers uncover situations of possible service misuse by their customers.\n",
      "Illustrated through topical examples, we consider the technical usage patterns\n",
      "that could signal situations warranting scrutiny, and raise some of the legal\n",
      "and technical challenges of monitoring for misuse. In all, by introducing this\n",
      "concept, we indicate a potential area for further inquiry from a range of\n",
      "perspectives. \n",
      "\n",
      "\n",
      "For training the sequence-to-sequence voice conversion model, we need to\n",
      "handle an issue of insufficient data about the number of speech pairs which\n",
      "consist of the same utterance. This study experimentally investigated the\n",
      "effects of Mel-spectrogram augmentation on training the sequence-to-sequence\n",
      "voice conversion (VC) model from scratch. For Mel-spectrogram augmentation, we\n",
      "adopted the policies proposed in SpecAugment. In addition, we proposed new\n",
      "policies (i.e., frequency warping, loudness and time length control) for more\n",
      "data variations. Moreover, to find the appropriate hyperparameters of\n",
      "augmentation policies without training the VC model, we proposed hyperparameter\n",
      "search strategy and the new metric for reducing experimental cost, namely\n",
      "deformation per deteriorating ratio. We compared the effect of these\n",
      "Mel-spectrogram augmentation methods based on various sizes of training set and\n",
      "augmentation policies. In the experimental results, the time axis warping based\n",
      "policies (i.e., time length control and time warping.) showed better\n",
      "performance than other policies. These results indicate that the use of the\n",
      "Mel-spectrogram augmentation is more beneficial for training the VC model. \n",
      "\n",
      "\n",
      "With the recent advancements of deep learning technologies, the performance\n",
      "of voice conversion (VC) in terms of quality and similarity has been\n",
      "significantly improved. However, heavy computations are generally required for\n",
      "deep-learning-based VC systems, which can cause notable latency and thus\n",
      "confine their deployments in real-world applications. Therefore, increasing\n",
      "online computation efficiency has become an important task. In this study, we\n",
      "propose a novel mixture-of-experts (MoE) based VC system. The MoE model uses a\n",
      "gating mechanism to specify optimal weights to feature maps to increase VC\n",
      "performance. In addition, assigning sparse constraints on the gating mechanism\n",
      "can accelerate online computation by skipping the convolution process by\n",
      "zeroing out redundant feature maps. Experimental results show that by\n",
      "specifying suitable sparse constraints, we can effectively increase the online\n",
      "computation efficiency with a notable 70% FLOPs (floating-point operations per\n",
      "second) reduction while improving the VC performance in both objective\n",
      "evaluations and human listening tests. \n",
      "\n",
      "\n",
      "We propose an algorithm that is capable of synthesizing high quality target\n",
      "speaker's singing voice given only their normal speech samples. The proposed\n",
      "algorithm first integrate speech and singing synthesis into a unified\n",
      "framework, and learns universal speaker embeddings that are shareable between\n",
      "speech and singing synthesis tasks. Specifically, the speaker embeddings\n",
      "learned from normal speech via the speech synthesis objective are shared with\n",
      "those learned from singing samples via the singing synthesis objective in the\n",
      "unified training framework. This makes the learned speaker embedding a\n",
      "transferable representation for both speaking and singing. We evaluate the\n",
      "proposed algorithm on singing voice conversion task where the content of\n",
      "original singing is covered with the timbre of another speaker's voice learned\n",
      "purely from their normal speech samples. Our experiments indicate that the\n",
      "proposed algorithm generates high-quality singing voices that sound highly\n",
      "similar to target speaker's voice given only his or her normal speech samples.\n",
      "We believe that proposed algorithm will open up new opportunities for singing\n",
      "synthesis and conversion for broader users and applications. \n",
      "\n",
      "\n",
      "We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)\n",
      "model based on the Transformer architecture with text-to-speech (TTS)\n",
      "pretraining. Seq2seq VC models are attractive owing to their ability to convert\n",
      "prosody. While seq2seq models based on recurrent neural networks (RNNs) and\n",
      "convolutional neural networks (CNNs) have been successfully applied to VC, the\n",
      "use of the Transformer network, which has shown promising results in various\n",
      "speech processing tasks, has not yet been investigated. Nonetheless, their\n",
      "data-hungry property and the mispronunciation of converted speech make seq2seq\n",
      "models far from practical. To this end, we propose a simple yet effective\n",
      "pretraining technique to transfer knowledge from learned TTS models, which\n",
      "benefit from large-scale, easily accessible TTS corpora. VC models initialized\n",
      "with such pretrained model parameters are able to generate effective hidden\n",
      "representations for high-fidelity, highly intelligible converted speech.\n",
      "Experimental results show that such a pretraining scheme can facilitate\n",
      "data-efficient training and outperform an RNN-based seq2seq VC model in terms\n",
      "of intelligibility, naturalness, and similarity. \n",
      "\n",
      "\n",
      "We present an approach to synthesize whisper by applying a handcrafted signal\n",
      "processing recipe and Voice Conversion (VC) techniques to convert normally\n",
      "phonated speech to whispered speech. We investigate using Gaussian Mixture\n",
      "Models (GMM) and Deep Neural Networks (DNN) to model the mapping between\n",
      "acoustic features of normal speech and those of whispered speech. We evaluate\n",
      "naturalness and speaker similarity of the converted whisper on an internal\n",
      "corpus and on the publicly available wTIMIT corpus. We show that applying VC\n",
      "techniques is significantly better than using rule-based signal processing\n",
      "methods and it achieves results that are indistinguishable from copy-synthesis\n",
      "of natural whisper recordings. We investigate the ability of the DNN model to\n",
      "generalize on unseen speakers, when trained with data from multiple speakers.\n",
      "We show that excluding the target speaker from the training set has little or\n",
      "no impact on the perceived naturalness and speaker similarity of the converted\n",
      "whisper. The proposed DNN method is used in the newly released Whisper Mode of\n",
      "Amazon Alexa. \n",
      "\n",
      "\n",
      "Recently, neural vocoders have been widely used in speech synthesis tasks,\n",
      "including text-to-speech and voice conversion. However, when encountering data\n",
      "distribution mismatch between training and inference, neural vocoders trained\n",
      "on real data often degrade in voice quality for unseen scenarios. In this\n",
      "paper, we train four common neural vocoders, including WaveNet, WaveRNN,\n",
      "FFTNet, Parallel WaveGAN alternately on five different datasets. To study the\n",
      "robustness of neural vocoders, we evaluate the models using acoustic features\n",
      "from seen/unseen speakers, seen/unseen languages, a text-to-speech model, and a\n",
      "voice conversion model. We found out that the speaker variety is much more\n",
      "important for achieving a universal vocoder than the language. Through our\n",
      "experiments, we show that WaveNet and WaveRNN are more suitable for\n",
      "text-to-speech models, while Parallel WaveGAN is more suitable for voice\n",
      "conversion applications. Great amount of subjective MOS results in naturalness\n",
      "for all vocoders are presented for future studies. \n",
      "\n",
      "\n",
      "Singing voice conversion is to convert a singer's voice to another one's\n",
      "voice without changing singing content. Recent work shows that unsupervised\n",
      "singing voice conversion can be achieved with an autoencoder-based approach\n",
      "[1]. However, the converted singing voice can be easily out of key, showing\n",
      "that the existing approach cannot model the pitch information precisely. In\n",
      "this paper, we propose to advance the existing unsupervised singing voice\n",
      "conversion method proposed in [1] to achieve more accurate pitch translation\n",
      "and flexible pitch manipulation. Specifically, the proposed PitchNet added an\n",
      "adversarially trained pitch regression network to enforce the encoder network\n",
      "to learn pitch invariant phoneme representation, and a separate module to feed\n",
      "pitch extracted from the source audio to the decoder network. Our evaluation\n",
      "shows that the proposed method can greatly improve the quality of the converted\n",
      "singing voice (2.92 vs 3.75 in MOS). We also demonstrate that the pitch of\n",
      "converted singing can be easily controlled during generation by changing the\n",
      "levels of the extracted pitch before passing it to the decoder network. \n",
      "\n",
      "\n",
      "We propose a flexible framework that deals with both singer conversion and\n",
      "singers vocal technique conversion. The proposed model is trained on\n",
      "non-parallel corpora, accommodates many-to-many conversion, and leverages\n",
      "recent advances of variational autoencoders. It employs separate encoders to\n",
      "learn disentangled latent representations of singer identity and vocal\n",
      "technique separately, with a joint decoder for reconstruction. Conversion is\n",
      "carried out by simple vector arithmetic in the learned latent spaces. Both a\n",
      "quantitative analysis as well as a visualization of the converted spectrograms\n",
      "show that our model is able to disentangle singer identity and vocal technique\n",
      "and successfully perform conversion of these attributes. To the best of our\n",
      "knowledge, this is the first work to jointly tackle conversion of singer\n",
      "identity and vocal technique based on a deep learning approach. \n",
      "\n",
      "\n",
      "Voice conversion (VC) is a task to transform a person's voice to different\n",
      "style while conserving linguistic contents. Previous state-of-the-art on VC is\n",
      "based on sequence-to-sequence (seq2seq) model, which could mislead linguistic\n",
      "information. There was an attempt to overcome it by using textual supervision,\n",
      "it requires explicit alignment which loses the benefit of using seq2seq model.\n",
      "In this paper, a voice converter using multitask learning with text-to-speech\n",
      "(TTS) is presented. The embedding space of seq2seq-based TTS has abundant\n",
      "information on the text. The role of the decoder of TTS is to convert embedding\n",
      "space to speech, which is same to VC. In the proposed model, the whole network\n",
      "is trained to minimize loss of VC and TTS. VC is expected to capture more\n",
      "linguistic information and to preserve training stability by multitask\n",
      "learning. Experiments of VC were performed on a male Korean emotional\n",
      "text-speech dataset, and it is shown that multitask learning is helpful to keep\n",
      "linguistic contents in VC. \n",
      "\n",
      "\n",
      "Have you ever wondered how a song might sound if performed by a different\n",
      "artist? In this work, we propose SCM-GAN, an end-to-end non-parallel song\n",
      "conversion system powered by generative adversarial and transfer learning that\n",
      "allows users to listen to a selected target singer singing any song. SCM-GAN\n",
      "first separates songs into vocals and instrumental music using a U-Net network,\n",
      "then converts the vocal segments to the target singer using advanced\n",
      "CycleGAN-VC, before merging the converted vocals with their corresponding\n",
      "background music. SCM-GAN is first initialized with feature representations\n",
      "learned from a state-of-the-art voice-to-voice conversion and then trained on a\n",
      "dataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set\n",
      "of metrics including global variance GV and modulation spectra MS on the 24\n",
      "Mel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and\n",
      "the MS by 13% on average. A subjective comparison is conducted to test the user\n",
      "satisfaction with the quality and the naturalness of the conversion. Results\n",
      "show above par similarity between SCM-GAN's output and the target (70\\% on\n",
      "average) as well as great naturalness of the converted songs. \n",
      "\n",
      "\n",
      "Automatic speaker verification (ASV) is one of the most natural and\n",
      "convenient means of biometric person recognition. Unfortunately, just like all\n",
      "other biometric systems, ASV is vulnerable to spoofing, also referred to as\n",
      "\"presentation attacks.\" These vulnerabilities are generally unacceptable and\n",
      "call for spoofing countermeasures or \"presentation attack detection\" systems.\n",
      "In addition to impersonation, ASV systems are vulnerable to replay, speech\n",
      "synthesis, and voice conversion attacks. The ASVspoof 2019 edition is the first\n",
      "to consider all three spoofing attack types within a single challenge. While\n",
      "they originate from the same source database and same underlying protocol, they\n",
      "are explored in two specific use case scenarios. Spoofing attacks within a\n",
      "logical access (LA) scenario are generated with the latest speech synthesis and\n",
      "voice conversion technologies, including state-of-the-art neural acoustic and\n",
      "waveform model techniques. Replay spoofing attacks within a physical access\n",
      "(PA) scenario are generated through carefully controlled simulations that\n",
      "support much more revealing analysis than possible previously. Also new to the\n",
      "2019 edition is the use of the tandem detection cost function metric, which\n",
      "reflects the impact of spoofing and countermeasures on the reliability of a\n",
      "fixed ASV system. This paper describes the database design, protocol, spoofing\n",
      "attack implementations, and baseline ASV and countermeasure results. It also\n",
      "describes a human assessment on spoofed data in logical access. It was\n",
      "demonstrated that the spoofing data in the ASVspoof 2019 database have varied\n",
      "degrees of perceived quality and similarity to the target speakers, including\n",
      "spoofed data that cannot be differentiated from bona-fide utterances even by\n",
      "human subjects. \n",
      "\n",
      "\n",
      "Automatic Speaker Verification (ASV) is the process of identifying a person\n",
      "based on the voice presented to a system. Different synthetic approaches allow\n",
      "spoofing to deceive ASV systems (ASVs), whether using techniques to imitate a\n",
      "voice or recunstruct the features. Attackers try to beat up the ASVs using four\n",
      "general techniques; impersonation, speech synthesis, voice conversion, and\n",
      "replay. The last technique is considered as a common and high potential tool\n",
      "for spoofing purposes since replay attacks are more accessible and require no\n",
      "technical knowledge from adversaries. In this study, we introduce a novel\n",
      "replay spoofing countermeasure for ASVs. Accordingly, we used the Constant Q\n",
      "Cepstral Coefficient (CQCC) features fed into an autoencoder to attain more\n",
      "informative features and to consider the noise information of spoofed\n",
      "utterances for discrimination purpose. Finally, different configurations of the\n",
      "Siamese network were used for the first time in this context for\n",
      "classification. The experiments performed on ASVspoof challenge 2019 dataset\n",
      "using Equal Error Rate (EER) and Tandem Detection Cost Function (t-DCF) as\n",
      "evaluation metrics show that the proposed system improved the results over the\n",
      "baseline by 10.73% and 0.2344 in terms of EER and t-DCF, respectively. \n",
      "\n",
      "\n",
      "In a typical voice conversion system, prior works utilize various acoustic\n",
      "features (e.g., the pitch, voiced/unvoiced flag, aperiodicity) of the source\n",
      "speech to control the prosody of generated waveform. However, the prosody is\n",
      "related with many factors, such as the intonation, stress and rhythm. It is a\n",
      "challenging task to perfectly describe the prosody through acoustic features.\n",
      "To deal with this problem, we propose prosody embeddings to model prosody.\n",
      "These embeddings are learned from the source speech in an unsupervised manner.\n",
      "We conduct experiments on our Mandarin corpus recoded by professional speakers.\n",
      "Experimental results demonstrate that the proposed method enables fine-grained\n",
      "control of the prosody. In challenging situations (such as the source speech is\n",
      "a singing song), our proposed method can also achieve promising results. \n",
      "\n",
      "\n",
      "This paper tackles GAN optimization and stability issues in the context of\n",
      "voice conversion. First, to simplify the conversion task, we propose to use\n",
      "spectral envelopes as inputs. Second we propose two adversarial weight training\n",
      "paradigms, the generalized weighted GAN and the generator impact GAN, both aim\n",
      "at reducing the impact of the generator on the discriminator, so both can learn\n",
      "more gradually and efficiently during training. Applying an energy constraint\n",
      "to the cycleGAN paradigm considerably improved conversion quality. A subjective\n",
      "experiment conducted on a voice conversion task on the voice conversion\n",
      "challenge 2018 dataset shows first that despite a significantly reduced network\n",
      "complexity, the proposed method achieves state-of-the-art results, and second\n",
      "that the proposed weighted GAN methods outperform a previously proposed one. \n",
      "\n",
      "\n",
      "Traditional voice conversion methods rely on parallel recordings of multiple\n",
      "speakers pronouncing the same sentences. For real-world applications however,\n",
      "parallel data is rarely available. We propose MelGAN-VC, a voice conversion\n",
      "method that relies on non-parallel speech data and is able to convert audio\n",
      "signals of arbitrary length from a source voice to a target voice. We firstly\n",
      "compute spectrograms from waveform data and then perform a domain translation\n",
      "using a Generative Adversarial Network (GAN) architecture. An additional\n",
      "siamese network helps preserving speech information in the translation process,\n",
      "without sacrificing the ability to flexibly model the style of the target\n",
      "speaker. We test our framework with a dataset of clean speech recordings, as\n",
      "well as with a collection of noisy real-world speech examples. Finally, we\n",
      "apply the same method to perform music style transfer, translating arbitrarily\n",
      "long music samples from one genre to another, and showing that our framework is\n",
      "flexible and can be used for audio manipulation applications different from\n",
      "voice conversion. \n",
      "\n",
      "\n",
      "This paper presents a cross-lingual voice conversion framework that adopts a\n",
      "modularized neural network. The modularized neural network has a common input\n",
      "structure that is shared for both languages, and two separate output modules,\n",
      "one for each language. The idea is motivated by the fact that phonetic systems\n",
      "of languages are similar because humans share a common vocal production system,\n",
      "but acoustic renderings, such as prosody and phonotactic, vary a lot from\n",
      "language to language. The modularized neural network is trained to map Phonetic\n",
      "PosteriorGram (PPG) to acoustic features for multiple speakers. It is\n",
      "conditioned on a speaker i-vector to generate the desired target voice. We\n",
      "validated the idea between English and Mandarin languages in objective and\n",
      "subjective tests. In addition, mixed-lingual PPG derived from a unified\n",
      "English-Mandarin acoustic model is proposed to capture the linguistic\n",
      "information from both languages. It is found that our proposed modularized\n",
      "neural network significantly outperforms the baseline approaches in terms of\n",
      "speech quality and speaker individuality, and mixed-lingual PPG representation\n",
      "further improves the conversion performance. \n",
      "\n",
      "\n",
      "In this work we introduce a semi-supervised approach to the voice conversion\n",
      "problem, in which speech from a source speaker is converted into speech of a\n",
      "target speaker. The proposed method makes use of both parallel and non-parallel\n",
      "utterances from the source and target simultaneously during training. This\n",
      "approach can be used to extend existing parallel data voice conversion systems\n",
      "such that they can be trained with semi-supervision. We show that incorporating\n",
      "semi-supervision improves the voice conversion performance compared to fully\n",
      "supervised training when the number of parallel utterances is limited as in\n",
      "many practical applications. Additionally, we find that increasing the number\n",
      "non-parallel utterances used in training continues to improve performance when\n",
      "the amount of parallel training data is held constant. \n",
      "\n",
      "\n",
      "Automatic speaker verification (ASV) systems in practice are greatly\n",
      "vulnerable to spoofing attacks. The latest voice conversion technologies are\n",
      "able to produce perceptually natural sounding speech that mimics any target\n",
      "speakers. However, the perceptual closeness to a speaker's identity may not be\n",
      "enough to deceive an ASV system. In this work, we propose a framework that uses\n",
      "the output scores of an ASV system as the feedback to a voice conversion\n",
      "system. The attacker framework is a black-box adversary that steals one's voice\n",
      "identity, because it does not require any knowledge about the ASV system but\n",
      "the system outputs. Experimental results conducted on ASVspoof 2019 database\n",
      "confirm that the proposed feedback-controlled voice conversion framework\n",
      "produces adversarial samples that are more deceptive than the straightforward\n",
      "voice conversion, thereby boosting the impostor ASV scores. Further, the\n",
      "perceptual evaluation studies reveal that converted speech does not adversely\n",
      "affect the voice quality from the baseline system. \n",
      "\n",
      "\n",
      "One of the obstacles in many-to-many voice conversion is the requirement of\n",
      "the parallel training data, which contain pairs of utterances with the same\n",
      "linguistic content spoken by different speakers. Since collecting such parallel\n",
      "data is a highly expensive task, many works attempted to use non-parallel\n",
      "training data for many-to-many voice conversion. One of such approaches is\n",
      "using the variational autoencoder (VAE). Though it can handle many-to-many\n",
      "voice conversion without the parallel training, the VAE based voice conversion\n",
      "methods suffer from low sound qualities of the converted speech. One of the\n",
      "major reasons is because the VAE learns only the self-reconstruction path. The\n",
      "conversion path is not trained at all. In this paper, we propose a cycle\n",
      "consistency loss for VAE to explicitly learn the conversion path. In addition,\n",
      "we propose to use multiple decoders to further improve the sound qualities of\n",
      "the conventional VAE based voice conversion methods. The effectiveness of the\n",
      "proposed method is validated using objective and the subjective evaluations. \n",
      "\n",
      "\n",
      "Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a\n",
      "similar objective, generating speech with a target voice. However, they are\n",
      "usually developed independently under vastly different frameworks. In this\n",
      "paper, we propose a methodology to bootstrap a VC system from a pretrained\n",
      "speaker-adaptive TTS model and unify the techniques as well as the\n",
      "interpretations of these two tasks. Moreover by offloading the heavy data\n",
      "demand to the training stage of the TTS model, our VC system can be built using\n",
      "a small amount of target speaker speech data. It also opens up the possibility\n",
      "of using speech in a foreign unseen language to build the system. Our\n",
      "subjective evaluations show that the proposed framework is able to not only\n",
      "achieve competitive performance in the standard intra-language scenario but\n",
      "also adapt and convert using speech utterances in an unseen language. \n",
      "\n",
      "\n",
      "Cross-lingual voice conversion (CLVC) is a quite challenging task since the\n",
      "source and target speakers speak different languages. This paper proposes a\n",
      "CLVC framework based on bottleneck features and deep neural network (DNN). In\n",
      "the proposed method, the bottleneck features extracted from a deep auto-encoder\n",
      "(DAE) are used to represent speaker-independent features of speech signals from\n",
      "different languages. A DNN model is trained to learn the mapping between\n",
      "bottleneck features and the corresponding spectral features of the target\n",
      "speaker. The proposed method can capture speaker-specific characteristics of a\n",
      "target speaker, and hence requires no speech data from source speaker during\n",
      "training. The performance of the proposed method is evaluated using data from\n",
      "three Indian languages: Telugu, Tamil and Malayalam. The experimental results\n",
      "show that the proposed method outperforms the baseline Gaussian mixture model\n",
      "(GMM)-based CLVC approach. \n",
      "\n",
      "\n",
      "Thanks to improvements in machine learning techniques, including deep\n",
      "learning, speech synthesis is becoming a machine learning task. To accelerate\n",
      "speech synthesis research, we are developing Japanese voice corpora reasonably\n",
      "accessible from not only academic institutions but also commercial companies.\n",
      "In 2017, we released the JSUT corpus, which contains 10 hours of reading-style\n",
      "speech uttered by a single speaker, for end-to-end text-to-speech synthesis.\n",
      "For more general use in speech synthesis research, e.g., voice conversion and\n",
      "multi-speaker modeling, in this paper, we construct the JVS corpus, which\n",
      "contains voice data of 100 speakers in three styles (normal, whisper, and\n",
      "falsetto). The corpus contains 30 hours of voice data including 22 hours of\n",
      "parallel normal voices. This paper describes how we designed the corpus and\n",
      "summarizes the specifications. The corpus is available at our project page. \n",
      "\n",
      "\n",
      "Voice-enabled interactions provide more human-like experiences in many\n",
      "popular IoT systems. Cloud-based speech analysis services extract useful\n",
      "information from voice input using speech recognition techniques. The voice\n",
      "signal is a rich resource that discloses several possible states of a speaker,\n",
      "such as emotional state, confidence and stress levels, physical condition, age,\n",
      "gender, and personal traits. Service providers can build a very accurate\n",
      "profile of a user's demographic category, personal preferences, and may\n",
      "compromise privacy. To address this problem, a privacy-preserving intermediate\n",
      "layer between users and cloud services is proposed to sanitize the voice input.\n",
      "It aims to maintain utility while preserving user privacy. It achieves this by\n",
      "collecting real time speech data and analyzes the signal to ensure privacy\n",
      "protection prior to sharing of this data with services providers. Precisely,\n",
      "the sensitive representations are extracted from the raw signal by using\n",
      "transformation functions and then wrapped it via voice conversion technology.\n",
      "Experimental evaluation based on emotion recognition to assess the efficacy of\n",
      "the proposed method shows that identification of sensitive emotional state of\n",
      "the speaker is reduced by ~96 %. \n",
      "\n",
      "\n",
      "This paper presents a new voice impersonation attack using voice conversion\n",
      "(VC). Enrolling personal voices for automatic speaker verification (ASV) offers\n",
      "natural and flexible biometric authentication systems. Basically, the ASV\n",
      "systems do not include the users' voice data. However, if the ASV system is\n",
      "unexpectedly exposed and hacked by a malicious attacker, there is a risk that\n",
      "the attacker will use VC techniques to reproduce the enrolled user's voices. We\n",
      "name this the ``verification-to-synthesis (V2S) attack'' and propose VC\n",
      "training with the ASV and pre-trained automatic speech recognition (ASR) models\n",
      "and without the targeted speaker's voice data. The VC model reproduces the\n",
      "targeted speaker's individuality by deceiving the ASV model and restores\n",
      "phonetic property of an input voice by matching phonetic posteriorgrams\n",
      "predicted by the ASR model. The experimental evaluation compares converted\n",
      "voices between the proposed method that does not use the targeted speaker's\n",
      "voice data and the standard VC that uses the data. The experimental results\n",
      "demonstrate that the proposed method performs comparably to the existing VC\n",
      "methods that trained using a very small amount of parallel voice data. \n",
      "\n",
      "\n",
      "Non-parallel multi-domain voice conversion (VC) is a technique for learning\n",
      "mappings among multiple domains without relying on parallel data. This is\n",
      "important but challenging owing to the requirement of learning multiple\n",
      "mappings and the non-availability of explicit supervision. Recently, StarGAN-VC\n",
      "has garnered attention owing to its ability to solve this problem only using a\n",
      "single generator. However, there is still a gap between real and converted\n",
      "speech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which\n",
      "are key components for achieving non-parallel multi-domain VC in a single\n",
      "model, and propose an improved variant called StarGAN-VC2. Particularly, we\n",
      "rethink conditional methods in two aspects: training objectives and network\n",
      "architectures. For the former, we propose a source-and-target conditional\n",
      "adversarial loss that allows all source domain data to be convertible to the\n",
      "target domain data. For the latter, we introduce a modulation-based conditional\n",
      "method that can transform the modulation of the acoustic feature in a\n",
      "domain-specific manner. We evaluated our methods on non-parallel multi-speaker\n",
      "VC. An objective evaluation demonstrates that our proposed methods improve\n",
      "speech quality in terms of both global and local structure measures.\n",
      "Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms\n",
      "StarGAN-VC in terms of naturalness and speaker similarity. The converted speech\n",
      "samples are provided at\n",
      "http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html. \n",
      "\n",
      "\n",
      "We present a modification to the spectrum differential based direct waveform\n",
      "modification for voice conversion (DIFFVC) so that it can be directly applied\n",
      "as a waveform generation module to voice conversion models. The recently\n",
      "proposed DIFFVC avoids the use of a vocoder, meanwhile preserves rich spectral\n",
      "details hence capable of generating high quality converted voice. To apply the\n",
      "DIFFVC framework, a model that can estimate the spectral differential from the\n",
      "F0 transformed input speech needs to be trained beforehand. This requirement\n",
      "imposes several constraints, including a limitation on the estimation model to\n",
      "parallel training and the need of extra training on each conversion pair, which\n",
      "make DIFFVC inflexible. Based on the above motivations, we propose a new DIFFVC\n",
      "framework based on an F0 transformation in the residual domain. By performing\n",
      "inverse filtering on the input signal followed by synthesis filtering on the F0\n",
      "transformed residual signal using the converted spectral features directly, the\n",
      "spectral conversion model does not need to be retrained or capable of\n",
      "predicting the spectral differential. We describe several details that need to\n",
      "be taken care of under this modification, and by applying our proposed method\n",
      "to a non-parallel, variational autoencoder (VAE)-based spectral conversion\n",
      "model, we demonstrate that this framework can be generalized to any spectral\n",
      "conversion model, and experimental evaluations show that it can outperform a\n",
      "baseline framework whose waveform generation process is carried out by a\n",
      "vocoder. \n",
      "\n",
      "\n",
      "In this paper, we present a novel technique for a non-parallel voice\n",
      "conversion (VC) with the use of cyclic variational autoencoder (CycleVAE)-based\n",
      "spectral modeling. In a variational autoencoder(VAE) framework, a latent space,\n",
      "usually with a Gaussian prior, is used to encode a set of input features. In a\n",
      "VAE-based VC, the encoded latent features are fed into a decoder, along with\n",
      "speaker-coding features, to generate estimated spectra with either the original\n",
      "speaker identity (reconstructed) or another speaker identity (converted). Due\n",
      "to the non-parallel modeling condition, the converted spectra can not be\n",
      "directly optimized, which heavily degrades the performance of a VAE-based VC.\n",
      "In this work, to overcome this problem, we propose to use CycleVAE-based\n",
      "spectral model that indirectly optimizes the conversion flow by recycling the\n",
      "converted features back into the system to obtain corresponding cyclic\n",
      "reconstructed spectra that can be directly optimized. The cyclic flow can be\n",
      "continued by using the cyclic reconstructed features as input for the next\n",
      "cycle. The experimental results demonstrate the effectiveness of the proposed\n",
      "CycleVAE-based VC, which yields higher accuracy of converted spectra, generates\n",
      "latent features with higher correlation degree, and significantly improves the\n",
      "quality and conversion accuracy of the converted speech. \n",
      "\n",
      "\n",
      "In this paper, we investigate the effectiveness of a quasi-periodic WaveNet\n",
      "(QPNet) vocoder combined with a statistical spectral conversion technique for a\n",
      "voice conversion task. The WaveNet (WN) vocoder has been applied as the\n",
      "waveform generation module in many different voice conversion frameworks and\n",
      "achieves significant improvement over conventional vocoders. However, because\n",
      "of the fixed dilated convolution and generic network architecture, the WN\n",
      "vocoder lacks robustness against unseen input features and often requires a\n",
      "huge network size to achieve acceptable speech quality. Such limitations\n",
      "usually lead to performance degradation in the voice conversion task. To\n",
      "overcome this problem, the QPNet vocoder is applied, which includes a\n",
      "pitch-dependent dilated convolution component to enhance the pitch\n",
      "controllability and attain a more compact network than the WN vocoder. In the\n",
      "proposed method, input spectral features are first converted using a framewise\n",
      "deep neural network, and then the QPNet vocoder generates converted speech\n",
      "conditioned on the linearly converted prosodic and transformed spectral\n",
      "features. The experimental results confirm that the QPNet vocoder achieves\n",
      "significantly better performance than the same-size WN vocoder while\n",
      "maintaining comparable speech quality to the double-size WN vocoder. Index\n",
      "Terms: WaveNet, vocoder, voice conversion, pitch-dependent dilated convolution,\n",
      "pitch controllability \n",
      "\n",
      "\n",
      "We present a voice conversion solution using recurrent sequence to sequence\n",
      "modeling for DNNs. Our solution takes advantage of recent advances in attention\n",
      "based modeling in the fields of Neural Machine Translation (NMT),\n",
      "Text-to-Speech (TTS) and Automatic Speech Recognition (ASR). The problem\n",
      "consists of converting between voices in a parallel setting when {\\it\n",
      "$<$source,target$>$} audio pairs are available. Our seq2seq architecture makes\n",
      "use of a hierarchical encoder to summarize input audio frames. On the decoder\n",
      "side, we use an attention based architecture used in recent TTS works. Since\n",
      "there is a dearth of large multispeaker voice conversion databases needed for\n",
      "training DNNs, we resort to training the network with a large single speaker\n",
      "dataset as an autoencoder. This is then adapted for the smaller multispeaker\n",
      "voice conversion datasets available for voice conversion. In contrast with\n",
      "other voice conversion works that use $F_0$, duration and linguistic features,\n",
      "our system uses mel spectrograms as the audio representation. Output mel frames\n",
      "are converted back to audio using a wavenet vocoder. \n",
      "\n",
      "\n",
      "The state-of-art models for speech synthesis and voice conversion are capable\n",
      "of generating synthetic speech that is perceptually indistinguishable from\n",
      "bonafide human speech. These methods represent a threat to the automatic\n",
      "speaker verification (ASV) systems. Additionally, replay attacks where the\n",
      "attacker uses a speaker to replay a previously recorded genuine human speech\n",
      "are also possible. We present our solution for the ASVSpoof2019 competition,\n",
      "which aims to develop countermeasure systems that distinguish between spoofing\n",
      "attacks and genuine speeches. Our model is inspired by the success of residual\n",
      "convolutional networks in many classification tasks. We build three variants of\n",
      "a residual convolutional neural network that accept different feature\n",
      "representations (MFCC, Log-magnitude STFT, and CQCC) of input. We compare the\n",
      "performance achieved by our model variants and the competition baseline models.\n",
      "In the logical access scenario, the fusion of our models has zero t-DCF cost\n",
      "and zero equal error rate (EER), as evaluated on the development set. On the\n",
      "evaluation set, our model fusion improves the t-DCF and EER by 25% compared to\n",
      "the baseline algorithms. Against physical access replay attacks, our model\n",
      "fusion improves the baseline algorithms t-DCF and EER scores by 71% and 75% on\n",
      "the evaluation set, respectively. \n",
      "\n",
      "\n",
      "This paper presents a method of sequence-to-sequence (seq2seq) voice\n",
      "conversion using non-parallel training data. In this method, disentangled\n",
      "linguistic and speaker representations are extracted from acoustic features,\n",
      "and voice conversion is achieved by preserving the linguistic representations\n",
      "of source utterances while replacing the speaker representations with the\n",
      "target ones. Our model is built under the framework of encoder-decoder neural\n",
      "networks. A recognition encoder is designed to learn the disentangled\n",
      "linguistic representations with two strategies. First, phoneme transcriptions\n",
      "of training data are introduced to provide the references for leaning\n",
      "linguistic representations of audio signals. Second, an adversarial training\n",
      "strategy is employed to further wipe out speaker information from the\n",
      "linguistic representations. Meanwhile, speaker representations are extracted\n",
      "from audio signals by a speaker encoder. The model parameters are estimated by\n",
      "two-stage training, including a pretraining stage using a multi-speaker dataset\n",
      "and a fine-tuning stage using the dataset of a specific conversion pair. Since\n",
      "both the recognition encoder and the decoder for recovering acoustic features\n",
      "are seq2seq neural networks, there are no constrains of frame alignment and\n",
      "frame-by-frame conversion in our proposed method. Experimental results showed\n",
      "that our method obtained higher similarity and naturalness than the best\n",
      "non-parallel voice conversion method in Voice Conversion Challenge 2018.\n",
      "Besides, the performance of our proposed method was closed to the\n",
      "state-of-the-art parallel seq2seq voice conversion method. \n",
      "\n",
      "\n",
      "End-to-end models for raw audio generation are a challenge, specially if they\n",
      "have to work with non-parallel data, which is a desirable setup in many\n",
      "situations. Voice conversion, in which a model has to impersonate a speaker in\n",
      "a recording, is one of those situations. In this paper, we propose Blow, a\n",
      "single-scale normalizing flow using hypernetwork conditioning to perform\n",
      "many-to-many voice conversion between raw audio. Blow is trained end-to-end,\n",
      "with non-parallel data, on a frame-by-frame basis using a single speaker\n",
      "identifier. We show that Blow compares favorably to existing flow-based\n",
      "architectures and other competitive baselines, obtaining equal or better\n",
      "performance in both objective and subjective evaluations. We further assess the\n",
      "impact of its main components with an ablation study, and quantify a number of\n",
      "properties such as the necessary amount of training data or the preference for\n",
      "source or target speakers. \n",
      "\n",
      "\n",
      "This paper evaluates the effectiveness of a Cycle-GAN based voice converter\n",
      "(VC) on four speaker identification (SID) systems and an automated speech\n",
      "recognition (ASR) system for various purposes. Audio samples converted by the\n",
      "VC model are classified by the SID systems as the intended target at up to 46%\n",
      "top-1 accuracy among more than 250 speakers. This encouraging result in\n",
      "imitating the target styles led us to investigate if converted (synthetic)\n",
      "samples can be used to improve ASR training. Unfortunately, adding synthetic\n",
      "data to the ASR training set only marginally improves word and character error\n",
      "rates. Our results indicate that even though VC models can successfully mimic\n",
      "the style of target speakers as measured by SID systems, improving ASR training\n",
      "with synthetic data from VC systems needs further research to establish its\n",
      "efficacy. \n",
      "\n",
      "\n",
      "We present an unsupervised end-to-end training scheme where we discover\n",
      "discrete subword units from speech without using any labels. The discrete\n",
      "subword units are learned under an ASR-TTS autoencoder reconstruction setting,\n",
      "where an ASR-Encoder is trained to discover a set of common linguistic units\n",
      "given a variety of speakers, and a TTS-Decoder trained to project the\n",
      "discovered units back to the designated speech. We propose a discrete encoding\n",
      "method, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\n",
      "differentiable. We found that the proposed encoding method offers automatic\n",
      "extraction of speech content from speaker style, and is sufficient to cover\n",
      "full linguistic content in a given language. Therefore, the TTS-Decoder can\n",
      "synthesize speech with the same content as the input of ASR-Encoder but with\n",
      "different speaker characteristics, which achieves voice conversion (VC). We\n",
      "further improve the quality of VC using adversarial training, where we train a\n",
      "TTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\n",
      "evaluations show that the proposed approach offers strong VC results as it\n",
      "eliminates speaker identity while preserving content within speech. In the\n",
      "ZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\n",
      "bitrate. \n",
      "\n",
      "\n",
      "Non-parallel many-to-many voice conversion, as well as zero-shot voice\n",
      "conversion, remain under-explored areas. Deep style transfer algorithms, such\n",
      "as generative adversarial networks (GAN) and conditional variational\n",
      "autoencoder (CVAE), are being applied as new solutions in this field. However,\n",
      "GAN training is sophisticated and difficult, and there is no strong evidence\n",
      "that its generated speech is of good perceptual quality. On the other hand,\n",
      "CVAE training is simple but does not come with the distribution-matching\n",
      "property of a GAN. In this paper, we propose a new style transfer scheme that\n",
      "involves only an autoencoder with a carefully designed bottleneck. We formally\n",
      "show that this scheme can achieve distribution-matching style transfer by\n",
      "training only on a self-reconstruction loss. Based on this scheme, we proposed\n",
      "AUTOVC, which achieves state-of-the-art results in many-to-many voice\n",
      "conversion with non-parallel data, and which is the first to perform zero-shot\n",
      "voice conversion. \n",
      "\n",
      "\n",
      "We present a method for converting the voices between a set of speakers. Our\n",
      "method is based on training multiple autoencoder paths, where there is a single\n",
      "speaker-independent encoder and multiple speaker-dependent decoders. The\n",
      "autoencoders are trained with an addition of an adversarial loss which is\n",
      "provided by an auxiliary classifier in order to guide the output of the encoder\n",
      "to be speaker independent. The training of the model is unsupervised in the\n",
      "sense that it does not require collecting the same utterances from the speakers\n",
      "nor does it require time aligning over phonemes. Due to the use of a single\n",
      "encoder, our method can generalize to converting the voice of out-of-training\n",
      "speakers to speakers in the training dataset. We present subjective tests\n",
      "corroborating the performance of our method. \n",
      "\n",
      "\n",
      "In this work, we investigate the effectiveness of two techniques for\n",
      "improving variational autoencoder (VAE) based voice conversion (VC). First, we\n",
      "reconsider the relationship between vocoder features extracted using the high\n",
      "quality vocoders adopted in conventional VC systems, and hypothesize that the\n",
      "spectral features are in fact F0 dependent. Such hypothesis implies that during\n",
      "the conversion phase, the latent codes and the converted features in VAE based\n",
      "VC are in fact source F0 dependent. To this end, we propose to utilize the F0\n",
      "as an additional input of the decoder. The model can learn to disentangle the\n",
      "latent code from the F0 and thus generates converted F0 dependent converted\n",
      "features. Second, to better capture temporal dependencies of the spectral\n",
      "features and the F0 pattern, we replace the frame wise conversion structure in\n",
      "the original VAE based VC framework with a fully convolutional network\n",
      "structure. Our experiments demonstrate that the degree of disentanglement as\n",
      "well as the naturalness of the converted speech are indeed improved. \n",
      "\n",
      "\n",
      "We present a Cycle-GAN based many-to-many voice conversion method that can\n",
      "convert between speakers that are not in the training set. This property is\n",
      "enabled through speaker embeddings generated by a neural network that is\n",
      "jointly trained with the Cycle-GAN. In contrast to prior work in this domain,\n",
      "our method enables conversion between an out-of-dataset speaker and a target\n",
      "speaker in either direction and does not require re-training. Out-of-dataset\n",
      "speaker conversion quality is evaluated using an independently trained speaker\n",
      "identification model, and shows good style conversion characteristics for\n",
      "previously unheard speakers. Subjective tests on human listeners show style\n",
      "conversion quality for in-dataset speakers is comparable to the\n",
      "state-of-the-art baseline model. \n",
      "\n",
      "\n",
      "Existing objective evaluation metrics for voice conversion (VC) are not\n",
      "always correlated with human perception. Therefore, training VC models with\n",
      "such criteria may not effectively improve naturalness and similarity of\n",
      "converted speech. In this paper, we propose deep learning-based assessment\n",
      "models to predict human ratings of converted speech. We adopt the convolutional\n",
      "and recurrent neural network models to build a mean opinion score (MOS)\n",
      "predictor, termed as MOSNet. The proposed models are tested on large-scale\n",
      "listening test results of the Voice Conversion Challenge (VCC) 2018.\n",
      "Experimental results show that the predicted scores of the proposed MOSNet are\n",
      "highly correlated with human MOS ratings at the system level while being fairly\n",
      "correlated with human MOS ratings at the utterance level. Meanwhile, we have\n",
      "modified MOSNet to predict the similarity scores, and the preliminary results\n",
      "show that the predicted scores are also fairly correlated with human ratings.\n",
      "These results confirm that the proposed models could be used as a computational\n",
      "evaluator to measure the MOS of VC systems to reduce the need for expensive\n",
      "human rating. \n",
      "\n",
      "\n",
      "Detecting spoofed utterances is a fundamental problem in voice-based\n",
      "biometrics. Spoofing can be performed either by logical accesses like speech\n",
      "synthesis, voice conversion or by physical accesses such as replaying the\n",
      "pre-recorded utterance. Inspired by the state-of-the-art \\emph{x}-vector based\n",
      "speaker verification approach, this paper proposes a time-delay shallow neural\n",
      "network (TD-SNN) for spoof detection for both logical and physical access. The\n",
      "novelty of the proposed TD-SNN system vis-a-vis conventional DNN systems is\n",
      "that it can handle variable length utterances during testing. Performance of\n",
      "the proposed TD-SNN systems and the baseline Gaussian mixture models (GMMs) is\n",
      "analyzed on the ASV-spoof-2019 dataset. The performance of the systems is\n",
      "measured in terms of the minimum normalized tandem detection cost function\n",
      "(min-t-DCF). When studied with individual features, the TD-SNN system\n",
      "consistently outperforms the GMM system for physical access. For logical\n",
      "access, GMM surpasses TD-SNN systems for certain individual features. When\n",
      "combined with the decision-level feature switching (DLFS) paradigm, the best\n",
      "TD-SNN system outperforms the best baseline GMM system on evaluation data with\n",
      "a relative improvement of 48.03\\% and 49.47\\% for both logical and physical\n",
      "access, respectively. \n",
      "\n",
      "\n",
      "We present a deep learning method for singing voice conversion. The proposed\n",
      "network is not conditioned on the text or on the notes, and it directly\n",
      "converts the audio of one singer to the voice of another. Training is performed\n",
      "without any form of supervision: no lyrics or any kind of phonetic features, no\n",
      "notes, and no matching samples between singers. The proposed network employs a\n",
      "single CNN encoder for all singers, a single WaveNet decoder, and a classifier\n",
      "that enforces the latent representation to be singer-agnostic. Each singer is\n",
      "represented by one embedding vector, which the decoder is conditioned on. In\n",
      "order to deal with relatively small datasets, we propose a new data\n",
      "augmentation scheme, as well as new training losses and protocols that are\n",
      "based on backtranslation. Our evaluation presents evidence that the conversion\n",
      "produces natural signing voices that are highly recognizable as the target\n",
      "singer. \n",
      "\n",
      "\n",
      "This paper describes the Speech Technology Center (STC) antispoofing systems\n",
      "submitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended\n",
      "version of the previous challenges and includes 2 evaluation conditions:\n",
      "logical access use-case scenario with speech synthesis and voice conversion\n",
      "attack types and physical access use-case scenario with replay attacks. During\n",
      "the challenge we developed anti-spoofing solutions for both scenarios. The\n",
      "proposed systems are implemented using deep learning approach and are based on\n",
      "different types of acoustic features. We enhanced Light CNN architecture\n",
      "previously considered by the authors for replay attacks detection and which\n",
      "performed high spoofing detection quality during the ASVspoof2017 challenge. In\n",
      "particular here we investigate the efficiency of angular margin based softmax\n",
      "activation for training robust deep Light CNN classifier to solve the\n",
      "mentioned-above tasks. Submitted systems achieved EER of 1.86% in logical\n",
      "access scenario and 0.54% in physical access scenario on the evaluation part of\n",
      "the Challenge corpora. High performance obtained for the unknown types of\n",
      "spoofing attacks demonstrates the stability of the offered approach in both\n",
      "evaluation conditions. \n",
      "\n",
      "\n",
      "Recently, voice conversion (VC) without parallel data has been successfully\n",
      "adapted to multi-target scenario in which a single model is trained to convert\n",
      "the input voice to many different speakers. However, such model suffers from\n",
      "the limitation that it can only convert the voice to the speakers in the\n",
      "training data, which narrows down the applicable scenario of VC. In this paper,\n",
      "we proposed a novel one-shot VC approach which is able to perform VC by only an\n",
      "example utterance from source and target speaker respectively, and the source\n",
      "and target speaker do not even need to be seen during training. This is\n",
      "achieved by disentangling speaker and content representations with instance\n",
      "normalization (IN). Objective and subjective evaluation shows that our model is\n",
      "able to generate the voice similar to target speaker. In addition to the\n",
      "performance measurement, we also demonstrate that this model is able to learn\n",
      "meaningful speaker representations without any supervision. \n",
      "\n",
      "\n",
      "Non-parallel voice conversion (VC) is a technique for learning the mapping\n",
      "from source to target speech without relying on parallel data. This is an\n",
      "important task, but it has been challenging due to the disadvantages of the\n",
      "training conditions. Recently, CycleGAN-VC has provided a breakthrough and\n",
      "performed comparably to a parallel VC method without relying on any extra data,\n",
      "modules, or time alignment procedures. However, there is still a large gap\n",
      "between the real target and converted speech, and bridging this gap remains a\n",
      "challenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved\n",
      "version of CycleGAN-VC incorporating three new techniques: an improved\n",
      "objective (two-step adversarial losses), improved generator (2-1-2D CNN), and\n",
      "improved discriminator (PatchGAN). We evaluated our method on a non-parallel VC\n",
      "task and analyzed the effect of each technique in detail. An objective\n",
      "evaluation showed that these techniques help bring the converted feature\n",
      "sequence closer to the target in terms of both global and local structures,\n",
      "which we assess by using Mel-cepstral distortion and modulation spectra\n",
      "distance, respectively. A subjective evaluation showed that CycleGAN-VC2\n",
      "outperforms CycleGAN-VC in terms of naturalness and similarity for every\n",
      "speaker pair, including intra-gender and inter-gender pairs. \n",
      "\n",
      "\n",
      "Humans are able to imagine a person's voice from the person's appearance and\n",
      "imagine the person's appearance from his/her voice. In this paper, we make the\n",
      "first attempt to develop a method that can convert speech into a voice that\n",
      "matches an input face image and generate a face image that matches the voice of\n",
      "the input speech by leveraging the correlation between faces and voices. We\n",
      "propose a model, consisting of a speech converter, a face encoder/decoder and a\n",
      "voice encoder. We use the latent code of an input face image encoded by the\n",
      "face encoder as the auxiliary input into the speech converter and train the\n",
      "speech converter so that the original latent code can be recovered from the\n",
      "generated speech by the voice encoder. We also train the face decoder along\n",
      "with the face encoder to ensure that the latent code will contain sufficient\n",
      "information to reconstruct the input face image. We confirmed experimentally\n",
      "that a speech converter trained in this way was able to convert input speech\n",
      "into a voice that matched an input face image and that the voice encoder and\n",
      "face decoder can be used to generate a face image that matches the voice of the\n",
      "input speech. \n",
      "\n",
      "\n",
      "This paper introduces Taco-VC, a novel architecture for voice conversion\n",
      "based on Tacotron synthesizer, which is a sequence-to-sequence with attention\n",
      "model. The training of multi-speaker voice conversion systems requires a large\n",
      "number of resources, both in training and corpus size. Taco-VC is implemented\n",
      "using a single speaker Tacotron synthesizer based on Phonetic PosteriorGrams\n",
      "(PPGs) and a single speaker WaveNet vocoder conditioned on mel spectrograms. To\n",
      "enhance the converted speech quality, and to overcome over-smoothing, the\n",
      "outputs of Tacotron are passed through a novel speechenhancement network, which\n",
      "is composed of a combination of the phoneme recognition and Tacotron networks.\n",
      "Our system is trained just with a single speaker corpus and adapts to new\n",
      "speakers using only a few minutes of training data. Using mid-size public\n",
      "datasets, our method outperforms the baseline in the VCC 2018 SPOKE\n",
      "non-parallel voice conversion task and achieves competitive results compared to\n",
      "multi-speaker networks trained on large private datasets. \n",
      "\n",
      "\n",
      "We present JHU's system submission to the ASVspoof 2019 Challenge:\n",
      "Anti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).\n",
      "Anti-spoofing has gathered more and more attention since the inauguration of\n",
      "the ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from\n",
      "all three major types: text-to-speech, voice conversion, and replay. Built upon\n",
      "previous research work on Deep Neural Network (DNN), ASSERT is a pipeline for\n",
      "DNN-based approach to anti-spoofing. ASSERT has four components: feature\n",
      "engineering, DNN models, network optimization and system combination, where the\n",
      "DNN models are variants of squeeze-excitation and residual networks. We\n",
      "conducted an ablation study of the effectiveness of each component on the\n",
      "ASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more\n",
      "than 93% and 17% relative improvements over the baseline systems in the two\n",
      "sub-challenges in ASVspooof 2019, ranking ASSERT one of the top performing\n",
      "systems. Code and pretrained models will be made publicly available. \n",
      "\n",
      "\n",
      "We investigated the training of a shared model for both text-to-speech (TTS)\n",
      "and voice conversion (VC) tasks. We propose using an extended model\n",
      "architecture of Tacotron, that is a multi-source sequence-to-sequence model\n",
      "with a dual attention mechanism as the shared model for both the TTS and VC\n",
      "tasks. This model can accomplish these two different tasks respectively\n",
      "according to the type of input. An end-to-end speech synthesis task is\n",
      "conducted when the model is given text as the input while a\n",
      "sequence-to-sequence voice conversion task is conducted when it is given the\n",
      "speech of a source speaker as the input. Waveform signals are generated by\n",
      "using WaveNet, which is conditioned by using a predicted mel-spectrogram. We\n",
      "propose jointly training a shared model as a decoder for a target speaker that\n",
      "supports multiple sources. Listening experiments show that our proposed\n",
      "multi-source encoder-decoder model can efficiently achieve both the TTS and VC\n",
      "tasks. \n",
      "\n",
      "\n",
      "Singing voice conversion is a task to convert a song sang by a source singer\n",
      "to the voice of a target singer. In this paper, we propose using a parallel\n",
      "data free, many-to-one voice conversion technique on singing voices. A phonetic\n",
      "posterior feature is first generated by decoding singing voices through a\n",
      "robust Automatic Speech Recognition Engine (ASR). Then, a trained Recurrent\n",
      "Neural Network (RNN) with a Deep Bidirectional Long Short Term Memory (DBLSTM)\n",
      "structure is used to model the mapping from person-independent content to the\n",
      "acoustic features of the target person. F0 and aperiodic are obtained through\n",
      "the original singing voice, and used with acoustic features to reconstruct the\n",
      "target singing voice through a vocoder. In the obtained singing voice, the\n",
      "targeted and sourced singers sound similar. To our knowledge, this is the first\n",
      "study that uses non parallel data to train a singing voice conversion system.\n",
      "Subjective evaluations demonstrate that the proposed method effectively\n",
      "converts singing voices. \n",
      "\n",
      "\n",
      "In a typical voice conversion system, vocoder is commonly used for\n",
      "speech-to-features analysis and features-to-speech synthesis. However, vocoder\n",
      "can be a source of speech quality degradation. This paper presents a\n",
      "vocoder-free voice conversion approach using WaveNet for non-parallel training\n",
      "data. Instead of dealing with the intermediate features, the proposed approach\n",
      "utilizes the WaveNet to map the Phonetic PosteriorGrams (PPGs) to the waveform\n",
      "samples directly. In this way, we avoid the estimation errors caused by vocoder\n",
      "and feature conversion. Additionally, as PPG is assumed to be speaker\n",
      "independent, the proposed method also reduces the feature mismatch problem in\n",
      "WaveNet vocoder based approaches. Experimental results conducted on the\n",
      "CMU-ARCTIC database show that the proposed approach significantly outperforms\n",
      "the baseline approaches in terms of speech quality. \n",
      "\n",
      "\n",
      "Voice-based biometric systems are highly prone to spoofing attacks. Recently,\n",
      "various countermeasures have been developed for detecting different kinds of\n",
      "attacks such as replay, speech synthesis (SS) and voice conversion (VC). Most\n",
      "of the existing studies are conducted with a specific training set defined by\n",
      "the evaluation protocol. However, for realistic scenarios, selecting\n",
      "appropriate training data is an open challenge for the system administrator.\n",
      "Motivated by this practical concern, this work investigates the generalization\n",
      "capability of spoofing countermeasures in restricted training conditions where\n",
      "speech from a broad attack types are left out in the training database. We\n",
      "demonstrate that different spoofing types have considerably different\n",
      "generalization capabilities. For this study, we analyze the performance using\n",
      "two kinds of features, mel-frequency cepstral coefficients (MFCCs) which are\n",
      "considered as baseline and recently proposed constant Q cepstral coefficients\n",
      "(CQCCs). The experiments are conducted with standard Gaussian mixture model -\n",
      "maximum likelihood (GMM-ML) classifier on two recently released spoofing\n",
      "corpora: ASVspoof 2015 and BTAS 2016 that includes cross-corpora performance\n",
      "analysis. Feature-level analysis suggests that static and dynamic coefficients\n",
      "of spectral features, both are important for detecting spoofing attacks in the\n",
      "real-life condition. \n",
      "\n",
      "\n",
      "This paper presents a refinement framework of WaveNet vocoders for\n",
      "variational autoencoder (VAE) based voice conversion (VC), which reduces the\n",
      "quality distortion caused by the mismatch between the training data and testing\n",
      "data. Conventional WaveNet vocoders are trained with natural acoustic features\n",
      "but conditioned on the converted features in the conversion stage for VC, and\n",
      "such a mismatch often causes significant quality and similarity degradation. In\n",
      "this work, we take advantage of the particular structure of VAEs to refine\n",
      "WaveNet vocoders with the self-reconstructed features generated by VAE, which\n",
      "are of similar characteristics with the converted features while having the\n",
      "same temporal structure with the target natural features. We analyze these\n",
      "features and show that the self-reconstructed features are similar to the\n",
      "converted features. Objective and subjective experimental results demonstrate\n",
      "the effectiveness of our proposed framework. \n",
      "\n",
      "\n",
      "This paper presents methods of making using of text supervision to improve\n",
      "the performance of sequence-to-sequence (seq2seq) voice conversion. Compared\n",
      "with conventional frame-to-frame voice conversion approaches, the seq2seq\n",
      "acoustic modeling method proposed in our previous work achieved higher\n",
      "naturalness and similarity. In this paper, we further improve its performance\n",
      "by utilizing the text transcriptions of parallel training data. First, a\n",
      "multi-task learning structure is designed which adds auxiliary classifiers to\n",
      "the middle layers of the seq2seq model and predicts linguistic labels as a\n",
      "secondary task. Second, a data-augmentation method is proposed which utilizes\n",
      "text alignment to produce extra parallel sequences for model training.\n",
      "Experiments are conducted to evaluate our proposed method with training sets at\n",
      "different sizes. Experimental results show that the multi-task learning with\n",
      "linguistic labels is effective at reducing the errors of seq2seq voice\n",
      "conversion. The data-augmentation method can further improve the performance of\n",
      "seq2seq voice conversion when only 50 or 100 training utterances are available. \n",
      "\n",
      "\n",
      "This paper describes a method based on a sequence-to-sequence learning\n",
      "(Seq2Seq) with attention and context preservation mechanism for voice\n",
      "conversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\n",
      "sequence modeling such as speech synthesis and recognition, machine\n",
      "translation, and image captioning. In contrast to current VC techniques, our\n",
      "method 1) stabilizes and accelerates the training procedure by considering\n",
      "guided attention and proposed context preservation losses, 2) allows not only\n",
      "spectral envelopes but also fundamental frequency contours and durations of\n",
      "speech to be converted, 3) requires no context information such as phoneme\n",
      "labels, and 4) requires no time-aligned source and target speech data in\n",
      "advance. In our experiment, the proposed VC framework can be trained in only\n",
      "one day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\n",
      "synthesized speech is higher than that of speech converted by Gaussian mixture\n",
      "model-based VC and is comparable to that of speech generated by recurrent\n",
      "neural network-based text-to-speech synthesis, which can be regarded as an\n",
      "upper limit on VC performance. \n",
      "\n",
      "\n",
      "This paper proposes a voice conversion (VC) method using sequence-to-sequence\n",
      "(seq2seq or S2S) learning, which flexibly converts not only the voice\n",
      "characteristics but also the pitch contour and duration of input speech. The\n",
      "proposed method, called ConvS2S-VC, has three key features. First, it uses a\n",
      "model with a fully convolutional architecture. This is particularly\n",
      "advantageous in that it is suitable for parallel computations using GPUs. It is\n",
      "also beneficial since it enables effective normalization techniques such as\n",
      "batch normalization to be used for all the hidden layers in the networks.\n",
      "Second, it achieves many-to-many conversion by simultaneously learning mappings\n",
      "among multiple speakers using only a single model instead of separately\n",
      "learning mappings between each speaker pair using a different model. This\n",
      "enables the model to fully utilize available training data collected from\n",
      "multiple speakers by capturing common latent features that can be shared across\n",
      "different speakers. Owing to this structure, our model works reasonably well\n",
      "even without source speaker information, thus making it able to handle\n",
      "any-to-many conversion tasks. Third, we introduce a mechanism, called the\n",
      "conditional batch normalization that switches batch normalization layers in\n",
      "accordance with the target speaker. This particular mechanism has been found to\n",
      "be extremely effective for our many-to-many conversion model. We conducted\n",
      "speaker identity conversion experiments and found that ConvS2S-VC obtained\n",
      "higher sound quality and speaker similarity than baseline methods. We also\n",
      "found from audio examples that it could perform well in various tasks including\n",
      "emotional expression conversion, electrolaryngeal speech enhancement, and\n",
      "English accent conversion. \n",
      "\n",
      "\n",
      "This paper focuses on using voice conversion (VC) to improve the speech\n",
      "intelligibility of surgical patients who have had parts of their articulators\n",
      "removed. Due to the difficulty of data collection, VC without parallel data is\n",
      "highly desired. Although techniques for unparallel VC, for example, CycleGAN,\n",
      "have been developed, they usually focus on transforming the speaker identity,\n",
      "and directly transforming the speech of one speaker to that of another speaker\n",
      "and as such do not address the task here. In this paper, we propose a new\n",
      "approach for unparallel VC. The proposed approach transforms impaired speech to\n",
      "normal speech while preserving the linguistic content and speaker\n",
      "characteristics. To our knowledge, this is the first end-to-end GAN-based\n",
      "unsupervised VC model applied to impaired speech. The experimental results show\n",
      "that the proposed approach outperforms CycleGAN. \n",
      "\n",
      "\n",
      "Sound source separation has attracted attention from Music Information\n",
      "Retrieval(MIR) researchers, since it is related to many MIR tasks such as\n",
      "automatic lyric transcription, singer identification, and voice conversion. In\n",
      "this paper, we propose an intuitive spectrogram-based model for source\n",
      "separation by adapting U-Net. We call it Spectrogram-Channels U-Net, which\n",
      "means each channel of the output corresponds to the spectrogram of separated\n",
      "source itself. The proposed model can be used for not only singing voice\n",
      "separation but also multi-instrument separation by changing only the number of\n",
      "output channels. In addition, we propose a loss function that balances volumes\n",
      "between different sources. Finally, we yield performance that is\n",
      "state-of-the-art on both separation tasks. \n",
      "\n",
      "\n",
      "In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork\n",
      "(SCENT) is presented for acoustic modeling in voice conversion. At training\n",
      "stage, a SCENT model is estimated by aligning the feature sequences of source\n",
      "and target speakers implicitly using attention mechanism. At conversion stage,\n",
      "acoustic features and durations of source utterances are converted\n",
      "simultaneously using the unified acoustic model. Mel-scale spectrograms are\n",
      "adopted as acoustic features which contain both excitation and vocal tract\n",
      "descriptions of speech signals. The bottleneck features extracted from source\n",
      "speech using an automatic speech recognition (ASR) model are appended as\n",
      "auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to\n",
      "reconstruct waveforms from the outputs of the SCENT model. It is worth noting\n",
      "that our proposed method can achieve appropriate duration conversion which is\n",
      "difficult in conventional methods. Experimental results show that our proposed\n",
      "method obtained better objective and subjective performance than the baseline\n",
      "methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as\n",
      "acoustic models. This proposed method also outperformed our previous work which\n",
      "achieved the top rank in Voice Conversion Challenge 2018. Ablation tests\n",
      "further confirmed the effectiveness of several components in our proposed\n",
      "method. \n",
      "\n",
      "\n",
      "So far, many of the deep learning approaches for voice conversion produce\n",
      "good quality speech by using a large amount of training data. This paper\n",
      "presents a Deep Bidirectional Long Short-Term Memory (DBLSTM) based voice\n",
      "conversion framework that can work with a limited amount of training data. We\n",
      "propose to implement a DBLSTM based average model that is trained with data\n",
      "from many speakers. Then, we propose to perform adaptation with a limited\n",
      "amount of target data. Last but not least, we propose an error reduction\n",
      "network that can improve the voice conversion quality even further. The\n",
      "proposed framework is motivated by three observations. Firstly, DBLSTM can\n",
      "achieve a remarkable voice conversion by considering the long-term dependencies\n",
      "of the speech utterance. Secondly, DBLSTM based average model can be easily\n",
      "adapted with a small amount of data, to achieve a speech that sounds closer to\n",
      "the target. Thirdly, an error reduction network can be trained with a small\n",
      "amount of training data, and can improve the conversion quality effectively.\n",
      "The experiments show that the proposed voice conversion framework is flexible\n",
      "to work with limited training data and outperforms the traditional frameworks\n",
      "in both objective and subjective evaluations. \n",
      "\n",
      "\n",
      "We propose a learning-based filter that allows us to directly modify a\n",
      "synthetic speech waveform into a natural speech waveform. Speech-processing\n",
      "systems using a vocoder framework such as statistical parametric speech\n",
      "synthesis and voice conversion are convenient especially for a limited number\n",
      "of data because it is possible to represent and process interpretable acoustic\n",
      "features over a compact space, such as the fundamental frequency (F0) and\n",
      "mel-cepstrum. However, a well-known problem that leads to the quality\n",
      "degradation of generated speech is an over-smoothing effect that eliminates\n",
      "some detailed structure of generated/converted acoustic features. To address\n",
      "this issue, we propose a synthetic-to-natural speech waveform conversion\n",
      "technique that uses cycle-consistent adversarial networks and which does not\n",
      "require any explicit assumption about speech waveform in adversarial learning.\n",
      "In contrast to current techniques, since our modification is performed at the\n",
      "waveform level, we expect that the proposed method will also make it possible\n",
      "to generate `vocoder-less' sounding speech even if the input speech is\n",
      "synthesized using a vocoder framework. The experimental results demonstrate\n",
      "that our proposed method can 1) alleviate the over-smoothing effect of the\n",
      "acoustic features despite the direct modification method used for the waveform\n",
      "and 2) greatly improve the naturalness of the generated speech sounds. \n",
      "\n",
      "\n",
      "An effective approach to non-parallel voice conversion (VC) is to utilize\n",
      "deep neural networks (DNNs), specifically variational auto encoders (VAEs), to\n",
      "model the latent structure of speech in an unsupervised manner. A previous\n",
      "study has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for\n",
      "VC. How- ever, VAE using other types of spectral features such as mel- cepstral\n",
      "coefficients (MCCs), which are related to human per- ception and have been\n",
      "widely used in VC, have not been prop- erly investigated. Instead of using one\n",
      "specific type of spectral feature, it is expected that VAE may benefit from\n",
      "using multi- ple types of spectral features simultaneously, thereby improving\n",
      "the capability of VAE for VC. To this end, we propose a novel VAE framework\n",
      "(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework\n",
      "utilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple\n",
      "objectives in order to constrain the behavior of the learned encoder and de-\n",
      "coder. Experimental results demonstrate that the proposed CD- VAE framework\n",
      "outperforms the conventional VAE framework in terms of subjective tests. \n",
      "\n",
      "\n",
      "Here we present a novel approach to conditioning the SampleRNN generative\n",
      "model for voice conversion (VC). Conventional methods for VC modify the\n",
      "perceived speaker identity by converting between source and target acoustic\n",
      "features. Our approach focuses on preserving voice content and depends on the\n",
      "generative network to learn voice style. We first train a multi-speaker\n",
      "SampleRNN model conditioned on linguistic features, pitch contour, and speaker\n",
      "identity using a multi-speaker speech corpus. Voice-converted speech is\n",
      "generated using linguistic features and pitch contour extracted from the source\n",
      "speaker, and the target speaker identity. We demonstrate that our system is\n",
      "capable of many-to-many voice conversion without requiring parallel data,\n",
      "enabling broad applications. Subjective evaluation demonstrates that our\n",
      "approach outperforms conventional VC methods. \n",
      "\n",
      "\n",
      "We study the problem of cross-lingual voice conversion in non-parallel speech\n",
      "corpora and one-shot learning setting. Most prior work require either parallel\n",
      "speech corpora or enough amount of training data from a target speaker.\n",
      "However, we convert an arbitrary sentences of an arbitrary source speaker to\n",
      "target speaker's given only one target speaker training utterance. To achieve\n",
      "this, we formulate the problem as learning disentangled speaker-specific and\n",
      "context-specific representations and follow the idea of [1] which uses\n",
      "Factorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE\n",
      "on multi-speaker training data, given arbitrary source and target speakers'\n",
      "utterance, we estimate those latent representations and then reconstruct the\n",
      "desired utterance of converted voice to that of target speaker. We investigate\n",
      "the effectiveness of the approach by conducting voice conversion experiments\n",
      "with varying size of training utterances and it was able to achieve reasonable\n",
      "performance with even just one training utterance. We also examine the speech\n",
      "representation and show that World vocoder outperforms Short-time Fourier\n",
      "Transform (STFT) used in [1]. Finally, in the subjective tests, for one\n",
      "language and cross-lingual voice conversion, our approach achieved\n",
      "significantly better or comparable results compared to VAE-STFT and GMM\n",
      "baselines in speech quality and similarity. \n",
      "\n",
      "\n",
      "This paper proposes a non-parallel many-to-many voice conversion (VC) method\n",
      "using a variant of the conditional variational autoencoder (VAE) called an\n",
      "auxiliary classifier VAE (ACVAE). The proposed method has three key features.\n",
      "First, it adopts fully convolutional architectures to construct the encoder and\n",
      "decoder networks so that the networks can learn conversion rules that capture\n",
      "time dependencies in the acoustic feature sequences of source and target\n",
      "speech. Second, it uses an information-theoretic regularization for the model\n",
      "training to ensure that the information in the attribute class label will not\n",
      "be lost in the conversion process. With regular CVAEs, the encoder and decoder\n",
      "are free to ignore the attribute class label input. This can be problematic\n",
      "since in such a situation, the attribute class label will have little effect on\n",
      "controlling the voice characteristics of input speech at test time. Such\n",
      "situations can be avoided by introducing an auxiliary classifier and training\n",
      "the encoder and decoder so that the attribute classes of the decoder outputs\n",
      "are correctly predicted by the classifier. Third, it avoids producing\n",
      "buzzy-sounding speech at test time by simply transplanting the spectral details\n",
      "of the input speech into its converted version. Subjective evaluation\n",
      "experiments revealed that this simple method worked reasonably well in a\n",
      "non-parallel many-to-many speaker identity conversion task. \n",
      "\n",
      "\n",
      "Speaking rate refers to the average number of phonemes within some unit time,\n",
      "while the rhythmic patterns refer to duration distributions for realizations of\n",
      "different phonemes within different phonetic structures. Both are key\n",
      "components of prosody in speech, which is different for different speakers.\n",
      "Models like cycle-consistent adversarial network (Cycle-GAN) and variational\n",
      "auto-encoder (VAE) have been successfully applied to voice conversion tasks\n",
      "without parallel data. However, due to the neural network architectures and\n",
      "feature vectors chosen for these approaches, the length of the predicted\n",
      "utterance has to be fixed to that of the input utterance, which limits the\n",
      "flexibility in mimicking the speaking rates and rhythmic patterns for the\n",
      "target speaker. On the other hand, sequence-to-sequence learning model was used\n",
      "to remove the above length constraint, but parallel training data are needed.\n",
      "In this paper, we propose an approach utilizing sequence-to-sequence model\n",
      "trained with unsupervised Cycle-GAN to perform the transformation between the\n",
      "phoneme posteriorgram sequences for different speakers. In this way, the length\n",
      "constraint mentioned above is removed to offer rhythm-flexible voice conversion\n",
      "without requiring parallel data. Preliminary evaluation on two datasets showed\n",
      "very encouraging results. \n",
      "\n",
      "\n",
      "This paper proposes a method that allows non-parallel many-to-many voice\n",
      "conversion (VC) by using a variant of a generative adversarial network (GAN)\n",
      "called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it\n",
      "(1) requires no parallel utterances, transcriptions, or time alignment\n",
      "procedures for speech generator training, (2) simultaneously learns\n",
      "many-to-many mappings across different attribute domains using a single\n",
      "generator network, (3) is able to generate converted speech signals quickly\n",
      "enough to allow real-time implementations and (4) requires only several minutes\n",
      "of training examples to generate reasonably realistic-sounding speech.\n",
      "Subjective evaluation experiments on a non-parallel many-to-many speaker\n",
      "identity conversion task revealed that the proposed method obtained higher\n",
      "sound quality and speaker similarity than a state-of-the-art method based on\n",
      "variational autoencoding GANs. \n",
      "\n",
      "\n",
      "The fundamental frequency (F0) contour of speech is a key aspect to represent\n",
      "speech prosody that finds use in speech and spoken language analysis such as\n",
      "voice conversion and speech synthesis as well as speaker and language\n",
      "identification. This work proposes new methods to estimate the F0 contour of\n",
      "speech using deep neural networks (DNNs) and recurrent neural networks (RNNs).\n",
      "They are trained using supervised learning with the ground truth of F0\n",
      "contours. The latest prior research addresses this problem first as a\n",
      "frame-by-frame-classification problem followed by sequence tracking using deep\n",
      "neural network hidden Markov model (DNN-HMM) hybrid architecture. This study,\n",
      "however, tackles the problem as a regression problem instead, in order to\n",
      "obtain F0 contours with higher frequency resolution from clean and noisy\n",
      "speech. Experiments using PTDB-TUG corpus contaminated with additive noise\n",
      "(NOISEX-92) show the proposed method improves gross pitch error (GPE) by more\n",
      "than 25 % at signal-to-noise ratios (SNRs) between -10 dB and +10 dB as\n",
      "compared with one of the most noise-robust F0 trackers, PEFAC. Furthermore, the\n",
      "performance on fine pitch error (FPE) is improved by approximately 20 % against\n",
      "a state-of-the-art DNN-HMM-based approach. \n",
      "\n",
      "\n",
      "In this paper, we propose a technique to alleviate the quality degradation\n",
      "caused by collapsed speech segments sometimes generated by the WaveNet vocoder.\n",
      "The effectiveness of the WaveNet vocoder for generating natural speech from\n",
      "acoustic features has been proved in recent works. However, it sometimes\n",
      "generates very noisy speech with collapsed speech segments when only a limited\n",
      "amount of training data is available or significant acoustic mismatches exist\n",
      "between the training and testing data. Such a limitation on the corpus and\n",
      "limited ability of the model can easily occur in some speech generation\n",
      "applications, such as voice conversion and speech enhancement. To address this\n",
      "problem, we propose a technique to automatically detect collapsed speech\n",
      "segments. Moreover, to refine the detected segments, we also propose a waveform\n",
      "generation technique for WaveNet using a linear predictive coding constraint.\n",
      "Verification and subjective tests are conducted to investigate the\n",
      "effectiveness of the proposed techniques. The verification results indicate\n",
      "that the detection technique can detect most collapsed segments. The subjective\n",
      "evaluations of voice conversion demonstrate that the generation technique\n",
      "significantly improves the speech quality while maintaining the same speaker\n",
      "similarity. \n",
      "\n",
      "\n",
      "Recent speech technology research has seen a growing interest in using\n",
      "WaveNets as statistical vocoders, i.e., generating speech waveforms from\n",
      "acoustic features. These models have been shown to improve the generated speech\n",
      "quality over classical vocoders in many tasks, such as text-to-speech synthesis\n",
      "and voice conversion. Furthermore, conditioning WaveNets with acoustic features\n",
      "allows sharing the waveform generator model across multiple speakers without\n",
      "additional speaker codes. However, multi-speaker WaveNet models require large\n",
      "amounts of training data and computation to cover the entire acoustic space.\n",
      "This paper proposes leveraging the source-filter model of speech production to\n",
      "more effectively train a speaker-independent waveform generator with limited\n",
      "resources. We present a multi-speaker 'GlotNet' vocoder, which utilizes a\n",
      "WaveNet to generate glottal excitation waveforms, which are then used to excite\n",
      "the corresponding vocal tract filter to produce speech. Listening tests show\n",
      "that the proposed model performs favourably to a direct WaveNet vocoder trained\n",
      "with the same model architecture and data. \n",
      "\n",
      "\n",
      "Voice conversion (VC) aims at conversion of speaker characteristic without\n",
      "altering content. Due to training data limitations and modeling imperfections,\n",
      "it is difficult to achieve believable speaker mimicry without introducing\n",
      "processing artifacts; performance assessment of VC, therefore, usually involves\n",
      "both speaker similarity and quality evaluation by a human panel. As a\n",
      "time-consuming, expensive, and non-reproducible process, it hinders rapid\n",
      "prototyping of new VC technology. We address artifact assessment using an\n",
      "alternative, objective approach leveraging from prior work on spoofing\n",
      "countermeasures (CMs) for automatic speaker verification. Therein, CMs are used\n",
      "for rejecting `fake' inputs such as replayed, synthetic or converted speech but\n",
      "their potential for automatic speech artifact assessment remains unknown. This\n",
      "study serves to fill that gap. As a supplement to subjective results for the\n",
      "2018 Voice Conversion Challenge (VCC'18) data, we configure a standard\n",
      "constant-Q cepstral coefficient CM to quantify the extent of processing\n",
      "artifacts. Equal error rate (EER) of the CM, a confusability index of VC\n",
      "samples with real human speech, serves as our artifact measure. Two clusters of\n",
      "VCC'18 entries are identified: low-quality ones with detectable artifacts (low\n",
      "EERs), and higher quality ones with less artifacts. None of the VCC'18 systems,\n",
      "however, is perfect: all EERs are < 30 % (the `ideal' value would be 50 %). Our\n",
      "preliminary findings suggest potential of CMs outside of their original\n",
      "application, as a supplemental optimization and benchmarking tool to enhance VC\n",
      "technology. \n",
      "\n",
      "\n",
      "We present the Voice Conversion Challenge 2018, designed as a follow up to\n",
      "the 2016 edition with the aim of providing a common framework for evaluating\n",
      "and comparing different state-of-the-art voice conversion (VC) systems. The\n",
      "objective of the challenge was to perform speaker conversion (i.e. transform\n",
      "the vocal identity) of a source speaker to a target speaker while maintaining\n",
      "linguistic information. As an update to the previous challenge, we considered\n",
      "both parallel and non-parallel data to form the Hub and Spoke tasks,\n",
      "respectively. A total of 23 teams from around the world submitted their\n",
      "systems, 11 of them additionally participated in the optional Spoke task. A\n",
      "large-scale crowdsourced perceptual evaluation was then carried out to rate the\n",
      "submitted converted speech in terms of naturalness and similarity to the target\n",
      "speaker identity. In this paper, we present a brief summary of the\n",
      "state-of-the-art techniques for VC, followed by a detailed explanation of the\n",
      "challenge tasks and the results that were obtained. \n",
      "\n",
      "\n",
      "Deep generative models have achieved great success in unsupervised learning\n",
      "with the ability to capture complex nonlinear relationships between latent\n",
      "generating factors and observations. Among them, a factorized hierarchical\n",
      "variational autoencoder (FHVAE) is a variational inference-based model that\n",
      "formulates a hierarchical generative process for sequential data. Specifically,\n",
      "an FHVAE model can learn disentangled and interpretable representations, which\n",
      "have been proven useful for numerous speech applications, such as speaker\n",
      "verification, robust speech recognition, and voice conversion. However, as we\n",
      "will elaborate in this paper, the training algorithm proposed in the original\n",
      "paper is not scalable to datasets of thousands of hours, which makes this model\n",
      "less applicable on a larger scale. After identifying limitations in terms of\n",
      "runtime, memory, and hyperparameter optimization, we propose a hierarchical\n",
      "sampling training algorithm to address all three issues. Our proposed method is\n",
      "evaluated comprehensively on a wide variety of datasets, ranging from 3 to\n",
      "1,000 hours and involving different types of generating factors, such as\n",
      "recording conditions and noise types. In addition, we also present a new\n",
      "visualization method for qualitatively evaluating the performance with respect\n",
      "to the interpretability and disentanglement. Models trained with our proposed\n",
      "algorithm demonstrate the desired characteristics on all the datasets. \n",
      "\n",
      "\n",
      "Recently, cycle-consistent adversarial network (Cycle-GAN) has been\n",
      "successfully applied to voice conversion to a different speaker without\n",
      "parallel data, although in those approaches an individual model is needed for\n",
      "each target speaker. In this paper, we propose an adversarial learning\n",
      "framework for voice conversion, with which a single model can be trained to\n",
      "convert the voice to many different speakers, all without parallel data, by\n",
      "separating the speaker characteristics from the linguistic content in speech\n",
      "signals. An autoencoder is first trained to extract speaker-independent latent\n",
      "representations and speaker embedding separately using another auxiliary\n",
      "speaker classifier to regularize the latent representation. The decoder then\n",
      "takes the speaker-independent latent representation and the target speaker\n",
      "embedding as the input to generate the voice of the target speaker with the\n",
      "linguistic content of the source utterance. The quality of decoder output is\n",
      "further improved by patching with the residual signal produced by another pair\n",
      "of generator and discriminator. A target speaker set size of 20 was tested in\n",
      "the preliminary experiments, and very good voice quality was obtained.\n",
      "Conventional voice conversion metrics are reported. We also show that the\n",
      "speaker information has been properly reduced from the latent representations. \n",
      "\n",
      "\n",
      "Although voice conversion (VC) algorithms have achieved remarkable success\n",
      "along with the development of machine learning, superior performance is still\n",
      "difficult to achieve when using nonparallel data. In this paper, we propose\n",
      "using a cycle-consistent adversarial network (CycleGAN) for nonparallel\n",
      "data-based VC training. A CycleGAN is a generative adversarial network (GAN)\n",
      "originally developed for unpaired image-to-image translation. A subjective\n",
      "evaluation of inter-gender conversion demonstrated that the proposed method\n",
      "significantly outperformed a method based on the Merlin open source neural\n",
      "network speech synthesis system (a parallel VC system adapted for our setup)\n",
      "and a GAN-based parallel VC system. This is the first research to show that the\n",
      "performance of a nonparallel VC method can exceed that of state-of-the-art\n",
      "parallel VC methods. \n",
      "\n",
      "\n",
      "Thanks to the growing availability of spoofing databases and rapid advances\n",
      "in using them, systems for detecting voice spoofing attacks are becoming more\n",
      "and more capable, and error rates close to zero are being reached for the\n",
      "ASVspoof2015 database. However, speech synthesis and voice conversion paradigms\n",
      "that are not considered in the ASVspoof2015 database are appearing. Such\n",
      "examples include direct waveform modelling and generative adversarial networks.\n",
      "We also need to investigate the feasibility of training spoofing systems using\n",
      "only low-quality found data. For that purpose, we developed a generative\n",
      "adversarial network-based speech enhancement system that improves the quality\n",
      "of speech data found in publicly available sources. Using the enhanced data, we\n",
      "trained state-of-the-art text-to-speech and voice conversion models and\n",
      "evaluated them in terms of perceptual speech quality and speaker similarity.\n",
      "The results show that the enhancement models significantly improved the SNR of\n",
      "low-quality degraded data found in publicly available sources and that they\n",
      "significantly improved the perceptual cleanliness of the source speech without\n",
      "significantly degrading the naturalness of the voice. However, the results also\n",
      "show limitations when generating speech with the low-quality found data. \n",
      "\n",
      "\n",
      "Time- and pitch-scale modifications of speech signals find important\n",
      "applications in speech synthesis, playback systems, voice conversion,\n",
      "learning/hearing aids, etc.. There is a requirement for computationally\n",
      "efficient and real-time implementable algorithms. In this paper, we propose a\n",
      "high quality and computationally efficient time- and pitch-scaling methodology\n",
      "based on the glottal closure instants (GCIs) or epochs in speech signals. The\n",
      "proposed algorithm, termed as epoch-synchronous overlap-add time/pitch-scaling\n",
      "(ESOLA-TS/PS), segments speech signals into overlapping short-time frames and\n",
      "then the adjacent frames are aligned with respect to the epochs and the frames\n",
      "are overlap-added to synthesize time-scale modified speech. Pitch scaling is\n",
      "achieved by resampling the time-scaled speech by a desired sampling factor. We\n",
      "also propose a concept of epoch embedding into speech signals, which\n",
      "facilitates the identification and time-stamping of samples corresponding to\n",
      "epochs and using them for time/pitch-scaling to multiple scaling factors\n",
      "whenever desired, thereby contributing to faster and efficient implementation.\n",
      "The results of perceptual evaluation tests reported in this paper indicate the\n",
      "superiority of ESOLA over state-of-the-art techniques. ESOLA significantly\n",
      "outperforms the conventional pitch synchronous overlap-add (PSOLA) techniques\n",
      "in terms of perceptual quality and intelligibility of the modified speech.\n",
      "Unlike the waveform similarity overlap-add (WSOLA) or synchronous overlap-add\n",
      "(SOLA) techniques, the ESOLA technique has the capability to do exact\n",
      "time-scaling of speech with high quality to any desired modification factor\n",
      "within a range of 0.5 to 2. Compared to synchronous overlap-add with fixed\n",
      "synthesis (SOLAFS), the ESOLA is computationally advantageous and at least\n",
      "three times faster. \n",
      "\n",
      "\n",
      "Inspired by recent work on neural network image generation which rely on\n",
      "backpropagation towards the network inputs, we present a proof-of-concept\n",
      "system for speech texture synthesis and voice conversion based on two\n",
      "mechanisms: approximate inversion of the representation learned by a speech\n",
      "recognition neural network, and on matching statistics of neuron activations\n",
      "between different source and target utterances. Similar to image texture\n",
      "synthesis and neural style transfer, the system works by optimizing a cost\n",
      "function with respect to the input waveform samples. To this end we use a\n",
      "differentiable mel-filterbank feature extraction pipeline and train a\n",
      "convolutional CTC speech recognition network. Our system is able to extract\n",
      "speaker characteristics from very limited amounts of target speaker data, as\n",
      "little as a few seconds, and can be used to generate realistic speech babble or\n",
      "reconstruct an utterance in a different voice. \n",
      "\n",
      "\n",
      "Voice input has been tremendously improving the user experience of mobile\n",
      "devices by freeing our hands from typing on the small screen. Speech\n",
      "recognition is the key technology that powers voice input, and it is usually\n",
      "outsourced to the cloud for the best performance. However, the cloud might\n",
      "compromise users' privacy by identifying their identities by voice, learning\n",
      "their sensitive input content via speech recognition, and then profiling the\n",
      "mobile users based on the content. In this paper, we design an intermediate\n",
      "between users and the cloud, named VoiceMask, to sanitize users' voice data\n",
      "before sending it to the cloud for speech recognition. We analyze the potential\n",
      "privacy risks and aim to protect users' identities and sensitive input content\n",
      "from being disclosed to the cloud. VoiceMask adopts a carefully designed voice\n",
      "conversion mechanism that is resistant to several attacks. Meanwhile, it\n",
      "utilizes an evolution-based keyword substitution technique to sanitize the\n",
      "voice input content. The two sanitization phases are all performed in the\n",
      "resource-limited mobile device while still maintaining the usability and\n",
      "accuracy of the cloud-supported speech recognition service. We implement the\n",
      "voice sanitizer on Android systems and present extensive experimental results\n",
      "that validate the effectiveness and efficiency of our app. It is demonstrated\n",
      "that we are able to reduce the chance of a user's voice being identified from\n",
      "50 people by 84% while keeping the drop of speech recognition accuracy within\n",
      "14.2%. \n",
      "\n",
      "\n",
      "We propose a parallel-data-free voice-conversion (VC) method that can learn a\n",
      "mapping from source to target speech without relying on parallel data. The\n",
      "proposed method is general purpose, high quality, and parallel-data free and\n",
      "works without any extra data, modules, or alignment procedure. It also avoids\n",
      "over-smoothing, which occurs in many conventional statistical model-based VC\n",
      "methods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial\n",
      "network (CycleGAN) with gated convolutional neural networks (CNNs) and an\n",
      "identity-mapping loss. A CycleGAN learns forward and inverse mappings\n",
      "simultaneously using adversarial and cycle-consistency losses. This makes it\n",
      "possible to find an optimal pseudo pair from unpaired data. Furthermore, the\n",
      "adversarial loss contributes to reducing over-smoothing of the converted\n",
      "feature sequence. We configure a CycleGAN with gated CNNs and train it with an\n",
      "identity-mapping loss. This allows the mapping function to capture sequential\n",
      "and hierarchical structures while preserving linguistic information. We\n",
      "evaluated our method on a parallel-data-free VC task. An objective evaluation\n",
      "showed that the converted feature sequence was near natural in terms of global\n",
      "variance and modulation spectra. A subjective evaluation showed that the\n",
      "quality of the converted speech was comparable to that obtained with a Gaussian\n",
      "mixture model-based method under advantageous conditions with parallel and\n",
      "twice the amount of data. \n",
      "\n",
      "\n",
      "A method for statistical parametric speech synthesis incorporating generative\n",
      "adversarial networks (GANs) is proposed. Although powerful deep neural networks\n",
      "(DNNs) techniques can be applied to artificially synthesize speech waveform,\n",
      "the synthetic speech quality is low compared with that of natural speech. One\n",
      "of the issues causing the quality degradation is an over-smoothing effect often\n",
      "observed in the generated speech parameters. A GAN introduced in this paper\n",
      "consists of two neural networks: a discriminator to distinguish natural and\n",
      "generated samples, and a generator to deceive the discriminator. In the\n",
      "proposed framework incorporating the GANs, the discriminator is trained to\n",
      "distinguish natural and generated speech parameters, while the acoustic models\n",
      "are trained to minimize the weighted sum of the conventional minimum generation\n",
      "loss and an adversarial loss for deceiving the discriminator. Since the\n",
      "objective of the GANs is to minimize the divergence (i.e., distribution\n",
      "difference) between the natural and generated speech parameters, the proposed\n",
      "method effectively alleviates the over-smoothing effect on the generated speech\n",
      "parameters. We evaluated the effectiveness for text-to-speech and voice\n",
      "conversion, and found that the proposed method can generate more natural\n",
      "spectral parameters and $F_0$ than conventional minimum generation error\n",
      "training algorithm regardless its hyper-parameter settings. Furthermore, we\n",
      "investigated the effect of the divergence of various GANs, and found that a\n",
      "Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms\n",
      "of improving synthetic speech quality. \n",
      "\n",
      "\n",
      "Voice conversion (VC) using sequence-to-sequence learning of context\n",
      "posterior probabilities is proposed. Conventional VC using shared context\n",
      "posterior probabilities predicts target speech parameters from the context\n",
      "posterior probabilities estimated from the source speech parameters. Although\n",
      "conventional VC can be built from non-parallel data, it is difficult to convert\n",
      "speaker individuality such as phonetic property and speaking rate contained in\n",
      "the posterior probabilities because the source posterior probabilities are\n",
      "directly used for predicting target speech parameters. In this work, we assume\n",
      "that the training data partly include parallel speech data and propose\n",
      "sequence-to-sequence learning between the source and target posterior\n",
      "probabilities. The conversion models perform non-linear and variable-length\n",
      "transformation from the source probability sequence to the target one. Further,\n",
      "we propose a joint training algorithm for the modules. In contrast to\n",
      "conventional VC, which separately trains the speech recognition that estimates\n",
      "posterior probabilities and the speech synthesis that predicts target speech\n",
      "parameters, our proposed method jointly trains these modules along with the\n",
      "proposed probability conversion modules. Experimental results demonstrate that\n",
      "our approach outperforms the conventional VC. \n",
      "\n",
      "\n",
      "Building a voice conversion (VC) system from non-parallel speech corpora is\n",
      "challenging but highly valuable in real application scenarios. In most\n",
      "situations, the source and the target speakers do not repeat the same texts or\n",
      "they may even speak different languages. In this case, one possible, although\n",
      "indirect, solution is to build a generative model for speech. Generative models\n",
      "focus on explaining the observations with latent variables instead of learning\n",
      "a pairwise transformation function, thereby bypassing the requirement of speech\n",
      "frame alignment. In this paper, we propose a non-parallel VC framework with a\n",
      "variational autoencoding Wasserstein generative adversarial network (VAW-GAN)\n",
      "that explicitly considers a VC objective when building the speech model.\n",
      "Experimental results corroborate the capability of our framework for building a\n",
      "VC system from unaligned data, and demonstrate improved conversion quality. \n",
      "\n",
      "\n",
      "Most of the existing studies on voice conversion (VC) are conducted in\n",
      "acoustically matched conditions between source and target signal. However, the\n",
      "robustness of VC methods in presence of mismatch remains unknown. In this\n",
      "paper, we report a comparative analysis of different VC techniques under\n",
      "mismatched conditions. The extensive experiments with five different VC\n",
      "techniques on CMU ARCTIC corpus suggest that performance of VC methods\n",
      "substantially degrades in noisy conditions. We have found that bilinear\n",
      "frequency warping with amplitude scaling (BLFWAS) outperforms other methods in\n",
      "most of the noisy conditions. We further explore the suitability of different\n",
      "speech enhancement techniques for robust conversion. The objective evaluation\n",
      "results indicate that spectral subtraction and log minimum mean square error\n",
      "(logMMSE) based speech enhancement techniques can be used to improve the\n",
      "performance in specific noisy conditions. \n",
      "\n",
      "\n",
      "The human auditory system is able to distinguish the vocal source of\n",
      "thousands of speakers, yet not much is known about what features the auditory\n",
      "system uses to do this. Fourier Transforms are capable of capturing the pitch\n",
      "and harmonic structure of the speaker but this alone proves insufficient at\n",
      "identifying speakers uniquely. The remaining structure, often referred to as\n",
      "timbre, is critical to identifying speakers but we understood little about it.\n",
      "In this paper we use recent advances in neural networks in order to manipulate\n",
      "the voice of one speaker into another by transforming not only the pitch of the\n",
      "speaker, but the timbre. We review generative models built with neural networks\n",
      "as well as architectures for creating neural networks that learn analogies. Our\n",
      "preliminary results converting voices from one speaker to another are\n",
      "encouraging. \n",
      "\n",
      "\n",
      "We propose a flexible framework for spectral conversion (SC) that facilitates\n",
      "training with unaligned corpora. Many SC frameworks require parallel corpora,\n",
      "phonetic alignments, or explicit frame-wise correspondence for learning\n",
      "conversion functions or for synthesizing a target spectrum with the aid of\n",
      "alignments. However, these requirements gravely limit the scope of practical\n",
      "applications of SC due to scarcity or even unavailability of parallel corpora.\n",
      "We propose an SC framework based on variational auto-encoder which enables us\n",
      "to exploit non-parallel corpora. The framework comprises an encoder that learns\n",
      "speaker-independent phonetic representations and a decoder that learns to\n",
      "reconstruct the designated speaker. It removes the requirement of parallel\n",
      "corpora or phonetic alignments to train a spectral conversion system. We report\n",
      "objective and subjective evaluations to validate our proposed method and\n",
      "compare it to SC methods that have access to aligned corpora. \n",
      "\n",
      "\n",
      "In this paper, we propose a dictionary update method for Nonnegative Matrix\n",
      "Factorization (NMF) with high dimensional data in a spectral conversion (SC)\n",
      "task. Voice conversion has been widely studied due to its potential\n",
      "applications such as personalized speech synthesis and speech enhancement.\n",
      "Exemplar-based NMF (ENMF) emerges as an effective and probably the simplest\n",
      "choice among all techniques for SC, as long as a source-target parallel speech\n",
      "corpus is given. ENMF-based SC systems usually need a large amount of bases\n",
      "(exemplars) to ensure the quality of the converted speech. However, a small and\n",
      "effective dictionary is desirable but hard to obtain via dictionary update, in\n",
      "particular when high-dimensional features such as STRAIGHT spectra are used.\n",
      "Therefore, we propose a dictionary update framework for NMF by means of an\n",
      "encoder-decoder reformulation. Regarding NMF as an encoder-decoder network\n",
      "makes it possible to exploit the whole parallel corpus more effectively and\n",
      "efficiently when applied to SC. Our experiments demonstrate significant gains\n",
      "of the proposed system with small dictionaries over conventional ENMF-based\n",
      "systems with dictionaries of same or much larger size. \n",
      "\n",
      "\n",
      "Now-a-days, speech-based biometric systems such as automatic speaker\n",
      "verification (ASV) are highly prone to spoofing attacks by an imposture. With\n",
      "recent development in various voice conversion (VC) and speech synthesis (SS)\n",
      "algorithms, these spoofing attacks can pose a serious potential threat to the\n",
      "current state-of-the-art ASV systems. To impede such attacks and enhance the\n",
      "security of the ASV systems, the development of efficient anti-spoofing\n",
      "algorithms is essential that can differentiate synthetic or converted speech\n",
      "from natural or human speech. In this paper, we propose a set of novel speech\n",
      "features for detecting spoofing attacks. The proposed features are computed\n",
      "using alternative frequency-warping technique and formant-specific block\n",
      "transformation of filter bank log energies. We have evaluated existing and\n",
      "proposed features against several kinds of synthetic speech data from ASVspoof\n",
      "2015 corpora. The results show that the proposed techniques outperform existing\n",
      "approaches for various spoofing attack detection task. The techniques\n",
      "investigated in this paper can also accurately classify natural and synthetic\n",
      "speech as equal error rates (EERs) of 0% have been achieved. \n",
      "\n",
      "\n",
      "Automatic speaker verification (ASV) technology is recently finding its way\n",
      "to end-user applications for secure access to personal data, smart services or\n",
      "physical facilities. Similar to other biometric technologies, speaker\n",
      "verification is vulnerable to spoofing attacks where an attacker masquerades as\n",
      "a particular target speaker via impersonation, replay, text-to-speech (TTS) or\n",
      "voice conversion (VC) techniques to gain illegitimate access to the system. We\n",
      "focus on TTS and VC that represent the most flexible, high-end spoofing\n",
      "attacks. Most of the prior studies on synthesized or converted speech detection\n",
      "report their findings using high-quality clean recordings. Meanwhile, the\n",
      "performance of spoofing detectors in the presence of additive noise, an\n",
      "important consideration in practical ASV implementations, remains largely\n",
      "unknown. To this end, we analyze the suitability of state-of-the-art synthetic\n",
      "speech detectors under additive noise with a special focus on front-end\n",
      "features. Our comparison includes eight acoustic feature sets, five related to\n",
      "spectral magnitude and three to spectral phase information. Our extensive\n",
      "experiments on ASVSpoof 2015 corpus reveal several important findings. Firstly,\n",
      "all the countermeasures break down even at relatively high signal-to-noise\n",
      "ratios (SNRs) and fail to generalize to noisy conditions. Secondly, speech\n",
      "enhancement is not found helpful. Thirdly, GMM back-end generally outperforms\n",
      "the more involved i-vector back-end. Fourthly, concerning the compared\n",
      "features, the Mel-frequency cepstral coefficients (MFCCs) and subband spectral\n",
      "centroid magnitude coefficients (SCMCs) perform the best on average though the\n",
      "winner method depends on SNR and noise type. Finally, a study with two score\n",
      "fusion strategies shows that combining different feature based systems improves\n",
      "recognition accuracy for known and unknown attacks in both clean and noisy\n",
      "conditions. \n",
      "\n",
      "\n",
      "Voice conversion methods have advanced rapidly over the last decade. Studies\n",
      "have shown that speaker characteristics are captured by spectral feature as\n",
      "well as various prosodic features. Most existing conversion methods focus on\n",
      "the spectral feature as it directly represents the timbre characteristics,\n",
      "while some conversion methods have focused only on the prosodic feature\n",
      "represented by the fundamental frequency. In this paper, a comprehensive\n",
      "framework using deep neural networks to convert both timbre and prosodic\n",
      "features is proposed. The timbre feature is represented by a high-resolution\n",
      "spectral feature. The prosodic features include F0, intensity and duration. It\n",
      "is well known that DNN is useful as a tool to model high-dimensional features.\n",
      "In this work, we show that DNN initialized by our proposed autoencoder\n",
      "pretraining yields good quality DNN conversion models. This pretraining is\n",
      "tailor-made for voice conversion and leverages on autoencoder to capture the\n",
      "generic spectral shape of source speech. Additionally, our framework uses\n",
      "segmental DNN models to capture the evolution of the prosodic features over\n",
      "time. To reconstruct the converted speech, the spectral feature produced by the\n",
      "DNN model is combined with the three prosodic features produced by the DNN\n",
      "segmental models. Our experimental results show that the application of both\n",
      "prosodic and high-resolution spectral features leads to quality converted\n",
      "speech as measured by objective evaluation and subjective listening tests. \n",
      "\n",
      "\n",
      "We propose a novel application based on acoustic-to-articulatory inversion\n",
      "towards quality assessment of voice converted speech. The ability of humans to\n",
      "speak effortlessly requires coordinated movements of various articulators,\n",
      "muscles, etc. This effortless movement contributes towards naturalness,\n",
      "intelligibility and speakers identity which is partially present in voice\n",
      "converted speech. Hence, during voice conversion, the information related to\n",
      "speech production is lost. In this paper, this loss is quantified for male\n",
      "voice, by showing increase in RMSE error for voice converted speech followed by\n",
      "showing decrease in mutual information. Similar results are obtained in case of\n",
      "female voice. This observation is extended by showing that articulatory\n",
      "features can be used as an objective measure. The effectiveness of proposed\n",
      "measure over MCD is illustrated by comparing their correlation with Mean\n",
      "Opinion Score. \n",
      "\n",
      "\n",
      "In this study, we investigate a solution to reduce the effect of one-to-many\n",
      "problem in voice conversion. One-to-many problem in VC happens when two very\n",
      "similar speech segments in source speaker have corresponding speech segments in\n",
      "target speaker that are not similar to each other. As a result, the mapper\n",
      "function usually over-smoothes the generated features in order to be similar to\n",
      "both target speech segments. In this study, we propose to equalize the formant\n",
      "location of source-target frame pairs using dynamic frequency warping in order\n",
      "to reduce the complexity. After the conversion, another dynamic frequency\n",
      "warping is further applied to reverse the effect of formant location\n",
      "equalization during the training. The subjective experiments showed that the\n",
      "proposed approach improves the speech quality significantly. \n",
      "\n",
      "\n",
      "Many existing speaker verification systems are reported to be vulnerable\n",
      "against different spoofing attacks, for example speaker-adapted speech\n",
      "synthesis, voice conversion, play back, etc. In order to detect these spoofed\n",
      "speech signals as a countermeasure, we propose a score level fusion approach\n",
      "with several different i-vector subsystems. We show that the acoustic level\n",
      "Mel-frequency cepstral coefficients (MFCC) features, the phase level modified\n",
      "group delay cepstral coefficients (MGDCC) and the phonetic level phoneme\n",
      "posterior probability (PPP) tandem features are effective for the\n",
      "countermeasure. Furthermore, feature level fusion of these features before\n",
      "i-vector modeling also enhance the performance. A polynomial kernel support\n",
      "vector machine is adopted as the supervised classifier. In order to enhance the\n",
      "generalizability of the countermeasure, we also adopted the cosine similarity\n",
      "and PLDA scoring as one-class classifications methods. By combining the\n",
      "proposed i-vector subsystems with the OpenSMILE baseline which covers the\n",
      "acoustic and prosodic information further improves the final performance. The\n",
      "proposed fusion system achieves 0.29% and 3.26% EER on the development and test\n",
      "set of the database provided by the INTERSPEECH 2015 automatic speaker\n",
      "verification spoofing and countermeasures challenge. \n",
      "\n",
      "\n",
      "This paper presents a new approach for a vocoder design based on full\n",
      "frequency masking by octaves in addition to a technique for spectral filling\n",
      "via beta probability distribution. Some psycho-acoustic characteristics of\n",
      "human hearing - inaudibility masking in frequency and phase - are used as a\n",
      "basis for the proposed algorithm. The results confirm that this technique may\n",
      "be useful to save bandwidth in applications requiring intelligibility. It is\n",
      "recommended for the legal eavesdropping of long voice conversations. \n",
      "\n",
      "\n",
      "This article surveys the various techniques adopted for optimising bandwidth\n",
      "for VoIP services over the period 1999-2014. The improvement of bandwidth can\n",
      "be realized through; silence suppression measure of repressing the silent\n",
      "portions (packets) in a voice conversation using Voice Activity Detection\n",
      "algorithm; by so doing, the transmission rate during the inactive periods of\n",
      "speech is reduced, and thus, the mean transmission rate can be reduced. A\n",
      "second measure is packet header reduction which defines a process of\n",
      "multiplexing and de-multiplexing packet headers to curb excesses. Voice/ Packet\n",
      "Header compression is considered the most productive of all the techniques,\n",
      "offering a scheme where VoIP packets are compressed from the 40 bytes of size\n",
      "to a smaller byte size of 2 bytes. When combined with aggregation, compression\n",
      "potentially yields a compressed size of up to 1 byte. In either case, bandwidth\n",
      "save is reached using compression and decompression codecs of varying data and\n",
      "bit rates. It is envisaged that an improvement in the performance of codecs\n",
      "would yield a better result in terms of enhancing results favourably in Voice\n",
      "over broadband networks \n",
      "\n",
      "\n",
      "The goal of cross-domain object matching (CDOM) is to find correspondence\n",
      "between two sets of objects in different domains in an unsupervised way. Photo\n",
      "album summarization is a typical application of CDOM, where photos are\n",
      "automatically aligned into a designed frame expressed in the Cartesian\n",
      "coordinate system. CDOM is usually formulated as finding a mapping from objects\n",
      "in one domain (photos) to objects in the other domain (frame) so that the\n",
      "pairwise dependency is maximized. A state-of-the-art CDOM method employs a\n",
      "kernel-based dependency measure, but it has a drawback that the kernel\n",
      "parameter needs to be determined manually. In this paper, we propose\n",
      "alternative CDOM methods that can naturally address the model selection\n",
      "problem. Through experiments on image matching, unpaired voice conversion, and\n",
      "photo album summarization tasks, the effectiveness of the proposed methods is\n",
      "demonstrated. \n",
      "\n",
      "\n",
      "Attackers may manipulate audio with the intent of presenting falsified\n",
      "reports, changing an opinion of a public figure, and winning influence and\n",
      "power. The prevalence of inauthentic multimedia continues to rise, so it is\n",
      "imperative to develop a set of tools that determines the legitimacy of media.\n",
      "We present a method that analyzes audio signals to determine whether they\n",
      "contain real human voices or fake human voices (i.e., voices generated by\n",
      "neural acoustic and waveform models). Instead of analyzing the audio signals\n",
      "directly, the proposed approach converts the audio signals into spectrogram\n",
      "images displaying frequency, intensity, and temporal content and evaluates them\n",
      "with a Convolutional Neural Network (CNN). Trained on both genuine human voice\n",
      "signals and synthesized voice signals, we show our approach achieves high\n",
      "accuracy on this classification task. \n",
      "\n",
      "\n",
      "Online platforms play a relevant role in the creation and diffusion of false\n",
      "or misleading news. Concerningly, the COVID-19 pandemic is shaping a\n",
      "communication network - barely considered in the literature - which reflects\n",
      "the emergence of collective attention towards a topic that rapidly gained\n",
      "universal interest. Here, we characterize the dynamics of this network on\n",
      "Twitter, analyzing how unreliable content distributes among its users. We find\n",
      "that a minority of accounts is responsible for the majority of the\n",
      "misinformation circulating online, and identify two categories of users: a few\n",
      "active ones, playing the role of \"creators\", and a majority playing the role of\n",
      "\"consumers\". The relative proportion of these groups ($\\approx$14% creators -\n",
      "86% consumers) appears stable over time: Consumers are mostly exposed to the\n",
      "opinions of a vocal minority of creators, that could be mistakenly understood\n",
      "as of representative of the majority of users. The corresponding pressure from\n",
      "a perceived majority is identified as a potential driver of the ongoing\n",
      "COVID-19 infodemic. \n",
      "\n",
      "\n",
      "In the area of Internet of Things (IoT) voice assistants have become an\n",
      "important interface to operate smart speakers, smartphones, and even\n",
      "automobiles. To save power and protect user privacy, voice assistants send\n",
      "commands to the cloud only if a small set of pre-registered wake-up words are\n",
      "detected. However, voice assistants are shown to be vulnerable to the FakeWake\n",
      "phenomena, whereby they are inadvertently triggered by innocent-sounding fuzzy\n",
      "words. In this paper, we present a systematic investigation of the FakeWake\n",
      "phenomena from three aspects. To start with, we design the first fuzzy word\n",
      "generator to automatically and efficiently produce fuzzy words instead of\n",
      "searching through a swarm of audio materials. We manage to generate 965 fuzzy\n",
      "words covering 8 most popular English and Chinese smart speakers. To explain\n",
      "the causes underlying the FakeWake phenomena, we construct an interpretable\n",
      "tree-based decision model, which reveals phonetic features that contribute to\n",
      "false acceptance of fuzzy words by wake-up word detectors. Finally, we propose\n",
      "remedies to mitigate the effect of FakeWake. The results show that the\n",
      "strengthened models are not only resilient to fuzzy words but also achieve\n",
      "better overall performance on original training datasets. \n",
      "\n",
      "\n",
      "Fake audio attack becomes a major threat to the speaker verification system.\n",
      "Although current detection approaches have achieved promising results on\n",
      "dataset-specific scenarios, they encounter difficulties on unseen spoofing\n",
      "data. Fine-tuning and retraining from scratch have been applied to incorporate\n",
      "new data. However, fine-tuning leads to performance degradation on previous\n",
      "data. Retraining takes a lot of time and computation resources. Besides,\n",
      "previous data are unavailable due to privacy in some situations. To solve the\n",
      "above problems, this paper proposes detecting fake without forgetting, a\n",
      "continual-learning-based method, to make the model learn new spoofing attacks\n",
      "incrementally. A knowledge distillation loss is introduced to loss function to\n",
      "preserve the memory of original model. Supposing the distribution of genuine\n",
      "voice is consistent among different scenarios, an extra embedding similarity\n",
      "loss is used as another constraint to further do a positive sample alignment.\n",
      "Experiments are conducted on the ASVspoof2019 dataset. The results show that\n",
      "our proposed method outperforms fine-tuning by the relative reduction of\n",
      "average equal error rate up to 81.62%. \n",
      "\n",
      "\n",
      "In this paper, we present a Distribution-Preserving Voice Anonymization\n",
      "technique, as our submission to the VoicePrivacy Challenge 2020. We observe\n",
      "that the challenge baseline system generates fake X-vectors which are very\n",
      "similar to each other, significantly more so than those extracted from organic\n",
      "speakers. This difference arises from averaging many X-vectors from a pool of\n",
      "speakers in the anonymization process, causing a loss of information. We\n",
      "propose a new method to generate fake X-vectors which overcomes these\n",
      "limitations by preserving the distributional properties of X-vectors and their\n",
      "intra-similarity. We use population data to learn the properties of the\n",
      "X-vector space, before fitting a generative model which we use to sample fake\n",
      "X-vectors. We show how this approach generates X-vectors that more closely\n",
      "follow the expected intra-similarity distribution of organic speaker X-vectors.\n",
      "Our method can be easily integrated with others as the anonymization component\n",
      "of the system and removes the need to distribute a pool of speakers to use\n",
      "during the anonymization. Our approach leads to an increase in EER of up to\n",
      "$19.4\\%$ in males and $11.1\\%$ in females in scenarios where enrollment and\n",
      "trial utterances are anonymized versus the baseline solution, demonstrating the\n",
      "diversity of our generated voices. \n",
      "\n",
      "\n",
      "Astroturfing, i.e., the fabrication of public discourse by private or\n",
      "state-controlled sponsors via the creation of fake online accounts, has become\n",
      "incredibly widespread in recent years. It gives a disproportionally strong\n",
      "voice to wealthy and technology-savvy actors, permits targeted attacks on\n",
      "public forums and could in the long run harm the trust users have in the\n",
      "internet as a communication platform. Countering these efforts without\n",
      "deanonymising the participants has not yet proven effective; however, we can\n",
      "raise the cost of astroturfing. Following the principle `one person, one\n",
      "voice', we introduce Trollthrottle, a protocol that limits the number of\n",
      "comments a single person can post on participating websites. Using direct\n",
      "anonymous attestation and a public ledger, the user is free to choose any\n",
      "nickname, but the number of comments is aggregated over all posts on all\n",
      "websites, no matter which nickname was used. We demonstrate the deployability\n",
      "of Trollthrottle by retrofitting it to the popular news aggregator website\n",
      "Reddit and by evaluating the cost of deployment for the scenario of a national\n",
      "newspaper (168k comments per day), an international newspaper (268k c/d) and\n",
      "Reddit itself (4.9M c/d). \n",
      "\n",
      "\n",
      "The COVID-19 pandemic has not only had severe political, economic, and\n",
      "societal effects, it has also affected media and communication systems in\n",
      "unprecedented ways. While traditional journalistic media has tried to adapt to\n",
      "the rapidly evolving situation, alternative news media on the Internet have\n",
      "given the events their own ideological spin. Such voices have been criticized\n",
      "for furthering societal confusion and spreading potentially dangerous \"fake\n",
      "news\" or conspiracy theories via social media and other online channels. The\n",
      "current study analyzes the factual basis of such fears in an initial\n",
      "computational content analysis of alternative news media's output on Facebook\n",
      "during the early Corona crisis, based on a large German data set from January\n",
      "to the second half of March 2020. Using computational content analysis,\n",
      "methods, reach, interactions, actors, and topics of the messages were examined,\n",
      "as well as the use of fabricated news and conspiracy theories. The analysis\n",
      "revealed that the alternative news media stay true to message patterns and\n",
      "ideological foundations identified in prior research. While they do not spread\n",
      "obvious lies, they are predominantly sharing overly critical, even\n",
      "anti-systemic messages, opposing the view of the mainstream news media and the\n",
      "political establishment. With this pandemic populism, they contribute to a\n",
      "contradictory, menacing, and distrusting worldview, as portrayed in detail in\n",
      "this analysis. \n",
      "\n",
      "\n",
      "With the shift of public discourse to social media, we see simultaneously an\n",
      "expansion of civic engagement as the bar to enter the conversation is lowered,\n",
      "and the reaction by both state and non-state adversaries of free speech to\n",
      "silence these voices. Traditional forms of censorship struggle in this new\n",
      "situation to enforce the preferred narrative of those in power. Consequently,\n",
      "they have developed new methods for controlling the conversation that use the\n",
      "social media platform itself.\n",
      "  Using the Central Asian republic of Kyrgyzstan as a main case study, this\n",
      "talk explores how this new form of \"subtle\" censorship relies on pretence and\n",
      "imitation, and why interdisciplinary methods of research are needed to grapple\n",
      "with it. We examine how \"fakeness\" in the form of fake news and profiles is\n",
      "used as methods of subtle censorship. \n",
      "\n",
      "\n",
      "Fake audio detection is expected to become an important research area in the\n",
      "field of smart speakers such as Google Home, Amazon Echo and chatbots developed\n",
      "for these platforms. This paper presents replay attack vulnerability of\n",
      "voice-driven interfaces and proposes a countermeasure to detect replay attack\n",
      "on these platforms. This paper presents a novel framework to model replay\n",
      "attack distortion, and then use a non-learning-based method for replay attack\n",
      "detection on smart speakers. The reply attack distortion is modeled as a\n",
      "higher-order nonlinearity in the replay attack audio. Higher-order spectral\n",
      "analysis (HOSA) is used to capture characteristics distortions in the replay\n",
      "audio. Effectiveness of the proposed countermeasure scheme is evaluated on\n",
      "original speech as well as corresponding replayed recordings. The replay attack\n",
      "recordings are successfully injected into the Google Home device via Amazon\n",
      "Alexa using the drop-in conferencing feature. \n",
      "\n",
      "\n",
      "Home Digital Voice Assistants (HDVAs) are getting popular in recent years.\n",
      "Users can control smart devices and get living assistance through those HDVAs\n",
      "(e.g., Amazon Alexa, Google Home) using voice. In this work, we study the\n",
      "insecurity of HDVA service by using Amazon Alexa as a case study. We disclose\n",
      "three security vulnerabilities which root in the insecure access control of\n",
      "Alexa services. We then exploit them to devise two proof-of-concept attacks,\n",
      "home burglary and fake order, where the adversary can remotely command the\n",
      "victim's Alexa device to open a door or place an order from Amazon.com. The\n",
      "insecure access control is that the Alexa device not only relies on a\n",
      "single-factor authentication but also takes voice commands even if no people\n",
      "are around. We thus argue that HDVAs should have another authentication factor,\n",
      "a physical presence based access control; that is, they can accept voice\n",
      "commands only when any person is detected nearby. To this end, we devise a\n",
      "Virtual Security Button (VSButton), which leverages the WiFi technology to\n",
      "detect indoor human motions. Once any indoor human motion is detected, the\n",
      "Alexa device is enabled to accept voice commands. Our evaluation results show\n",
      "that it can effectively differentiate indoor motions from the cases of no\n",
      "motion and outdoor motions in both the laboratory and real world settings. \n",
      "\n",
      "\n",
      "The telephony over IP (ToIP) is becoming a new trend in technology widely\n",
      "used nowadays in almost all business sectors. Its concepts rely on transiting\n",
      "the telephone communications through the IP network. Today, this technology is\n",
      "deployed increasingly what the cause of emergence of companies is offering this\n",
      "service as Switzernet. For several highly demanded destinations, recently fake\n",
      "vendors appeared in the market offering voice termination but providing only\n",
      "false answer supervision. The answered signal is returned immediately and calls\n",
      "are being charged without being connected. Different techniques are used to\n",
      "keep the calling party on the line. One of these techniques is to play a record\n",
      "of a ring back tone (while the call is already being charged). Another, more\n",
      "sophisticated technique is to play a human voice randomly picked up from a set\n",
      "of records containing contents similar to: hello, hello, I cannot hear you\n",
      "Apart the fact that the fallaciously established calls are charged at rates of\n",
      "real calls, such malicious routes seriously handicap the switching process. The\n",
      "system does not detect a failure on signaling level and is unable to attempt\n",
      "the call via backup routes, the call technically being already connected. Once\n",
      "the call flow falls into such trap, the calls will continue being routed via\n",
      "the fraudulent route until a manual intervention. \n",
      "\n",
      "\n",
      "Transferring knowledge from an image synthesis model trained on a large\n",
      "dataset is a promising direction for learning generative image models from\n",
      "various domains efficiently. While previous works have studied GAN models, we\n",
      "present a recipe for learning vision transformers by generative knowledge\n",
      "transfer. We base our framework on state-of-the-art generative vision\n",
      "transformers that represent an image as a sequence of visual tokens to the\n",
      "autoregressive or non-autoregressive transformers. To adapt to a new domain, we\n",
      "employ prompt tuning, which prepends learnable tokens called prompt to the\n",
      "image token sequence, and introduce a new prompt design for our task. We study\n",
      "on a variety of visual domains, including visual task adaptation\n",
      "benchmark~\\cite{zhai2019large}, with varying amount of training images, and\n",
      "show effectiveness of knowledge transfer and a significantly better image\n",
      "generation quality over existing works. \n",
      "\n",
      "\n",
      "The field of image synthesis has made great strides in the last couple of\n",
      "years. Recent models are capable of generating images with astonishing quality.\n",
      "Fine-grained evaluation of these models on some interesting categories such as\n",
      "faces is still missing. Here, we conduct a quantitative comparison of three\n",
      "popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their\n",
      "ability to generate photorealistic faces in the wild. We find that Stable\n",
      "Diffusion generates better faces than the other systems, according to the FID\n",
      "score. We also introduce a dataset of generated faces in the wild dubbed GFW,\n",
      "including a total of 15,076 faces. Furthermore, we hope that our study spurs\n",
      "follow-up research in assessing the generative models and improving them. Data\n",
      "and code are available at data and code, respectively. \n",
      "\n",
      "\n",
      "3D-aware image synthesis aims at learning a generative model that can render\n",
      "photo-realistic 2D images while capturing decent underlying 3D shapes. A\n",
      "popular solution is to adopt the generative adversarial network (GAN) and\n",
      "replace the generator with a 3D renderer, where volume rendering with neural\n",
      "radiance field (NeRF) is commonly used. Despite the advancement of synthesis\n",
      "quality, existing methods fail to obtain moderate 3D shapes. We argue that,\n",
      "considering the two-player game in the formulation of GANs, only making the\n",
      "generator 3D-aware is not enough. In other words, displacing the generative\n",
      "mechanism only offers the capability, but not the guarantee, of producing\n",
      "3D-aware images, because the supervision of the generator primarily comes from\n",
      "the discriminator. To address this issue, we propose GeoD through learning a\n",
      "geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides\n",
      "differentiating real and fake samples from the 2D image space, the\n",
      "discriminator is additionally asked to derive the geometry information from the\n",
      "inputs, which is then applied as the guidance of the generator. Such a simple\n",
      "yet effective design facilitates learning substantially more accurate 3D\n",
      "shapes. Extensive experiments on various generator architectures and training\n",
      "datasets verify the superiority of GeoD over state-of-the-art alternatives.\n",
      "Moreover, our approach is registered as a general framework such that a more\n",
      "capable discriminator (i.e., with a third task of novel view synthesis beyond\n",
      "domain classification and geometry extraction) can further assist the generator\n",
      "with a better multi-view consistency. \n",
      "\n",
      "\n",
      "Offline reinforcement learning (Offline RL) suffers from the innate\n",
      "distributional shift as it cannot interact with the physical environment during\n",
      "training. To alleviate such limitation, state-based offline RL leverages a\n",
      "learned dynamics model from the logged experience and augments the predicted\n",
      "state transition to extend the data distribution. For exploiting such benefit\n",
      "also on the image-based RL, we firstly propose a generative model, S2P\n",
      "(State2Pixel), which synthesizes the raw pixel of the agent from its\n",
      "corresponding state. It enables bridging the gap between the state and the\n",
      "image domain in RL algorithms, and virtually exploring unseen image\n",
      "distribution via model-based transition in the state space. Through\n",
      "experiments, we confirm that our S2P-based image synthesis not only improves\n",
      "the image-based offline RL performance but also shows powerful generalization\n",
      "capability on unseen tasks. \n",
      "\n",
      "\n",
      "Recent breakthroughs in text-to-image synthesis have been driven by diffusion\n",
      "models trained on billions of image-text pairs. Adapting this approach to 3D\n",
      "synthesis would require large-scale datasets of labeled 3D data and efficient\n",
      "architectures for denoising 3D data, neither of which currently exist. In this\n",
      "work, we circumvent these limitations by using a pretrained 2D text-to-image\n",
      "diffusion model to perform text-to-3D synthesis. We introduce a loss based on\n",
      "probability density distillation that enables the use of a 2D diffusion model\n",
      "as a prior for optimization of a parametric image generator. Using this loss in\n",
      "a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a\n",
      "Neural Radiance Field, or NeRF) via gradient descent such that its 2D\n",
      "renderings from random angles achieve a low loss. The resulting 3D model of the\n",
      "given text can be viewed from any angle, relit by arbitrary illumination, or\n",
      "composited into any 3D environment. Our approach requires no 3D training data\n",
      "and no modifications to the image diffusion model, demonstrating the\n",
      "effectiveness of pretrained image diffusion models as priors. \n",
      "\n",
      "\n",
      "Synthetic data generated by generative models can enhance the performance and\n",
      "capabilities of data-hungry deep learning models in medical imaging. However,\n",
      "there is (1) limited availability of (synthetic) datasets and (2) generative\n",
      "models are complex to train, which hinders their adoption in research and\n",
      "clinical applications. To reduce this entry barrier, we propose medigan, a\n",
      "one-stop shop for pretrained generative models implemented as an open-source\n",
      "framework-agnostic Python library. medigan allows researchers and developers to\n",
      "create, increase, and domain-adapt their training data in just a few lines of\n",
      "code. Guided by design decisions based on gathered end-user requirements, we\n",
      "implement medigan based on modular components for generative model (i)\n",
      "execution, (ii) visualisation, (iii) search & ranking, and (iv) contribution.\n",
      "The library's scalability and design is demonstrated by its growing number of\n",
      "integrated and readily-usable pretrained generative models consisting of 21\n",
      "models utilising 9 different Generative Adversarial Network architectures\n",
      "trained on 11 datasets from 4 domains, namely, mammography, endoscopy, x-ray,\n",
      "and MRI. Furthermore, 3 applications of medigan are analysed in this work,\n",
      "which include (a) enabling community-wide sharing of restricted data, (b)\n",
      "investigating generative model evaluation metrics, and (c) improving clinical\n",
      "downstream tasks. In (b), extending on common medical image synthesis\n",
      "assessment and reporting standards, we show Fr\\'echet Inception Distance\n",
      "variability based on image normalisation and radiology-specific feature\n",
      "extraction. \n",
      "\n",
      "\n",
      "MRI and CT are most widely used medical imaging modalities. It is often\n",
      "necessary to acquire multi-modality images for diagnosis and treatment such as\n",
      "radiotherapy planning. However, multi-modality imaging is not only costly but\n",
      "also introduces misalignment between MRI and CT images. To address this\n",
      "challenge, computational conversion is a viable approach between MRI and CT\n",
      "images, especially from MRI to CT images. In this paper, we propose to use an\n",
      "emerging deep learning framework called diffusion and score-matching models in\n",
      "this context. Specifically, we adapt denoising diffusion probabilistic and\n",
      "score-matching models, use four different sampling strategies, and compare\n",
      "their performance metrics with that using a convolutional neural network and a\n",
      "generative adversarial network model. Our results show that the diffusion and\n",
      "score-matching models generate better synthetic CT images than the CNN and GAN\n",
      "models. Furthermore, we investigate the uncertainties associated with the\n",
      "diffusion and score-matching networks using the Monte-Carlo method, and improve\n",
      "the results by averaging their Monte-Carlo outputs. Our study suggests that\n",
      "diffusion and score-matching models are powerful to generate high quality\n",
      "images conditioned on an image obtained using a complementary imaging modality,\n",
      "analytically rigorous with clear explainability, and highly competitive with\n",
      "CNNs and GANs for image synthesis. \n",
      "\n",
      "\n",
      "Regularized Maximum Likelihood (RML) techniques are a class of image\n",
      "synthesis methods that have the potential to achieve improved angular\n",
      "resolution and image fidelity compared to traditional image synthesis methods\n",
      "like CLEAN when applied to sub-mm interferometric observations. We used the\n",
      "GPU-accelerated open source Python package MPoL to explore the influence of\n",
      "various RML prior distributions (maximum entropy, sparsity, total variation,\n",
      "and total squared variation) on images reconstructed from ALMA continuum\n",
      "observations of the protoplanetary disk hosted by HD 143006. We developed a\n",
      "K-fold process for the image validation procedure cross-validation (CV) and\n",
      "explored both uniform and \"dartboard\" styles of visibility sampling within the\n",
      "validation process. Using CV to find optimal hyperparameter values for the test\n",
      "case of total squared variation regularization, we discovered that a wide range\n",
      "of hyperparameter values (spanning roughly an order of magnitude) correspond to\n",
      "models with strong predictive power for visibilities across unsampled or\n",
      "sparsely sampled spatial frequencies. We also provide a comparison of RML and\n",
      "CLEAN images for the protoplanetary disk around HD 143006, finding that RML\n",
      "imaging improves the spatial resolution of the image by about a factor of 3.\n",
      "Lastly, we distill general recommendations for building an RML workflow for\n",
      "image synthesis of ALMA protoplanetary disk observations, including\n",
      "recommendations for incorporating CV most effectively. Using RML methods to\n",
      "improve the resolution of protoplanetary disk observations will enable new\n",
      "science requiring high resolution images, including the detection of\n",
      "protoplanets embedded within disks. \n",
      "\n",
      "\n",
      "We offer a method for one-shot image synthesis that allows controlling\n",
      "manipulations of a single image by inverting a quasi-robust classifier equipped\n",
      "with strong regularizers. Our proposed method, entitled Magic, samples\n",
      "structured gradients from a pre-trained quasi-robust classifier to better\n",
      "preserve the input semantics while preserving its classification accuracy,\n",
      "thereby guaranteeing credibility in the synthesis. Unlike current methods that\n",
      "use complex primitives to supervise the process or use attention maps as a weak\n",
      "supervisory signal, Magic aggregates gradients over the input, driven by a\n",
      "guide binary mask that enforces a strong, spatial prior. Magic implements a\n",
      "series of manipulations with a single framework achieving shape and location\n",
      "control, intense non-rigid shape deformations, and copy/move operations in the\n",
      "presence of repeating objects and gives users firm control over the synthesis\n",
      "by requiring simply specifying binary guide masks. Our study and findings are\n",
      "supported by various qualitative comparisons with the state-of-the-art on the\n",
      "same images sampled from ImageNet and quantitative analysis using machine\n",
      "perception along with a user survey of 100+ participants that endorse our\n",
      "synthesis quality. \n",
      "\n",
      "\n",
      "Discriminator plays a vital role in training generative adversarial networks\n",
      "(GANs) via distinguishing real and synthesized samples. While the real data\n",
      "distribution remains the same, the synthesis distribution keeps varying because\n",
      "of the evolving generator, and thus effects a corresponding change to the\n",
      "bi-classification task for the discriminator. We argue that a discriminator\n",
      "with an on-the-fly adjustment on its capacity can better accommodate such a\n",
      "time-varying task. A comprehensive empirical study confirms that the proposed\n",
      "training strategy, termed as DynamicD, improves the synthesis performance\n",
      "without incurring any additional computation cost or training objectives. Two\n",
      "capacity adjusting schemes are developed for training GANs under different data\n",
      "regimes: i) given a sufficient amount of training data, the discriminator\n",
      "benefits from a progressively increased learning capacity, and ii) when the\n",
      "training data is limited, gradually decreasing the layer width mitigates the\n",
      "over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware\n",
      "image synthesis tasks conducted on a range of datasets substantiate the\n",
      "generalizability of our DynamicD as well as its substantial improvement over\n",
      "the baselines. Furthermore, DynamicD is synergistic to other\n",
      "discriminator-improving approaches (including data augmentation, regularizers,\n",
      "and pre-training), and brings continuous performance gain when combined for\n",
      "learning GANs. \n",
      "\n",
      "\n",
      "In order to achieve good performance and generalisability, medical image\n",
      "segmentation models should be trained on sizeable datasets with sufficient\n",
      "variability. Due to ethics and governance restrictions, and the costs\n",
      "associated with labelling data, scientific development is often stifled, with\n",
      "models trained and tested on limited data. Data augmentation is often used to\n",
      "artificially increase the variability in the data distribution and improve\n",
      "model generalisability. Recent works have explored deep generative models for\n",
      "image synthesis, as such an approach would enable the generation of an\n",
      "effectively infinite amount of varied data, addressing the generalisability and\n",
      "data access problems. However, many proposed solutions limit the user's control\n",
      "over what is generated. In this work, we propose brainSPADE, a model which\n",
      "combines a synthetic diffusion-based label generator with a semantic image\n",
      "generator. Our model can produce fully synthetic brain labels on-demand, with\n",
      "or without pathology of interest, and then generate a corresponding MRI image\n",
      "of an arbitrary guided style. Experiments show that brainSPADE synthetic data\n",
      "can be used to train segmentation models with performance comparable to that of\n",
      "models trained on real data. \n",
      "\n",
      "\n",
      "Joint synthesis of images and segmentation masks with generative adversarial\n",
      "networks (GANs) is promising to reduce the effort needed for collecting image\n",
      "data with pixel-wise annotations. However, to learn high-fidelity image-mask\n",
      "synthesis, existing GAN approaches first need a pre-training phase requiring\n",
      "large amounts of image data, which limits their utilization in restricted image\n",
      "domains. In this work, we take a step to reduce this limitation, introducing\n",
      "the task of one-shot image-mask synthesis. We aim to generate diverse images\n",
      "and their segmentation masks given only a single labelled example, and\n",
      "assuming, contrary to previous models, no access to any pre-training data. To\n",
      "this end, inspired by the recent architectural developments of single-image\n",
      "GANs, we introduce our OSMIS model which enables the synthesis of segmentation\n",
      "masks that are precisely aligned to the generated images in the one-shot\n",
      "regime. Besides achieving the high fidelity of generated masks, OSMIS\n",
      "outperforms state-of-the-art single-image GAN models in image synthesis quality\n",
      "and diversity. In addition, despite not using any additional data, OSMIS\n",
      "demonstrates an impressive ability to serve as a source of useful data\n",
      "augmentation for one-shot segmentation applications, providing performance\n",
      "gains that are complementary to standard data augmentation techniques. Code is\n",
      "available at https://github.com/ boschresearch/one-shot-synthesis \n",
      "\n",
      "\n",
      "Recent advances in text-to-image synthesis have led to large pretrained\n",
      "transformers with excellent capabilities to generate visualizations from a\n",
      "given text. However, these models are ill-suited for specialized tasks like\n",
      "story visualization, which requires an agent to produce a sequence of images\n",
      "given a corresponding sequence of captions, forming a narrative. Moreover, we\n",
      "find that the story visualization task fails to accommodate generalization to\n",
      "unseen plots and characters in new narratives. Hence, we first propose the task\n",
      "of story continuation, where the generated visual story is conditioned on a\n",
      "source image, allowing for better generalization to narratives with new\n",
      "characters. Then, we enhance or 'retro-fit' the pretrained text-to-image\n",
      "synthesis models with task-specific modules for (a) sequential image generation\n",
      "and (b) copying relevant elements from an initial frame. Then, we explore\n",
      "full-model finetuning, as well as prompt-based tuning for parameter-efficient\n",
      "adaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on\n",
      "two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset\n",
      "DiDeMoSV collected from a video-captioning dataset. We also develop a model\n",
      "StoryGANc based on Generative Adversarial Networks (GAN) for story\n",
      "continuation, and compare it with the StoryDALL-E model to demonstrate the\n",
      "advantages of our approach. We show that our retro-fitting approach outperforms\n",
      "GAN-based models for story continuation and facilitates copying of visual\n",
      "elements from the source image, thereby improving continuity in the generated\n",
      "visual story. Finally, our analysis suggests that pretrained transformers\n",
      "struggle to comprehend narratives containing several characters. Overall, our\n",
      "work demonstrates that pretrained text-to-image synthesis models can be adapted\n",
      "for complex and low-resource tasks like story continuation. \n",
      "\n",
      "\n",
      "Multiple Sclerosis (MS) is a chronic progressive neurological disease\n",
      "characterized by the development of lesions in the white matter of the brain.\n",
      "T2-fluid-attenuated inversion recovery (FLAIR) brain magnetic resonance imaging\n",
      "(MRI) provides superior visualization and characterization of MS lesions,\n",
      "relative to other MRI modalities. Longitudinal brain FLAIR MRI in MS, involving\n",
      "repetitively imaging a patient over time, provides helpful information for\n",
      "clinicians towards monitoring disease progression. Predicting future whole\n",
      "brain MRI examinations with variable time lag has only been attempted in\n",
      "limited applications, such as healthy aging and structural degeneration in\n",
      "Alzheimer's Disease. In this article, we present novel modifications to deep\n",
      "learning architectures for MS FLAIR image synthesis, in order to support\n",
      "prediction of longitudinal images in a flexible continuous way. This is\n",
      "achieved with learned transposed convolutions, which support modelling time as\n",
      "a spatially distributed array with variable temporal properties at different\n",
      "spatial locations. Thus, this approach can theoretically model\n",
      "spatially-specific time-dependent brain development, supporting the modelling\n",
      "of more rapid growth at appropriate physical locations, such as the site of an\n",
      "MS brain lesion. This approach also supports the clinician user to define how\n",
      "far into the future a predicted examination should target. Accurate prediction\n",
      "of future rounds of imaging can inform clinicians of potentially poor patient\n",
      "outcomes, which may be able to contribute to earlier treatment and better\n",
      "prognoses. Four distinct deep learning architectures have been developed. The\n",
      "ISBI2015 longitudinal MS dataset was used to validate and compare our proposed\n",
      "approaches. Results demonstrate that a modified ACGAN achieves the best\n",
      "performance and reduces variability in model accuracy. \n",
      "\n",
      "\n",
      "We propose a method for synthesizing cardiac MR images with plausible heart\n",
      "shapes and realistic appearances for the purpose of generating labeled data for\n",
      "deep-learning (DL) training. It breaks down the image synthesis into label\n",
      "deformation and label-to-image translation tasks. The former is achieved via\n",
      "latent space interpolation in a VAE model, while the latter is accomplished via\n",
      "a conditional GAN model. We devise an approach for label manipulation in the\n",
      "latent space of the trained VAE model, namely pathology synthesis, aiming to\n",
      "synthesize a series of pseudo-pathological synthetic subjects with\n",
      "characteristics of a desired heart disease. Furthermore, we propose to model\n",
      "the relationship between 2D slices in the latent space of the VAE via\n",
      "estimating the correlation coefficient matrix between the latent vectors and\n",
      "utilizing it to correlate elements of randomly drawn samples before decoding to\n",
      "image space. This simple yet effective approach results in generating 3D\n",
      "consistent subjects from 2D slice-by-slice generations. Such an approach could\n",
      "provide a solution to diversify and enrich the available database of cardiac MR\n",
      "images and to pave the way for the development of generalizable DL-based image\n",
      "analysis algorithms. The code will be available at\n",
      "https://github.com/sinaamirrajab/CardiacPathologySynthesis. \n",
      "\n",
      "\n",
      "3D-aware generative models have demonstrated their superb performance to\n",
      "generate 3D neural radiance fields (NeRF) from a collection of monocular 2D\n",
      "images even for topology-varying object categories. However, these methods\n",
      "still lack the capability to separately control the shape and appearance of the\n",
      "objects in the generated radiance fields. In this paper, we propose a\n",
      "generative model for synthesizing radiance fields of topology-varying objects\n",
      "with disentangled shape and appearance variations. Our method generates\n",
      "deformable radiance fields, which builds the dense correspondence between the\n",
      "density fields of the objects and encodes their appearances in a shared\n",
      "template field. Our disentanglement is achieved in an unsupervised manner\n",
      "without introducing extra labels to previous 3D-aware GAN training. We also\n",
      "develop an effective image inversion scheme for reconstructing the radiance\n",
      "field of an object in a real monocular image and manipulating its shape and\n",
      "appearance. Experiments show that our method can successfully learn the\n",
      "generative model from unstructured monocular images and well disentangle the\n",
      "shape and appearance for objects (e.g., chairs) with large topological\n",
      "variance. The model trained on synthetic data can faithfully reconstruct the\n",
      "real object in a given single image and achieve high-quality texture and shape\n",
      "editing results. \n",
      "\n",
      "\n",
      "Semantic image synthesis enables control over unconditional image generation\n",
      "by allowing guidance on what is being generated. We conditionally synthesize\n",
      "the latent space from a vector quantized model (VQ-model) pre-trained to\n",
      "autoencode images. Instead of training an autoregressive Transformer on\n",
      "separately learned conditioning latents and image latents, we find that jointly\n",
      "learning the conditioning and image latents significantly improves the modeling\n",
      "capabilities of the Transformer model. While our jointly trained VQ-model\n",
      "achieves a similar reconstruction performance to a vanilla VQ-model for both\n",
      "semantic and image latents, tying the two modalities at the autoencoding stage\n",
      "proves to be an important ingredient to improve autoregressive modeling\n",
      "performance. We show that our model improves semantic image synthesis using\n",
      "autoregressive models on popular semantic image datasets ADE20k, Cityscapes and\n",
      "COCO-Stuff. \n",
      "\n",
      "\n",
      "Scene-text image synthesis techniques aimed at naturally composing text\n",
      "instances on background scene images are very appealing for training deep\n",
      "neural networks because they can provide accurate and comprehensive annotation\n",
      "information. Prior studies have explored generating synthetic text images on\n",
      "two-dimensional and three-dimensional surfaces based on rules derived from\n",
      "real-world observations. Some of these studies have proposed generating\n",
      "scene-text images from learning; however, owing to the absence of a suitable\n",
      "training dataset, unsupervised frameworks have been explored to learn from\n",
      "existing real-world data, which may not result in a robust performance. To ease\n",
      "this dilemma and facilitate research on learning-based scene text synthesis, we\n",
      "propose DecompST, a real-world dataset prepared using public benchmarks, with\n",
      "three types of annotations: quadrilateral-level BBoxes, stroke-level text\n",
      "masks, and text-erased images. Using the DecompST dataset, we propose an image\n",
      "synthesis engine that includes a text location proposal network (TLPNet) and a\n",
      "text appearance adaptation network (TAANet). TLPNet first predicts the suitable\n",
      "regions for text embedding. TAANet then adaptively changes the geometry and\n",
      "color of the text instance according to the context of the background. Our\n",
      "comprehensive experiments verified the effectiveness of the proposed method for\n",
      "generating pretraining data for scene text detectors. \n",
      "\n",
      "\n",
      "Existing deep networks for histopathology image synthesis cannot generate\n",
      "accurate boundaries for clustered nuclei and cannot output image styles that\n",
      "align with different organs. To address these issues, we propose a style-guided\n",
      "instance-adaptive normalization (SIAN) to synthesize realistic color\n",
      "distributions and textures for different organs. SIAN contains four phases,\n",
      "semantization, stylization, instantiation, and modulation. The four phases work\n",
      "together and are integrated into a generative network to embed image semantics,\n",
      "style, and instance-level boundaries. Experimental results demonstrate the\n",
      "effectiveness of all components in SIAN, and show that the proposed method\n",
      "outperforms the state-of-the-art conditional GANs for histopathology image\n",
      "synthesis using the Frechet Inception Distance (FID), structural similarity\n",
      "Index (SSIM), detection quality(DQ), segmentation quality(SQ), and panoptic\n",
      "quality(PQ). Furthermore, the performance of a segmentation network could be\n",
      "significantly improved by incorporating synthetic images generated using SIAN. \n",
      "\n",
      "\n",
      "Diffusion models (DMs) have shown great potential for high-quality image\n",
      "synthesis. However, when it comes to producing images with complex scenes, how\n",
      "to properly describe both image global structures and object details remains a\n",
      "challenging task. In this paper, we present Frido, a Feature Pyramid Diffusion\n",
      "model performing a multi-scale coarse-to-fine denoising process for image\n",
      "synthesis. Our model decomposes an input image into scale-dependent vector\n",
      "quantized features, followed by a coarse-to-fine gating for producing image\n",
      "output. During the above multi-scale representation learning stage, additional\n",
      "input conditions like text, scene graph, or image layout can be further\n",
      "exploited. Thus, Frido can be also applied for conditional or cross-modality\n",
      "image synthesis. We conduct extensive experiments over various unconditioned\n",
      "and conditional image generation tasks, ranging from text-to-image synthesis,\n",
      "layout-to-image, scene-graph-to-image, to label-to-image. More specifically, we\n",
      "achieved state-of-the-art FID scores on five benchmarks, namely layout-to-image\n",
      "on COCO and OpenImages, scene-graph-to-image on COCO and Visual Genome, and\n",
      "label-to-image on COCO. Code is available at\n",
      "https://github.com/davidhalladay/Frido. \n",
      "\n",
      "\n",
      "Generating images from hand-drawings is a crucial and fundamental task in\n",
      "content creation. The translation is difficult as there exist infinite\n",
      "possibilities and the different users usually expect different outcomes.\n",
      "Therefore, we propose a unified framework supporting a three-dimensional\n",
      "control over the image synthesis from sketches and strokes based on diffusion\n",
      "models. Users can not only decide the level of faithfulness to the input\n",
      "strokes and sketches, but also the degree of realism, as the user inputs are\n",
      "usually not consistent with the real images. Qualitative and quantitative\n",
      "experiments demonstrate that our framework achieves state-of-the-art\n",
      "performance while providing flexibility in generating customized images with\n",
      "control over shape, color, and realism. Moreover, our method unleashes\n",
      "applications such as editing on real images, generation with partial sketches\n",
      "and strokes, and multi-domain multi-modal synthesis. \n",
      "\n",
      "\n",
      "Cross-modality image synthesis is an active research topic with multiple\n",
      "medical clinically relevant applications. Recently, methods allowing training\n",
      "with paired but misaligned data have started to emerge. However, no robust and\n",
      "well-performing methods applicable to a wide range of real world data sets\n",
      "exist. In this work, we propose a generic solution to the problem of\n",
      "cross-modality image synthesis with paired but non-aligned data by introducing\n",
      "new deformation equivariance encouraging loss functions. The method consists of\n",
      "joint training of an image synthesis network together with separate\n",
      "registration networks and allows adversarial training conditioned on the input\n",
      "even with misaligned data. The work lowers the bar for new clinical\n",
      "applications by allowing effortless training of cross-modality image synthesis\n",
      "networks for more difficult data sets and opens up opportunities for the\n",
      "development of new generic learning based cross-modality registration\n",
      "algorithms. \n",
      "\n",
      "\n",
      "Visual counterfeits are increasingly causing an existential conundrum in\n",
      "mainstream media with rapid evolution in neural image synthesis methods. Though\n",
      "detection of such counterfeits has been a taxing problem in the image forensics\n",
      "community, a recent class of forensic detectors -- universal detectors -- are\n",
      "able to surprisingly spot counterfeit images regardless of generator\n",
      "architectures, loss functions, training datasets, and resolutions. This\n",
      "intriguing property suggests the possible existence of transferable forensic\n",
      "features (T-FF) in universal detectors. In this work, we conduct the first\n",
      "analytical study to discover and understand T-FF in universal detectors. Our\n",
      "contributions are 2-fold: 1) We propose a novel forensic feature relevance\n",
      "statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2)\n",
      "Our qualitative and quantitative investigations uncover an unexpected finding:\n",
      "color is a critical T-FF in universal detectors. Code and models are available\n",
      "at https://keshik6.github.io/transferable-forensic-features/ \n",
      "\n",
      "\n",
      "Face image synthesis has progressed beyond the point at which humans can\n",
      "effectively distinguish authentic faces from synthetically generated ones.\n",
      "Recently developed synthetic face image detectors boast \"better-than-human\"\n",
      "discriminative ability, especially those guided by human perceptual\n",
      "intelligence during the model's training process. In this paper, we investigate\n",
      "whether these human-guided synthetic face detectors can assist non-expert human\n",
      "operators in the task of synthetic image detection when compared to models\n",
      "trained without human-guidance. We conducted a large-scale experiment with more\n",
      "than 1,560 subjects classifying whether an image shows an authentic or\n",
      "synthetically-generated face, and annotate regions that supported their\n",
      "decisions. In total, 56,015 annotations across 3,780 unique face images were\n",
      "collected. All subjects first examined samples without any AI support, followed\n",
      "by samples given (a) the AI's decision (\"synthetic\" or \"authentic\"), (b) class\n",
      "activation maps illustrating where the model deems salient for its decision, or\n",
      "(c) both the AI's decision and AI's saliency map. Synthetic faces were\n",
      "generated with six modern Generative Adversarial Networks. Interesting\n",
      "observations from this experiment include: (1) models trained with\n",
      "human-guidance offer better support to human examination of face images when\n",
      "compared to models trained traditionally using cross-entropy loss, (2) binary\n",
      "decisions presented to humans offers better support than saliency maps, (3)\n",
      "understanding the AI's accuracy helps humans to increase trust in a given model\n",
      "and thus increase their overall accuracy. This work demonstrates that although\n",
      "humans supported by machines achieve better-than-random accuracy of synthetic\n",
      "face detection, the ways of supplying humans with AI support and of building\n",
      "trust are key factors determining high effectiveness of the human-AI tandem. \n",
      "\n",
      "\n",
      "Foreground-aware image synthesis aims to generate images as well as their\n",
      "foreground masks. A common approach is to formulate an image as an masked\n",
      "blending of a foreground image and a background image. It is a challenging\n",
      "problem because it is prone to reach the trivial solution where either image\n",
      "overwhelms the other, i.e., the masks become completely full or empty, and the\n",
      "foreground and background are not meaningfully separated. We present FurryGAN\n",
      "with three key components: 1) imposing both the foreground image and the\n",
      "composite image to be realistic, 2) designing a mask as a combination of coarse\n",
      "and fine masks, and 3) guiding the generator by an auxiliary mask predictor in\n",
      "the discriminator. Our method produces realistic images with remarkably\n",
      "detailed alpha masks which cover hair, fur, and whiskers in a fully\n",
      "unsupervised manner. \n",
      "\n",
      "\n",
      "Text-to-image synthesis aims to generate a photo-realistic and semantic\n",
      "consistent image from a specific text description. The images synthesized by\n",
      "off-the-shelf models usually contain limited components compared with the\n",
      "corresponding image and text description, which decreases the image quality and\n",
      "the textual-visual consistency. To address this issue, we propose a novel\n",
      "Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*,\n",
      "which introduces a dual vision-language matching mechanism to strengthen the\n",
      "image quality and semantic consistency. The dual vision-language matching\n",
      "mechanism considers textual-visual matching between the generated image and the\n",
      "corresponding text description, and visual-visual consistent constraints\n",
      "between the synthesized image and the real image. Given a specific text\n",
      "description, VLMGAN* firstly encodes it into textual features and then feeds\n",
      "them to a dual vision-language matching-based generative model to synthesize a\n",
      "photo-realistic and textual semantic consistent image. Besides, the popular\n",
      "evaluation metrics for text-to-image synthesis are borrowed from simple image\n",
      "generation, which mainly evaluates the reality and diversity of the synthesized\n",
      "images. Therefore, we introduce a metric named Vision-Language Matching Score\n",
      "(VLMS) to evaluate the performance of text-to-image synthesis which can\n",
      "consider both the image quality and the semantic consistency between\n",
      "synthesized image and the description. The proposed dual multi-level\n",
      "vision-language matching strategy can be applied to other text-to-image\n",
      "synthesis methods. We implement this strategy on two popular baselines, which\n",
      "are marked with ${\\text{VLMGAN}_{+\\text{AttnGAN}}}$ and\n",
      "${\\text{VLMGAN}_{+\\text{DFGAN}}}$. The experimental results on two widely-used\n",
      "datasets show that the model achieves significant improvements over other\n",
      "state-of-the-art methods. \n",
      "\n",
      "\n",
      "Denoising diffusion probabilistic models (DDPMs) are a recent family of\n",
      "generative models that achieve state-of-the-art results. In order to obtain\n",
      "class-conditional generation, it was suggested to guide the diffusion process\n",
      "by gradients from a time-dependent classifier. While the idea is theoretically\n",
      "sound, deep learning-based classifiers are infamously susceptible to\n",
      "gradient-based adversarial attacks. Therefore, while traditional classifiers\n",
      "may achieve good accuracy scores, their gradients are possibly unreliable and\n",
      "might hinder the improvement of the generation results. Recent work discovered\n",
      "that adversarially robust classifiers exhibit gradients that are aligned with\n",
      "human perception, and these could better guide a generative process towards\n",
      "semantically meaningful images. We utilize this observation by defining and\n",
      "training a time-dependent adversarially robust classifier and use it as\n",
      "guidance for a generative diffusion model. In experiments on the highly\n",
      "challenging and diverse ImageNet dataset, our scheme introduces significantly\n",
      "more intelligible intermediate gradients, better alignment with theoretical\n",
      "findings, as well as improved generation results under several evaluation\n",
      "metrics. Furthermore, we conduct an opinion survey whose findings indicate that\n",
      "human raters prefer our method's results. \n",
      "\n",
      "\n",
      "The vast progress in synthetic image synthesis enables the generation of\n",
      "facial images in high resolution and photorealism. In biometric applications,\n",
      "the main motivation for using synthetic data is to solve the shortage of\n",
      "publicly-available biometric data while reducing privacy risks when processing\n",
      "such sensitive information. These advantages are exploited in this work by\n",
      "simulating human face ageing with recent face age modification algorithms to\n",
      "generate mated samples, thereby studying the impact of ageing on the\n",
      "performance of an open-source biometric recognition system. Further, a real\n",
      "dataset is used to evaluate the effects of short-term ageing, comparing the\n",
      "biometric performance to the synthetic domain. The main findings indicate that\n",
      "short-term ageing in the range of 1-5 years has only minor effects on the\n",
      "general recognition performance. However, the correct verification of mated\n",
      "faces with long-term age differences beyond 20 years poses still a significant\n",
      "challenge and requires further investigation. \n",
      "\n",
      "\n",
      "Controllable image synthesis with user scribbles is a topic of keen interest\n",
      "in the computer vision community. In this paper, for the first time we study\n",
      "the problem of photorealistic image synthesis from incomplete and primitive\n",
      "human paintings. In particular, we propose a novel approach paint2pix, which\n",
      "learns to predict (and adapt) \"what a user wants to draw\" from rudimentary\n",
      "brushstroke inputs, by learning a mapping from the manifold of incomplete human\n",
      "paintings to their realistic renderings. When used in conjunction with recent\n",
      "works in autonomous painting agents, we show that paint2pix can be used for\n",
      "progressive image synthesis from scratch. During this process, paint2pix allows\n",
      "a novice user to progressively synthesize the desired image output, while\n",
      "requiring just few coarse user scribbles to accurately steer the trajectory of\n",
      "the synthesis process. Furthermore, we find that our approach also forms a\n",
      "surprisingly convenient approach for real image editing, and allows the user to\n",
      "perform a diverse range of custom fine-grained edits through the addition of\n",
      "only a few well-placed brushstrokes. Supplemental video and demo are available\n",
      "at https://1jsingh.github.io/paint2pix \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are state of the art for image\n",
      "synthesis. Here, we present dapi2ck, a novel GAN-based approach to synthesize\n",
      "cytokeratin (CK) staining from immunofluorescent (IF) DAPI staining of nuclei\n",
      "in non-small cell lung cancer (NSCLC) images. We use the synthetic CK to\n",
      "segment epithelial regions, which, compared to expert annotations, yield\n",
      "equally good results as segmentation on stained CK. Considering the limited\n",
      "number of markers in a multiplexed IF (mIF) panel, our approach allows to\n",
      "replace CK by another marker addressing the complexity of the tumor\n",
      "micro-environment (TME) to facilitate patient selection for immunotherapies. In\n",
      "contrast to stained CK, dapi2ck does not suffer from issues like unspecific CK\n",
      "staining or loss of tumoral CK expression. \n",
      "\n",
      "\n",
      "Human matting refers to extracting human parts from natural images with high\n",
      "quality, including human detail information such as hair, glasses, hat, etc.\n",
      "This technology plays an essential role in image synthesis and visual effects\n",
      "in the film industry. When the green screen is not available, the existing\n",
      "human matting methods need the help of additional inputs (such as trimap,\n",
      "background image, etc.), or the model with high computational cost and complex\n",
      "network structure, which brings great difficulties to the application of human\n",
      "matting in practice. To alleviate such problems, most existing methods (such as\n",
      "MODNet) use multi-branches to pave the way for matting through segmentation,\n",
      "but these methods do not make full use of the image features and only utilize\n",
      "the prediction results of the network as guidance information. Therefore, we\n",
      "propose a module to generate foreground probability map and add it to MODNet to\n",
      "obtain Semantic Guided Matting Net (SGM-Net). Under the condition of only one\n",
      "image, we can realize the human matting task. We verify our method on the\n",
      "P3M-10k dataset. Compared with the benchmark, our method has significantly\n",
      "improved in various evaluation indicators. \n",
      "\n",
      "\n",
      "The crux of text-to-image synthesis stems from the difficulty of preserving\n",
      "the cross-modality semantic consistency between the input text and the\n",
      "synthesized image. Typical methods, which seek to model the text-to-image\n",
      "mapping directly, could only capture keywords in the text that indicates common\n",
      "objects or actions but fail to learn their spatial distribution patterns. An\n",
      "effective way to circumvent this limitation is to generate an image layout as\n",
      "guidance, which is attempted by a few methods. Nevertheless, these methods fail\n",
      "to generate practically effective layouts due to the diversity of input text\n",
      "and object location. In this paper we push for effective modeling in both\n",
      "text-to-layout generation and layout-to-image synthesis. Specifically, we\n",
      "formulate the text-to-layout generation as a sequence-to-sequence modeling\n",
      "task, and build our model upon Transformer to learn the spatial relationships\n",
      "between objects by modeling the sequential dependencies between them. In the\n",
      "stage of layout-to-image synthesis, we focus on learning the textual-visual\n",
      "semantic alignment per object in the layout to precisely incorporate the input\n",
      "text into the layout-to-image synthesizing process. To evaluate the quality of\n",
      "generated layout, we design a new metric specifically, dubbed Layout Quality\n",
      "Score, which considers both the absolute distribution errors of bounding boxes\n",
      "in the layout and the mutual spatial relationships between them. Extensive\n",
      "experiments on three datasets demonstrate the superior performance of our\n",
      "method over state-of-the-art methods on both predicting the layout and\n",
      "synthesizing the image from the given text. \n",
      "\n",
      "\n",
      "Cross-modal fashion image synthesis has emerged as one of the most promising\n",
      "directions in the generation domain due to the vast untapped potential of\n",
      "incorporating multiple modalities and the wide range of fashion image\n",
      "applications. To facilitate accurate generation, cross-modal synthesis methods\n",
      "typically rely on Contrastive Language-Image Pre-training (CLIP) to align\n",
      "textual and garment information. In this work, we argue that simply aligning\n",
      "texture and garment information is not sufficient to capture the semantics of\n",
      "the visual information and therefore propose MaskCLIP. MaskCLIP decomposes the\n",
      "garments into semantic parts, ensuring fine-grained and semantically accurate\n",
      "alignment between the visual and text information. Building on MaskCLIP, we\n",
      "propose ARMANI, a unified cross-modal fashion designer with part-level\n",
      "garment-text alignment. ARMANI discretizes an image into uniform tokens based\n",
      "on a learned cross-modal codebook in its first stage and uses a Transformer to\n",
      "model the distribution of image tokens for a real image given the tokens of the\n",
      "control signals in its second stage. Contrary to prior approaches that also\n",
      "rely on two-stage paradigms, ARMANI introduces textual tokens into the\n",
      "codebook, making it possible for the model to utilize fine-grain semantic\n",
      "information to generate more realistic images. Further, by introducing a\n",
      "cross-modal Transformer, ARMANI is versatile and can accomplish image synthesis\n",
      "from various control signals, such as pure text, sketch images, and partial\n",
      "images. Extensive experiments conducted on our newly collected cross-modal\n",
      "fashion dataset demonstrate that ARMANI generates photo-realistic images in\n",
      "diverse synthesis tasks and outperforms existing state-of-the-art cross-modal\n",
      "image synthesis approaches.Our code is available at\n",
      "https://github.com/Harvey594/ARMANI. \n",
      "\n",
      "\n",
      "The discovery of patient-specific imaging markers that are predictive of\n",
      "future disease outcomes can help us better understand individual-level\n",
      "heterogeneity of disease evolution. In fact, deep learning models that can\n",
      "provide data-driven personalized markers are much more likely to be adopted in\n",
      "medical practice. In this work, we demonstrate that data-driven biomarker\n",
      "discovery can be achieved through a counterfactual synthesis process. We show\n",
      "how a deep conditional generative model can be used to perturb local imaging\n",
      "features in baseline images that are pertinent to subject-specific future\n",
      "disease evolution and result in a counterfactual image that is expected to have\n",
      "a different future outcome. Candidate biomarkers, therefore, result from\n",
      "examining the set of features that are perturbed in this process. Through\n",
      "several experiments on a large-scale, multi-scanner, multi-center multiple\n",
      "sclerosis (MS) clinical trial magnetic resonance imaging (MRI) dataset of\n",
      "relapsing-remitting (RRMS) patients, we demonstrate that our model produces\n",
      "counterfactuals with changes in imaging features that reflect established\n",
      "clinical markers predictive of future MRI lesional activity at the population\n",
      "level. Additional qualitative results illustrate that our model has the\n",
      "potential to discover novel and subject-specific predictive markers of future\n",
      "activity. \n",
      "\n",
      "\n",
      "Recent large-scale text-driven synthesis models have attracted much attention\n",
      "thanks to their remarkable capabilities of generating highly diverse images\n",
      "that follow given text prompts. Such text-based synthesis methods are\n",
      "particularly appealing to humans who are used to verbally describe their\n",
      "intent. Therefore, it is only natural to extend the text-driven image synthesis\n",
      "to text-driven image editing. Editing is challenging for these generative\n",
      "models, since an innate property of an editing technique is to preserve most of\n",
      "the original image, while in the text-based models, even a small modification\n",
      "of the text prompt often leads to a completely different outcome.\n",
      "State-of-the-art methods mitigate this by requiring the users to provide a\n",
      "spatial mask to localize the edit, hence, ignoring the original structure and\n",
      "content within the masked region. In this paper, we pursue an intuitive\n",
      "prompt-to-prompt editing framework, where the edits are controlled by text\n",
      "only. To this end, we analyze a text-conditioned model in depth and observe\n",
      "that the cross-attention layers are the key to controlling the relation between\n",
      "the spatial layout of the image to each word in the prompt. With this\n",
      "observation, we present several applications which monitor the image synthesis\n",
      "by editing the textual prompt only. This includes localized editing by\n",
      "replacing a word, global editing by adding a specification, and even delicately\n",
      "controlling the extent to which a word is reflected in the image. We present\n",
      "our results over diverse images and prompts, demonstrating high-quality\n",
      "synthesis and fidelity to the edited prompts. \n",
      "\n",
      "\n",
      "Controllable person image synthesis task enables a wide range of applications\n",
      "through explicit control over body pose and appearance. In this paper, we\n",
      "propose a cross attention based style distribution module that computes between\n",
      "the source semantic styles and target pose for pose transfer. The module\n",
      "intentionally selects the style represented by each semantic and distributes\n",
      "them according to the target pose. The attention matrix in cross attention\n",
      "expresses the dynamic similarities between the target pose and the source\n",
      "styles for all semantics. Therefore, it can be utilized to route the color and\n",
      "texture from the source image, and is further constrained by the target parsing\n",
      "map to achieve a clearer objective. At the same time, to encode the source\n",
      "appearance accurately, the self attention among different semantic styles is\n",
      "also added. The effectiveness of our model is validated quantitatively and\n",
      "qualitatively on pose transfer and virtual try-on tasks. \n",
      "\n",
      "\n",
      "The capacity to learn incrementally from an online stream of data is an\n",
      "envied trait of human learners, as deep neural networks typically suffer from\n",
      "catastrophic forgetting and stability-plasticity dilemma. Several works have\n",
      "previously explored incremental few-shot learning, a task with greater\n",
      "challenges due to data constraint, mostly in classification setting with mild\n",
      "success. In this work, we study the underrepresented task of generative\n",
      "incremental few-shot learning. To effectively handle the inherent challenges of\n",
      "incremental learning and few-shot learning, we propose a novel framework named\n",
      "ConPro that leverages the two-player nature of GANs. Specifically, we design a\n",
      "conservative generator that preserves past knowledge in parameter and compute\n",
      "efficient manner, and a progressive discriminator that learns to reason\n",
      "semantic distances between past and present task samples, minimizing\n",
      "overfitting with few data points and pursuing good forward transfer. We present\n",
      "experiments to validate the effectiveness of ConPro. \n",
      "\n",
      "\n",
      "In this paper, we present the Multi-Forgery Detection Challenge held\n",
      "concurrently with the IEEE Computer Society Workshop on Biometrics at CVPR\n",
      "2022. Our Multi-Forgery Detection Challenge aims to detect automatic image\n",
      "manipulations including but not limited to image editing, image synthesis,\n",
      "image generation, image photoshop, etc. Our challenge has attracted 674 teams\n",
      "from all over the world, with about 2000 valid result submission counts. We\n",
      "invited the Top 10 teams to present their solutions to the challenge, from\n",
      "which three teams are awarded prizes in the grand finale. In this paper, we\n",
      "present the solutions from the Top 3 teams, in order to boost the research work\n",
      "in the field of image forgery detection. \n",
      "\n",
      "\n",
      "This work presents a novel framework CISFA (Contrastive Image synthesis and\n",
      "Self-supervised Feature Adaptation)that builds on image domain translation and\n",
      "unsupervised feature adaptation for cross-modality biomedical image\n",
      "segmentation. Different from existing works, we use a one-sided generative\n",
      "model and add a weighted patch-wise contrastive loss between sampled patches of\n",
      "the input image and the corresponding synthetic image, which serves as shape\n",
      "constraints. Moreover, we notice that the generated images and input images\n",
      "share similar structural information but are in different modalities. As such,\n",
      "we enforce contrastive losses on the generated images and the input images to\n",
      "train the encoder of a segmentation model to minimize the discrepancy between\n",
      "paired images in the learned embedding space. Compared with existing works that\n",
      "rely on adversarial learning for feature adaptation, such a method enables the\n",
      "encoder to learn domain-independent features in a more explicit way. We\n",
      "extensively evaluate our methods on segmentation tasks containing CT and MRI\n",
      "images for abdominal cavities and whole hearts. Experimental results show that\n",
      "the proposed framework not only outputs synthetic images with less distortion\n",
      "of organ shapes, but also outperforms state-of-the-art domain adaptation\n",
      "methods by a large margin. \n",
      "\n",
      "\n",
      "Novel architectures have recently improved generative image synthesis leading\n",
      "to excellent visual quality in various tasks. Of particular note is the field\n",
      "of ``AI-Art'', which has seen unprecedented growth with the emergence of\n",
      "powerful multimodal models such as CLIP. By combining speech and image\n",
      "synthesis models, so-called ``prompt-engineering'' has become established, in\n",
      "which carefully selected and composed sentences are used to achieve a certain\n",
      "visual style in the synthesized image. In this note, we present an alternative\n",
      "approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set\n",
      "of nearest neighbors is retrieved from an external database during training for\n",
      "each training instance, and the diffusion model is conditioned on these\n",
      "informative samples. During inference (sampling), we replace the retrieval\n",
      "database with a more specialized database that contains, for example, only\n",
      "images of a particular visual style. This provides a novel way to prompt a\n",
      "general trained model after training and thereby specify a particular visual\n",
      "style. As shown by our experiments, this approach is superior to specifying the\n",
      "visual style within the text prompt. We open-source code and model weights at\n",
      "https://github.com/CompVis/latent-diffusion . \n",
      "\n",
      "\n",
      "Obtaining the ground truth labels from a video is challenging since the\n",
      "manual annotation of pixel-wise flow labels is prohibitively expensive and\n",
      "laborious. Besides, existing approaches try to adapt the trained model on\n",
      "synthetic datasets to authentic videos, which inevitably suffers from domain\n",
      "discrepancy and hinders the performance for real-world applications. To solve\n",
      "these problems, we propose RealFlow, an Expectation-Maximization based\n",
      "framework that can create large-scale optical flow datasets directly from any\n",
      "unlabeled realistic videos. Specifically, we first estimate optical flow\n",
      "between a pair of video frames, and then synthesize a new image from this pair\n",
      "based on the predicted flow. Thus the new image pairs and their corresponding\n",
      "flows can be regarded as a new training set. Besides, we design a Realistic\n",
      "Image Pair Rendering (RIPR) module that adopts softmax splatting and\n",
      "bi-directional hole filling techniques to alleviate the artifacts of the image\n",
      "synthesis. In the E-step, RIPR renders new images to create a large quantity of\n",
      "training data. In the M-step, we utilize the generated training data to train\n",
      "an optical flow network, which can be used to estimate optical flows in the\n",
      "next E-step. During the iterative learning steps, the capability of the flow\n",
      "network is gradually improved, so is the accuracy of the flow, as well as the\n",
      "quality of the synthesized dataset. Experimental results show that RealFlow\n",
      "outperforms previous dataset generation methods by a considerably large margin.\n",
      "Moreover, based on the generated dataset, our approach achieves\n",
      "state-of-the-art performance on two standard benchmarks compared with both\n",
      "supervised and unsupervised optical flow methods. Our code and dataset are\n",
      "available at https://github.com/megvii-research/RealFlow \n",
      "\n",
      "\n",
      "Deep generative models have achieved conspicuous progress in realistic image\n",
      "synthesis with multifarious conditional inputs, while generating diverse yet\n",
      "high-fidelity images remains a grand challenge in conditional image generation.\n",
      "This paper presents a versatile framework for conditional image generation\n",
      "which incorporates the inductive bias of CNNs and powerful sequence modeling of\n",
      "auto-regression that naturally leads to diverse image generation. Instead of\n",
      "independently quantizing the features of multiple domains as in prior research,\n",
      "we design an integrated quantization scheme with a variational regularizer that\n",
      "mingles the feature discretization in multiple domains, and markedly boosts the\n",
      "auto-regressive modeling performance. Notably, the variational regularizer\n",
      "enables to regularize feature distributions in incomparable latent spaces by\n",
      "penalizing the intra-domain variations of distributions. In addition, we design\n",
      "a Gumbel sampling strategy that allows to incorporate distribution uncertainty\n",
      "into the auto-regressive training procedure. The Gumbel sampling substantially\n",
      "mitigates the exposure bias that often incurs misalignment between the training\n",
      "and inference stages and severely impairs the inference performance. Extensive\n",
      "experiments over multiple conditional image generation tasks show that our\n",
      "method achieves superior diverse image generation performance qualitatively and\n",
      "quantitatively as compared with the state-of-the-art. \n",
      "\n",
      "\n",
      "Over the years, 2D GANs have achieved great successes in photorealistic\n",
      "portrait generation. However, they lack 3D understanding in the generation\n",
      "process, thus they suffer from multi-view inconsistency problem. To alleviate\n",
      "the issue, many 3D-aware GANs have been proposed and shown notable results, but\n",
      "3D GANs struggle with editing semantic attributes. The controllability and\n",
      "interpretability of 3D GANs have not been much explored. In this work, we\n",
      "propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware\n",
      "GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of\n",
      "discovering semantic attributes during training and controlling them in an\n",
      "unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN\n",
      "to obtain a high-fidelity 3D-controllable generator. Unlike existing\n",
      "latent-based methods allowing implicit pose control, the proposed\n",
      "3D-controllable StyleGAN enables explicit pose control over portrait\n",
      "generation. This distillation allows direct compatibility between 3D control\n",
      "and many StyleGAN-based techniques (e.g., inversion and stylization), and also\n",
      "brings an advantage in terms of computational resources. Our codes are\n",
      "available at https://github.com/jgkwak95/SURF-GAN. \n",
      "\n",
      "\n",
      "Lung nodule detection in chest X-ray (CXR) images is common to early\n",
      "screening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis\n",
      "(CAD) systems can support radiologists for nodule screening in CXR. However, it\n",
      "requires large-scale and diverse medical data with high-quality annotations to\n",
      "train such robust and accurate CADs. To alleviate the limited availability of\n",
      "such datasets, lung nodule synthesis methods are proposed for the sake of data\n",
      "augmentation. Nevertheless, previous methods lack the ability to generate\n",
      "nodules that are realistic with the size attribute desired by the detector. To\n",
      "address this issue, we introduce a novel lung nodule synthesis framework in\n",
      "this paper, which decomposes nodule attributes into three main aspects\n",
      "including shape, size, and texture, respectively. A GAN-based Shape Generator\n",
      "firstly models nodule shapes by generating diverse shape masks. The following\n",
      "Size Modulation then enables quantitative control on the diameters of the\n",
      "generated nodule shapes in pixel-level granularity. A coarse-to-fine gated\n",
      "convolutional Texture Generator finally synthesizes visually plausible nodule\n",
      "textures conditioned on the modulated shape masks. Moreover, we propose to\n",
      "synthesize nodule CXR images by controlling the disentangled nodule attributes\n",
      "for data augmentation, in order to better compensate for the nodules that are\n",
      "easily missed in the detection task. Our experiments demonstrate the enhanced\n",
      "image quality, diversity, and controllability of the proposed lung nodule\n",
      "synthesis framework. We also validate the effectiveness of our data\n",
      "augmentation on greatly improving nodule detection performance. \n",
      "\n",
      "\n",
      "Imputation of missing images via source-to-target modality translation can\n",
      "facilitate downstream tasks in medical imaging. A pervasive approach for\n",
      "synthesizing target images involves one-shot mapping through generative\n",
      "adversarial networks (GAN). Yet, GAN models that implicitly characterize the\n",
      "image distribution can suffer from limited sample fidelity and diversity. Here,\n",
      "we propose a novel method based on adversarial diffusion modeling, SynDiff, for\n",
      "improved reliability in medical image synthesis. To capture a direct correlate\n",
      "of the image distribution, SynDiff leverages a conditional diffusion process to\n",
      "progressively map noise and source images onto the target image. For fast and\n",
      "accurate image sampling during inference, large diffusion steps are coupled\n",
      "with adversarial projections in the reverse diffusion direction. To enable\n",
      "training on unpaired datasets, a cycle-consistent architecture is devised with\n",
      "two coupled diffusion processes to synthesize the target given source and the\n",
      "source given target. Extensive assessments are reported on the utility of\n",
      "SynDiff against competing GAN and diffusion models in multi-contrast MRI and\n",
      "MRI-CT translation. Our demonstrations indicate that SynDiff offers superior\n",
      "performance against competing baselines both qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "Recently, diffusion models have shown remarkable results in image synthesis\n",
      "by gradually removing noise and amplifying signals. Although the simple\n",
      "generative process surprisingly works well, is this the best way to generate\n",
      "image data? For instance, despite the fact that human perception is more\n",
      "sensitive to the low frequencies of an image, diffusion models themselves do\n",
      "not consider any relative importance of each frequency component. Therefore, to\n",
      "incorporate the inductive bias for image data, we propose a novel generative\n",
      "process that synthesizes images in a coarse-to-fine manner. First, we\n",
      "generalize the standard diffusion models by enabling diffusion in a rotated\n",
      "coordinate system with different velocities for each component of the vector.\n",
      "We further propose a blur diffusion as a special case, where each frequency\n",
      "component of an image is diffused at different speeds. Specifically, the\n",
      "proposed blur diffusion consists of a forward process that blurs an image and\n",
      "adds noise gradually, after which a corresponding reverse process deblurs an\n",
      "image and removes noise progressively. Experiments show that the proposed model\n",
      "outperforms the previous method in FID on LSUN bedroom and church datasets.\n",
      "Code is available at https://github.com/sangyun884/blur-diffusion. \n",
      "\n",
      "\n",
      "Based on the seminal work on Array-RQMC methods and rank-1 lattice sequences\n",
      "by Pierre L'Ecuyer and collaborators, we introduce efficient deterministic\n",
      "algorithms for image synthesis. Enumerating a low discrepancy sequence along\n",
      "the Hilbert curve superimposed on the raster of pixels of an image, we achieve\n",
      "noise characteristics that are desirable with respect to the human visual\n",
      "system, especially at very low sampling rates. As compared to the state of the\n",
      "art, our simple algorithms neither require randomization, nor costly\n",
      "optimization, nor lookup tables. We analyze correlations of space-filling\n",
      "curves and low discrepancy sequences, and demonstrate the benefits of the new\n",
      "algorithms in a professional, massively parallel light transport simulation and\n",
      "rendering system. \n",
      "\n",
      "\n",
      "Manipulating latent code in generative adversarial networks (GANs) for facial\n",
      "image synthesis mainly focuses on continuous attribute synthesis (e.g., age,\n",
      "pose and emotion), while discrete attribute synthesis (like face mask and\n",
      "eyeglasses) receives less attention. Directly applying existing works to facial\n",
      "discrete attributes may cause inaccurate results. In this work, we propose an\n",
      "innovative framework to tackle challenging facial discrete attribute synthesis\n",
      "via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly\n",
      "decompose the discrete attribute representation into two components, i.e. the\n",
      "semantic prior basis and offset latent representation. The semantic prior basis\n",
      "shows an initializing direction for manipulating face representation in the\n",
      "latent space. The offset latent presentation obtained by 3D-aware semantic\n",
      "fusion network is proposed to adjust prior basis. In addition, the fusion\n",
      "network integrates 3D embedding for better identity preservation and discrete\n",
      "attribute synthesis. The combination of prior basis and offset latent\n",
      "representation enable our method to synthesize photo-realistic face images with\n",
      "discrete attributes. Notably, we construct a large and valuable dataset MEGN\n",
      "(Face Mask and Eyeglasses images crawled from Google and Naver) for completing\n",
      "the lack of discrete attributes in the existing dataset. Extensive qualitative\n",
      "and quantitative experiments demonstrate the state-of-the-art performance of\n",
      "our method. Our code is available at: https://github.com/MontaEllis/SD-GAN. \n",
      "\n",
      "\n",
      "Synthesizing a realistic image from textual description is a major challenge\n",
      "in computer vision. Current text to image synthesis approaches falls short of\n",
      "producing a highresolution image that represent a text descriptor. Most\n",
      "existing studies rely either on Generative Adversarial Networks (GANs) or\n",
      "Variational Auto Encoders (VAEs). GANs has the capability to produce sharper\n",
      "images but lacks the diversity of outputs, whereas VAEs are good at producing a\n",
      "diverse range of outputs, but the images generated are often blurred. Taking\n",
      "into account the relative advantages of both GANs and VAEs, we proposed a new\n",
      "stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture\n",
      "for synthesizing images conditioned on a text description. This study uses\n",
      "Conditional VAEs as an initial generator to produce a high-level sketch of the\n",
      "text descriptor. This high-level sketch output from first stage and a text\n",
      "descriptor is used as an input to the conditional GAN network. The second stage\n",
      "GAN produces a 256x256 high resolution image. The proposed architecture\n",
      "benefits from a conditioning augmentation and a residual block on the\n",
      "Conditional GAN network to achieve the results. Multiple experiments were\n",
      "conducted using CUB and Oxford-102 dataset and the result of the proposed\n",
      "approach is compared against state-ofthe-art techniques such as StackGAN. The\n",
      "experiments illustrate that the proposed method generates a high-resolution\n",
      "image conditioned on text descriptions and yield competitive results based on\n",
      "Inception and Frechet Inception Score using both datasets \n",
      "\n",
      "\n",
      "We demonstrate a physics-aware transformer for feature-based data fusion from\n",
      "cameras with diverse resolution, color spaces, focal planes, focal lengths, and\n",
      "exposure. We also demonstrate a scalable solution for synthetic training data\n",
      "generation for the transformer using open-source computer graphics software. We\n",
      "demonstrate image synthesis on arrays with diverse spectral responses,\n",
      "instantaneous field of view and frame rate. \n",
      "\n",
      "\n",
      "Medical image synthesis has attracted increasing attention because it could\n",
      "generate missing image data, improving diagnosis and benefits many downstream\n",
      "tasks. However, so far the developed synthesis model is not adaptive to unseen\n",
      "data distribution that presents domain shift, limiting its applicability in\n",
      "clinical routine. This work focuses on exploring domain adaptation (DA) of 3D\n",
      "image-to-image synthesis models. First, we highlight the technical difference\n",
      "in DA between classification, segmentation and synthesis models. Second, we\n",
      "present a novel efficient adaptation approach based on 2D variational\n",
      "autoencoder which approximates 3D distributions. Third, we present empirical\n",
      "studies on the effect of the amount of adaptation data and the key\n",
      "hyper-parameters. Our results show that the proposed approach can significantly\n",
      "improve the synthesis accuracy on unseen domains in a 3D setting. The code is\n",
      "publicly available at\n",
      "https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis \n",
      "\n",
      "\n",
      "Deep Learning-based image synthesis techniques have been applied in\n",
      "healthcare research for generating medical images to support open research.\n",
      "Training generative adversarial neural networks (GAN) usually requires large\n",
      "amounts of training data. Federated learning (FL) provides a way of training a\n",
      "central model using distributed data from different medical institutions while\n",
      "keeping raw data locally. However, FL is vulnerable to backdoor attack, an\n",
      "adversarial by poisoning training data, given the central server cannot access\n",
      "the original data directly. Most backdoor attack strategies focus on\n",
      "classification models and centralized domains. In this study, we propose a way\n",
      "of attacking federated GAN (FedGAN) by treating the discriminator with a\n",
      "commonly used data poisoning strategy in backdoor attack classification models.\n",
      "We demonstrate that adding a small trigger with size less than 0.5 percent of\n",
      "the original image size can corrupt the FL-GAN model. Based on the proposed\n",
      "attack, we provide two effective defense strategies: global malicious detection\n",
      "and local training regularization. We show that combining the two defense\n",
      "strategies yields a robust medical image generation. \n",
      "\n",
      "\n",
      "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable\n",
      "success in various image generation tasks compared with Generative Adversarial\n",
      "Nets (GANs). Recent work on semantic image synthesis mainly follows the\n",
      "\\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality\n",
      "or diversity of generated images. In this paper, we propose a novel framework\n",
      "based on DDPM for semantic image synthesis. Unlike previous conditional\n",
      "diffusion model directly feeds the semantic layout and noisy image as input to\n",
      "a U-Net structure, which may not fully leverage the information in the input\n",
      "semantic mask, our framework processes semantic layout and noisy image\n",
      "differently. It feeds noisy image to the encoder of the U-Net structure while\n",
      "the semantic layout to the decoder by multi-layer spatially-adaptive\n",
      "normalization operators. To further improve the generation quality and semantic\n",
      "interpretability in semantic image synthesis, we introduce the classifier-free\n",
      "guidance sampling strategy, which acknowledge the scores of an unconditional\n",
      "model for sampling process. Extensive experiments on three benchmark datasets\n",
      "demonstrate the effectiveness of our proposed method, achieving\n",
      "state-of-the-art performance in terms of fidelity~(FID) and diversity~(LPIPS). \n",
      "\n",
      "\n",
      "Generative models have emerged as an essential building block for many image\n",
      "synthesis and editing tasks. Recent advances in this field have also enabled\n",
      "high-quality 3D or video content to be generated that exhibits either\n",
      "multi-view or temporal consistency. With our work, we explore 4D generative\n",
      "adversarial networks (GANs) that learn unconditional generation of 3D-aware\n",
      "videos. By combining neural implicit representations with time-aware\n",
      "discriminator, we develop a GAN framework that synthesizes 3D video supervised\n",
      "only with monocular videos. We show that our method learns a rich embedding of\n",
      "decomposable 3D structures and motions that enables new visual effects of\n",
      "spatio-temporal renderings while producing imagery with quality comparable to\n",
      "that of existing 3D or video GANs. \n",
      "\n",
      "\n",
      "We introduce ArtBench-10, the first class-balanced, high-quality, cleanly\n",
      "annotated, and standardized dataset for benchmarking artwork generation. It\n",
      "comprises 60,000 images of artwork from 10 distinctive artistic styles, with\n",
      "5,000 training images and 1,000 testing images per style. ArtBench-10 has\n",
      "several advantages over previous artwork datasets. Firstly, it is\n",
      "class-balanced while most previous artwork datasets suffer from the long tail\n",
      "class distributions. Secondly, the images are of high quality with clean\n",
      "annotations. Thirdly, ArtBench-10 is created with standardized data collection,\n",
      "annotation, filtering, and preprocessing procedures. We provide three versions\n",
      "of the dataset with different resolutions ($32\\times32$, $256\\times256$, and\n",
      "original image size), formatted in a way that is easy to be incorporated by\n",
      "popular machine learning frameworks. We also conduct extensive benchmarking\n",
      "experiments using representative image synthesis models with ArtBench-10 and\n",
      "present in-depth analysis. The dataset is available at\n",
      "https://github.com/liaopeiyuan/artbench under a Fair Use license. \n",
      "\n",
      "\n",
      "The destitution of image data and corresponding expert annotations limit the\n",
      "training capacities of AI diagnostic models and potentially inhibit their\n",
      "performance. To address such a problem of data and label scarcity, generative\n",
      "models have been developed to augment the training datasets. Previously\n",
      "proposed generative models usually require manually adjusted annotations (e.g.,\n",
      "segmentation masks) or need pre-labeling. However, studies have found that\n",
      "these pre-labeling based methods can induce hallucinating artifacts, which\n",
      "might mislead the downstream clinical tasks, while manual adjustment could be\n",
      "onerous and subjective. To avoid manual adjustment and pre-labeling, we propose\n",
      "a novel controllable and simultaneous synthesizer (dubbed CS$^2$) in this study\n",
      "to generate both realistic images and corresponding annotations at the same\n",
      "time. Our CS$^2$ model is trained and validated using high resolution CT (HRCT)\n",
      "data collected from COVID-19 patients to realize an efficient infections\n",
      "segmentation with minimal human intervention. Our contributions include 1) a\n",
      "conditional image synthesis network that receives both style information from\n",
      "reference CT images and structural information from unsupervised segmentation\n",
      "masks, and 2) a corresponding segmentation mask synthesis network to\n",
      "automatically segment these synthesized images simultaneously. Our experimental\n",
      "studies on HRCT scans collected from COVID-19 patients demonstrate that our\n",
      "CS$^2$ model can lead to realistic synthesized datasets and promising\n",
      "segmentation results of COVID infections compared to the state-of-the-art\n",
      "nnUNet trained and fine-tuned in a fully supervised manner. \n",
      "\n",
      "\n",
      "Object cut-and-paste has become a promising approach to efficiently generate\n",
      "large sets of labeled training data. It involves compositing foreground object\n",
      "masks onto background images. The background images, when congruent with the\n",
      "objects, provide helpful context information for training object recognition\n",
      "models. While the approach can easily generate large labeled data, finding\n",
      "congruent context images for downstream tasks has remained an elusive problem.\n",
      "In this work, we propose a new paradigm for automatic context image generation\n",
      "at scale. At the core of our approach lies utilizing an interplay between\n",
      "language description of context and language-driven image generation. Language\n",
      "description of a context is provided by applying an image captioning method on\n",
      "a small set of images representing the context. These language descriptions are\n",
      "then used to generate diverse sets of context images using the language-based\n",
      "DALL-E image generation framework. These are then composited with objects to\n",
      "provide an augmented training set for a classifier. We demonstrate the\n",
      "advantages of our approach over the prior context image generation approaches\n",
      "on four object detection datasets. Furthermore, we also highlight the\n",
      "compositional nature of our data generation approach on out-of-distribution and\n",
      "zero-shot data generation scenarios. \n",
      "\n",
      "\n",
      "Generative Adversarial Network (GAN) is one of the state-of-the-art\n",
      "generative models for realistic image synthesis. While training and evaluating\n",
      "GAN becomes increasingly important, the current GAN research ecosystem does not\n",
      "provide reliable benchmarks for which the evaluation is conducted consistently\n",
      "and fairly. Furthermore, because there are few validated GAN implementations,\n",
      "researchers devote considerable time to reproducing baselines. We study the\n",
      "taxonomy of GAN approaches and present a new open-source library named\n",
      "StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4\n",
      "adversarial losses, 13 regularization modules, 3 differentiable augmentations,\n",
      "7 evaluation metrics, and 5 evaluation backbones. With our training and\n",
      "evaluation protocol, we present a large-scale benchmark using various datasets\n",
      "(CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3\n",
      "different evaluation backbones (InceptionV3, SwAV, and Swin Transformer).\n",
      "Unlike other benchmarks used in the GAN community, we train representative\n",
      "GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training\n",
      "pipeline and quantify generation performance with 7 evaluation metrics. The\n",
      "benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL,\n",
      "ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations,\n",
      "training, and evaluation scripts with the pre-trained weights. StudioGAN is\n",
      "available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN. \n",
      "\n",
      "\n",
      "Evaluation metrics in image synthesis play a key role to measure performances\n",
      "of generative models. However, most metrics mainly focus on image fidelity.\n",
      "Existing diversity metrics are derived by comparing distributions, and thus\n",
      "they cannot quantify the diversity or rarity degree of each generated image. In\n",
      "this work, we propose a new evaluation metric, called `rarity score', to\n",
      "measure the individual rarity of each image synthesized by generative models.\n",
      "We first show empirical observation that common samples are close to each other\n",
      "and rare samples are far from each other in nearest-neighbor distances of\n",
      "feature space. We then use our metric to demonstrate that the extent to which\n",
      "different generative models produce rare images can be effectively compared. We\n",
      "also propose a method to compare rarities between datasets that share the same\n",
      "concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in\n",
      "different designs of feature spaces to better understand the relationship\n",
      "between feature spaces and resulting sparse images. Code will be publicly\n",
      "available online for the research community. \n",
      "\n",
      "\n",
      "Capitalizing on the recent advances in image generation models, existing\n",
      "controllable face image synthesis methods are able to generate high-fidelity\n",
      "images with some levels of controllability, e.g., controlling the shapes,\n",
      "expressions, textures, and poses of the generated face images. However, these\n",
      "methods focus on 2D image generative models, which are prone to producing\n",
      "inconsistent face images under large expression and pose changes. In this\n",
      "paper, we propose a new NeRF-based conditional 3D face synthesis framework,\n",
      "which enables 3D controllability over the generated face images by imposing\n",
      "explicit 3D conditions from 3D face priors. At its core is a conditional\n",
      "Generative Occupancy Field (cGOF) that effectively enforces the shape of the\n",
      "generated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve\n",
      "accurate control over fine-grained 3D face shapes of the synthesized image, we\n",
      "additionally incorporate a 3D landmark loss as well as a volume warping loss\n",
      "into our synthesis algorithm. Experiments validate the effectiveness of the\n",
      "proposed method, which is able to generate high-fidelity face images and shows\n",
      "more precise 3D controllability than state-of-the-art 2D-based controllable\n",
      "face synthesis methods. Find code and demo at\n",
      "https://keqiangsun.github.io/projects/cgof. \n",
      "\n",
      "\n",
      "Diffusion probabilistic models (DPMs) have become a popular approach to\n",
      "conditional generation, due to their promising results and support for\n",
      "cross-modal synthesis. A key desideratum in conditional synthesis is to achieve\n",
      "high correspondence between the conditioning input and generated output. Most\n",
      "existing methods learn such relationships implicitly, by incorporating the\n",
      "prior into the variational lower bound. In this work, we take a different route\n",
      "-- we enhance input-output connections by maximizing their mutual information\n",
      "using contrastive learning. To this end, we introduce a Conditional Discrete\n",
      "Contrastive Diffusion (CDCD) loss and design two contrastive diffusion\n",
      "mechanisms to effectively incorporate it into the denoising process. We\n",
      "formulate CDCD by connecting it with the conventional variational objectives.\n",
      "We demonstrate the efficacy of our approach in evaluations with three diverse,\n",
      "multimodal conditional synthesis tasks: dance-to-music generation,\n",
      "text-to-image synthesis, and class-conditioned image synthesis. On each, we\n",
      "achieve state-of-the-art or higher synthesis quality and improve the\n",
      "input-output correspondence. Furthermore, the proposed approach improves the\n",
      "convergence of diffusion models, reducing the number of required diffusion\n",
      "steps by more than 35% on two benchmarks, significantly increasing the\n",
      "inference speed. \n",
      "\n",
      "\n",
      "State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to\n",
      "parameterize 3D radiance fields. While demonstrating impressive results,\n",
      "querying an MLP for every sample along each ray leads to slow rendering.\n",
      "Therefore, existing approaches often render low-resolution feature maps and\n",
      "process them with an upsampling network to obtain the final image. Albeit\n",
      "efficient, neural rendering often entangles viewpoint and content such that\n",
      "changing the camera pose results in unwanted changes of geometry or appearance.\n",
      "Motivated by recent results in voxel-based novel view synthesis, we investigate\n",
      "the utility of sparse voxel grid representations for fast and 3D-consistent\n",
      "generative modeling in this paper. Our results demonstrate that monolithic MLPs\n",
      "can indeed be replaced by 3D convolutions when combining sparse voxel grids\n",
      "with progressive growing, free space pruning and appropriate regularization. To\n",
      "obtain a compact representation of the scene and allow for scaling to higher\n",
      "voxel resolutions, our model disentangles the foreground object (modeled in 3D)\n",
      "from the background (modeled in 2D). In contrast to existing approaches, our\n",
      "method requires only a single forward pass to generate a full 3D scene. It\n",
      "hence allows for efficient rendering from arbitrary viewpoints while yielding\n",
      "3D consistent results with high visual fidelity. \n",
      "\n",
      "\n",
      "Image synthesis under multi-modal priors is a useful and challenging task\n",
      "that has received increasing attention in recent years. A major challenge in\n",
      "using generative models to accomplish this task is the lack of paired data\n",
      "containing all modalities (i.e. priors) and corresponding outputs. In recent\n",
      "work, a variational auto-encoder (VAE) model was trained in a weakly supervised\n",
      "manner to address this challenge. Since the generative power of VAEs is usually\n",
      "limited, it is difficult for this method to synthesize images belonging to\n",
      "complex distributions. To this end, we propose a solution based on a denoising\n",
      "diffusion probabilistic models to synthesise images under multi-model priors.\n",
      "Based on the fact that the distribution over each time step in the diffusion\n",
      "model is Gaussian, in this work we show that there exists a closed-form\n",
      "expression to the generate the image corresponds to the given modalities. The\n",
      "proposed solution does not require explicit retraining for all modalities and\n",
      "can leverage the outputs of individual modalities to generate realistic images\n",
      "according to different constraints. We conduct studies on two real-world\n",
      "datasets to demonstrate the effectiveness of our approach \n",
      "\n",
      "\n",
      "Score-based generative models (SGMs) have recently emerged as a promising\n",
      "class of generative models. The key idea is to produce high-quality images by\n",
      "recurrently adding Gaussian noises and gradients to a Gaussian sample until\n",
      "converging to the target distribution, a.k.a. the diffusion sampling. To ensure\n",
      "stability of convergence in sampling and generation quality, however, this\n",
      "sequential sampling process has to take a small step size and many sampling\n",
      "iterations (e.g., 2000). Several acceleration methods have been proposed with\n",
      "focus on low-resolution generation. In this work, we consider the acceleration\n",
      "of high-resolution generation with SGMs, a more challenging yet more important\n",
      "problem. We prove theoretically that this slow convergence drawback is\n",
      "primarily due to the ignorance of the target distribution. Further, we\n",
      "introduce a novel Target Distribution Aware Sampling (TDAS) method by\n",
      "leveraging the structural priors in space and frequency domains. Extensive\n",
      "experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can\n",
      "consistently accelerate state-of-the-art SGMs, particularly on more challenging\n",
      "high resolution (1024x1024) image generation tasks by up to 18.4x, whilst\n",
      "largely maintaining the synthesis quality. With fewer sampling iterations, TDAS\n",
      "can still generate good quality images. In contrast, the existing methods\n",
      "degrade drastically or even fails completely \n",
      "\n",
      "\n",
      "Many flagship smartphone cameras now use a dedicated neural image signal\n",
      "processor (ISP) to render noisy raw sensor images to the final processed\n",
      "output. Training nightmode ISP networks relies on large-scale datasets of image\n",
      "pairs with: (1) a noisy raw image captured with a short exposure and a high ISO\n",
      "gain; and (2) a ground truth low-noise raw image captured with a long exposure\n",
      "and low ISO that has been rendered through the ISP. Capturing such image pairs\n",
      "is tedious and time-consuming, requiring careful setup to ensure alignment\n",
      "between the image pairs. In addition, ground truth images are often prone to\n",
      "motion blur due to the long exposure. To address this problem, we propose a\n",
      "method that synthesizes nighttime images from daytime images. Daytime images\n",
      "are easy to capture, exhibit low-noise (even on smartphone cameras) and rarely\n",
      "suffer from motion blur. We outline a processing framework to convert daytime\n",
      "raw images to have the appearance of realistic nighttime raw images with\n",
      "different levels of noise. Our procedure allows us to easily produce aligned\n",
      "noisy and clean nighttime image pairs. We show the effectiveness of our\n",
      "synthesis framework by training neural ISPs for nightmode rendering.\n",
      "Furthermore, we demonstrate that using our synthetic nighttime images together\n",
      "with small amounts of real data (e.g., 5% to 10%) yields performance almost on\n",
      "par with training exclusively on real nighttime images. Our dataset and code\n",
      "are available at https://github.com/SamsungLabs/day-to-night. \n",
      "\n",
      "\n",
      "In this work, we propose and validate a framework to leverage language-image\n",
      "pretraining representations for training-free zero-shot sketch-to-image\n",
      "synthesis. We show that disentangled content and style representations can be\n",
      "utilized to guide image generators to employ them as sketch-to-image generators\n",
      "without (re-)training any parameters. Our approach for disentangling style and\n",
      "content entails a simple method consisting of elementary arithmetic assuming\n",
      "compositionality of information in representations of input sketches. Our\n",
      "results demonstrate that this approach is competitive with state-of-the-art\n",
      "instance-level open-domain sketch-to-image models, while only depending on\n",
      "pretrained off-the-shelf models and a fraction of the data. \n",
      "\n",
      "\n",
      "Recently most successful image synthesis models are multi stage process to\n",
      "combine the advantages of different methods, which always includes a VAE-like\n",
      "model for faithfully reconstructing embedding to image and a prior model to\n",
      "generate image embedding. At the same time, diffusion models have shown be\n",
      "capacity to generate high-quality synthetic images. Our work proposes a VQ-VAE\n",
      "architecture model with a diffusion decoder (DiVAE) to work as the\n",
      "reconstructing component in image synthesis. We explore how to input image\n",
      "embedding into diffusion model for excellent performance and find that simple\n",
      "modification on diffusion's UNet can achieve it. Training on ImageNet, Our\n",
      "model achieves state-of-the-art results and generates more photorealistic\n",
      "images specifically. In addition, we apply the DiVAE with an Auto-regressive\n",
      "generator on conditional synthesis tasks to perform more human-feeling and\n",
      "detailed samples. \n",
      "\n",
      "\n",
      "Vector quantized diffusion (VQ-Diffusion) is a powerful generative model for\n",
      "text-to-image synthesis, but sometimes can still generate low-quality samples\n",
      "or weakly correlated images with text input. We find these issues are mainly\n",
      "due to the flawed sampling strategy. In this paper, we propose two important\n",
      "techniques to further improve the sample quality of VQ-Diffusion. 1) We explore\n",
      "classifier-free guidance sampling for discrete denoising diffusion model and\n",
      "propose a more general and effective implementation of classifier-free\n",
      "guidance. 2) We present a high-quality inference strategy to alleviate the\n",
      "joint distribution issue in VQ-Diffusion. Finally, we conduct experiments on\n",
      "various datasets to validate their effectiveness and show that the improved\n",
      "VQ-Diffusion suppresses the vanilla version by large margins. We achieve an\n",
      "8.44 FID score on MSCOCO, surpassing VQ-Diffusion by 5.42 FID score. When\n",
      "trained on ImageNet, we dramatically improve the FID score from 11.89 to 4.83,\n",
      "demonstrating the superiority of our proposed techniques. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) can synthesize abundant\n",
      "photo-realistic synthetic aperture radar (SAR) images. Some recent GANs (e.g.,\n",
      "InfoGAN), are even able to edit specific properties of the synthesized images\n",
      "by introducing latent codes. It is crucial for SAR image synthesis since the\n",
      "targets in real SAR images are with different properties due to the imaging\n",
      "mechanism. Despite the success of InfoGAN in manipulating properties, there\n",
      "still lacks a clear explanation of how these latent codes affect synthesized\n",
      "properties, thus editing specific properties usually relies on empirical\n",
      "trials, unreliable and time-consuming. In this paper, we show that latent codes\n",
      "are disentangled to affect the properties of SAR images in a non-linear manner.\n",
      "By introducing some property estimators for latent codes, we are able to\n",
      "provide a completely analytical nonlinear model to decompose the entangled\n",
      "causality between latent codes and different properties. The qualitative and\n",
      "quantitative experimental results further reveal that the properties can be\n",
      "calculated by latent codes, inversely, the satisfying latent codes can be\n",
      "estimated given desired properties. In this case, properties can be manipulated\n",
      "by latent codes as we expect. \n",
      "\n",
      "\n",
      "The impressive success of style-based GANs (StyleGANs) in high-fidelity image\n",
      "synthesis has motivated research to understand the semantic properties of their\n",
      "latent spaces. Recently, a close relationship was observed between the\n",
      "semantically disentangled local perturbations and the local PCA components in\n",
      "the learned latent space $\\mathcal{W}$. However, understanding the number of\n",
      "disentangled perturbations remains challenging. Building upon this observation,\n",
      "we propose a local dimension estimation algorithm for an arbitrary intermediate\n",
      "layer in a pre-trained GAN model. The estimated intrinsic dimension corresponds\n",
      "to the number of disentangled local perturbations. In this perspective, we\n",
      "analyze the intermediate layers of the mapping network in StyleGANs. Our\n",
      "analysis clarifies the success of $\\mathcal{W}$-space in StyleGAN and suggests\n",
      "an alternative. Moreover, the intrinsic dimension estimation opens the\n",
      "possibility of unsupervised evaluation of global-basis-compatibility and\n",
      "disentanglement for a latent space. Our proposed metric, called Distortion,\n",
      "measures an inconsistency of intrinsic tangent space on the learned latent\n",
      "space. The metric is purely geometric and does not require any additional\n",
      "attribute information. Nevertheless, the metric shows a high correlation with\n",
      "the global-basis-compatibility and supervised disentanglement score. Our\n",
      "findings pave the way towards an unsupervised selection of globally\n",
      "disentangled latent space among the intermediate latent spaces in a GAN. \n",
      "\n",
      "\n",
      "Variational autoencoders (VAEs) are a popular class of deep generative models\n",
      "with many variants and a wide range of applications. Improvements upon the\n",
      "standard VAE mostly focus on the modelling of the posterior distribution over\n",
      "the latent space and the properties of the neural network decoder. In contrast,\n",
      "improving the model for the observational distribution is rarely considered and\n",
      "typically defaults to a pixel-wise independent categorical or normal\n",
      "distribution. In image synthesis, sampling from such distributions produces\n",
      "spatially-incoherent results with uncorrelated pixel noise, resulting in only\n",
      "the sample mean being somewhat useful as an output prediction. In this paper,\n",
      "we aim to stay true to VAE theory by improving the samples from the\n",
      "observational distribution. We propose an alternative model for the observation\n",
      "space, encoding spatial dependencies via a low-rank parameterisation. We\n",
      "demonstrate that this new observational distribution has the ability to capture\n",
      "relevant covariance between pixels, resulting in spatially-coherent samples. In\n",
      "contrast to pixel-wise independent distributions, our samples seem to contain\n",
      "semantically meaningful variations from the mean allowing the prediction of\n",
      "multiple plausible outputs with a single forward pass. \n",
      "\n",
      "\n",
      "With the rapid development of the Metaverse, virtual humans have emerged, and\n",
      "human image synthesis and editing techniques, such as pose transfer, have\n",
      "recently become popular. Most of the existing techniques rely on GANs, which\n",
      "can generate good human images even with large variants and occlusions. But\n",
      "from our best knowledge, the existing state-of-the-art method still has the\n",
      "following problems: the first is that the rendering effect of the synthetic\n",
      "image is not realistic, such as poor rendering of some regions. And the second\n",
      "is that the training of GAN is unstable and slow to converge, such as model\n",
      "collapse. Based on the above two problems, we propose several methods to solve\n",
      "them. To improve the rendering effect, we use the Residual Fast Fourier\n",
      "Transform Block to replace the traditional Residual Block. Then, spectral\n",
      "normalization and Wasserstein distance are used to improve the speed and\n",
      "stability of GAN training. Experiments demonstrate that the methods we offer\n",
      "are effective at solving the problems listed above, and we get state-of-the-art\n",
      "scores in LPIPS and PSNR. \n",
      "\n",
      "\n",
      "We present Imagen, a text-to-image diffusion model with an unprecedented\n",
      "degree of photorealism and a deep level of language understanding. Imagen\n",
      "builds on the power of large transformer language models in understanding text\n",
      "and hinges on the strength of diffusion models in high-fidelity image\n",
      "generation. Our key discovery is that generic large language models (e.g. T5),\n",
      "pretrained on text-only corpora, are surprisingly effective at encoding text\n",
      "for image synthesis: increasing the size of the language model in Imagen boosts\n",
      "both sample fidelity and image-text alignment much more than increasing the\n",
      "size of the image diffusion model. Imagen achieves a new state-of-the-art FID\n",
      "score of 7.27 on the COCO dataset, without ever training on COCO, and human\n",
      "raters find Imagen samples to be on par with the COCO data itself in image-text\n",
      "alignment. To assess text-to-image models in greater depth, we introduce\n",
      "DrawBench, a comprehensive and challenging benchmark for text-to-image models.\n",
      "With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,\n",
      "Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen\n",
      "over other models in side-by-side comparisons, both in terms of sample quality\n",
      "and image-text alignment. See https://imagen.research.google/ for an overview\n",
      "of the results. \n",
      "\n",
      "\n",
      "Here we present a structural similarity index measure (SSIM) guided\n",
      "conditional Generative Adversarial Network (cGAN) that generatively performs\n",
      "image-to-image (i2i) synthesis to generate photo-accurate protein channels in\n",
      "multiplexed spatial proteomics images. This approach can be utilized to\n",
      "accurately generate missing spatial proteomics channels that were not included\n",
      "during experimental data collection either at the bench or the clinic.\n",
      "Experimental spatial proteomic data from the Human BioMolecular Atlas Program\n",
      "(HuBMAP) was used to generate spatial representations of missing proteins\n",
      "through a U-Net based image synthesis pipeline. HuBMAP channels were\n",
      "hierarchically clustered by the (SSIM) as a heuristic to obtain the minimal set\n",
      "needed to recapitulate the underlying biology represented by the spatial\n",
      "landscape of proteins. We subsequently prove that our SSIM based architecture\n",
      "allows for scaling of generative image synthesis to slides with up to 100\n",
      "channels, which is better than current state of the art algorithms which are\n",
      "limited to data with 11 channels. We validate these claims by generating a new\n",
      "experimental spatial proteomics data set from human lung adenocarcinoma tissue\n",
      "sections and show that a model trained on HuBMAP can accurately synthesize\n",
      "channels from our new data set. The ability to recapitulate experimental data\n",
      "from sparsely stained multiplexed histological slides containing spatial\n",
      "proteomic will have tremendous impact on medical diagnostics and drug\n",
      "development, and also raises important questions on the medical ethics of\n",
      "utilizing data produced by generative image synthesis in the clinical setting.\n",
      "The algorithm that we present in this paper will allow researchers and\n",
      "clinicians to save time and costs in proteomics based histological staining\n",
      "while also increasing the amount of data that they can generate through their\n",
      "experiments. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GAN) have motivated a rapid growth of the\n",
      "domain of computer image synthesis. As almost all the existing image synthesis\n",
      "algorithms consider an image as a pixel matrix, the high-resolution image\n",
      "synthesis is complicated.A good alternative can be vector images. However, they\n",
      "belong to the highly sophisticated parametric space, which is a restriction for\n",
      "solving the task of synthesizing vector graphics by GANs. In this paper, we\n",
      "consider a specific application domain that softens this restriction\n",
      "dramatically allowing the usage of vector image synthesis.\n",
      "  Music cover images should meet the requirements of Internet streaming\n",
      "services and printing standards, which imply high resolution of graphic\n",
      "materials without any additional requirements on the content of such images.\n",
      "Existing music cover image generation services do not analyze tracks\n",
      "themselves; however, some services mostly consider only genre tags. To generate\n",
      "music covers as vector images that reflect the music and consist of simple\n",
      "geometric objects, we suggest a GAN-based algorithm called CoverGAN. The\n",
      "assessment of resulting images is based on their correspondence to the music\n",
      "compared with AttnGAN and DALL-E text-to-image generation according to title or\n",
      "lyrics. Moreover, the significance of the patterns found by CoverGAN has been\n",
      "evaluated in terms of the correspondence of the generated cover images to the\n",
      "musical tracks. Listeners evaluate the music covers generated by the proposed\n",
      "algorithm as quite satisfactory and corresponding to the tracks. Music cover\n",
      "images generation code and demo are available at\n",
      "https://github.com/IzhanVarsky/CoverGAN. \n",
      "\n",
      "\n",
      "Despite recent success in conditional image synthesis, prevalent input\n",
      "conditions such as semantics and edges are not clear enough to express `Linear\n",
      "(Ridges)' and `Planar (Scale)' representations. To address this problem, we\n",
      "propose a novel framework StyLandGAN, which synthesizes desired landscape\n",
      "images using a depth map which has higher expressive power. Our StyleLandGAN is\n",
      "extended from the unconditional generation model to accept input conditions. We\n",
      "also propose a '2-phase inference' pipeline which generates diverse depth maps\n",
      "and shifts local parts so that it can easily reflect user's intend. As a\n",
      "comparison, we modified the existing semantic image synthesis models to accept\n",
      "a depth map as well. Experimental results show that our method is superior to\n",
      "existing methods in quality, diversity, and depth-accuracy. \n",
      "\n",
      "\n",
      "Text-to-image synthesis has made a giant leap towards becoming a mainstream\n",
      "phenomenon since 2021. With text-to-image systems, anybody can create digital\n",
      "images and artworks. This provokes the question of whether text-to-image art is\n",
      "creative. This paper expounds on the nature of human creativity involved in\n",
      "text-to-image art with a specific focus on the practice of \"prompt\n",
      "engineering\". The paper argues that the current product-centered view of\n",
      "creativity may fall short in the context of text-to-image generation. A case\n",
      "exemplifying this shortcoming is provided and the importance of online\n",
      "communities for the creative ecosystem of text-to-image art is highlighted. We\n",
      "provide a high-level summary of this online ecosystem drawing on Rhodes's\n",
      "conceptual model of creativity. We provide a discussion on the challenges for\n",
      "evaluating the creativity of text-to-image generation and discuss opportunities\n",
      "for research on text-to-image art in the field of Human-Computer Interaction\n",
      "(HCI). \n",
      "\n",
      "\n",
      "Neural Radiance Fields (NeRF) have emerged as a potent paradigm for\n",
      "representing scenes and synthesizing photo-realistic images. A main limitation\n",
      "of conventional NeRFs is that they often fail to produce high-quality\n",
      "renderings under novel viewpoints that are significantly different from the\n",
      "training viewpoints. In this paper, instead of exploiting few-shot image\n",
      "synthesis, we study the novel view extrapolation setting that (1) the training\n",
      "images can well describe an object, and (2) there is a notable discrepancy\n",
      "between the training and test viewpoints' distributions. We present RapNeRF\n",
      "(RAy Priors) as a solution. Our insight is that the inherent appearances of a\n",
      "3D surface's arbitrary visible projections should be consistent. We thus\n",
      "propose a random ray casting policy that allows training unseen views using\n",
      "seen views. Furthermore, we show that a ray atlas pre-computed from the\n",
      "observed rays' viewing directions could further enhance the rendering quality\n",
      "for extrapolated views. A main limitation is that RapNeRF would remove the\n",
      "strong view-dependent effects because it leverages the multi-view consistency\n",
      "property. \n",
      "\n",
      "\n",
      "Researchers have explored various ways to generate realistic images from\n",
      "freehand sketches, e.g., for objects and human faces. However, how to generate\n",
      "realistic human body images from sketches is still a challenging problem. It\n",
      "is, first because of the sensitivity to human shapes, second because of the\n",
      "complexity of human images caused by body shape and pose changes, and third\n",
      "because of the domain gap between realistic images and freehand sketches. In\n",
      "this work, we present DeepPortraitDrawing, a deep generative framework for\n",
      "converting roughly drawn sketches to realistic human body images. To encode\n",
      "complicated body shapes under various poses, we take a local-to-global\n",
      "approach. Locally, we employ semantic part auto-encoders to construct\n",
      "part-level shape spaces, which are useful for refining the geometry of an input\n",
      "pre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial\n",
      "transformer network to refine the structure of body parts by adjusting their\n",
      "spatial locations and relative proportions. Finally, we use a global synthesis\n",
      "network for the sketch-to-image translation task, and a face refinement network\n",
      "to enhance facial details. Extensive experiments have shown that given roughly\n",
      "sketched human portraits, our method produces more realistic images than the\n",
      "state-of-the-art sketch-to-image synthesis techniques. \n",
      "\n",
      "\n",
      "Deep learning greatly improved the realism of animatable human models by\n",
      "learning geometry and appearance from collections of 3D scans, template meshes,\n",
      "and multi-view imagery. High-resolution models enable photo-realistic avatars\n",
      "but at the cost of requiring studio settings not available to end users. Our\n",
      "goal is to create avatars directly from raw images without relying on expensive\n",
      "studio setups and surface tracking. While a few such approaches exist, those\n",
      "have limited generalization capabilities and are prone to learning spurious\n",
      "(chance) correlations between irrelevant body parts, resulting in implausible\n",
      "deformations and missing body parts on unseen poses. We introduce a three-stage\n",
      "method that induces two inductive biases to better disentangled pose-dependent\n",
      "deformation. First, we model correlations of body parts explicitly with a graph\n",
      "neural network. Second, to further reduce the effect of chance correlations, we\n",
      "introduce localized per-bone features that use a factorized volumetric\n",
      "representation and a new aggregation function. We demonstrate that our model\n",
      "produces realistic body shapes under challenging unseen poses and shows\n",
      "high-quality image synthesis. Our proposed representation strikes a better\n",
      "trade-off between model capacity, expressiveness, and robustness than competing\n",
      "methods. Project website: https://lemonatsu.github.io/danbo. \n",
      "\n",
      "\n",
      "Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical\n",
      "practice as each contrast provides complementary information. However, the\n",
      "availability of each contrast may vary amongst patients in reality. This poses\n",
      "challenges to both radiologists and automated image analysis algorithms. A\n",
      "general approach for tackling this problem is missing data imputation, which\n",
      "aims to synthesize the missing contrasts from existing ones. While several\n",
      "convolutional neural network (CNN) based algorithms have been proposed, they\n",
      "suffer from the fundamental limitations of CNN models, such as requirement for\n",
      "fixed numbers of input and output channels, inability to capture long-range\n",
      "dependencies, and lack of interpretability. In this paper, we formulate missing\n",
      "data imputation as a sequence-to-sequence learning problem and propose a\n",
      "multi-contrast multi-scale Transformer (MMT), which can take any subset of\n",
      "input contrasts and synthesize those that are missing. MMT consists of a\n",
      "multi-scale Transformer encoder that builds hierarchical representations of\n",
      "inputs combined with a multi-scale Transformer decoder that generates the\n",
      "outputs in a coarse-to-fine fashion. Thanks to the proposed multi-contrast Swin\n",
      "Transformer blocks, it can efficiently capture intra- and inter-contrast\n",
      "dependencies for accurate image synthesis. Moreover, MMT is inherently\n",
      "interpretable. It allows us to understand the importance of each input contrast\n",
      "in different regions by analyzing the in-built attention maps of Transformer\n",
      "blocks in the decoder. Extensive experiments on two large-scale multi-contrast\n",
      "MRI datasets demonstrate that MMT outperforms the state-of-the-art methods\n",
      "quantitatively and qualitatively. \n",
      "\n",
      "\n",
      "Isolating and controlling specific features in the outputs of generative\n",
      "models in a user-friendly way is a difficult and open-ended problem. We develop\n",
      "techniques that allow an oracle user to generate an image they are envisioning\n",
      "in their head by answering a sequence of relative queries of the form\n",
      "\\textit{\"do you prefer image $a$ or image $b$?\"} Our framework consists of a\n",
      "Conditional VAE that uses the collected relative queries to partition the\n",
      "latent space into preference-relevant features and non-preference-relevant\n",
      "features. We then use the user's responses to relative queries to determine the\n",
      "preference-relevant features that correspond to their envisioned output image.\n",
      "Additionally, we develop techniques for modeling the uncertainty in images'\n",
      "predicted preference-relevant features, allowing our framework to generalize to\n",
      "scenarios in which the relative query training set contains noise. \n",
      "\n",
      "\n",
      "Rendering on conventional computers is capable of generating realistic\n",
      "imagery, but the computational complexity of these light transport algorithms\n",
      "is a limiting factor of image synthesis. Quantum computers have the potential\n",
      "to significantly improve rendering performance through reducing the underlying\n",
      "complexity of the algorithms behind light transport. This paper investigates\n",
      "hybrid quantum-classical algorithms for ray tracing, a core component of most\n",
      "rendering techniques. Through a practical implementation of quantum ray tracing\n",
      "in a 3D environment, we show quantum approaches provide a quadratic improvement\n",
      "in query complexity compared to the equivalent classical approach. Based on\n",
      "domain specific knowledge, we then propose algorithms to significantly reduce\n",
      "the computation required for quantum ray tracing through exploiting image space\n",
      "coherence and a principled termination criteria for quantum searching. We show\n",
      "results for both Whitted style ray tracing, and for accelerating ray tracing\n",
      "operations when performing classical Monte Carlo integration for area lights\n",
      "and indirect illumination. \n",
      "\n",
      "\n",
      "Recent efforts on scene text erasing have shown promising results. However,\n",
      "existing methods require rich yet costly label annotations to obtain robust\n",
      "models, which limits the use for practical applications. To this end, we study\n",
      "an unsupervised scenario by proposing a novel Self-supervised Text Erasing\n",
      "(STE) framework that jointly learns to synthesize training images with erasure\n",
      "ground-truth and accurately erase texts in the real world. We first design a\n",
      "style-aware image synthesis function to generate synthetic images with diverse\n",
      "styled texts based on two synthetic mechanisms. To bridge the text style gap\n",
      "between the synthetic and real-world data, a policy network is constructed to\n",
      "control the synthetic mechanisms by picking style parameters with the guidance\n",
      "of two specifically designed rewards. The synthetic training images with\n",
      "erasure ground-truth are then fed to train a coarse-to-fine erasing network. To\n",
      "produce better erasing outputs, a triplet erasure loss is designed to enforce\n",
      "the refinement stage to recover background textures. Moreover, we provide a new\n",
      "dataset (called PosterErase), which contains 60K high-resolution posters with\n",
      "texts and is more challenging for the text erasing task. The proposed method\n",
      "has been extensively evaluated with both PosterErase and the widely-used\n",
      "SCUT-Enstext dataset. Notably, on PosterErase, our unsupervised method achieves\n",
      "5.07 in terms of FID, with a relative performance of 20.9% over existing\n",
      "supervised baselines. \n",
      "\n",
      "\n",
      "In recent years, generative adversarial networks (GANs) have gained\n",
      "tremendous popularity for potential applications in medical imaging, such as\n",
      "medical image synthesis, restoration, reconstruction, translation, as well as\n",
      "objective image quality assessment. Despite the impressive progress in\n",
      "generating high-resolution, perceptually realistic images, it is not clear if\n",
      "modern GANs reliably learn the statistics that are meaningful to a downstream\n",
      "medical imaging application. In this work, the ability of a state-of-the-art\n",
      "GAN to learn the statistics of canonical stochastic image models (SIMs) that\n",
      "are relevant to objective assessment of image quality is investigated. It is\n",
      "shown that although the employed GAN successfully learned several basic first-\n",
      "and second-order statistics of the specific medical SIMs under consideration\n",
      "and generated images with high perceptual quality, it failed to correctly learn\n",
      "several per-image statistics pertinent to the these SIMs, highlighting the\n",
      "urgent need to assess medical image GANs in terms of objective measures of\n",
      "image quality. \n",
      "\n",
      "\n",
      "Generative image synthesis with diffusion models has recently achieved\n",
      "excellent visual quality in several tasks such as text-based or\n",
      "class-conditional image synthesis. Much of this success is due to a dramatic\n",
      "increase in the computational capacity invested in training these models. This\n",
      "work presents an alternative approach: inspired by its successful application\n",
      "in natural language processing, we propose to complement the diffusion model\n",
      "with a retrieval-based approach and to introduce an explicit memory in the form\n",
      "of an external database. During training, our diffusion model is trained with\n",
      "similar visual features retrieved via CLIP and from the neighborhood of each\n",
      "training instance. By leveraging CLIP's joint image-text embedding space, our\n",
      "model achieves highly competitive performance on tasks for which it has not\n",
      "been explicitly trained, such as class-conditional or text-image synthesis, and\n",
      "can be conditioned on both text and image embeddings. Moreover, we can apply\n",
      "our approach to unconditional generation, where it achieves state-of-the-art\n",
      "performance. Our approach incurs low computational and memory overheads and is\n",
      "easy to implement. We discuss its relationship to concurrent work and will\n",
      "publish code and pretrained models soon. \n",
      "\n",
      "\n",
      "Text-to-image synthesis aims to generate natural images conditioned on text\n",
      "descriptions. The main difficulty of this task lies in effectively fusing text\n",
      "information into the image synthesis process. Existing methods usually\n",
      "adaptively fuse suitable text information into the synthesis process with\n",
      "multiple isolated fusion blocks (e.g., Conditional\n",
      "  Batch Normalization and Instance Normalization). However, isolated fusion\n",
      "blocks not only conflict with each other but also increase the difficulty of\n",
      "training (see first page of the supplementary). To address these issues, we\n",
      "propose a Recurrent Affine Transformation (RAT) for Generative Adversarial\n",
      "Networks that connects all the fusion blocks with a recurrent neural network to\n",
      "model their long-term dependency. Besides, to improve semantic consistency\n",
      "between texts and synthesized images, we incorporate a spatial attention model\n",
      "in the discriminator. Being aware of matching image regions, text descriptions\n",
      "supervise the generator to synthesize more relevant image contents. Extensive\n",
      "experiments on the CUB, Oxford-102 and COCO datasets demonstrate the\n",
      "superiority of the proposed model in comparison to state-of-the-art models\n",
      "\\footnote{https://github.com/senmaoy/Recurrent-Affine-Transformation-for-Text-to-image-Synthesis.git} \n",
      "\n",
      "\n",
      "Deep neural advancements have recently brought remarkable image synthesis\n",
      "performance to the field of image inpainting. The adaptation of generative\n",
      "adversarial networks (GAN) in particular has accelerated significant progress\n",
      "in high-quality image reconstruction. However, although many notable GAN-based\n",
      "networks have been proposed for image inpainting, still pixel artifacts or\n",
      "color inconsistency occur in synthesized images during the generation process,\n",
      "which are usually called fake textures. To reduce pixel inconsistency disorder\n",
      "resulted from fake textures, we introduce a GAN-based model using dynamic\n",
      "attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake\n",
      "texture and products dynamic attention maps to diminish pixel inconsistency\n",
      "from the feature maps in the generator. Evaluation results on CelebA-HQ and\n",
      "Places2 datasets with other image inpainting approaches show the superiority of\n",
      "our network. \n",
      "\n",
      "\n",
      "We introduce a segmentation-guided approach to synthesise images that\n",
      "integrate features from two distinct domains. Images synthesised by our\n",
      "dual-domain model belong to one domain within the semantic mask, and to another\n",
      "in the rest of the image - smoothly integrated. We build on the successes of\n",
      "few-shot StyleGAN and single-shot semantic segmentation to minimise the amount\n",
      "of training required in utilising two domains. The method combines a few-shot\n",
      "cross-domain StyleGAN with a latent optimiser to achieve images containing\n",
      "features of two distinct domains. We use a segmentation-guided perceptual loss,\n",
      "which compares both pixel-level and activations between domain-specific and\n",
      "dual-domain synthetic images. Results demonstrate qualitatively and\n",
      "quantitatively that our model is capable of synthesising dual-domain images on\n",
      "a variety of objects (faces, horses, cats, cars), domains (natural, caricature,\n",
      "sketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). The code\n",
      "is publicly available at:\n",
      "https://github.com/denabazazian/Dual-Domain-Synthesis. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have achieved remarkable achievements\n",
      "in image synthesis. These successes of GANs rely on large scale datasets,\n",
      "requiring too much cost. With limited training data, how to stable the training\n",
      "process of GANs and generate realistic images have attracted more attention.\n",
      "The challenges of Data-Efficient GANs (DE-GANs) mainly arise from three\n",
      "aspects: (i) Mismatch Between Training and Target Distributions, (ii)\n",
      "Overfitting of the Discriminator, and (iii) Imbalance Between Latent and Data\n",
      "Spaces. Although many augmentation and pre-training strategies have been\n",
      "proposed to alleviate these issues, there lacks a systematic survey to\n",
      "summarize the properties, challenges, and solutions of DE-GANs. In this paper,\n",
      "we revisit and define DE-GANs from the perspective of distribution\n",
      "optimization. We conclude and analyze the challenges of DE-GANs. Meanwhile, we\n",
      "propose a taxonomy, which classifies the existing methods into three\n",
      "categories: Data Selection, GANs Optimization, and Knowledge Sharing. Last but\n",
      "not the least, we attempt to highlight the current problems and the future\n",
      "directions. \n",
      "\n",
      "\n",
      "Recent advances in deep learning, such as powerful generative models and\n",
      "joint text-image embeddings, have provided the computational creativity\n",
      "community with new tools, opening new perspectives for artistic pursuits.\n",
      "Text-to-image synthesis approaches that operate by generating images from text\n",
      "cues provide a case in point. These images are generated with a latent vector\n",
      "that is progressively refined to agree with text cues. To do so, patches are\n",
      "sampled within the generated image, and compared with the text prompts in the\n",
      "common text-image embedding space; The latent vector is then updated, using\n",
      "gradient descent, to reduce the mean (average) distance between these patches\n",
      "and text cues. While this approach provides artists with ample freedom to\n",
      "customize the overall appearance of images, through their choice in generative\n",
      "models, the reliance on a simple criterion (mean of distances) often causes\n",
      "mode collapse: The entire image is drawn to the average of all text cues,\n",
      "thereby losing their diversity. To address this issue, we propose using\n",
      "matching techniques found in the optimal transport (OT) literature, resulting\n",
      "in images that are able to reflect faithfully a wide diversity of prompts. We\n",
      "provide numerous illustrations showing that OT avoids some of the pitfalls\n",
      "arising from estimating vectors with mean distances, and demonstrate the\n",
      "capacity of our proposed method to perform better in experiments, qualitatively\n",
      "and quantitatively. \n",
      "\n",
      "\n",
      "Generative models operate at fixed resolution, even though natural images\n",
      "come in a variety of sizes. As high-resolution details are downsampled away and\n",
      "low-resolution images are discarded altogether, precious supervision is lost.\n",
      "We argue that every pixel matters and create datasets with variable-size\n",
      "images, collected at their native resolutions. To take advantage of varied-size\n",
      "data, we introduce continuous-scale training, a process that samples patches at\n",
      "random scales to train a new generator with variable output resolutions. First,\n",
      "conditioning the generator on a target scale allows us to generate higher\n",
      "resolution images than previously possible, without adding layers to the model.\n",
      "Second, by conditioning on continuous coordinates, we can sample patches that\n",
      "still obey a consistent global layout, which also allows for scalable training\n",
      "at higher resolutions. Controlled FFHQ experiments show that our method can\n",
      "take advantage of multi-resolution training data better than discrete\n",
      "multi-scale approaches, achieving better FID scores and cleaner high-frequency\n",
      "details. We also train on other natural image domains including churches,\n",
      "mountains, and birds, and demonstrate arbitrary scale synthesis with both\n",
      "coherent global layouts and realistic local details, going beyond 2K resolution\n",
      "in our experiments. Our project page is available at:\n",
      "https://chail.github.io/anyres-gan/. \n",
      "\n",
      "\n",
      "Ultrasound (US) imaging is widely used for anatomical structure inspection in\n",
      "clinical diagnosis. The training of new sonographers and deep learning based\n",
      "algorithms for US image analysis usually requires a large amount of data.\n",
      "However, obtaining and labeling large-scale US imaging data are not easy tasks,\n",
      "especially for diseases with low incidence. Realistic US image synthesis can\n",
      "alleviate this problem to a great extent. In this paper, we propose a\n",
      "generative adversarial network (GAN) based image synthesis framework. Our main\n",
      "contributions include: 1) we present the first work that can synthesize\n",
      "realistic B-mode US images with high-resolution and customized texture editing\n",
      "features; 2) to enhance structural details of generated images, we propose to\n",
      "introduce auxiliary sketch guidance into a conditional GAN. We superpose the\n",
      "edge sketch onto the object mask and use the composite mask as the network\n",
      "input; 3) to generate high-resolution US images, we adopt a progressive\n",
      "training strategy to gradually generate high-resolution images from\n",
      "low-resolution images. In addition, a feature loss is proposed to minimize the\n",
      "difference of high-level features between the generated and real images, which\n",
      "further improves the quality of generated images; 4) the proposed US image\n",
      "synthesis method is quite universal and can also be generalized to the US\n",
      "images of other anatomical structures besides the three ones tested in our\n",
      "study (lung, hip joint, and ovary); 5) extensive experiments on three large US\n",
      "image datasets are conducted to validate our method. Ablation studies,\n",
      "customized texture editing, user studies, and segmentation tests demonstrate\n",
      "promising results of our method in synthesizing realistic US images. \n",
      "\n",
      "\n",
      "3D-aware image synthesis aims to generate images of objects from multiple\n",
      "views by learning a 3D representation. However, one key challenge remains:\n",
      "existing approaches lack geometry constraints, hence usually fail to generate\n",
      "multi-view consistent images. To address this challenge, we propose Multi-View\n",
      "Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware\n",
      "image synthesis with geometry constraints. By leveraging the underlying 3D\n",
      "geometry information of generated images, i.e., depth and camera transformation\n",
      "matrix, we explicitly establish stereo correspondence between views to perform\n",
      "multi-view joint optimization. In particular, we enforce the photometric\n",
      "consistency between pairs of views and integrate a stereo mixup mechanism into\n",
      "the training process, encouraging the model to reason about the correct 3D\n",
      "shape. Besides, we design a two-stage training strategy with feature-level\n",
      "multi-view joint optimization to improve the image quality. Extensive\n",
      "experiments on three datasets demonstrate that MVCGAN achieves the\n",
      "state-of-the-art performance for 3D-aware image synthesis. \n",
      "\n",
      "\n",
      "Existing face swap methods rely heavily on large-scale networks for adequate\n",
      "capacity to generate visually plausible results, which inhibits its\n",
      "applications on resource-constraint platforms. In this work, we propose\n",
      "MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile\n",
      "devices with much fewer parameters while achieving competitive performance. A\n",
      "lightweight encoder-decoder structure is designed especially for image\n",
      "synthesis tasks, which is only 10.2MB and can run on mobile devices at a\n",
      "real-time speed. To tackle the unstability of training such a small network, we\n",
      "construct the FSTriplets dataset utilizing facial attribute editing techniques.\n",
      "FSTriplets provides source-target-result training triplets, yielding\n",
      "pixel-level labels thus for the first time making the training process\n",
      "supervised. We also designed multi-scale gradient losses for efficient\n",
      "back-propagation, resulting in faster and better convergence. Experimental\n",
      "results show that our model reaches comparable performance towards\n",
      "state-of-the-art methods, while significantly reducing the number of network\n",
      "parameters. Codes and the dataset have been released. \n",
      "\n",
      "\n",
      "We deal with the controllable person image synthesis task which aims to\n",
      "re-render a human from a reference image with explicit control over body pose\n",
      "and appearance. Observing that person images are highly structured, we propose\n",
      "to generate desired images by extracting and distributing semantic entities of\n",
      "reference images. To achieve this goal, a neural texture extraction and\n",
      "distribution operation based on double attention is described. This operation\n",
      "first extracts semantic neural textures from reference feature maps. Then, it\n",
      "distributes the extracted neural textures according to the spatial\n",
      "distributions learned from target poses. Our model is trained to predict human\n",
      "images in arbitrary poses, which encourages it to extract disentangled and\n",
      "expressive neural textures representing the appearance of different semantic\n",
      "entities. The disentangled representation further enables explicit appearance\n",
      "control. Neural textures of different reference images can be fused to control\n",
      "the appearance of the interested areas. Experimental comparisons show the\n",
      "superiority of the proposed model. Code is available at\n",
      "https://github.com/RenYurui/Neural-Texture-Extraction-Distribution. \n",
      "\n",
      "\n",
      "Human visual sensitivity to spatial details declines towards the periphery.\n",
      "Novel image synthesis techniques, so-called foveated rendering, exploit this\n",
      "observation and reduce the spatial resolution of synthesized images for the\n",
      "periphery, avoiding the synthesis of high-spatial-frequency details that are\n",
      "costly to generate but not perceived by a viewer. However, contemporary\n",
      "techniques do not make a clear distinction between the range of spatial\n",
      "frequencies that must be reproduced and those that can be omitted. For a given\n",
      "eccentricity, there is a range of frequencies that are detectable but not\n",
      "resolvable. While the accurate reproduction of these frequencies is not\n",
      "required, an observer can detect their absence if completely omitted. We use\n",
      "this observation to improve the performance of existing foveated rendering\n",
      "techniques. We demonstrate that this specific range of frequencies can be\n",
      "efficiently replaced with procedural noise whose parameters are carefully tuned\n",
      "to image content and human perception. Consequently, these frequencies do not\n",
      "have to be synthesized during rendering, allowing more aggressive foveation,\n",
      "and they can be replaced by noise generated in a less expensive post-processing\n",
      "step, leading to improved performance of the rendering system. Our main\n",
      "contribution is a perceptually-inspired technique for deriving the parameters\n",
      "of the noise required for the enhancement and its calibration. The method\n",
      "operates on rendering output and runs at rates exceeding 200FPS at 4K\n",
      "resolution, making it suitable for integration with real-time foveated\n",
      "rendering systems for VR and AR devices. We validate our results and compare\n",
      "them to the existing contrast enhancement technique in user experiments. \n",
      "\n",
      "\n",
      "Existing text-guided image manipulation methods aim to modify the appearance\n",
      "of the image or to edit a few objects in a virtual or simple scenario, which is\n",
      "far from practical application. In this work, we study a novel task on\n",
      "text-guided image manipulation on the entity level in the real world. The task\n",
      "imposes three basic requirements, (1) to edit the entity consistent with the\n",
      "text descriptions, (2) to preserve the text-irrelevant regions, and (3) to\n",
      "merge the manipulated entity into the image naturally. To this end, we propose\n",
      "a new transformer-based framework based on the two-stage image synthesis\n",
      "method, namely \\textbf{ManiTrans}, which can not only edit the appearance of\n",
      "entities but also generate new entities corresponding to the text guidance. Our\n",
      "framework incorporates a semantic alignment module to locate the image regions\n",
      "to be manipulated, and a semantic loss to help align the relationship between\n",
      "the vision and language. We conduct extensive experiments on the real datasets,\n",
      "CUB, Oxford, and COCO datasets to verify that our method can distinguish the\n",
      "relevant and irrelevant regions and achieve more precise and flexible\n",
      "manipulation compared with baseline methods. The project homepage is\n",
      "\\url{https://jawang19.github.io/manitrans}. \n",
      "\n",
      "\n",
      "Modern generative models, such as generative adversarial networks (GANs),\n",
      "hold tremendous promise for several areas of medical imaging, such as\n",
      "unconditional medical image synthesis, image restoration, reconstruction and\n",
      "translation, and optimization of imaging systems. However, procedures for\n",
      "establishing stochastic image models (SIMs) using GANs remain generic and do\n",
      "not address specific issues relevant to medical imaging. In this work,\n",
      "canonical SIMs that simulate realistic vessels in angiography images are\n",
      "employed to evaluate procedures for establishing SIMs using GANs. The GAN-based\n",
      "SIM is compared to the canonical SIM based on its ability to reproduce those\n",
      "statistics that are meaningful to the particular medically realistic SIM\n",
      "considered. It is shown that evaluating GANs using classical metrics and\n",
      "medically relevant metrics may lead to different conclusions about the fidelity\n",
      "of the trained GANs. This work highlights the need for the development of\n",
      "objective metrics for evaluating GANs. \n",
      "\n",
      "\n",
      "In many practical applications of long-range imaging such as biometrics and\n",
      "surveillance, thermal imagining modalities are often used to capture images in\n",
      "low-light and nighttime conditions. However, such imaging systems often suffer\n",
      "from atmospheric turbulence, which introduces severe blur and deformation\n",
      "artifacts to the captured images. Such an issue is unavoidable in long-range\n",
      "imaging and significantly decreases the face verification accuracy. In this\n",
      "paper, we first investigate the problem with a turbulence simulation method on\n",
      "real-world thermal images. An end-to-end reconstruction method is then proposed\n",
      "which can directly transform thermal images into visible-spectrum images by\n",
      "utilizing natural image priors based on a pre-trained StyleGAN2 network.\n",
      "Compared with the existing two-steps methods of consecutive turbulence\n",
      "mitigation and thermal to visible image translation, our method is demonstrated\n",
      "to be effective in terms of both the visual quality of the reconstructed\n",
      "results and face verification accuracy. Moreover, to the best of our knowledge,\n",
      "this is the first work that studies the problem of thermal to visible image\n",
      "translation under atmospheric turbulence. \n",
      "\n",
      "\n",
      "Semantic image synthesis is a challenging task with many practical\n",
      "applications. Albeit remarkable progress has been made in semantic image\n",
      "synthesis with spatially-adaptive normalization and existing methods normalize\n",
      "the feature activations under the coarse-level guidance (e.g., semantic class).\n",
      "However, different parts of a semantic object (e.g., wheel and window of car)\n",
      "are quite different in structures and textures, making blurry synthesis results\n",
      "usually inevitable due to the missing of fine-grained guidance. In this paper,\n",
      "we propose a novel normalization module, termed as REtrieval-based Spatially\n",
      "AdaptIve normaLization (RESAIL), for introducing pixel level fine-grained\n",
      "guidance to the normalization architecture. Specifically, we first present a\n",
      "retrieval paradigm by finding a content patch of the same semantic class from\n",
      "training set with the most similar shape to each test semantic mask. Then,\n",
      "RESAIL is presented to use the retrieved patch for guiding the feature\n",
      "normalization of corresponding region, and can provide pixel level fine-grained\n",
      "guidance, thereby greatly mitigating blurry synthesis results. Moreover,\n",
      "distorted ground-truth images are also utilized as alternatives of\n",
      "retrieval-based guidance for feature normalization, further benefiting model\n",
      "training and improving visual quality of generated images. Experiments on\n",
      "several challenging datasets show that our RESAIL performs favorably against\n",
      "state-of-the-arts in terms of quantitative metrics, visual quality, and\n",
      "subjective evaluation. The source code and pre-trained models will be publicly\n",
      "available. \n",
      "\n",
      "\n",
      "Positional encodings have enabled recent works to train a single adversarial\n",
      "network that can generate images of different scales. However, these approaches\n",
      "are either limited to a set of discrete scales or struggle to maintain good\n",
      "perceptual quality at the scales for which the model is not trained explicitly.\n",
      "We propose the design of scale-consistent positional encodings invariant to our\n",
      "generator's layers transformations. This enables the generation of\n",
      "arbitrary-scale images even at scales unseen during training. Moreover, we\n",
      "incorporate novel inter-scale augmentations into our pipeline and partial\n",
      "generation training to facilitate the synthesis of consistent images at\n",
      "arbitrary scales. Lastly, we show competitive results for a continuum of scales\n",
      "on various commonly used datasets for image synthesis. \n",
      "\n",
      "\n",
      "Despite astonishing progress, generating realistic images of complex scenes\n",
      "remains a challenging problem. Recently, layout-to-image synthesis approaches\n",
      "have attracted much interest by conditioning the generator on a list of\n",
      "bounding boxes and corresponding class labels. However, previous approaches are\n",
      "very restrictive because the set of labels is fixed a priori. Meanwhile,\n",
      "text-to-image synthesis methods have substantially improved and provide a\n",
      "flexible way for conditional image generation. In this work, we introduce dense\n",
      "text-to-image (DT2I) synthesis as a new task to pave the way toward more\n",
      "intuitive image generation. Furthermore, we propose DTC-GAN, a novel method to\n",
      "generate images from semantically rich region descriptions, and a multi-modal\n",
      "region feature matching loss to encourage semantic image-text matching. Our\n",
      "results demonstrate the capability of our approach to generate plausible images\n",
      "of complex scenes using region captions. \n",
      "\n",
      "\n",
      "The microstructure is significant for exploring the physical properties of\n",
      "hardened cement paste. In general, the microstructures of hardened cement paste\n",
      "are obtained by microscopy. As a popular method, scanning electron microscopy\n",
      "(SEM) can acquire high-quality 2D images but fails to obtain 3D\n",
      "microstructures.Although several methods, such as microtomography (Micro-CT)\n",
      "and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM), can acquire 3D\n",
      "microstructures, these fail to obtain high-quality 3D images or consume\n",
      "considerable cost. To address these issues, a method based on solid texture\n",
      "synthesis is proposed, synthesizing high-quality 3D microstructural image of\n",
      "hardened cement paste. This method includes 2D backscattered electron (BSE)\n",
      "image acquisition and 3D microstructure synthesis phases. In the approach, the\n",
      "synthesis model is based on solid texture synthesis, capturing microstructure\n",
      "information of the acquired 2D BSE image and generating high-quality 3D\n",
      "microstructures. In experiments, the method is verified on actual 3D Micro-CT\n",
      "images and 2D BSE images. Finally, qualitative experiments demonstrate that the\n",
      "3D microstructures generated by our method have similar visual characteristics\n",
      "to the given 2D example. Furthermore, quantitative experiments prove that the\n",
      "synthetic 3D results are consistent with the actual instance in terms of\n",
      "porosity, particle size distribution, and grey scale co-occurrence matrix. \n",
      "\n",
      "\n",
      "The effects of adversarial training on semantic segmentation networks has not\n",
      "been thoroughly explored. While previous work has shown that\n",
      "adversarially-trained image classifiers can be used to perform image synthesis,\n",
      "we have yet to understand how best to leverage an adversarially-trained\n",
      "segmentation network to do the same. Using a simple optimizer, we demonstrate\n",
      "that adversarially-trained semantic segmentation networks can be used to\n",
      "perform image inpainting and generation. Our experiments demonstrate that\n",
      "adversarially-trained segmentation networks are more robust and indeed exhibit\n",
      "perceptually-aligned gradients which help in producing plausible image\n",
      "inpaintings. We seek to place additional weight behind the hypothesis that\n",
      "adversarially robust models exhibit gradients that are more\n",
      "perceptually-aligned with human vision. Through image synthesis, we argue that\n",
      "perceptually-aligned gradients promote a better understanding of a neural\n",
      "network's learned representations and aid in making neural networks more\n",
      "interpretable. \n",
      "\n",
      "\n",
      "Recent years have witnessed substantial progress in semantic image synthesis,\n",
      "it is still challenging in synthesizing photo-realistic images with rich\n",
      "details. Most previous methods focus on exploiting the given semantic map,\n",
      "which just captures an object-level layout for an image. Obviously, a\n",
      "fine-grained part-level semantic layout will benefit object details generation,\n",
      "and it can be roughly inferred from an object's shape. In order to exploit the\n",
      "part-level layouts, we propose a Shape-aware Position Descriptor (SPD) to\n",
      "describe each pixel's positional feature, where object shape is explicitly\n",
      "encoded into the SPD feature. Furthermore, a Semantic-shape Adaptive Feature\n",
      "Modulation (SAFM) block is proposed to combine the given semantic map and our\n",
      "positional features to produce adaptively modulated features. Extensive\n",
      "experiments demonstrate that the proposed SPD and SAFM significantly improve\n",
      "the generation of objects with rich details. Moreover, our method performs\n",
      "favorably against the SOTA methods in terms of quantitative and qualitative\n",
      "evaluation. The source code and model are available at\n",
      "https://github.com/cszy98/SAFM. \n",
      "\n",
      "\n",
      "Although progress has been made for text-to-image synthesis, previous methods\n",
      "fall short of generalizing to unseen or underrepresented attribute compositions\n",
      "in the input text. Lacking compositionality could have severe implications for\n",
      "robustness and fairness, e.g., inability to synthesize the face images of\n",
      "underrepresented demographic groups. In this paper, we introduce a new\n",
      "framework, StyleT2I, to improve the compositionality of text-to-image\n",
      "synthesis. Specifically, we propose a CLIP-guided Contrastive Loss to better\n",
      "distinguish different compositions among different sentences. To further\n",
      "improve the compositionality, we design a novel Semantic Matching Loss and a\n",
      "Spatial Constraint to identify attributes' latent directions for intended\n",
      "spatial region manipulations, leading to better disentangled latent\n",
      "representations of attributes. Based on the identified latent directions of\n",
      "attributes, we propose Compositional Attribute Adjustment to adjust the latent\n",
      "code, resulting in better compositionality of image synthesis. In addition, we\n",
      "leverage the $\\ell_2$-norm regularization of identified latent directions (norm\n",
      "penalty) to strike a nice balance between image-text alignment and image\n",
      "fidelity. In the experiments, we devise a new dataset split and an evaluation\n",
      "metric to evaluate the compositionality of text-to-image synthesis models. The\n",
      "results show that StyleT2I outperforms previous approaches in terms of the\n",
      "consistency between the input text and synthesized images and achieves higher\n",
      "fidelity. \n",
      "\n",
      "\n",
      "Existing text-to-image synthesis methods generally are only applicable to\n",
      "words in the training dataset. However, human faces are so variable to be\n",
      "described with limited words. So this paper proposes the first free-style\n",
      "text-to-face method namely AnyFace enabling much wider open world applications\n",
      "such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel\n",
      "two-stream framework for face image synthesis and manipulation given arbitrary\n",
      "descriptions of the human face. Specifically, one stream performs text-to-face\n",
      "generation and the other conducts face image reconstruction. Facial text and\n",
      "image features are extracted using the CLIP (Contrastive Language-Image\n",
      "Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD)\n",
      "module is designed to align the linguistic and visual features across these two\n",
      "streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model\n",
      "fine-grained features and improve facial diversity. Extensive experiments on\n",
      "Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of\n",
      "AnyFace over state-of-the-art methods. AnyFace can achieve high-quality,\n",
      "high-resolution, and high-diversity face synthesis and manipulation results\n",
      "without any constraints on the number and content of input captions. \n",
      "\n",
      "\n",
      "Traffic light recognition, as a critical component of the perception module\n",
      "of self-driving vehicles, plays a vital role in the intelligent transportation\n",
      "systems. The prevalent deep learning based traffic light recognition methods\n",
      "heavily hinge on the large quantity and rich diversity of training data.\n",
      "However, it is quite challenging to collect data in various rare scenarios such\n",
      "as flashing, blackout or extreme weather, thus resulting in the imbalanced\n",
      "distribution of training data and consequently the degraded performance in\n",
      "recognizing rare classes. In this paper, we seek to improve traffic light\n",
      "recognition by leveraging data synthesis. Inspired by the generative\n",
      "adversarial networks (GANs), we propose a novel traffic light generation\n",
      "approach TL-GAN to synthesize the data of rare classes to improve traffic light\n",
      "recognition for autonomous driving. TL-GAN disentangles traffic light sequence\n",
      "generation into image synthesis and sequence assembling. In the image synthesis\n",
      "stage, our approach enables conditional generation to allow full control of the\n",
      "color of the generated traffic light images. In the sequence assembling stage,\n",
      "we design the style mixing and adaptive template to synthesize realistic and\n",
      "diverse traffic light sequences. Extensive experiments show that the proposed\n",
      "TL-GAN renders remarkable improvement over the baseline without using the\n",
      "generated data, leading to the state-of-the-art performance in comparison with\n",
      "the competing algorithms that are used for general image synthesis and data\n",
      "imbalance tackling. \n",
      "\n",
      "\n",
      "In this work, we propose a non-iterative Gaussian transformation strategy\n",
      "based on copula function, which doesn't require some commonly seen restrictive\n",
      "assumptions in the previous studies such as the elliptically symmetric\n",
      "distribution assumption and the linear independent component analysis\n",
      "assumption. Theoretical properties guarantee the proposed strategy can exactly\n",
      "transfer any random variable vector with a continuous multivariate distribution\n",
      "to a variable vector that follows a multivariate Gaussian distribution.\n",
      "Simulation studies also demonstrate the outperformance of such a strategy\n",
      "compared to some other methods like Box-Cox Gaussianization and radial\n",
      "Gaussianization. An application for probability density estimation for image\n",
      "synthesis is also shown. \n",
      "\n",
      "\n",
      "In this paper, we tackle the problem of synthesizing a ground-view panorama\n",
      "image conditioned on a top-view aerial image, which is a challenging problem\n",
      "due to the large gap between the two image domains with different view-points.\n",
      "Instead of learning cross-view mapping in a feedforward pass, we propose a\n",
      "novel adversarial feedback GAN framework named PanoGAN with two key components:\n",
      "an adversarial feedback module and a dual branch discrimination strategy.\n",
      "First, the aerial image is fed into the generator to produce a target panorama\n",
      "image and its associated segmentation map in favor of model training with\n",
      "layout semantics. Second, the feature responses of the discriminator encoded by\n",
      "our adversarial feedback module are fed back to the generator to refine the\n",
      "intermediate representations, so that the generation performance is continually\n",
      "improved through an iterative generation process. Third, to pursue\n",
      "high-fidelity and semantic consistency of the generated panorama image, we\n",
      "propose a pixel-segmentation alignment mechanism under the dual branch\n",
      "discrimiantion strategy to facilitate cooperation between the generator and the\n",
      "discriminator. Extensive experimental results on two challenging cross-view\n",
      "image datasets show that PanoGAN enables high-quality panorama image generation\n",
      "with more convincing details than state-of-the-art approaches. The source code\n",
      "and trained models are available at \\url{https://github.com/sswuai/PanoGAN}. \n",
      "\n",
      "\n",
      "The removal of non-brain signal from magnetic resonance imaging (MRI) data,\n",
      "known as skull-stripping, is an integral component of many neuroimage analysis\n",
      "streams. Despite their abundance, popular classical skull-stripping methods are\n",
      "usually tailored to images with specific acquisition properties, namely\n",
      "near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are\n",
      "prevalent in research settings. As a result, existing tools tend to adapt\n",
      "poorly to other image types, such as stacks of thick slices acquired with fast\n",
      "spin-echo (FSE) MRI that are common in the clinic. While learning-based\n",
      "approaches for brain extraction have gained traction in recent years, these\n",
      "methods face a similar burden, as they are only effective for image types seen\n",
      "during the training procedure. To achieve robust skull-stripping across a\n",
      "landscape of imaging protocols, we introduce SynthStrip, a rapid,\n",
      "learning-based brain-extraction tool. By leveraging anatomical segmentations to\n",
      "generate an entirely synthetic training dataset with anatomies, intensity\n",
      "distributions, and artifacts that far exceed the realistic range of medical\n",
      "images, SynthStrip learns to successfully generalize to a variety of real\n",
      "acquired brain images, removing the need for training data with target\n",
      "contrasts. We demonstrate the efficacy of SynthStrip for a diverse set of image\n",
      "acquisitions and resolutions across subject populations, ranging from newborn\n",
      "to adult. We show substantial improvements in accuracy over popular\n",
      "skull-stripping baselines -- all with a single trained model. Our method and\n",
      "labeled evaluation data are available at https://w3id.org/synthstrip. \n",
      "\n",
      "\n",
      "This work digs into a root question in human perception: can face geometry be\n",
      "gleaned from one's voices? Previous works that study this question only adopt\n",
      "developments in image synthesis and convert voices into face images to show\n",
      "correlations, but working on the image domain unavoidably involves predicting\n",
      "attributes that voices cannot hint, including facial textures, hairstyles, and\n",
      "backgrounds. We instead investigate the ability to reconstruct 3D faces to\n",
      "concentrate on only geometry, which is much more physiologically grounded. We\n",
      "propose our analysis framework, Cross-Modal Perceptionist, under both\n",
      "supervised and unsupervised learning. First, we construct a dataset,\n",
      "Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,\n",
      "making supervised learning possible. Second, we use a knowledge distillation\n",
      "mechanism to study whether face geometry can still be gleaned from voices\n",
      "without paired voices and 3D face data under limited availability of 3D face\n",
      "scans. We break down the core question into four parts and perform visual and\n",
      "numerical analyses as responses to the core question. Our findings echo those\n",
      "in physiology and neuroscience about the correlation between voices and facial\n",
      "structures. The work provides future human-centric cross-modal learning with\n",
      "explainable foundations. See our project page:\n",
      "https://choyingw.github.io/works/Voice2Mesh/index.html \n",
      "\n",
      "\n",
      "Video frame interpolation (VFI) aims to improve the temporal resolution of a\n",
      "video sequence. Most of the existing deep learning based VFI methods adopt\n",
      "off-the-shelf optical flow algorithms to estimate the bidirectional flows and\n",
      "interpolate the missing frames accordingly. Though having achieved a great\n",
      "success, these methods require much human experience to tune the bidirectional\n",
      "flows and often generate unpleasant results when the estimated flows are not\n",
      "accurate. In this work, we rethink the VFI problem and formulate it as a\n",
      "continuous image transition (CIT) task, whose key issue is to transition an\n",
      "image from one space to another space continuously. More specifically, we learn\n",
      "to implicitly decouple the images into a translatable flow space and a\n",
      "non-translatable feature space. The former depicts the translatable states\n",
      "between the given images, while the later aims to reconstruct the intermediate\n",
      "features that cannot be directly translated. In this way, we can easily perform\n",
      "image interpolation in the flow space and intermediate image synthesis in the\n",
      "feature space, obtaining a CIT model. The proposed space decoupled learning\n",
      "(SDL) approach is simple to implement, while it provides an effective framework\n",
      "to a variety of CIT problems beyond VFI, such as style transfer and image\n",
      "morphing. Our extensive experiments on a variety of CIT tasks demonstrate the\n",
      "superiority of SDL to existing methods. The source code and models can be found\n",
      "at \\url{https://github.com/yangxy/SDL}. \n",
      "\n",
      "\n",
      "Perceiving the similarity between images has been a long-standing and\n",
      "fundamental problem underlying various visual generation tasks. Predominant\n",
      "approaches measure the inter-image distance by computing pointwise absolute\n",
      "deviations, which tends to estimate the median of instance distributions and\n",
      "leads to blurs and artifacts in the generated images. This paper presents\n",
      "MoNCE, a versatile metric that introduces image contrast to learn a calibrated\n",
      "metric for the perception of multifaceted inter-image distances. Unlike vanilla\n",
      "contrast which indiscriminately pushes negative samples from the anchor\n",
      "regardless of their similarity, we propose to re-weight the pushing force of\n",
      "negative samples adaptively according to their similarity to the anchor, which\n",
      "facilitates the contrastive learning from informative negative samples. Since\n",
      "multiple patch-level contrastive objectives are involved in image distance\n",
      "measurement, we introduce optimal transport in MoNCE to modulate the pushing\n",
      "force of negative samples collaboratively across multiple contrastive\n",
      "objectives. Extensive experiments over multiple image translation tasks show\n",
      "that the proposed MoNCE outperforms various prevailing metrics substantially.\n",
      "The code is available at https://github.com/fnzhan/MoNCE. \n",
      "\n",
      "\n",
      "Spatial commonsense, the knowledge about spatial position and relationship\n",
      "between objects (like the relative size of a lion and a girl, and the position\n",
      "of a boy relative to a bicycle when cycling), is an important part of\n",
      "commonsense knowledge. Although pretrained language models (PLMs) succeed in\n",
      "many NLP tasks, they are shown to be ineffective in spatial commonsense\n",
      "reasoning. Starting from the observation that images are more likely to exhibit\n",
      "spatial commonsense than texts, we explore whether models with visual signals\n",
      "learn more spatial commonsense than text-based PLMs. We propose a spatial\n",
      "commonsense benchmark that focuses on the relative scales of objects, and the\n",
      "positional relationship between people and objects under different actions. We\n",
      "probe PLMs and models with visual signals, including vision-language pretrained\n",
      "models and image synthesis models, on this benchmark, and find that image\n",
      "synthesis models are more capable of learning accurate and consistent spatial\n",
      "knowledge than other models. The spatial knowledge from image synthesis models\n",
      "also helps in natural language understanding tasks that require spatial\n",
      "commonsense. \n",
      "\n",
      "\n",
      "Style-guided text image generation tries to synthesize text image by\n",
      "imitating reference image's appearance while keeping text content unaltered.\n",
      "The text image appearance includes many aspects. In this paper, we focus on\n",
      "transferring style image's background and foreground color patterns to the\n",
      "content image to generate photo-realistic text image. To achieve this goal, we\n",
      "propose 1) a content-style cross attention based pixel sampling approach to\n",
      "roughly mimicking the style text image's background; 2) a pixel-wise style\n",
      "modulation technique to transfer varying color patterns of the style image to\n",
      "the content image spatial-adaptively; 3) a cross attention based multi-scale\n",
      "style fusion approach to solving text foreground misalignment issue between\n",
      "style and content images; 4) an image patch shuffling strategy to create style,\n",
      "content and ground truth image tuples for training. Experimental results on\n",
      "Chinese handwriting text image synthesis with SCUT-HCCDoc and CASIA-OLHWDB\n",
      "datasets demonstrate that the proposed method can improve the quality of\n",
      "synthetic text images and make them more photo-realistic. \n",
      "\n",
      "\n",
      "The increasing availability of wireless access points (APs) is leading\n",
      "towards human sensing applications based on Wi-Fi signals as support or\n",
      "alternative tools to the widespread visual sensors, where the signals enable to\n",
      "address well-known vision-related problems such as illumination changes or\n",
      "occlusions. Indeed, using image synthesis techniques to translate radio\n",
      "frequencies to the visible spectrum can become essential to obtain otherwise\n",
      "unavailable visual data. This domain-to-domain translation is feasible because\n",
      "both objects and people affect electromagnetic waves, causing radio and optical\n",
      "frequencies variations. In literature, models capable of inferring\n",
      "radio-to-visual features mappings have gained momentum in the last few years\n",
      "since frequency changes can be observed in the radio domain through the channel\n",
      "state information (CSI) of Wi-Fi APs, enabling signal-based feature extraction,\n",
      "e.g., amplitude. On this account, this paper presents a novel two-branch\n",
      "generative neural network that effectively maps radio data into visual\n",
      "features, following a teacher-student design that exploits a cross-modality\n",
      "supervision strategy. The latter conditions signal-based features in the visual\n",
      "domain to completely replace visual data. Once trained, the proposed method\n",
      "synthesizes human silhouette and skeleton videos using exclusively Wi-Fi\n",
      "signals. The approach is evaluated on publicly available data, where it obtains\n",
      "remarkable results for both silhouette and skeleton videos generation,\n",
      "demonstrating the effectiveness of the proposed cross-modality supervision\n",
      "strategy. \n",
      "\n",
      "\n",
      "Deep generative models, like GANs, have considerably improved the state of\n",
      "the art in image synthesis, and are able to generate near photo-realistic\n",
      "images in structured domains such as human faces. Based on this success, recent\n",
      "work on image editing proceeds by projecting images to the GAN latent space and\n",
      "manipulating the latent vector. However, these approaches are limited in that\n",
      "only images from a narrow domain can be transformed, and with only a limited\n",
      "number of editing operations. We propose FlexIT, a novel method which can take\n",
      "any input image and a user-defined text instruction for editing. Our method\n",
      "achieves flexible and natural editing, pushing the limits of semantic image\n",
      "translation. First, FlexIT combines the input image and text into a single\n",
      "target point in the CLIP multimodal embedding space. Via the latent space of an\n",
      "auto-encoder, we iteratively transform the input image toward the target point,\n",
      "ensuring coherence and quality with a variety of novel regularization terms. We\n",
      "propose an evaluation protocol for semantic image translation, and thoroughly\n",
      "evaluate our method on ImageNet. Code will be made publicly available. \n",
      "\n",
      "\n",
      "In this paper we present a compositing image synthesis method that generates\n",
      "RGB canvases with well aligned segmentation maps and sparse depth maps, coupled\n",
      "with an in-painting network that transforms the RGB canvases into high quality\n",
      "RGB images and the sparse depth maps into pixel-wise dense depth maps. We\n",
      "benchmark our method in terms of structural alignment and image quality,\n",
      "showing an increase in mIoU over SOTA by 3.7 percentage points and a highly\n",
      "competitive FID. Furthermore, we analyse the quality of the generated data as\n",
      "training data for semantic segmentation and depth completion, and show that our\n",
      "approach is more suited for this purpose than other methods. \n",
      "\n",
      "\n",
      "Magnetic Resonance Imaging (MRI) typically recruits multiple sequences\n",
      "(defined here as \"modalities\"). As each modality is designed to offer different\n",
      "anatomical and functional clinical information, there are evident disparities\n",
      "in the imaging content across modalities. Inter- and intra-modality affine and\n",
      "non-rigid image registration is an essential medical image analysis process in\n",
      "clinical imaging, as for example before imaging biomarkers need to be derived\n",
      "and clinically evaluated across different MRI modalities, time phases and\n",
      "slices. Although commonly needed in real clinical scenarios, affine and\n",
      "non-rigid image registration is not extensively investigated using a single\n",
      "unsupervised model architecture. In our work, we present an un-supervised deep\n",
      "learning registration methodology which can accurately model affine and\n",
      "non-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a\n",
      "fundamental inter-modality registration property that is not considered in deep\n",
      "learning registration algorithms. To address inverse-consistency, our\n",
      "methodology performs bi-directional cross-modality image synthesis to learn\n",
      "modality-invariant latent rep-resentations, while involves two factorised\n",
      "transformation networks and an inverse-consistency loss to learn\n",
      "topology-preserving anatomical transformations. Overall, our model (named\n",
      "\"FIRE\") shows improved performances against the reference standard baseline\n",
      "method on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI\n",
      "data experiments. \n",
      "\n",
      "\n",
      "Interactive image synthesis from user-guided input is a challenging task when\n",
      "users wish to control the scene structure of a generated image with\n",
      "ease.Although remarkable progress has been made on layout-based image synthesis\n",
      "approaches, in order to get realistic fake image in interactive scene, existing\n",
      "methods require high-precision inputs, which probably need adjustment several\n",
      "times and are unfriendly to novice users. When placement of bounding boxes is\n",
      "subject to perturbation, layout-based models suffer from \"missing regions\" in\n",
      "the constructed semantic layouts and hence undesirable artifacts in the\n",
      "generated images. In this work, we propose Panoptic Layout Generative\n",
      "Adversarial Networks (PLGAN) to address this challenge. The PLGAN employs\n",
      "panoptic theory which distinguishes object categories between \"stuff\" with\n",
      "amorphous boundaries and \"things\" with well-defined shapes, such that stuff and\n",
      "instance layouts are constructed through separate branches and later fused into\n",
      "panoptic layouts. In particular, the stuff layouts can take amorphous shapes\n",
      "and fill up the missing regions left out by the instance layouts. We\n",
      "experimentally compare our PLGAN with state-of-the-art layout-based models on\n",
      "the COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN\n",
      "are not only visually demonstrated but quantitatively verified in terms of\n",
      "inception score, Fr\\'echet inception distance, classification accuracy score,\n",
      "and coverage. \n",
      "\n",
      "\n",
      "In the past decades, the excessive use of the last-generation GAN (Generative\n",
      "Adversarial Networks) models in computer vision has enabled the creation of\n",
      "artificial face images that are visually indistinguishable from genuine ones.\n",
      "These images are particularly used in adversarial settings to create fake\n",
      "social media accounts and other fake online profiles. Such malicious activities\n",
      "can negatively impact the trustworthiness of users identities. On the other\n",
      "hand, the recent development of GAN models may create high-quality face images\n",
      "without evidence of spatial artifacts. Therefore, reassembling uniform color\n",
      "channel correlations is a challenging research problem. To face these\n",
      "challenges, we need to develop efficient tools able to differentiate between\n",
      "fake and authentic face images. In this chapter, we propose a new strategy to\n",
      "differentiate GAN-generated images from authentic images by leveraging spectral\n",
      "band discrepancies, focusing on artificial face image synthesis. In particular,\n",
      "we enable the digital preservation of face images using the Cross-band\n",
      "co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these\n",
      "techniques and feed them to a Convolutional Neural Networks (CNN) architecture\n",
      "to identify the real from artificial faces. Additionally, we show that the\n",
      "performance boost is particularly significant and achieves more than 92% in\n",
      "different post-processing environments. Finally, we provide several research\n",
      "observations demonstrating that this strategy improves a comparable detection\n",
      "method based only on intra-band spatial co-occurrences. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have established themselves as a\n",
      "prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating\n",
      "case study, owing to its remarkable visual quality and an ability to support a\n",
      "large array of downstream tasks. This state-of-the-art report covers the\n",
      "StyleGAN architecture, and the ways it has been employed since its conception,\n",
      "while also analyzing its severe limitations. It aims to be of use for both\n",
      "newcomers, who wish to get a grasp of the field, and for more experienced\n",
      "readers that might benefit from seeing current research trends and existing\n",
      "tools laid out. Among StyleGAN's most interesting aspects is its learned latent\n",
      "space. Despite being learned with no supervision, it is surprisingly\n",
      "well-behaved and remarkably disentangled. Combined with StyleGAN's visual\n",
      "quality, these properties gave rise to unparalleled editing capabilities.\n",
      "However, the control offered by StyleGAN is inherently limited to the\n",
      "generator's learned distribution, and can only be applied to images generated\n",
      "by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world\n",
      "scenarios, the study of GAN inversion and latent space embedding has quickly\n",
      "gained in popularity. Meanwhile, this same study has helped shed light on the\n",
      "inner workings and limitations of StyleGAN. We map out StyleGAN's impressive\n",
      "story through these investigations, and discuss the details that have made\n",
      "StyleGAN the go-to generator. We further elaborate on the visual priors\n",
      "StyleGAN constructs, and discuss their use in downstream discriminative tasks.\n",
      "Looking forward, we point out StyleGAN's limitations and speculate on current\n",
      "trends and promising directions for future research, such as task and target\n",
      "specific fine-tuning. \n",
      "\n",
      "\n",
      "We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image\n",
      "synthesis framework that generates non-repetitive images with 4K or higher\n",
      "resolution from a single training image. OUR-GAN generates a visually coherent\n",
      "image at low resolution and then gradually increases the resolution by\n",
      "super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize\n",
      "large-scale shapes with fine details while maintaining long-range coherence,\n",
      "which is difficult with conventional generative models that generate large\n",
      "images based on the patch distribution learned from relatively small images.\n",
      "OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or\n",
      "higher UHR images with limited memory, preventing discontinuity at the\n",
      "boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity\n",
      "by adding vertical positional embeddings to the feature maps. In experiments on\n",
      "the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual\n",
      "coherency, and diversity compared with existing methods. The synthesized images\n",
      "are presented at https://anonymous-62348.github.io. \n",
      "\n",
      "\n",
      "We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object\n",
      "or a scene of a specific class, conditioned on a single input image. This is a\n",
      "challenging task, as training NeRF requires multiple views of the same scene,\n",
      "coupled with corresponding poses, which are hard to obtain. Our method is based\n",
      "on $\\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,\n",
      "which maps random latent codes to radiance fields of a class of objects. We\n",
      "jointly optimize (1) the $\\pi$-GAN objective to utilize its high-fidelity\n",
      "3D-aware generation and (2) a carefully designed reconstruction objective. The\n",
      "latter includes an encoder coupled with $\\pi$-GAN generator to form an\n",
      "auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is\n",
      "unsupervised, capable of being trained with independent images without 3D,\n",
      "multi-view, or pose supervision. Applications of our pipeline include 3d avatar\n",
      "generation, object-centric novel view synthesis with a single input image, and\n",
      "3d-aware super-resolution, to name a few. \n",
      "\n",
      "\n",
      "Text-to-image generation intends to automatically produce a photo-realistic\n",
      "image, conditioned on a textual description. It can be potentially employed in\n",
      "the field of art creation, data augmentation, photo-editing, etc. Although many\n",
      "efforts have been dedicated to this task, it remains particularly challenging\n",
      "to generate believable, natural scenes. To facilitate the real-world\n",
      "applications of text-to-image synthesis, we focus on studying the following\n",
      "three issues: 1) How to ensure that generated samples are believable, realistic\n",
      "or natural? 2) How to exploit the latent space of the generator to edit a\n",
      "synthesized image? 3) How to improve the explainability of a text-to-image\n",
      "generation framework? In this work, we constructed two novel data sets (i.e.,\n",
      "the Good & Bad bird and face data sets) consisting of successful as well as\n",
      "unsuccessful generated samples, according to strict criteria. To effectively\n",
      "and efficiently acquire high-quality images by increasing the probability of\n",
      "generating Good latent codes, we use a dedicated Good/Bad classifier for\n",
      "generated images. It is based on a pre-trained front end and fine-tuned on the\n",
      "basis of the proposed Good & Bad data set. After that, we present a novel\n",
      "algorithm which identifies semantically-understandable directions in the latent\n",
      "space of a conditional text-to-image GAN architecture by performing independent\n",
      "component analysis on the pre-trained weight values of the generator.\n",
      "Furthermore, we develop a background-flattening loss (BFL), to improve the\n",
      "background appearance in the edited image. Subsequently, we introduce linear\n",
      "interpolation analysis between pairs of keywords. This is extended into a\n",
      "similar triangular `linguistic' interpolation in order to take a deep look into\n",
      "what a text-to-image synthesis model has learned within the linguistic\n",
      "embeddings. Our data set is available at\n",
      "https://zenodo.org/record/6283798#.YhkN_ujMI2w. \n",
      "\n",
      "\n",
      "StyleGAN is known to produce high-fidelity images, while also offering\n",
      "unprecedented semantic editing. However, these fascinating abilities have been\n",
      "demonstrated only on a limited set of datasets, which are usually structurally\n",
      "aligned and well curated. In this paper, we show how StyleGAN can be adapted to\n",
      "work on raw uncurated images collected from the Internet. Such image\n",
      "collections impose two main challenges to StyleGAN: they contain many outlier\n",
      "images, and are characterized by a multi-modal distribution. Training StyleGAN\n",
      "on such raw image collections results in degraded image synthesis quality. To\n",
      "meet these challenges, we proposed a StyleGAN-based self-distillation approach,\n",
      "which consists of two main components: (i) A generative-based self-filtering of\n",
      "the dataset to eliminate outlier images, in order to generate an adequate\n",
      "training set, and (ii) Perceptual clustering of the generated images to detect\n",
      "the inherent data modalities, which are then employed to improve StyleGAN's\n",
      "\"truncation trick\" in the image synthesis process. The presented technique\n",
      "enables the generation of high-quality images, while minimizing the loss in\n",
      "diversity of the data. Through qualitative and quantitative evaluation, we\n",
      "demonstrate the power of our approach to new challenging and diverse domains\n",
      "collected from the Internet. New datasets and pre-trained models are available\n",
      "at https://self-distilled-stylegan.github.io/ . \n",
      "\n",
      "\n",
      "Transformers have dominated the field of natural language processing, and\n",
      "recently impacted the computer vision area. In the field of medical image\n",
      "analysis, Transformers have also been successfully applied to full-stack\n",
      "clinical applications, including image synthesis/reconstruction, registration,\n",
      "segmentation, detection, and diagnosis. Our paper aims to promote awareness and\n",
      "application of Transformers in the field of medical image analysis.\n",
      "Specifically, we first overview the core concepts of the attention mechanism\n",
      "built into Transformers and other basic components. Second, we review various\n",
      "Transformer architectures tailored for medical image applications and discuss\n",
      "their limitations. Within this review, we investigate key challenges revolving\n",
      "around the use of Transformers in different learning paradigms, improving the\n",
      "model efficiency, and their coupling with other techniques. We hope this review\n",
      "can give a comprehensive picture of Transformers to the readers in the field of\n",
      "medical image analysis. \n",
      "\n",
      "\n",
      "The goal of this thesis is to present my research contributions towards\n",
      "solving various visual synthesis and generation tasks, comprising image\n",
      "translation, image completion, and completed scene decomposition. This thesis\n",
      "consists of five pieces of work, each of which presents a new learning-based\n",
      "approach for synthesizing images with plausible content as well as visually\n",
      "realistic appearance. Each work demonstrates the superiority of the proposed\n",
      "approach on image synthesis, with some further contributing to other tasks,\n",
      "such as depth estimation. \n",
      "\n",
      "\n",
      "Deep learning based fall detection is one of the crucial tasks for\n",
      "intelligent video surveillance systems, which aims to detect unintentional\n",
      "falls of humans and alarm dangerous situations. In this work, we propose a\n",
      "simple and efficient framework to detect falls through a single and small-sized\n",
      "convolutional neural network. To this end, we first introduce a new image\n",
      "synthesis method that represents human motion in a single frame. This\n",
      "simplifies the fall detection task as an image classification task. Besides,\n",
      "the proposed synthetic data generation method enables to generate a sufficient\n",
      "amount of training dataset, resulting in satisfactory performance even with the\n",
      "small model. At the inference step, we also represent real human motion in a\n",
      "single image by estimating mean of input frames. In the experiment, we conduct\n",
      "both qualitative and quantitative evaluations on URFD and AIHub airport\n",
      "datasets to show the effectiveness of our method. \n",
      "\n",
      "\n",
      "MRI entails a great amount of cost, time and effort for the generation of all\n",
      "the modalities that are recommended for efficient diagnosis and treatment\n",
      "planning. Recent advancements in deep learning research show that generative\n",
      "models have achieved substantial improvement in the aspects of style transfer\n",
      "and image synthesis. In this work, we formulate generating the missing MR\n",
      "modality from existing MR modalities as an imputation problem using style\n",
      "transfer. With a multiple-to-one mapping, we model a network that accommodates\n",
      "domain specific styles in generating the target image. We analyse the style\n",
      "diversity both within and across MR modalities. Our model is tested on the\n",
      "BraTS'18 dataset and the results obtained are observed to be on par with the\n",
      "state-of-the-art in terms of visual metrics, SSIM and PSNR. After being\n",
      "evaluated by two expert radiologists, we show that our model is efficient,\n",
      "extendable, and suitable for clinical applications. \n",
      "\n",
      "\n",
      "Despite the recent advancement of Generative Adversarial Networks (GANs) in\n",
      "learning 3D-aware image synthesis from 2D data, existing methods fail to model\n",
      "indoor scenes due to the large diversity of room layouts and the objects\n",
      "inside. We argue that indoor scenes do not have a shared intrinsic structure,\n",
      "and hence only using 2D images cannot adequately guide the model with the 3D\n",
      "geometry. In this work, we fill in this gap by introducing depth as a 3D prior.\n",
      "Compared with other 3D data formats, depth better fits the convolution-based\n",
      "generation mechanism and is more easily accessible in practice. Specifically,\n",
      "we propose a dual-path generator, where one path is responsible for depth\n",
      "generation, whose intermediate features are injected into the other path as the\n",
      "condition for appearance rendering. Such a design eases the 3D-aware synthesis\n",
      "with explicit geometry information. Meanwhile, we introduce a switchable\n",
      "discriminator both to differentiate real v.s. fake domains and to predict the\n",
      "depth from a given input. In this way, the discriminator can take the spatial\n",
      "arrangement into account and advise the generator to learn an appropriate depth\n",
      "condition. Extensive experimental results suggest that our approach is capable\n",
      "of synthesizing indoor scenes with impressively good quality and 3D\n",
      "consistency, significantly outperforming state-of-the-art alternatives. \n",
      "\n",
      "\n",
      "The existence of completely aligned and paired multi-modal neuroimaging data\n",
      "has proved its effectiveness in diagnosis of brain diseases. However,\n",
      "collecting the full set of well-aligned and paired data is impractical or even\n",
      "luxurious, since the practical difficulties may include high cost, long time\n",
      "acquisition, image corruption, and privacy issues. A realistic solution is to\n",
      "explore either an unsupervised learning or a semi-supervised learning to\n",
      "synthesize the absent neuroimaging data. In this paper, we tend to approach\n",
      "multi-modality brain image synthesis task from different perspectives, which\n",
      "include the level of supervision, the range of modality synthesis, and the\n",
      "synthesis-based downstream tasks. Particularly, we provide in-depth analysis on\n",
      "how cross-modality brain image synthesis can improve the performance of\n",
      "different downstream tasks. Finally, we evaluate the challenges and provide\n",
      "several open directions for this community. All resources are available at\n",
      "https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis \n",
      "\n",
      "\n",
      "Generative transformers have experienced rapid popularity growth in the\n",
      "computer vision community in synthesizing high-fidelity and high-resolution\n",
      "images. The best generative transformer models so far, however, still treat an\n",
      "image naively as a sequence of tokens, and decode an image sequentially\n",
      "following the raster scan ordering (i.e. line-by-line). We find this strategy\n",
      "neither optimal nor efficient. This paper proposes a novel image synthesis\n",
      "paradigm using a bidirectional transformer decoder, which we term MaskGIT.\n",
      "During training, MaskGIT learns to predict randomly masked tokens by attending\n",
      "to tokens in all directions. At inference time, the model begins with\n",
      "generating all tokens of an image simultaneously, and then refines the image\n",
      "iteratively conditioned on the previous generation. Our experiments demonstrate\n",
      "that MaskGIT significantly outperforms the state-of-the-art transformer model\n",
      "on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x.\n",
      "Besides, we illustrate that MaskGIT can be easily extended to various image\n",
      "editing tasks, such as inpainting, extrapolation, and image manipulation. \n",
      "\n",
      "\n",
      "In this work, we address an important problem of optical see through (OST)\n",
      "augmented reality: non-negative image synthesis. Most of the image generation\n",
      "methods fail under this condition, since they assume full control over each\n",
      "pixel and cannot create darker pixels by adding light. In order to solve the\n",
      "non-negative image generation problem in AR image synthesis, prior works have\n",
      "attempted to utilize optical illusion to simulate human vision but fail to\n",
      "preserve lightness constancy well under situations such as high dynamic range.\n",
      "In our paper, we instead propose a method that is able to preserve lightness\n",
      "constancy at a local level, thus capturing high frequency details. Compared\n",
      "with existing work, our method shows strong performance in image-to-image\n",
      "translation tasks, particularly in scenarios such as large scale images, high\n",
      "resolution images, and high dynamic range image transfer. \n",
      "\n",
      "\n",
      "Invertible neural networks based on Coupling Flows CFlows) have various\n",
      "applications such as image synthesis and data compression. The approximation\n",
      "universality for CFlows is of paramount importance to ensure the model\n",
      "expressiveness. In this paper, we prove that CFlows can approximate any\n",
      "diffeomorphism in C^k-norm if its layers can approximate certain\n",
      "single-coordinate transforms. Specifically, we derive that a composition of\n",
      "affine coupling layers and invertible linear transforms achieves this\n",
      "universality. Furthermore, in parametric cases where the diffeomorphism depends\n",
      "on some extra parameters, we prove the corresponding approximation theorems for\n",
      "our proposed parametric coupling flows named Para-CFlows. In practice, we apply\n",
      "Para-CFlows as a neural surrogate model in contextual Bayesian optimization\n",
      "tasks, to demonstrate its superiority over other neural surrogate models in\n",
      "terms of optimization performance. \n",
      "\n",
      "\n",
      "In applications such as optical see-through and projector augmented reality,\n",
      "producing images amounts to solving non-negative image generation, where one\n",
      "can only add light to an existing image. Most image generation methods,\n",
      "however, are ill-suited to this problem setting, as they make the assumption\n",
      "that one can assign arbitrary color to each pixel. In fact, naive application\n",
      "of existing methods fails even in simple domains such as MNIST digits, since\n",
      "one cannot create darker pixels by adding light. We know, however, that the\n",
      "human visual system can be fooled by optical illusions involving certain\n",
      "spatial configurations of brightness and contrast. Our key insight is that one\n",
      "can leverage this behavior to produce high quality images with negligible\n",
      "artifacts. For example, we can create the illusion of darker patches by\n",
      "brightening surrounding pixels. We propose a novel optimization procedure to\n",
      "produce images that satisfy both semantic and non-negativity constraints. Our\n",
      "approach can incorporate existing state-of-the-art methods, and exhibits strong\n",
      "performance in a variety of tasks including image-to-image translation and\n",
      "style transfer. \n",
      "\n",
      "\n",
      "Computer graphics has experienced a recent surge of data-centric approaches\n",
      "for photorealistic and controllable content creation. StyleGAN in particular\n",
      "sets new standards for generative modeling regarding image quality and\n",
      "controllability. However, StyleGAN's performance severely degrades on large\n",
      "unstructured datasets such as ImageNet. StyleGAN was designed for\n",
      "controllability; hence, prior works suspect its restrictive design to be\n",
      "unsuitable for diverse datasets. In contrast, we find the main limiting factor\n",
      "to be the current training strategy. Following the recently introduced\n",
      "Projected GAN paradigm, we leverage powerful neural network priors and a\n",
      "progressive growing strategy to successfully train the latest StyleGAN3\n",
      "generator on ImageNet. Our final model, StyleGAN-XL, sets a new\n",
      "state-of-the-art on large-scale image synthesis and is the first to generate\n",
      "images at a resolution of $1024^2$ at such a dataset scale. We demonstrate that\n",
      "this model can invert and edit images beyond the narrow domain of portraits or\n",
      "specific object classes. \n",
      "\n",
      "\n",
      "The existence of completely aligned and paired multi-modal neuroimaging data\n",
      "has proved its effectiveness in the diagnosis of brain diseases. However,\n",
      "collecting the full set of well-aligned and paired data is impractical, since\n",
      "the practical difficulties may include high cost, long time acquisition, image\n",
      "corruption, and privacy issues. Previously, the misaligned unpaired\n",
      "neuroimaging data (termed as MUD) are generally treated as noisy label.\n",
      "However, such a noisy label-based method fail to accomplish well when\n",
      "misaligned data occurs distortions severely. For example, the angle of rotation\n",
      "is different. In this paper, we propose a novel federated self-supervised\n",
      "learning (FedMed) for brain image synthesis. An affine transform loss (ATL) was\n",
      "formulated to make use of severely distorted images without violating privacy\n",
      "legislation for the hospital. We then introduce a new data augmentation\n",
      "procedure for self-supervised training and fed it into three auxiliary heads,\n",
      "namely auxiliary rotation, auxiliary translation and auxiliary scaling heads.\n",
      "The proposed method demonstrates the advanced performance in both the quality\n",
      "of our synthesized results under a severely misaligned and unpaired data\n",
      "setting, and better stability than other GAN-based algorithms. The proposed\n",
      "method also reduces the demand for deformable registration while encouraging to\n",
      "leverage the misaligned and unpaired data. Experimental results verify the\n",
      "outstanding performance of our learning paradigm compared to other\n",
      "state-of-the-art approaches. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have shown promise in augmenting\n",
      "datasets and boosting convolutional neural networks' (CNN) performance on image\n",
      "classification tasks. But they introduce more hyperparameters to tune as well\n",
      "as the need for additional time and computational power to train supplementary\n",
      "to the CNN. In this work, we examine the potential for Auxiliary-Classifier\n",
      "GANs (AC-GANs) as a 'one-stop-shop' architecture for image classification,\n",
      "particularly in low data regimes. Additionally, we explore modifications to the\n",
      "typical AC-GAN framework, changing the generator's latent space sampling scheme\n",
      "and employing a Wasserstein loss with gradient penalty to stabilize the\n",
      "simultaneous training of image synthesis and classification. Through\n",
      "experiments on images of varying resolutions and complexity, we demonstrate\n",
      "that AC-GANs show promise in image classification, achieving competitive\n",
      "performance with standard CNNs. These methods can be employed as an\n",
      "'all-in-one' framework with particular utility in the absence of large amounts\n",
      "of training data. \n",
      "\n",
      "\n",
      "Utilizing multi-modal neuroimaging data has been proved to be effective to\n",
      "investigate human cognitive activities and certain pathologies. However, it is\n",
      "not practical to obtain the full set of paired neuroimaging data centrally\n",
      "since the collection faces several constraints, e.g., high examination cost,\n",
      "long acquisition time, and image corruption. In addition, these data are\n",
      "dispersed into different medical institutions and thus cannot be aggregated for\n",
      "centralized training considering the privacy issues. There is a clear need to\n",
      "launch a federated learning and facilitate the integration of the dispersed\n",
      "data from different institutions. In this paper, we propose a new benchmark for\n",
      "federated domain translation on unsupervised brain image synthesis (termed as\n",
      "FedMed-GAN) to bridge the gap between federated learning and medical GAN.\n",
      "FedMed-GAN mitigates the mode collapse without sacrificing the performance of\n",
      "generators, and is widely applied to different proportions of unpaired and\n",
      "paired data with variation adaptation property. We treat the gradient penalties\n",
      "by federally averaging algorithm and then leveraging differential privacy\n",
      "gradient descent to regularize the training dynamics. A comprehensive\n",
      "evaluation is provided for comparing FedMed-GAN and other centralized methods,\n",
      "which shows the new state-of-the-art performance by our FedMed-GAN. Our code\n",
      "has been released on the website: https://github.com/M-3LAB/FedMed-GAN \n",
      "\n",
      "\n",
      "Proposed in 2014, Generative Adversarial Networks (GAN) initiated a fresh\n",
      "interest in generative modelling. They immediately achieved state-of-the-art in\n",
      "image synthesis, image-to-image translation, text-to-image generation, image\n",
      "inpainting and have been used in sciences ranging from medicine to high-energy\n",
      "particle physics. Despite their popularity and ability to learn arbitrary\n",
      "distributions, GAN have not been widely applied in recommender systems (RS).\n",
      "Moreover, only few of the techniques that have introduced GAN in RS have\n",
      "employed them directly as a collaborative filtering (CF) model. In this work we\n",
      "propose a new GAN-based approach that learns user and item latent factors in a\n",
      "matrix factorization setting for the generic top-N recommendation problem.\n",
      "Following the vector-wise GAN training approach for RS introduced by CFGAN, we\n",
      "identify 2 unique issues when utilizing GAN for CF. We propose solutions for\n",
      "both of them by using an autoencoder as discriminator and incorporating an\n",
      "additional loss function for the generator. We evaluate our model, GANMF,\n",
      "through well-known datasets in the RS community and show improvements over\n",
      "traditional CF approaches and GAN-based models. Through an ablation study on\n",
      "the components of GANMF we aim to understand the effects of our architectural\n",
      "choices. Finally, we provide a qualitative evaluation of the matrix\n",
      "factorization performance of GANMF. \n",
      "\n",
      "\n",
      "Diffusion Probabilistic models have been shown to generate state-of-the-art\n",
      "results on several competitive image synthesis benchmarks but lack a\n",
      "low-dimensional, interpretable latent space, and are slow at generation. On the\n",
      "other hand, Variational Autoencoders (VAEs) typically have access to a\n",
      "low-dimensional latent space but exhibit poor sample quality. Despite recent\n",
      "advances, VAEs usually require high-dimensional hierarchies of the latent codes\n",
      "to generate high-quality samples. We present DiffuseVAE, a novel generative\n",
      "framework that integrates VAE within a diffusion model framework, and leverage\n",
      "this to design a novel conditional parameterization for diffusion models. We\n",
      "show that the resulting model can improve upon the unconditional diffusion\n",
      "model in terms of sampling efficiency while also equipping diffusion models\n",
      "with the low-dimensional VAE inferred latent code. Furthermore, we show that\n",
      "the proposed model can generate high-resolution samples and exhibits synthesis\n",
      "quality comparable to state-of-the-art models on standard benchmarks. Lastly,\n",
      "we show that the proposed method can be used for controllable image synthesis\n",
      "and also exhibits out-of-the-box capabilities for downstream tasks like image\n",
      "super-resolution and denoising. For reproducibility, our source code is\n",
      "publicly available at \\url{https://github.com/kpandey008/DiffuseVAE}. \n",
      "\n",
      "\n",
      "Conventional methods for the image-text generation tasks mainly tackle the\n",
      "naturally bidirectional generation tasks separately, focusing on designing\n",
      "task-specific frameworks to improve the quality and fidelity of the generated\n",
      "samples. Recently, Vision-Language Pre-training models have greatly improved\n",
      "the performance of the image-to-text generation tasks, but large-scale\n",
      "pre-training models for text-to-image synthesis task are still under-developed.\n",
      "In this paper, we propose ERNIE-ViLG, a unified generative pre-training\n",
      "framework for bidirectional image-text generation with transformer model. Based\n",
      "on the image quantization models, we formulate both image generation and text\n",
      "generation as autoregressive generative tasks conditioned on the text/image\n",
      "input. The bidirectional image-text generative modeling eases the semantic\n",
      "alignments across vision and language. For the text-to-image generation\n",
      "process, we further propose an end-to-end training method to jointly learn the\n",
      "visual sequence generator and the image reconstructor. To explore the landscape\n",
      "of large-scale pre-training for bidirectional text-image generation, we train a\n",
      "10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million\n",
      "(Chinese) image-text pairs which achieves state-of-the-art performance for both\n",
      "text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for\n",
      "text-to-image synthesis and best results on COCO-CN and AIC-ICC for image\n",
      "captioning. \n",
      "\n",
      "\n",
      "Learning light-weight yet expressive deep networks in both image synthesis\n",
      "and image recognition remains a challenging problem. Inspired by a more recent\n",
      "observation that it is the data-specificity that makes the multi-head\n",
      "self-attention (MHSA) in the Transformer model so powerful, this paper proposes\n",
      "to extend the widely adopted light-weight Squeeze-Excitation (SE) module to be\n",
      "spatially-adaptive to reinforce its data specificity, as a convolutional\n",
      "alternative of the MHSA, while retaining the efficiency of SE and the inductive\n",
      "basis of convolution. It presents two designs of spatially-adaptive\n",
      "squeeze-excitation (SASE) modules for image synthesis and image recognition\n",
      "respectively. For image synthesis tasks, the proposed SASE is tested in both\n",
      "low-shot and one-shot learning tasks. It shows better performance than prior\n",
      "arts. For image recognition tasks, the proposed SASE is used as a drop-in\n",
      "replacement for convolution layers in ResNets and achieves much better accuracy\n",
      "than the vanilla ResNets, and slightly better than the MHSA counterparts such\n",
      "as the Swin-Transformer and Pyramid-Transformer in the ImageNet-1000 dataset,\n",
      "with significantly smaller models. \n",
      "\n",
      "\n",
      "As information exists in various modalities in real world, effective\n",
      "interaction and fusion among multimodal information plays a key role for the\n",
      "creation and perception of multimodal data in computer vision and deep learning\n",
      "research. With superb power in modelling the interaction among multimodal\n",
      "information, multimodal image synthesis and editing has become a hot research\n",
      "topic in recent years. Instead of providing explicit guidance for network\n",
      "training, multimodal guidance offers intuitive and flexible means for image\n",
      "synthesis and editing. On the other hand, this field is also facing several\n",
      "challenges in alignment of features with inherent modality gaps, synthesis of\n",
      "high-resolution images, faithful evaluation metrics, etc. In this survey, we\n",
      "comprehensively contextualize the advance of the recent multimodal image\n",
      "synthesis and editing and formulate taxonomies according to data modality and\n",
      "model architectures. We start with an introduction to different types of\n",
      "guidance modalities in image synthesis and editing. We then describe multimodal\n",
      "image synthesis and editing approaches extensively with detailed frameworks\n",
      "including Generative Adversarial Networks (GANs), Auto-regressive models,\n",
      "Diffusion models, Neural Radiance Fields (NeRF) and other methods. This is\n",
      "followed by a comprehensive description of benchmark datasets and corresponding\n",
      "evaluation metrics as widely adopted in multimodal image synthesis and editing,\n",
      "as well as detailed comparisons of various synthesis methods with analysis of\n",
      "respective advantages and limitations. Finally, we provide insights about the\n",
      "current research challenges and possible directions for future research. We\n",
      "hope this survey could lay a sound and valuable foundation for future\n",
      "development of multimodal image synthesis and editing. A project associated\n",
      "with this survey is available at https://github.com/fnzhan/MISE. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are the driving force behind the\n",
      "state-of-the-art in image generation. Despite their ability to synthesize\n",
      "high-resolution photo-realistic images, generating content with on-demand\n",
      "conditioning of different granularity remains a challenge. This challenge is\n",
      "usually tackled by annotating massive datasets with the attributes of interest,\n",
      "a laborious task that is not always a viable option. Therefore, it is vital to\n",
      "introduce control into the generation process of unsupervised generative\n",
      "models. In this work, we focus on controllable image generation by leveraging\n",
      "GANs that are well-trained in an unsupervised fashion. To this end, we discover\n",
      "that the representation space of intermediate layers of the generator forms a\n",
      "number of clusters that separate the data according to semantically meaningful\n",
      "attributes (e.g., hair color and pose). By conditioning on the cluster\n",
      "assignments, the proposed method is able to control the semantic class of the\n",
      "generated image. Our approach enables sampling from each cluster by Implicit\n",
      "Maximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach\n",
      "on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using\n",
      "different pre-trained generative models. The results highlight the ability of\n",
      "our approach to condition image generation on attributes like gender, pose and\n",
      "hair style on faces, as well as a variety of features on different object\n",
      "classes. \n",
      "\n",
      "\n",
      "Building upon the recent progress in novel view synthesis, we propose its\n",
      "application to improve monocular depth estimation. In particular, we propose a\n",
      "novel training method split in three main steps. First, the prediction results\n",
      "of a monocular depth network are warped to an additional view point. Second, we\n",
      "apply an additional image synthesis network, which corrects and improves the\n",
      "quality of the warped RGB image. The output of this network is required to look\n",
      "as similar as possible to the ground-truth view by minimizing the pixel-wise\n",
      "RGB reconstruction error. Third, we reapply the same monocular depth estimation\n",
      "onto the synthesized second view point and ensure that the depth predictions\n",
      "are consistent with the associated ground truth depth. Experimental results\n",
      "prove that our method achieves state-of-the-art or comparable performance on\n",
      "the KITTI and NYU-Depth-v2 datasets with a lightweight and simple vanilla U-Net\n",
      "architecture. \n",
      "\n",
      "\n",
      "Despite the tantalizing success in a broad of vision tasks, transformers have\n",
      "not yet demonstrated on-par ability as ConvNets in high-resolution image\n",
      "generative modeling. In this paper, we seek to explore using pure transformers\n",
      "to build a generative adversarial network for high-resolution image synthesis.\n",
      "To this end, we believe that local attention is crucial to strike the balance\n",
      "between computational efficiency and modeling capacity. Hence, the proposed\n",
      "generator adopts Swin transformer in a style-based architecture. To achieve a\n",
      "larger receptive field, we propose double attention which simultaneously\n",
      "leverages the context of the local and the shifted windows, leading to improved\n",
      "generation quality. Moreover, we show that offering the knowledge of the\n",
      "absolute position that has been lost in window-based transformers greatly\n",
      "benefits the generation quality. The proposed StyleSwin is scalable to high\n",
      "resolutions, with both the coarse geometry and fine structures benefit from the\n",
      "strong expressivity of transformers. However, blocking artifacts occur during\n",
      "high-resolution synthesis because performing the local attention in a\n",
      "block-wise manner may break the spatial coherency. To solve this, we\n",
      "empirically investigate various solutions, among which we find that employing a\n",
      "wavelet discriminator to examine the spectral discrepancy effectively\n",
      "suppresses the artifacts. Extensive experiments show the superiority over prior\n",
      "transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The\n",
      "StyleSwin, without complex training strategies, excels over StyleGAN on\n",
      "CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the\n",
      "promise of using transformers for high-resolution image generation. The code\n",
      "and models will be available at https://github.com/microsoft/StyleSwin. \n",
      "\n",
      "\n",
      "Making generative models 3D-aware bridges the 2D image space and the 3D\n",
      "physical world yet remains challenging. Recent attempts equip a Generative\n",
      "Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D\n",
      "coordinates to pixel values, as a 3D prior. However, the implicit function in\n",
      "NeRF has a very local receptive field, making the generator hard to become\n",
      "aware of the global structure. Meanwhile, NeRF is built on volume rendering\n",
      "which can be too costly to produce high-resolution results, increasing the\n",
      "optimization difficulty. To alleviate these two problems, we propose a novel\n",
      "framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,\n",
      "through explicitly learning a structural representation and a textural\n",
      "representation. We first learn a feature volume to represent the underlying\n",
      "structure, which is then converted to a feature field using a NeRF-like model.\n",
      "The feature field is further accumulated into a 2D feature map as the textural\n",
      "representation, followed by a neural renderer for appearance synthesis. Such a\n",
      "design enables independent control of the shape and the appearance. Extensive\n",
      "experiments on a wide range of datasets show that our approach achieves\n",
      "sufficiently higher image quality and better 3D control than the previous\n",
      "methods. \n",
      "\n",
      "\n",
      "By decomposing the image formation process into a sequential application of\n",
      "denoising autoencoders, diffusion models (DMs) achieve state-of-the-art\n",
      "synthesis results on image data and beyond. Additionally, their formulation\n",
      "allows for a guiding mechanism to control the image generation process without\n",
      "retraining. However, since these models typically operate directly in pixel\n",
      "space, optimization of powerful DMs often consumes hundreds of GPU days and\n",
      "inference is expensive due to sequential evaluations. To enable DM training on\n",
      "limited computational resources while retaining their quality and flexibility,\n",
      "we apply them in the latent space of powerful pretrained autoencoders. In\n",
      "contrast to previous work, training diffusion models on such a representation\n",
      "allows for the first time to reach a near-optimal point between complexity\n",
      "reduction and detail preservation, greatly boosting visual fidelity. By\n",
      "introducing cross-attention layers into the model architecture, we turn\n",
      "diffusion models into powerful and flexible generators for general conditioning\n",
      "inputs such as text or bounding boxes and high-resolution synthesis becomes\n",
      "possible in a convolutional manner. Our latent diffusion models (LDMs) achieve\n",
      "a new state of the art for image inpainting and highly competitive performance\n",
      "on various tasks, including unconditional image generation, semantic scene\n",
      "synthesis, and super-resolution, while significantly reducing computational\n",
      "requirements compared to pixel-based DMs. Code is available at\n",
      "https://github.com/CompVis/latent-diffusion . \n",
      "\n",
      "\n",
      "Diffusion models have recently been shown to generate high-quality synthetic\n",
      "images, especially when paired with a guidance technique to trade off diversity\n",
      "for fidelity. We explore diffusion models for the problem of text-conditional\n",
      "image synthesis and compare two different guidance strategies: CLIP guidance\n",
      "and classifier-free guidance. We find that the latter is preferred by human\n",
      "evaluators for both photorealism and caption similarity, and often produces\n",
      "photorealistic samples. Samples from a 3.5 billion parameter text-conditional\n",
      "diffusion model using classifier-free guidance are favored by human evaluators\n",
      "to those from DALL-E, even when the latter uses expensive CLIP reranking.\n",
      "Additionally, we find that our models can be fine-tuned to perform image\n",
      "inpainting, enabling powerful text-driven image editing. We train a smaller\n",
      "model on a filtered dataset and release the code and weights at\n",
      "https://github.com/openai/glide-text2im. \n",
      "\n",
      "\n",
      "Existing methods for image synthesis utilized a style encoder based on stacks\n",
      "of convolutions and pooling layers to generate style codes from input images.\n",
      "However, the encoded vectors do not necessarily contain local information of\n",
      "the corresponding images since small-scale objects are tended to \"wash away\"\n",
      "through such downscaling procedures. In this paper, we propose deep image\n",
      "synthesis with superpixel based style encoder, named as SuperStyleNet. First,\n",
      "we directly extract the style codes from the original image based on\n",
      "superpixels to consider local objects. Second, we recover spatial relationships\n",
      "in vectorized style codes based on graphical analysis. Thus, the proposed\n",
      "network achieves high-quality image synthesis by mapping the style codes into\n",
      "semantic labels. Experimental results show that the proposed method outperforms\n",
      "state-of-the-art ones in terms of visual quality and quantitative measurements.\n",
      "Furthermore, we achieve elaborate spatial style editing by adjusting style\n",
      "codes. \n",
      "\n",
      "\n",
      "Score-based generative models (SGMs) have demonstrated remarkable synthesis\n",
      "quality. SGMs rely on a diffusion process that gradually perturbs the data\n",
      "towards a tractable distribution, while the generative model learns to denoise.\n",
      "The complexity of this denoising task is, apart from the data distribution\n",
      "itself, uniquely determined by the diffusion process. We argue that current\n",
      "SGMs employ overly simplistic diffusions, leading to unnecessarily complex\n",
      "denoising processes, which limit generative modeling performance. Based on\n",
      "connections to statistical mechanics, we propose a novel critically-damped\n",
      "Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior\n",
      "performance. CLD can be interpreted as running a joint diffusion in an extended\n",
      "space, where the auxiliary variables can be considered \"velocities\" that are\n",
      "coupled to the data variables as in Hamiltonian dynamics. We derive a novel\n",
      "score matching objective for CLD and show that the model only needs to learn\n",
      "the score function of the conditional distribution of the velocity given data,\n",
      "an easier task than learning scores of the data directly. We also derive a new\n",
      "sampling scheme for efficient synthesis from CLD-based diffusion models. We\n",
      "find that CLD outperforms previous SGMs in synthesis quality for similar\n",
      "network architectures and sampling compute budgets. We show that our novel\n",
      "sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\n",
      "framework provides new insights into score-based denoising diffusion models and\n",
      "can be readily used for high-resolution image synthesis. Project page and code:\n",
      "https://nv-tlabs.github.io/CLD-SGM. \n",
      "\n",
      "\n",
      "Controllable image synthesis models allow creation of diverse images based on\n",
      "text instructions or guidance from a reference image. Recently, denoising\n",
      "diffusion probabilistic models have been shown to generate more realistic\n",
      "imagery than prior methods, and have been successfully demonstrated in\n",
      "unconditional and class-conditional settings. We investigate fine-grained,\n",
      "continuous control of this model class, and introduce a novel unified framework\n",
      "for semantic diffusion guidance, which allows either language or image\n",
      "guidance, or both. Guidance is injected into a pretrained unconditional\n",
      "diffusion model using the gradient of image-text or image matching scores. We\n",
      "explore CLIP-based language guidance as well as both content and style-based\n",
      "image guidance in a unified framework. Our text-guided synthesis approach can\n",
      "be applied to datasets without associated text annotations. We conduct\n",
      "experiments on FFHQ and LSUN datasets, and show results on fine-grained\n",
      "text-guided image synthesis, synthesis of images related to a style or content\n",
      "reference image, and examples with both textual and image guidance. \n",
      "\n",
      "\n",
      "A powerful simulator highly decreases the need for real-world tests when\n",
      "training and evaluating autonomous vehicles. Data-driven simulators flourished\n",
      "with the recent advancement of conditional Generative Adversarial Networks\n",
      "(cGANs), providing high-fidelity images. The main challenge is synthesizing\n",
      "photorealistic images while following given constraints. In this work, we\n",
      "propose to improve the quality of generated images by rethinking the\n",
      "discriminator architecture. The focus is on the class of problems where images\n",
      "are generated given semantic inputs, such as scene segmentation maps or human\n",
      "body poses. We build on successful cGAN models to propose a new\n",
      "semantically-aware discriminator that better guides the generator. We aim to\n",
      "learn a shared latent representation that encodes enough information to jointly\n",
      "do semantic segmentation, content reconstruction, along with a coarse-to-fine\n",
      "grained adversarial reasoning. The achieved improvements are generic and simple\n",
      "enough to be applied to any architecture of conditional image synthesis. We\n",
      "demonstrate the strength of our method on the scene, building, and human\n",
      "synthesis tasks across three different datasets. The code is available at\n",
      "https://github.com/vita-epfl/SemDisc. \n",
      "\n",
      "\n",
      "Existing conditional image synthesis frameworks generate images based on user\n",
      "inputs in a single modality, such as text, segmentation, sketch, or style\n",
      "reference. They are often unable to leverage multimodal user inputs when\n",
      "available, which reduces their practicality. To address this limitation, we\n",
      "propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN)\n",
      "framework, which can synthesize images conditioned on multiple input modalities\n",
      "or any subset of them, even the empty set. PoE-GAN consists of a\n",
      "product-of-experts generator and a multimodal multiscale projection\n",
      "discriminator. Through our carefully designed training scheme, PoE-GAN learns\n",
      "to synthesize images with high quality and diversity. Besides advancing the\n",
      "state of the art in multimodal conditional image synthesis, PoE-GAN also\n",
      "outperforms the best existing unimodal conditional image synthesis approaches\n",
      "when tested in the unimodal setting. The project website is available at\n",
      "https://deepimagination.github.io/PoE-GAN . \n",
      "\n",
      "\n",
      "A comprehensive understanding of vision and language and their interrelation\n",
      "are crucial to realize the underlying similarities and differences between\n",
      "these modalities and to learn more generalized, meaningful representations. In\n",
      "recent years, most of the works related to Text-to-Image synthesis and\n",
      "Image-to-Text generation, focused on supervised generative deep architectures\n",
      "to solve the problems, where very little interest was placed on learning the\n",
      "similarities between the embedding spaces across modalities. In this paper, we\n",
      "propose a novel self-supervised deep learning based approach towards learning\n",
      "the cross-modal embedding spaces; for both image to text and text to image\n",
      "generations. In our approach, we first obtain dense vector representations of\n",
      "images using StackGAN-based autoencoder model and also dense vector\n",
      "representations on sentence-level utilizing LSTM based text-autoencoder; then\n",
      "we study the mapping from embedding space of one modality to embedding space of\n",
      "the other modality utilizing GAN and maximum mean discrepancy based generative\n",
      "networks. We, also demonstrate that our model learns to generate textual\n",
      "description from image data as well as images from textual data both\n",
      "qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "Assessing the performance of Generative Adversarial Networks (GANs) has been\n",
      "an important topic due to its practical significance. Although several\n",
      "evaluation metrics have been proposed, they generally assess the quality of the\n",
      "whole generated image distribution. For Reference-guided Image Synthesis (RIS)\n",
      "tasks, i.e., rendering a source image in the style of another reference image,\n",
      "where assessing the quality of a single generated image is crucial, these\n",
      "metrics are not applicable. In this paper, we propose a general learning-based\n",
      "framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively\n",
      "evaluate the quality of a single generated image. Notably, the training of RISA\n",
      "does not require human annotations. In specific, the training data for RISA are\n",
      "acquired by the intermediate models from the training procedure in RIS, and\n",
      "weakly annotated by the number of models' iterations, based on the positive\n",
      "correlation between image quality and iterations. As this annotation is too\n",
      "coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise\n",
      "interpolation scheme to refine the coarse labels, and 2) multiple binary\n",
      "classifiers to replace a na\\\"ive regressor. In addition, an unsupervised\n",
      "contrastive loss is introduced to effectively capture the style similarity\n",
      "between a generated image and its reference image. Empirical results on various\n",
      "datasets demonstrate that RISA is highly consistent with human preference and\n",
      "transfers well across models. \n",
      "\n",
      "\n",
      "We focus on controllable disentangled representation learning (C-Dis-RL),\n",
      "where users can control the partition of the disentangled latent space to\n",
      "factorize dataset attributes (concepts) for downstream tasks. Two general\n",
      "problems remain under-explored in current methods: (1) They lack comprehensive\n",
      "disentanglement constraints, especially missing the minimization of mutual\n",
      "information between different attributes across latent and observation domains.\n",
      "(2) They lack convexity constraints, which is important for meaningfully\n",
      "manipulating specific attributes for downstream tasks. To encourage both\n",
      "comprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet\n",
      "efficient method: Controllable Interpolation Regularization (CIR), which\n",
      "creates a positive loop where disentanglement and convexity can help each\n",
      "other. Specifically, we conduct controlled interpolation in latent space during\n",
      "training, and we reuse the encoder to help form a 'perfect disentanglement'\n",
      "regularization. In that case, (a) disentanglement loss implicitly enlarges the\n",
      "potential understandable distribution to encourage convexity; (b) convexity can\n",
      "in turn improve robust and precise disentanglement. CIR is a general module and\n",
      "we merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to\n",
      "show the compatibility and effectiveness. Qualitative and quantitative\n",
      "experiments show improvement in C-Dis-RL and latent convexity by CIR. This\n",
      "further improves downstream tasks: controllable image synthesis, cross-modality\n",
      "image translation, and zero-shot synthesis. \n",
      "\n",
      "\n",
      "We propose a parametric model that maps free-view images into a vector space\n",
      "of coded facial shape, expression and appearance with a neural radiance field,\n",
      "namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial\n",
      "shape, expression and appearance along with space coordinate and view direction\n",
      "as input to an MLP, and outputs the radiance of the space point for\n",
      "photo-realistic image synthesis. Compared with conventional 3D morphable models\n",
      "(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic\n",
      "facial details even for eyes, mouths, and beards. Also, continuous face\n",
      "morphing can be easily achieved by interpolating the input shape, expression\n",
      "and appearance codes. By introducing identity-specific modulation and texture\n",
      "encoder, our model synthesizes accurate photometric details and shows strong\n",
      "representation ability. Our model shows strong ability on multiple applications\n",
      "including image-based fitting, random generation, face rigging, face editing,\n",
      "and novel view synthesis. Experiments show that our method achieves higher\n",
      "representation ability than previous parametric models, and achieves\n",
      "competitive performance in several applications. To the best of our knowledge,\n",
      "our work is the first facial parametric model built upon a neural radiance\n",
      "field that can be used in fitting, generation and manipulation. The code and\n",
      "data is available at https://github.com/zhuhao-nju/mofanerf. \n",
      "\n",
      "\n",
      "Recent studies have shown that StyleGANs provide promising prior models for\n",
      "downstream tasks on image synthesis and editing. However, since the latent\n",
      "codes of StyleGANs are designed to control global styles, it is hard to achieve\n",
      "a fine-grained control over synthesized images. We present SemanticStyleGAN,\n",
      "where a generator is trained to model local semantic parts separately and\n",
      "synthesizes images in a compositional way. The structure and texture of\n",
      "different local parts are controlled by corresponding latent codes.\n",
      "Experimental results demonstrate that our model provides a strong\n",
      "disentanglement between different spatial areas. When combined with editing\n",
      "methods designed for StyleGANs, it can achieve a more fine-grained control to\n",
      "edit synthesized or real images. The model can also be extended to other\n",
      "domains via transfer learning. Thus, as a generic prior model with built-in\n",
      "disentanglement, it could facilitate the development of GAN-based applications\n",
      "and enable more potential downstream tasks. \n",
      "\n",
      "\n",
      "Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D\n",
      "representations from 2D images, have recently been shown to produce realistic\n",
      "images representing rigid/semi-rigid objects, such as human faces or cars.\n",
      "However, they usually struggle to generate high-quality images representing\n",
      "non-rigid objects, such as the human body, which is of a great interest for\n",
      "many computer graphics applications. This paper proposes a 3D-aware\n",
      "Semantic-Guided Generative Model (3D-SGAN) for human image synthesis, which\n",
      "combines a GNeRF with a texture generator. The former learns an implicit 3D\n",
      "representation of the human body and outputs a set of 2D semantic segmentation\n",
      "masks. The latter transforms these semantic masks into a real image, adding a\n",
      "realistic texture to the human appearance. Without requiring additional 3D\n",
      "information, our model can learn 3D human representations with a\n",
      "photo-realistic, controllable generation. Our experiments on the DeepFashion\n",
      "dataset show that 3D-SGAN significantly outperforms the most recent baselines.\n",
      "The code is available at https://github.com/zhangqianhui/3DSGAN \n",
      "\n",
      "\n",
      "In this paper, we conduct a study on the state-of-the-art methods for\n",
      "text-to-image synthesis and propose a framework to evaluate these methods. We\n",
      "consider syntheses where an image contains a single or multiple objects. Our\n",
      "study outlines several issues in the current evaluation pipeline: (i) for image\n",
      "quality assessment, a commonly used metric, e.g., Inception Score (IS), is\n",
      "often either miscalibrated for the single-object case or misused for the\n",
      "multi-object case; (ii) for text relevance and object accuracy assessment,\n",
      "there is an overfitting phenomenon in the existing R-precision (RP) and\n",
      "Semantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object\n",
      "case, many vital factors for evaluation, e.g., object fidelity, positional\n",
      "alignment, counting alignment, are largely dismissed; (iv) the ranking of the\n",
      "methods based on current metrics is highly inconsistent with real images. To\n",
      "overcome these issues, we propose a combined set of existing and new metrics to\n",
      "systematically evaluate the methods. For existing metrics, we offer an improved\n",
      "version of IS named IS* by using temperature scaling to calibrate the\n",
      "confidence of the classifier used by IS; we also propose a solution to mitigate\n",
      "the overfitting issues of RP and SOA. For new metrics, we develop counting\n",
      "alignment, positional alignment, object-centric IS, and object-centric FID\n",
      "metrics for evaluating the multi-object case. We show that benchmarking with\n",
      "our bag of metrics results in a highly consistent ranking among existing\n",
      "methods that is well-aligned with human evaluation. As a by-product, we create\n",
      "AttnGAN++, a simple but strong baseline for the benchmark by stabilizing the\n",
      "training of AttnGAN using spectral normalization. We also release our toolbox,\n",
      "so-called TISE, for advocating fair and consistent evaluation of text-to-image\n",
      "models. \n",
      "\n",
      "\n",
      "We present the vector quantized diffusion (VQ-Diffusion) model for\n",
      "text-to-image generation. This method is based on a vector quantized\n",
      "variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional\n",
      "variant of the recently developed Denoising Diffusion Probabilistic Model\n",
      "(DDPM). We find that this latent-space method is well-suited for text-to-image\n",
      "generation tasks because it not only eliminates the unidirectional bias with\n",
      "existing methods but also allows us to incorporate a mask-and-replace diffusion\n",
      "strategy to avoid the accumulation of errors, which is a serious problem with\n",
      "existing methods. Our experiments show that the VQ-Diffusion produces\n",
      "significantly better text-to-image generation results when compared with\n",
      "conventional autoregressive (AR) models with similar numbers of parameters.\n",
      "Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can\n",
      "handle more complex scenes and improve the synthesized image quality by a large\n",
      "margin. Finally, we show that the image generation computation in our method\n",
      "can be made highly efficient by reparameterization. With traditional AR\n",
      "methods, the text-to-image generation time increases linearly with the output\n",
      "image resolution and hence is quite time consuming even for normal size images.\n",
      "The VQ-Diffusion allows us to achieve a better trade-off between quality and\n",
      "speed. Our experiments indicate that the VQ-Diffusion model with the\n",
      "reparameterization is fifteen times faster than traditional AR methods while\n",
      "achieving a better image quality. \n",
      "\n",
      "\n",
      "Data-driven paradigms using machine learning are becoming ubiquitous in image\n",
      "processing and communications. In particular, image-to-image (I2I) translation\n",
      "is a generic and widely used approach to image processing problems, such as\n",
      "image synthesis, style transfer, and image restoration. At the same time,\n",
      "neural image compression has emerged as a data-driven alternative to\n",
      "traditional coding approaches in visual communications. In this paper, we study\n",
      "the combination of these two paradigms into a joint I2I compression and\n",
      "translation framework, focusing on multi-domain image synthesis. We first\n",
      "propose distributed I2I translation by integrating quantization and entropy\n",
      "coding into an I2I translation framework (i.e. I2Icodec). In practice, the\n",
      "image compression functionality (i.e. autoencoding) is also desirable,\n",
      "requiring to deploy alongside I2Icodec a regular image codec. Thus, we further\n",
      "propose a unified framework that allows both translation and autoencoding\n",
      "capabilities in a single codec. Adaptive residual blocks conditioned on the\n",
      "translation/compression mode provide flexible adaptation to the desired\n",
      "functionality. The experiments show promising results in both I2I translation\n",
      "and image compression using a single model. \n",
      "\n",
      "\n",
      "Synthetic images created by generative models increase in quality and\n",
      "expressiveness as newer models utilize larger datasets and novel architectures.\n",
      "Although this photorealism is a positive side-effect from a creative\n",
      "standpoint, it becomes problematic when such generative models are used for\n",
      "impersonation without consent. Most of these approaches are built on the\n",
      "partial transfer between source and target pairs, or they generate completely\n",
      "new samples based on an ideal distribution, still resembling the closest real\n",
      "sample in the dataset. We propose MixSyn (read as \" mixin' \") for learning\n",
      "novel fuzzy compositions from multiple sources and creating novel images as a\n",
      "mix of image regions corresponding to the compositions. MixSyn not only\n",
      "combines uncorrelated regions from multiple source masks into a coherent\n",
      "semantic composition, but also generates mask-aware high quality\n",
      "reconstructions of non-existing images. We compare MixSyn to state-of-the-art\n",
      "single-source sequential generation and collage generation approaches in terms\n",
      "of quality, diversity, realism, and expressive power; while also showcasing\n",
      "interactive synthesis, mix & match, and edit propagation tasks, with no mask\n",
      "dependency. \n",
      "\n",
      "\n",
      "This paper addresses the problem of finding interpretable directions in the\n",
      "latent space of pre-trained Generative Adversarial Networks (GANs) to\n",
      "facilitate controllable image synthesis. Such interpretable directions\n",
      "correspond to transformations that can affect both the style and geometry of\n",
      "the synthetic images. However, existing approaches that utilise linear\n",
      "techniques to find these transformations often fail to provide an intuitive way\n",
      "to separate these two sources of variation. To address this, we propose to a)\n",
      "perform a multilinear decomposition of the tensor of intermediate\n",
      "representations, and b) use a tensor-based regression to map directions found\n",
      "using this decomposition to the latent space. Our scheme allows for both linear\n",
      "edits corresponding to the individual modes of the tensor, and non-linear ones\n",
      "that model the multiplicative interactions between them. We show experimentally\n",
      "that we can utilise the former to better separate style- from geometry-based\n",
      "transformations, and the latter to generate an extended set of possible\n",
      "transformations in comparison to prior works. We demonstrate our approach's\n",
      "efficacy both quantitatively and qualitatively compared to the current\n",
      "state-of-the-art. \n",
      "\n",
      "\n",
      "Producing diverse and realistic images with generative models such as GANs\n",
      "typically requires large scale training with vast amount of images. GANs\n",
      "trained with limited data can easily memorize few training samples and display\n",
      "undesirable properties like \"stairlike\" latent space where interpolation in the\n",
      "latent space yields discontinuous transitions in the output space. In this\n",
      "work, we consider a challenging task of pretraining-free few-shot image\n",
      "synthesis, and seek to train existing generative models with minimal\n",
      "overfitting and mode collapse. We propose mixup-based distance regularization\n",
      "on the feature space of both a generator and the counterpart discriminator that\n",
      "encourages the two players to reason not only about the scarce observed data\n",
      "points but the relative distances in the feature space they reside. Qualitative\n",
      "and quantitative evaluation on diverse datasets demonstrates that our method is\n",
      "generally applicable to existing models to enhance both fidelity and diversity\n",
      "under few-shot setting. Code is available. \n",
      "\n",
      "\n",
      "While modern image translation techniques can create photorealistic synthetic\n",
      "images, they have limited style controllability, thus could suffer from\n",
      "translation errors. In this work, we show that the activation function is one\n",
      "of the crucial components in controlling the direction of image synthesis.\n",
      "Specifically, we explicitly demonstrated that the slope parameters of the\n",
      "rectifier could change the data distribution and be used independently to\n",
      "control the direction of translation. To improve the style controllability, two\n",
      "simple but effective techniques are proposed, including Adaptive ReLU (AdaReLU)\n",
      "and structural adaptive function. The AdaReLU can dynamically adjust the slope\n",
      "parameters according to the target style and can be utilized to increase the\n",
      "controllability by combining with Adaptive Instance Normalization (AdaIN).\n",
      "Meanwhile, the structural adaptative function enables rectifiers to manipulate\n",
      "the structure of feature maps more effectively. It is composed of the proposed\n",
      "structural convolution (StruConv), an efficient convolutional module that can\n",
      "choose the area to be activated based on the mean and variance specified by\n",
      "AdaIN. Extensive experiments show that the proposed techniques can greatly\n",
      "increase the network controllability and output diversity in style-based image\n",
      "translation tasks. \n",
      "\n",
      "\n",
      "Training supervised image synthesis models requires a critic to compare two\n",
      "images: the ground truth to the result. Yet, this basic functionality remains\n",
      "an open problem. A popular line of approaches uses the L1 (mean absolute error)\n",
      "loss, either in the pixel or the feature space of pretrained deep networks.\n",
      "However, we observe that these losses tend to produce overly blurry and grey\n",
      "images, and other techniques such as GANs need to be employed to fight these\n",
      "artifacts. In this work, we introduce an information theory based approach to\n",
      "measuring similarity between two images. We argue that a good reconstruction\n",
      "should have high mutual information with the ground truth. This view enables\n",
      "learning a lightweight critic to \"calibrate\" a feature space in a contrastive\n",
      "manner, such that reconstructions of corresponding spatial patches are brought\n",
      "together, while other patches are repulsed. We show that our formulation\n",
      "immediately boosts the perceptual realism of output images when used as a\n",
      "drop-in replacement for the L1 loss, with or without an additional GAN loss. \n",
      "\n",
      "\n",
      "Generative models have been applied in the medical imaging domain for various\n",
      "image recognition and synthesis tasks. However, a more controllable and\n",
      "interpretable image synthesis model is still lacking yet necessary for\n",
      "important applications such as assisting in medical training. In this work, we\n",
      "leverage the efficient self-attention and contrastive learning modules and\n",
      "build upon state-of-the-art generative adversarial networks (GANs) to achieve\n",
      "an attribute-aware image synthesis model, termed AttributeGAN, which can\n",
      "generate high-quality histopathology images based on multi-attribute inputs. In\n",
      "comparison to existing single-attribute conditional generative models, our\n",
      "proposed model better reflects input attributes and enables smoother\n",
      "interpolation among attribute values. We conduct experiments on a\n",
      "histopathology dataset containing stained H&E images of urothelial carcinoma\n",
      "and demonstrate the effectiveness of our proposed model via comprehensive\n",
      "quantitative and qualitative comparisons with state-of-the-art models as well\n",
      "as different variants of our model. Code is available at\n",
      "https://github.com/karenyyy/MICCAI2021AttributeGAN. \n",
      "\n",
      "\n",
      "We present neural radiance fields (NeRF) with templates, dubbed\n",
      "Template-NeRF, for modeling appearance and geometry and generating dense shape\n",
      "correspondences simultaneously among objects of the same category from only\n",
      "multi-view posed images, without the need of either 3D supervision or\n",
      "ground-truth correspondence knowledge. The learned dense correspondences can be\n",
      "readily used for various image-based tasks such as keypoint detection, part\n",
      "segmentation, and texture transfer that previously require specific model\n",
      "designs. Our method can also accommodate annotation transfer in a one or\n",
      "few-shot manner, given only one or a few instances of the category. Using\n",
      "periodic activation and feature-wise linear modulation (FiLM) conditioning, we\n",
      "introduce deep implicit templates on 3D data into the 3D-aware image synthesis\n",
      "pipeline NeRF. By representing object instances within the same category as\n",
      "shape and appearance variation of a shared NeRF template, our proposed method\n",
      "can achieve dense shape correspondences reasoning on images for a wide range of\n",
      "object classes. We demonstrate the results and applications on both synthetic\n",
      "and real-world data with competitive results compared with other methods based\n",
      "on 3D information. \n",
      "\n",
      "\n",
      "Convolutional Gridding is a technique (algorithm) extensively used in Radio\n",
      "Interferometric Image Synthesis for fast inversion of functions sampled with\n",
      "irregular intervals on the Fourier plane. In this thesis, we propose some\n",
      "modifications to the technique to execute faster on a GPU. These modifications\n",
      "give rise to \\textit{Hybrid Gridding} and \\textit{Pruned NN Interpolation},\n",
      "which take advantage of the oversampling of the Gridding Convolutional Function\n",
      "in Convolutional Gridding to try to make gridding faster with no reduction in\n",
      "the quality of the output. Our experiments showed that given the right\n",
      "conditions, Hybrid Gridding executes up to $6.8\\times$ faster than\n",
      "Convolutional Gridding, and Pruned NN Interpolation is generally slower than\n",
      "Hybrid Gridding.\n",
      "  The two new techniques feature the downsampling of an oversampled grid\n",
      "through convolution to accelerate the Fourier inversion. It is a well-known\n",
      "approximate technique which suffers from aliasing. In this thesis, we are\n",
      "re-proposing the technique as a \\textit{Convolution-Based FFT Pruning}\n",
      "algorithm able to suppress aliasing below arithmetic noise. The algorithm uses\n",
      "the recently discovered least-misfit gridding functions, which through our\n",
      "experiments gave promising results, although not as good as expected from the\n",
      "related published work on the stated gridding functions. Nevertheless, our\n",
      "experiments showed that, given the right conditions, Convolutional-Based\n",
      "Pruning reduces the Fourier inversion execution time on a GPU by approximately\n",
      "a factor of $8\\times$. \n",
      "\n",
      "\n",
      "We present a new perspective of achieving image synthesis by viewing this\n",
      "task as a visual token generation problem. Different from existing paradigms\n",
      "that directly synthesize a full image from a single input (e.g., a latent\n",
      "code), the new formulation enables a flexible local manipulation for different\n",
      "image regions, which makes it possible to learn content-aware and fine-grained\n",
      "style control for image synthesis. Specifically, it takes as input a sequence\n",
      "of latent tokens to predict the visual tokens for synthesizing an image. Under\n",
      "this perspective, we propose a token-based generator (i.e.,TokenGAN).\n",
      "Particularly, the TokenGAN inputs two semantically different visual tokens,\n",
      "i.e., the learned constant content tokens and the style tokens from the latent\n",
      "space. Given a sequence of style tokens, the TokenGAN is able to control the\n",
      "image synthesis by assigning the styles to the content tokens by attention\n",
      "mechanism with a Transformer. We conduct extensive experiments and show that\n",
      "the proposed TokenGAN has achieved state-of-the-art results on several\n",
      "widely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with\n",
      "different resolutions. In particular, the generator is able to synthesize\n",
      "high-fidelity images with 1024x1024 size, dispensing with convolutions\n",
      "entirely. \n",
      "\n",
      "\n",
      "Objectives. We generate via advanced Deep Learning (DL) techniques artificial\n",
      "leaf images in an automatized way. We aim to dispose of a source of training\n",
      "samples for AI applications for modern crop management. Such applications\n",
      "require large amounts of data and, while leaf images are not truly scarce,\n",
      "image collection and annotation remains a very time--consuming process. Data\n",
      "scarcity can be addressed by augmentation techniques consisting in simple\n",
      "transformations of samples belonging to a small dataset, but the richness of\n",
      "the augmented data is limited: this motivates the search for alternative\n",
      "approaches. Methods. Pursuing an approach based on DL generative models, we\n",
      "propose a Leaf-to-Leaf Translation (L2L) procedure structured in two steps:\n",
      "first, a residual variational autoencoder architecture generates synthetic leaf\n",
      "skeletons (leaf profile and veins) starting from companions binarized skeletons\n",
      "of real images. In a second step, we perform translation via a Pix2pix\n",
      "framework, which uses conditional generator adversarial networks to reproduce\n",
      "the colorization of leaf blades, preserving the shape and the venation pattern.\n",
      "Results. The L2L procedure generates synthetic images of leaves with a\n",
      "realistic appearance. We address the performance measurement both in a\n",
      "qualitative and a quantitative way; for this latter evaluation, we employ a DL\n",
      "anomaly detection strategy which quantifies the degree of anomaly of synthetic\n",
      "leaves with respect to real samples. Conclusions. Generative DL approaches have\n",
      "the potential to be a new paradigm to provide low-cost meaningful synthetic\n",
      "samples for computer-aided applications. The present L2L approach represents a\n",
      "step towards this goal, being able to generate synthetic samples with a\n",
      "relevant qualitative and quantitative resemblance to real leaves. \n",
      "\n",
      "\n",
      "We introduce Seamless Satellite-image Synthesis (SSS), a novel neural\n",
      "architecture to create scale-and-space continuous satellite textures from\n",
      "cartographic data. While 2D map data is cheap and easily synthesized, accurate\n",
      "satellite imagery is expensive and often unavailable or out of date. Our\n",
      "approach generates seamless textures over arbitrarily large spatial extents\n",
      "which are consistent through scale-space. To overcome tile size limitations in\n",
      "image-to-image translation approaches, SSS learns to remove seams between tiled\n",
      "images in a semantically meaningful manner. Scale-space continuity is achieved\n",
      "by a hierarchy of networks conditioned on style and cartographic data. Our\n",
      "qualitative and quantitative evaluations show that our system improves over the\n",
      "state-of-the-art in several key areas. We show applications to texturing\n",
      "procedurally generation maps and interactive satellite image manipulation. \n",
      "\n",
      "\n",
      "Morphing is the process of combining two or more subjects in an image in\n",
      "order to create a new identity which contains features of both individuals.\n",
      "Morphed images can fool Facial Recognition Systems (FRS) into falsely accepting\n",
      "multiple people, leading to failures in national security. As morphed image\n",
      "synthesis becomes easier, it is vital to expand the research community's\n",
      "available data to help combat this dilemma. In this paper, we explore\n",
      "combination of two methods for morphed image generation, those of geometric\n",
      "transformation (warping and blending to create morphed images) and photometric\n",
      "perturbation. We leverage both methods to generate high-quality adversarially\n",
      "perturbed morphs from the FERET, FRGC, and FRLL datasets. The final images\n",
      "retain high similarity to both input subjects while resulting in minimal\n",
      "artifacts in the visual domain. Images are synthesized by fusing the wavelet\n",
      "sub-bands from the two look-alike subjects, and then adversarially perturbed to\n",
      "create highly convincing imagery to deceive both humans and deep morph\n",
      "detectors. \n",
      "\n",
      "\n",
      "Recent advances in generative adversarial networks (GANs) have led to\n",
      "remarkable achievements in face image synthesis. While methods that use\n",
      "style-based GANs can generate strikingly photorealistic face images, it is\n",
      "often difficult to control the characteristics of the generated faces in a\n",
      "meaningful and disentangled way. Prior approaches aim to achieve such semantic\n",
      "control and disentanglement within the latent space of a previously trained\n",
      "GAN. In contrast, we propose a framework that a priori models physical\n",
      "attributes of the face such as 3D shape, albedo, pose, and lighting explicitly,\n",
      "thus providing disentanglement by design. Our method, MOST-GAN, integrates the\n",
      "expressive power and photorealism of style-based GANs with the physical\n",
      "disentanglement and flexibility of nonlinear 3D morphable models, which we\n",
      "couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves\n",
      "photorealistic manipulation of portrait images with fully disentangled 3D\n",
      "control over their physical attributes, enabling extreme manipulation of\n",
      "lighting, facial expression, and pose variations up to full profile view. \n",
      "\n",
      "\n",
      "The advent of generative radiance fields has significantly promoted the\n",
      "development of 3D-aware image synthesis. The cumulative rendering process in\n",
      "radiance fields makes training these generative models much easier since\n",
      "gradients are distributed over the entire volume, but leads to diffused object\n",
      "surfaces. In the meantime, compared to radiance fields occupancy\n",
      "representations could inherently ensure deterministic surfaces. However, if we\n",
      "directly apply occupancy representations to generative models, during training\n",
      "they will only receive sparse gradients located on object surfaces and\n",
      "eventually suffer from the convergence problem. In this paper, we propose\n",
      "Generative Occupancy Fields (GOF), a novel model based on generative radiance\n",
      "fields that can learn compact object surfaces without impeding its training\n",
      "convergence. The key insight of GOF is a dedicated transition from the\n",
      "cumulative rendering in radiance fields to rendering with only the surface\n",
      "points as the learned surface gets more and more accurate. In this way, GOF\n",
      "combines the merits of two representations in a unified framework. In practice,\n",
      "the training-time transition of start from radiance fields and march to\n",
      "occupancy representations is achieved in GOF by gradually shrinking the\n",
      "sampling region in its rendering process from the entire volume to a minimal\n",
      "neighboring region around the surface. Through comprehensive experiments on\n",
      "multiple datasets, we demonstrate that GOF can synthesize high-quality images\n",
      "with 3D consistency and simultaneously learn compact and smooth object\n",
      "surfaces. Code, models, and demo videos are available at\n",
      "https://sheldontsui.github.io/projects/GOF \n",
      "\n",
      "\n",
      "The advancement of generative radiance fields has pushed the boundary of\n",
      "3D-aware image synthesis. Motivated by the observation that a 3D object should\n",
      "look realistic from multiple viewpoints, these methods introduce a multi-view\n",
      "constraint as regularization to learn valid 3D radiance fields from 2D images.\n",
      "Despite the progress, they often fall short of capturing accurate 3D shapes due\n",
      "to the shape-color ambiguity, limiting their applicability in downstream tasks.\n",
      "In this work, we address this ambiguity by proposing a novel shading-guided\n",
      "generative implicit model that is able to learn a starkly improved shape\n",
      "representation. Our key insight is that an accurate 3D shape should also yield\n",
      "a realistic rendering under different lighting conditions. This multi-lighting\n",
      "constraint is realized by modeling illumination explicitly and performing\n",
      "shading with various lighting conditions. Gradients are derived by feeding the\n",
      "synthesized images to a discriminator. To compensate for the additional\n",
      "computational burden of calculating surface normals, we further devise an\n",
      "efficient volume rendering strategy via surface tracking, reducing the training\n",
      "and inference time by 24% and 48%, respectively. Our experiments on multiple\n",
      "datasets show that the proposed approach achieves photorealistic 3D-aware image\n",
      "synthesis while capturing accurate underlying 3D shapes. We demonstrate\n",
      "improved performance of our approach on 3D shape reconstruction against\n",
      "existing methods, and show its applicability on image relighting. Our code will\n",
      "be released at https://github.com/XingangPan/ShadeGAN. \n",
      "\n",
      "\n",
      "Existing deep learning-based approaches for histopathology image analysis\n",
      "require large annotated training sets to achieve good performance; but\n",
      "annotating histopathology images is slow and resource-intensive. Conditional\n",
      "generative adversarial networks have been applied to generate synthetic\n",
      "histopathology images to alleviate this issue, but current approaches fail to\n",
      "generate clear contours for overlapped and touching nuclei. In this study, We\n",
      "propose a sharpness loss regularized generative adversarial network to\n",
      "synthesize realistic histopathology images. The proposed network uses\n",
      "normalized nucleus distance map rather than the binary mask to encode nuclei\n",
      "contour information. The proposed sharpness loss enhances the contrast of\n",
      "nuclei contour pixels. The proposed method is evaluated using four image\n",
      "quality metrics and segmentation results on two public datasets. Both\n",
      "quantitative and qualitative results demonstrate that the proposed approach can\n",
      "generate realistic histopathology images with clear nuclei contours. \n",
      "\n",
      "\n",
      "Transformer becomes prevalent in computer vision, especially for high-level\n",
      "vision tasks. However, adopting Transformer in the generative adversarial\n",
      "network (GAN) framework is still an open yet challenging problem. In this\n",
      "paper, we conduct a comprehensive empirical study to investigate the properties\n",
      "of Transformer in GAN for high-fidelity image synthesis. Our analysis\n",
      "highlights and reaffirms the importance of feature locality in image\n",
      "generation, although the merits of the locality are well known in the\n",
      "classification task. Perhaps more interestingly, we find the residual\n",
      "connections in self-attention layers harmful for learning Transformer-based\n",
      "discriminators and conditional generators. We carefully examine the influence\n",
      "and propose effective ways to mitigate the negative impacts. Our study leads to\n",
      "a new alternative design of Transformers in GAN, a convolutional neural network\n",
      "(CNN)-free generator termed as STrans-G, which achieves competitive results in\n",
      "both unconditional and conditional image generations. The Transformer-based\n",
      "discriminator, STrans-D, also significantly reduces its gap against the\n",
      "CNN-based discriminators. \n",
      "\n",
      "\n",
      "In this paper, we propose a method to add constraints that are\n",
      "un-formulatable in generative adversarial networks (GAN)-based arbitrary size\n",
      "RAW Bayer image generation. It is shown theoretically that by using the\n",
      "transformed data in GAN training, it is able to improve the learning of the\n",
      "original data distribution, owing to the invariant of Jensen-Shannon (JS)\n",
      "divergence between two distributions under invertible and differentiable\n",
      "transformation. Benefiting from the proposed method, RAW Bayer pattern images\n",
      "can be generated by configuring the transformation as demosaicing. It is shown\n",
      "that by adding another transformation, the proposed method is able to\n",
      "synthesize high-quality RAW Bayer images with arbitrary size. Experimental\n",
      "results show that images generated by the proposed method outperform the\n",
      "existing methods in the Fr\\'echet inception distance (FID) score, peak signal\n",
      "to noise ratio (PSNR), and mean structural similarity (MSSIM), and the training\n",
      "process is more stable. To the best knowledge of the authors, there is no\n",
      "open-source, large-scale image dataset in the RAW Bayer domain, which is\n",
      "crucial for research works aiming to explore the image signal processing (ISP)\n",
      "pipeline design for computer vision tasks. Converting the existing commonly\n",
      "used color image datasets to their corresponding RAW Bayer versions, the\n",
      "proposed method can be a promising solution to the RAW image dataset problem.\n",
      "We also show in the experiments that, by training object detection frameworks\n",
      "using the synthesized RAW Bayer images, they can be used in an end-to-end\n",
      "manner (from RAW images to vision tasks) with negligible performance\n",
      "degradation. \n",
      "\n",
      "\n",
      "Translating face sketches to photo-realistic faces is an interesting and\n",
      "essential task in many applications like law enforcement and the digital\n",
      "entertainment industry. One of the most important challenges of this task is\n",
      "the inherent differences between the sketch and the real image such as the lack\n",
      "of color and details of the skin tissue in the sketch. With the advent of\n",
      "adversarial generative models, an increasing number of methods have been\n",
      "proposed for sketch-to-image synthesis. However, these models still suffer from\n",
      "limitations such as the large number of paired data required for training, the\n",
      "low resolution of the produced images, or the unrealistic appearance of the\n",
      "generated images. In this paper, we propose a method for converting an input\n",
      "facial sketch to a colorful photo without the need for any paired dataset. To\n",
      "do so, we use a pre-trained face photo generating model to synthesize\n",
      "high-quality natural face photos and employ an optimization procedure to keep\n",
      "high-fidelity to the input sketch. We train a network to map the facial\n",
      "features extracted from the input sketch to a vector in the latent space of the\n",
      "face generating model. Also, we study different optimization criteria and\n",
      "compare the results of the proposed model with those of the state-of-the-art\n",
      "models quantitatively and qualitatively. The proposed model achieved 0.655 in\n",
      "the SSIM index and 97.59% rank-1 face recognition rate with higher quality of\n",
      "the produced images. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have made a dramatic leap in\n",
      "high-fidelity image synthesis and stylized face generation. Recently, a\n",
      "layer-swapping mechanism has been developed to improve the stylization\n",
      "performance. However, this method is incapable of fitting arbitrary styles in a\n",
      "single model and requires hundreds of style-consistent training images for each\n",
      "style. To address the above issues, we propose BlendGAN for arbitrary stylized\n",
      "face generation by leveraging a flexible blending strategy and a generic\n",
      "artistic dataset. Specifically, we first train a self-supervised style encoder\n",
      "on the generic artistic dataset to extract the representations of arbitrary\n",
      "styles. In addition, a weighted blending module (WBM) is proposed to blend face\n",
      "and style representations implicitly and control the arbitrary stylization\n",
      "effect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified\n",
      "model while avoiding case-by-case preparation of style-consistent training\n",
      "images. To this end, we also present a novel large-scale artistic face dataset\n",
      "AAHQ. Extensive experiments demonstrate that BlendGAN outperforms\n",
      "state-of-the-art methods in terms of visual quality and style diversity for\n",
      "both latent-guided and reference-guided stylized face synthesis. \n",
      "\n",
      "\n",
      "The image synthesis technique is relatively well established which can\n",
      "generate facial images that are indistinguishable even by human beings.\n",
      "However, all of these approaches uses gradients to condition the output,\n",
      "resulting in the outputting the same image with the same input. Also, they can\n",
      "only generate images with basic expression or mimic an expression instead of\n",
      "generating compound expression. In real life, however, human expressions are of\n",
      "great diversity and complexity. In this paper, we propose an evolutionary\n",
      "algorithm (EA) assisted GAN, named EvoGAN, to generate various compound\n",
      "expressions with any accurate target compound expression. EvoGAN uses an EA to\n",
      "search target results in the data distribution learned by GAN. Specifically, we\n",
      "use the Facial Action Coding System (FACS) as the encoding of an EA and use a\n",
      "pre-trained GAN to generate human facial images, and then use a pre-trained\n",
      "classifier to recognize the expression composition of the synthesized images as\n",
      "the fitness function to guide the search of the EA. Combined random searching\n",
      "algorithm, various images with the target expression can be easily sythesized.\n",
      "Quantitative and Qualitative results are presented on several compound\n",
      "expressions, and the experimental results demonstrate the feasibility and the\n",
      "potential of EvoGAN. \n",
      "\n",
      "\n",
      "While much research has been done in text-to-image synthesis, little work has\n",
      "been done to explore the usage of linguistic structure of the input text. Such\n",
      "information is even more important for story visualization since its inputs\n",
      "have an explicit narrative structure that needs to be translated into an image\n",
      "sequence (or visual story). Prior work in this domain has shown that there is\n",
      "ample room for improvement in the generated image sequence in terms of visual\n",
      "quality, consistency and relevance. In this paper, we first explore the use of\n",
      "constituency parse trees using a Transformer-based recurrent architecture for\n",
      "encoding structured input. Second, we augment the structured input with\n",
      "commonsense information and study the impact of this external knowledge on the\n",
      "generation of visual story. Third, we also incorporate visual structure via\n",
      "bounding boxes and dense captioning to provide feedback about the\n",
      "characters/objects in generated images within a dual learning setup. We show\n",
      "that off-the-shelf dense-captioning models trained on Visual Genome can improve\n",
      "the spatial structure of images from a different target domain without needing\n",
      "fine-tuning. We train the model end-to-end using intra-story contrastive loss\n",
      "(between words and image sub-regions) and show significant improvements in\n",
      "several metrics (and human evaluation) for multiple datasets. Finally, we\n",
      "provide an analysis of the linguistic and visuo-spatial information. Code and\n",
      "data: https://github.com/adymaharana/VLCStoryGan. \n",
      "\n",
      "\n",
      "We present SSOD, the first end-to-end analysis-by synthesis framework with\n",
      "controllable GANs for the task of self-supervised object detection. We use\n",
      "collections of real world images without bounding box annotations to learn to\n",
      "synthesize and detect objects. We leverage controllable GANs to synthesize\n",
      "images with pre-defined object properties and use them to train object\n",
      "detectors. We propose a tight end-to-end coupling of the synthesis and\n",
      "detection networks to optimally train our system. Finally, we also propose a\n",
      "method to optimally adapt SSOD to an intended target data without requiring\n",
      "labels for it. For the task of car detection, on the challenging KITTI and\n",
      "Cityscapes datasets, we show that SSOD outperforms the prior state-of-the-art\n",
      "purely image-based self-supervised object detection method Wetectron. Even\n",
      "without requiring any 3D CAD assets, it also surpasses the state-of-the-art\n",
      "rendering based method Meta-Sim2. Our work advances the field of\n",
      "self-supervised object detection by introducing a successful new paradigm of\n",
      "using controllable GAN-based image synthesis for it and by significantly\n",
      "improving the baseline accuracy of the task. We open-source our code at\n",
      "https://github.com/NVlabs/SSOD. \n",
      "\n",
      "\n",
      "The style-based GAN (StyleGAN) architecture achieved state-of-the-art results\n",
      "for generating high-quality images, but it lacks explicit and precise control\n",
      "over camera poses. The recently proposed NeRF-based GANs made great progress\n",
      "towards 3D-aware generators, but they are unable to generate high-quality\n",
      "images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that\n",
      "is composed of a shallow NeRF network and a deep implicit neural representation\n",
      "(INR) network. The generator synthesizes each pixel value independently without\n",
      "any spatial convolution or upsampling operation. In addition, we diagnose the\n",
      "problem of mirror symmetry that implies a suboptimal solution and solve it by\n",
      "introducing an auxiliary discriminator. Trained on raw, single-view images,\n",
      "CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of\n",
      "6.97 for images at the $256\\times256$ resolution on FFHQ. We also demonstrate\n",
      "several interesting directions for CIPS-3D such as transfer learning and\n",
      "3D-aware face stylization. The synthesis results are best viewed as videos, so\n",
      "we recommend the readers to check our github project at\n",
      "https://github.com/PeterouZh/CIPS-3D \n",
      "\n",
      "\n",
      "Much of the state-of-the-art in image synthesis inspired by real artwork are\n",
      "either entirely generative by filtered random noise or inspired by the transfer\n",
      "of style. This work explores the application of image inpainting to continue\n",
      "famous artworks and produce generative art with a Conditional GAN. During the\n",
      "training stage of the process, the borders of images are cropped, leaving only\n",
      "the centre. An inpainting GAN is then tasked with learning to reconstruct the\n",
      "original image from the centre crop by way of minimising both adversarial and\n",
      "absolute difference losses, which are analysed by both their Fr\\'echet\n",
      "Inception Distances and manual observations which are presented. Once the\n",
      "network is trained, images are then resized rather than cropped and presented\n",
      "as input to the generator. Following the learning process, the generator then\n",
      "creates new images by continuing from the edges of the original piece. Three\n",
      "experiments are performed with datasets of 4766 landscape paintings\n",
      "(impressionism and romanticism), 1167 Ukiyo-e works from the Japanese Edo\n",
      "period, and 4968 abstract artworks. Results show that geometry and texture\n",
      "(including canvas and paint) as well as scenery such as sky, clouds, water,\n",
      "land (including hills and mountains), grass, and flowers are implemented by the\n",
      "generator when extending real artworks. In the Ukiyo-e experiments, it was\n",
      "observed that features such as written text were generated even in cases where\n",
      "the original image did not have any, due to the presence of an unpainted border\n",
      "within the input image. \n",
      "\n",
      "\n",
      "We propose StyleNeRF, a 3D-aware generative model for photo-realistic\n",
      "high-resolution image synthesis with high multi-view consistency, which can be\n",
      "trained on unstructured 2D images. Existing approaches either cannot synthesize\n",
      "high-resolution images with fine details or yield noticeable 3D-inconsistent\n",
      "artifacts. In addition, many of them lack control over style attributes and\n",
      "explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF)\n",
      "into a style-based generator to tackle the aforementioned challenges, i.e.,\n",
      "improving rendering efficiency and 3D consistency for high-resolution image\n",
      "generation. We perform volume rendering only to produce a low-resolution\n",
      "feature map and progressively apply upsampling in 2D to address the first\n",
      "issue. To mitigate the inconsistencies caused by 2D upsampling, we propose\n",
      "multiple designs, including a better upsampler and a new regularization loss.\n",
      "With these designs, StyleNeRF can synthesize high-resolution images at\n",
      "interactive rates while preserving 3D consistency at high quality. StyleNeRF\n",
      "also enables control of camera poses and different levels of styles, which can\n",
      "generalize to unseen views. It also supports challenging tasks, including\n",
      "zoom-in and-out, style mixing, inversion, and semantic editing. \n",
      "\n",
      "\n",
      "Synthesizing high-quality, realistic images from text-descriptions is a\n",
      "challenging task, and current methods synthesize images from text in a\n",
      "multi-stage manner, typically by first generating a rough initial image and\n",
      "then refining image details at subsequent stages. However, existing methods\n",
      "that follow this paradigm suffer from three important limitations. Firstly,\n",
      "they synthesize initial images without attempting to separate image attributes\n",
      "at a word-level. As a result, object attributes of initial images (that provide\n",
      "a basis for subsequent refinement) are inherently entangled and ambiguous in\n",
      "nature. Secondly, by using common text-representations for all regions, current\n",
      "methods prevent us from interpreting text in fundamentally different ways at\n",
      "different parts of an image. Different image regions are therefore only allowed\n",
      "to assimilate the same type of information from text at each refinement stage.\n",
      "Finally, current methods generate refinement features only once at each\n",
      "refinement stage and attempt to address all image aspects in a single shot.\n",
      "This single-shot refinement limits the precision with which each refinement\n",
      "stage can learn to improve the prior image. Our proposed method introduces\n",
      "three novel components to address these shortcomings: (1) An initial generation\n",
      "stage that explicitly generates separate sets of image features for each word\n",
      "n-gram. (2) A spatial dynamic memory module for refinement of images. (3) An\n",
      "iterative multi-headed mechanism to make it easier to improve upon multiple\n",
      "image aspects. Experimental results demonstrate that our Multi-Headed Spatial\n",
      "Dynamic Memory image refinement with our Multi-Tailed Word-level Initial\n",
      "Generation (MSMT-GAN) performs favourably against the previous state of the art\n",
      "on the CUB and COCO datasets. \n",
      "\n",
      "\n",
      "We focus on the problem of novel-view human action synthesis. Given an action\n",
      "video, the goal is to generate the same action from an unseen viewpoint.\n",
      "Naturally, novel view video synthesis is more challenging than image synthesis.\n",
      "It requires the synthesis of a sequence of realistic frames with temporal\n",
      "coherency. Besides, transferring the different actions to a novel target view\n",
      "requires awareness of action category and viewpoint change simultaneously. To\n",
      "address these challenges, we propose a novel framework named Pose-guided Action\n",
      "Separable Generative Adversarial Net (PAS-GAN), which utilizes pose to\n",
      "alleviate the difficulty of this task. First, we propose a recurrent\n",
      "pose-transformation module which transforms actions from the source view to the\n",
      "target view and generates novel view pose sequence in 2D coordinate space.\n",
      "Second, a well-transformed pose sequence enables us to separatethe action and\n",
      "background in the target view. We employ a novel local-global spatial\n",
      "transformation module to effectively generate sequential video features in the\n",
      "target view using these action and background features. Finally, the generated\n",
      "video features are used to synthesize human action with the help of a 3D\n",
      "decoder. Moreover, to focus on dynamic action in the video, we propose a novel\n",
      "multi-scale action-separable loss which further improves the video quality. We\n",
      "conduct extensive experiments on two large-scale multi-view human action\n",
      "datasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN\n",
      "which outperforms existing approaches. \n",
      "\n",
      "\n",
      "Automatic font generation based on deep learning has aroused a lot of\n",
      "interest in the last decade. However, only a few recently-reported approaches\n",
      "are capable of directly generating vector glyphs and their results are still\n",
      "far from satisfactory. In this paper, we propose a novel method, DeepVecFont,\n",
      "to effectively resolve this problem. Using our method, for the first time,\n",
      "visually-pleasing vector glyphs whose quality and compactness are both\n",
      "comparable to human-designed ones can be automatically generated. The key idea\n",
      "of our DeepVecFont is to adopt the techniques of image synthesis, sequence\n",
      "modeling and differentiable rasterization to exhaustively exploit the\n",
      "dual-modality information (i.e., raster images and vector outlines) of vector\n",
      "fonts. The highlights of this paper are threefold. First, we design a\n",
      "dual-modality learning strategy which utilizes both image-aspect and\n",
      "sequence-aspect features of fonts to synthesize vector glyphs. Second, we\n",
      "provide a new generative paradigm to handle unstructured data (e.g., vector\n",
      "glyphs) by randomly sampling plausible synthesis results to get the optimal one\n",
      "which is further refined under the guidance of generated structured data (e.g.,\n",
      "glyph images). Finally, qualitative and quantitative experiments conducted on a\n",
      "publicly-available dataset demonstrate that our method obtains high-quality\n",
      "synthesis results in the applications of vector font generation and\n",
      "interpolation, significantly outperforming the state of the art. \n",
      "\n",
      "\n",
      "We propose a new approach for high resolution semantic image synthesis. It\n",
      "consists of one base image generator and multiple class-specific generators.\n",
      "The base generator generates high quality images based on a segmentation map.\n",
      "To further improve the quality of different objects, we create a bank of\n",
      "Generative Adversarial Networks (GANs) by separately training class-specific\n",
      "models. This has several benefits including -- dedicated weights for each\n",
      "class; centrally aligned data for each model; additional training data from\n",
      "other sources, potential of higher resolution and quality; and easy\n",
      "manipulation of a specific object in the scene. Experiments show that our\n",
      "approach can generate high quality images in high resolution while having\n",
      "flexibility of object-level control by using class-specific generators. \n",
      "\n",
      "\n",
      "Accurate and automated super-resolution image synthesis is highly desired\n",
      "since it has the great potential to circumvent the need for acquiring high-cost\n",
      "medical scans and a time-consuming preprocessing pipeline of neuroimaging data.\n",
      "However, existing deep learning frameworks are solely designed to predict\n",
      "high-resolution (HR) image from a low-resolution (LR) one, which limits their\n",
      "generalization ability to brain graphs (i.e., connectomes). A small body of\n",
      "works has focused on superresolving brain graphs where the goal is to predict a\n",
      "HR graph from a single LR graph. Although promising, existing works mainly\n",
      "focus on superresolving graphs belonging to the same domain (e.g., functional),\n",
      "overlooking the domain fracture existing between multimodal brain data\n",
      "distributions (e.g., morphological and structural). To this aim, we propose a\n",
      "novel inter-domain adaptation framework namely, Learn to SuperResolve Brain\n",
      "Graphs with Knowledge Distillation Network (L2S-KDnet), which adopts a\n",
      "teacher-student paradigm to superresolve brain graphs. Our teacher network is a\n",
      "graph encoder-decoder that firstly learns the LR brain graph embeddings, and\n",
      "secondly learns how to align the resulting latent representations to the HR\n",
      "ground truth data distribution using an adversarial regularization. Ultimately,\n",
      "it decodes the HR graphs from the aligned embeddings. Next, our student network\n",
      "learns the knowledge of the aligned brain graphs as well as the topological\n",
      "structure of the predicted HR graphs transferred from the teacher. We further\n",
      "leverage the decoder of the teacher to optimize the student network. L2S-KDnet\n",
      "presents the first TS architecture tailored for brain graph super-resolution\n",
      "synthesis that is based on inter-domain alignment. Our experimental results\n",
      "demonstrate substantial performance gains over benchmark methods. \n",
      "\n",
      "\n",
      "Segmentation of Prostate Cancer (PCa) tissues from Gleason graded\n",
      "histopathology images is vital for accurate diagnosis. Although deep learning\n",
      "(DL) based segmentation methods achieve state-of-the-art accuracy, they rely on\n",
      "large datasets with manual annotations. We propose a method to synthesize for\n",
      "PCa histopathology images by learning the geometrical relationship between\n",
      "different disease labels using self-supervised learning. We use a weakly\n",
      "supervised segmentation approach that uses Gleason score to segment the\n",
      "diseased regions and the resulting segmentation map is used to train a Shape\n",
      "Restoration Network (ShaRe-Net) to predict missing mask segments in a\n",
      "self-supervised manner. Using DenseUNet as the backbone generator architecture\n",
      "we incorporate latent variable sampling to inject diversity in the image\n",
      "generation process and thus improve robustness. Experiments on multiple\n",
      "histopathology datasets demonstrate the superiority of our method over\n",
      "competing image synthesis methods for segmentation tasks. Ablation studies show\n",
      "the benefits of integrating geometry and diversity in generating high-quality\n",
      "images, and our self-supervised approach with limited class-labeled data\n",
      "achieves similar performance as fully supervised learning. \n",
      "\n",
      "\n",
      "Semantic Image Synthesis (SIS) is a subclass of image-to-image translation\n",
      "where a photorealistic image is synthesized from a segmentation mask. SIS has\n",
      "mostly been addressed as a supervised problem. However, state-of-the-art\n",
      "methods depend on a huge amount of labeled data and cannot be applied in an\n",
      "unpaired setting. On the other hand, generic unpaired image-to-image\n",
      "translation frameworks underperform in comparison, because they color-code\n",
      "semantic layouts and feed them to traditional convolutional networks, which\n",
      "then learn correspondences in appearance instead of semantic content. In this\n",
      "initial work, we propose a new Unsupervised paradigm for Semantic Image\n",
      "Synthesis (USIS) as a first step towards closing the performance gap between\n",
      "paired and unpaired settings. Notably, the framework deploys a SPADE generator\n",
      "that learns to output images with visually separable semantic classes using a\n",
      "self-supervised segmentation loss. Furthermore, in order to match the color and\n",
      "texture distribution of real images without losing high-frequency information,\n",
      "we propose to use whole image wavelet-based discrimination. We test our\n",
      "methodology on 3 challenging datasets and demonstrate its ability to generate\n",
      "multimodal photorealistic images with an improved quality in the unpaired\n",
      "setting. \n",
      "\n",
      "\n",
      "In recent years, conditional image synthesis has attracted growing attention\n",
      "due to its controllability in the image generation process. Although recent\n",
      "works have achieved realistic results, most of them fail to handle fine-grained\n",
      "styles with subtle details. To address this problem, a novel normalization\n",
      "module, named DRAN, is proposed. It learns fine-grained style representation,\n",
      "while maintaining the robustness to general styles. Specifically, we first\n",
      "introduce a multi-level structure, Spatiality-Aware Pyramid Pooling, to guide\n",
      "the model to learn coarse-to-fine features. Then, to adaptively fuse different\n",
      "levels of styles, we propose Dynamic Gating, making it possible to choose\n",
      "different styles according to different spatial regions. To evaluate the\n",
      "effectiveness and generalization ability of DRAN, we conduct a set of\n",
      "experiments on makeup transfer and semantic image synthesis. Quantitative and\n",
      "qualitative experiments show that equipped with DRAN, the baseline models are\n",
      "able to achieve significant improvement in complex style transfer and texture\n",
      "details reconstruction. \n",
      "\n",
      "\n",
      "Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable\n",
      "of learning neural implicit representations, are not performant for internal\n",
      "image synthesis applications. Convolutional Neural Networks (CNNs) are\n",
      "typically used instead for a variety of internal generative tasks, at the cost\n",
      "of a larger model. We propose Neural Knitwork, an architecture for neural\n",
      "implicit representation learning of natural images that achieves image\n",
      "synthesis by optimizing the distribution of image patches in an adversarial\n",
      "manner and by enforcing consistency between the patch predictions. To the best\n",
      "of our knowledge, this is the first implementation of a coordinate-based MLP\n",
      "tailored for synthesis tasks such as image inpainting, super-resolution, and\n",
      "denoising. We demonstrate the utility of the proposed technique by training on\n",
      "these three tasks. The results show that modeling natural images using patches,\n",
      "rather than pixels, produces results of higher fidelity. The resulting model\n",
      "requires 80% fewer parameters than alternative CNN-based solutions while\n",
      "achieving comparable performance and training time. \n",
      "\n",
      "\n",
      "Explanation techniques that synthesize small, interpretable changes to a\n",
      "given image while producing desired changes in the model prediction have become\n",
      "popular for introspecting black-box models. Commonly referred to as\n",
      "counterfactuals, the synthesized explanations are required to contain\n",
      "discernible changes (for easy interpretability) while also being realistic\n",
      "(consistency to the data manifold). In this paper, we focus on the case where\n",
      "we have access only to the trained deep classifier and not the actual training\n",
      "data. While the problem of inverting deep models to synthesize images from the\n",
      "training distribution has been explored, our goal is to develop a deep\n",
      "inversion approach to generate counterfactual explanations for a given query\n",
      "image. Despite their effectiveness in conditional image synthesis, we show that\n",
      "existing deep inversion methods are insufficient for producing meaningful\n",
      "counterfactuals. We propose DISC (Deep Inversion for Synthesizing\n",
      "Counterfactuals) that improves upon deep inversion by utilizing (a) stronger\n",
      "image priors, (b) incorporating a novel manifold consistency objective and (c)\n",
      "adopting a progressive optimization strategy. We find that, in addition to\n",
      "producing visually meaningful explanations, the counterfactuals from DISC are\n",
      "effective at learning classifier decision boundaries and are robust to unknown\n",
      "test-time corruptions. \n",
      "\n",
      "\n",
      "Multiplex immunofluorescence (MxIF) is an emerging imaging technique that\n",
      "produces the high sensitivity and specificity of single-cell mapping. With a\n",
      "tenet of 'seeing is believing', MxIF enables iterative staining and imaging\n",
      "extensive antibodies, which provides comprehensive biomarkers to segment and\n",
      "group different cells on a single tissue section. However, considerable\n",
      "depletion of the scarce tissue is inevitable from extensive rounds of staining\n",
      "and bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging\n",
      "can globally fail for particular rounds ('missing stain''). In this work, we\n",
      "focus on the 'missing stain' issue. It would be appealing to develop digital\n",
      "image synthesis approaches to restore missing stain images without losing more\n",
      "tissue physically. Herein, we aim to develop image synthesis approaches for\n",
      "eleven MxIF structural molecular markers (i.e., epithelial and stromal) on real\n",
      "samples. We propose a novel multi-channel high-resolution image synthesis\n",
      "approach, called pixN2N-HD, to tackle possible missing stain scenarios via a\n",
      "high-resolution generative adversarial network (GAN). Our contribution is\n",
      "three-fold: (1) a single deep network framework is proposed to tackle missing\n",
      "stain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four\n",
      "years of computational time to 20 hours when covering all possible missing\n",
      "stains scenarios, with up to five missing stains (e.g., '(N-1)-to-1',\n",
      "'(N-2)-to-2'); and (3) this work is the first comprehensive experimental study\n",
      "of investigating cross-stain synthesis in MxIF. Our results elucidate a\n",
      "promising direction of advancing MxIF imaging with deep image synthesis. \n",
      "\n",
      "\n",
      "Recent deep generative models allow real-time generation of hair images from\n",
      "sketch inputs. Existing solutions often require a user-provided binary mask to\n",
      "specify a target hair shape. This not only costs users extra labor but also\n",
      "fails to capture complicated hair boundaries. Those solutions usually encode\n",
      "hair structures via orientation maps, which, however, are not very effective to\n",
      "encode complex structures. We observe that colored hair sketches already\n",
      "implicitly define target hair shapes as well as hair appearance and are more\n",
      "flexible to depict hair structures than orientation maps. Based on these\n",
      "observations, we present SketchHairSalon, a two-stage framework for generating\n",
      "realistic hair images directly from freehand sketches depicting desired hair\n",
      "structure and appearance. At the first stage, we train a network to predict a\n",
      "hair matte from an input hair sketch, with an optional set of non-hair strokes.\n",
      "At the second stage, another network is trained to synthesize the structure and\n",
      "appearance of hair images from the input sketch and the generated matte. To\n",
      "make the networks in the two stages aware of long-term dependency of strokes,\n",
      "we apply self-attention modules to them. To train these networks, we present a\n",
      "new dataset containing thousands of annotated hair sketch-image pairs and\n",
      "corresponding hair mattes. Two efficient methods for sketch completion are\n",
      "proposed to automatically complete repetitive braided parts and hair strokes,\n",
      "respectively, thus reducing the workload of users. Based on the trained\n",
      "networks and the two sketch completion strategies, we build an intuitive\n",
      "interface to allow even novice users to design visually pleasing hair images\n",
      "exhibiting various hair structures and appearance via freehand sketches. The\n",
      "qualitative and quantitative evaluations show the advantages of the proposed\n",
      "system over the existing or alternative solutions. \n",
      "\n",
      "\n",
      "In this paper, we present a novel approach to synthesize realistic images\n",
      "based on their semantic layouts. It hypothesizes that for objects with similar\n",
      "appearance, they share similar representation. Our method establishes\n",
      "dependencies between regions according to their appearance correlation,\n",
      "yielding both spatially variant and associated representations. Conditioning on\n",
      "these features, we propose a dynamic weighted network constructed by spatially\n",
      "conditional computation (with both convolution and normalization). More than\n",
      "preserving semantic distinctions, the given dynamic network strengthens\n",
      "semantic relevance, benefiting global structure and detail synthesis. We\n",
      "demonstrate that our method gives the compelling generation performance\n",
      "qualitatively and quantitatively with extensive experiments on benchmarks. \n",
      "\n",
      "\n",
      "We present an algorithm for re-rendering a person from a single image under\n",
      "arbitrary poses. Existing methods often have difficulties in hallucinating\n",
      "occluded contents photo-realistically while preserving the identity and fine\n",
      "details in the source image. We first learn to inpaint the correspondence field\n",
      "between the body surface texture and the source image with a human body\n",
      "symmetry prior. The inpainted correspondence field allows us to transfer/warp\n",
      "local features extracted from the source to the target view even under large\n",
      "pose changes. Directly mapping the warped local features to an RGB image using\n",
      "a simple CNN decoder often leads to visible artifacts. Thus, we extend the\n",
      "StyleGAN generator so that it takes pose as input (for controlling poses) and\n",
      "introduces a spatially varying modulation for the latent space using the warped\n",
      "local features (for controlling appearances). We show that our method compares\n",
      "favorably against the state-of-the-art algorithms in both quantitative\n",
      "evaluation and visual comparison. \n",
      "\n",
      "\n",
      "Scene depth estimation from stereo and monocular imagery is critical for\n",
      "extracting 3D information for downstream tasks such as scene understanding.\n",
      "Recently, learning-based methods for depth estimation have received much\n",
      "attention due to their high performance and flexibility in hardware choice.\n",
      "However, collecting ground truth data for supervised training of these\n",
      "algorithms is costly or outright impossible. This circumstance suggests a need\n",
      "for alternative learning approaches that do not require corresponding depth\n",
      "measurements. Indeed, self-supervised learning of depth estimation provides an\n",
      "increasingly popular alternative. It is based on the idea that observed frames\n",
      "can be synthesized from neighboring frames if accurate depth of the scene is\n",
      "known - or in this case, estimated. We show empirically that - contrary to\n",
      "common belief - improvements in image synthesis do not necessitate improvement\n",
      "in depth estimation. Rather, optimizing for image synthesis can result in\n",
      "diverging performance with respect to the main prediction objective - depth. We\n",
      "attribute this diverging phenomenon to aleatoric uncertainties, which originate\n",
      "from data. Based on our experiments on four datasets (spanning street, indoor,\n",
      "and medical) and five architectures (monocular and stereo), we conclude that\n",
      "this diverging phenomenon is independent of the dataset domain and not\n",
      "mitigated by commonly used regularization techniques. To underscore the\n",
      "importance of this finding, we include a survey of methods which use image\n",
      "synthesis, totaling 127 papers over the last six years. This observed\n",
      "divergence has not been previously reported or studied in depth, suggesting\n",
      "room for future improvement of self-supervised approaches which might be\n",
      "impacted the finding. \n",
      "\n",
      "\n",
      "Deep learning is a technique for machine learning using multi-layer neural\n",
      "networks. It has been used for image synthesis and image recognition, but in\n",
      "recent years, it has also been used for various social detection and social\n",
      "labeling. In this analysis, we compared (1) the number of Iterations per minute\n",
      "between the GPU and CPU when using the VGG model and the NIN model, and (2) the\n",
      "number of Iterations per minute by the number of pixels when using the VGG\n",
      "model, using an image with 128 pixels. When the number of pixels was 64 or 128,\n",
      "the processing time was almost the same when using the GPU, but when the number\n",
      "of pixels was changed to 256, the number of iterations per minute decreased and\n",
      "the processing time increased by about three times. In this case study, since\n",
      "the number of pixels becomes core dumping when the number of pixels is 512 or\n",
      "more, we can consider that we should consider improvement in the vector\n",
      "calculation part. If we aim to achieve 8K highly saturated computer graphics\n",
      "using neural networks, we will need to consider an environment that allows\n",
      "computation even when the size of the image becomes even more highly saturated\n",
      "and massive, and parallel computation when performing image recognition and\n",
      "tuning. \n",
      "\n",
      "\n",
      "Virtual try-on is a promising application of computer graphics and human\n",
      "computer interaction that can have a profound real-world impact especially\n",
      "during this pandemic. Existing image-based works try to synthesize a try-on\n",
      "image from a single image of a target garment, but it inherently limits the\n",
      "ability to react to possible interactions. It is difficult to reproduce the\n",
      "change of wrinkles caused by pose and body size change, as well as pulling and\n",
      "stretching of the garment by hand. In this paper, we propose an alternative per\n",
      "garment capture and synthesis workflow to handle such rich interactions by\n",
      "training the model with many systematically captured images. Our workflow is\n",
      "composed of two parts: garment capturing and clothed person image synthesis. We\n",
      "designed an actuated mannequin and an efficient capturing process that collects\n",
      "the detailed deformations of the target garments under diverse body sizes and\n",
      "poses. Furthermore, we proposed to use a custom-designed measurement garment,\n",
      "and we captured paired images of the measurement garment and the target\n",
      "garments. We then learn a mapping between the measurement garment and the\n",
      "target garments using deep image-to-image translation. The customer can then\n",
      "try on the target garments interactively during online shopping. \n",
      "\n",
      "\n",
      "Pre-training visual and textual representations from large-scale image-text\n",
      "pairs is becoming a standard approach for many downstream vision-language\n",
      "tasks. The transformer-based models learn inter and intra-modal attention\n",
      "through a list of self-supervised learning tasks. This paper proposes LAViTeR,\n",
      "a novel architecture for visual and textual representation learning. The main\n",
      "module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\n",
      "GAN-based image synthesis and Image Captioning. We also propose a new\n",
      "evaluation metric measuring the similarity between the learnt visual and\n",
      "textual embedding. The experimental results on two public datasets, CUB and\n",
      "MS-COCO, demonstrate superior visual and textual representation alignment in\n",
      "the joint feature embedding space \n",
      "\n",
      "\n",
      "Text-to-image synthesis aims to generate a photo-realistic image from a given\n",
      "natural language description. Previous works have made significant progress\n",
      "with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to\n",
      "generate intact objects or clear textures (Fig 1). To address this issue, we\n",
      "propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a\n",
      "high-quality image by integrating two techniques: a self-supervised\n",
      "discriminator and a feature-aware loss. First, we design a self-supervised\n",
      "discriminator with an auxiliary decoder so that the discriminator can extract\n",
      "better representation. Secondly, we introduce a feature-aware loss to provide\n",
      "the generator more direct supervision by employing the feature representation\n",
      "from the self-supervised discriminator. Experiments on the MS-COCO dataset show\n",
      "that our proposed method significantly advances the state-of-the-art FID score\n",
      "from 28.92 to 24.58. \n",
      "\n",
      "\n",
      "As an effective way to integrate the information contained in multiple\n",
      "medical images under different modalities, medical image synthesis and fusion\n",
      "have emerged in various clinical applications such as disease diagnosis and\n",
      "treatment planning. In this paper, an invertible and variable augmented network\n",
      "(iVAN) is proposed for medical image synthesis and fusion. In iVAN, the channel\n",
      "number of the network input and output is the same through variable\n",
      "augmentation technology, and data relevance is enhanced, which is conducive to\n",
      "the generation of characterization information. Meanwhile, the invertible\n",
      "network is used to achieve the bidirectional inference processes. Due to the\n",
      "invertible and variable augmentation schemes, iVAN can not only be applied to\n",
      "the mappings of multi-input to one-output and multi-input to multi-output, but\n",
      "also be applied to one-input to multi-output. Experimental results demonstrated\n",
      "that the proposed method can obtain competitive or superior performance in\n",
      "comparison to representative medical image synthesis and fusion methods. \n",
      "\n",
      "\n",
      "Text-to-image synthesis refers to generating an image from a given text\n",
      "description, the key goal of which lies in photo realism and semantic\n",
      "consistency. Previous methods usually generate an initial image with sentence\n",
      "embedding and then refine it with fine-grained word embedding. Despite the\n",
      "significant progress, the 'aspect' information (e.g., red eyes) contained in\n",
      "the text, referring to several words rather than a word that depicts 'a\n",
      "particular part or feature of something', is often ignored, which is highly\n",
      "helpful for synthesizing image details. How to make better utilization of\n",
      "aspect information in text-to-image synthesis still remains an unresolved\n",
      "challenge. To address this problem, in this paper, we propose a Dynamic\n",
      "Aspect-awarE GAN (DAE-GAN) that represents text information comprehensively\n",
      "from multiple granularities, including sentence-level, word-level, and\n",
      "aspect-level. Moreover, inspired by human learning behaviors, we develop a\n",
      "novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an\n",
      "Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement\n",
      "(ALR) module are alternately employed. AGR utilizes word-level embedding to\n",
      "globally enhance the previously generated image, while ALR dynamically employs\n",
      "aspect-level embedding to refine image details from a local perspective.\n",
      "Finally, a corresponding matching loss function is designed to ensure the\n",
      "text-image semantic consistency at different levels. Extensive experiments on\n",
      "two well-studied and publicly available datasets (i.e., CUB-200 and COCO)\n",
      "demonstrate the superiority and rationality of our method. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have revolutionized image synthesis\n",
      "through many applications like face generation, photograph editing, and image\n",
      "super-resolution. Image synthesis using GANs has predominantly been uni-modal,\n",
      "with few approaches that can synthesize images from text or other data modes.\n",
      "Text-to-image synthesis, especially text-to-face synthesis, has promising use\n",
      "cases of robust face-generation from eye witness accounts and augmentation of\n",
      "the reading experience with visual cues. However, only a couple of datasets\n",
      "provide consolidated face data and textual descriptions for text-to-face\n",
      "synthesis. Moreover, these textual annotations are less extensive and\n",
      "descriptive, which reduces the diversity of faces generated from it. This paper\n",
      "empirically proves that increasing the number of facial attributes in each\n",
      "textual description helps GANs generate more diverse and real-looking faces. To\n",
      "prove this, we propose a new methodology that focuses on using structured\n",
      "textual descriptions. We also consolidate a Multi-Attributed and Structured\n",
      "Text-to-face (MAST) dataset consisting of high-quality images with structured\n",
      "textual annotations and make it available to researchers to experiment and\n",
      "build upon. Lastly, we report benchmark Frechet's Inception Distance (FID),\n",
      "Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for\n",
      "the MAST dataset. \n",
      "\n",
      "\n",
      "Generative adversarial networks have been widely used in image synthesis in\n",
      "recent years and the quality of the generated image has been greatly improved.\n",
      "However, the flexibility to control and decouple facial attributes (e.g., eyes,\n",
      "nose, mouth) is still limited. In this paper, we propose a novel approach,\n",
      "called ChildGAN, to generate a child's image according to the images of parents\n",
      "with heredity prior. The main idea is to disentangle the latent space of a\n",
      "pre-trained generation model and precisely control the face attributes of child\n",
      "images with clear semantics. We use distances between face landmarks as pseudo\n",
      "labels to figure out the most influential semantic vectors of the corresponding\n",
      "face attributes by calculating the gradient of latent vectors to pseudo labels.\n",
      "Furthermore, we disentangle the semantic vectors by weighting irrelevant\n",
      "features and orthogonalizing them with Schmidt Orthogonalization. Finally, we\n",
      "fuse the latent vector of the parents by leveraging the disentangled semantic\n",
      "vectors under the guidance of biological genetic laws. Extensive experiments\n",
      "demonstrate that our approach outperforms the existing methods with encouraging\n",
      "results. \n",
      "\n",
      "\n",
      "Synthetic-to-real transfer learning is a framework in which a synthetically\n",
      "generated dataset is used to pre-train a model to improve its performance on\n",
      "real vision tasks. The most significant advantage of using synthetic images is\n",
      "that the ground-truth labels are automatically available, enabling unlimited\n",
      "expansion of the data size without human cost. However, synthetic data may have\n",
      "a huge domain gap, in which case increasing the data size does not improve the\n",
      "performance. How can we know that? In this study, we derive a simple scaling\n",
      "law that predicts the performance from the amount of pre-training data. By\n",
      "estimating the parameters of the law, we can judge whether we should increase\n",
      "the data or change the setting of image synthesis. Further, we analyze the\n",
      "theory of transfer learning by considering learning dynamics and confirm that\n",
      "the derived generalization bound is consistent with our empirical findings. We\n",
      "empirically validated our scaling law on various experimental settings of\n",
      "benchmark tasks, model sizes, and complexities of synthetic images. \n",
      "\n",
      "\n",
      "Recent conditional image synthesis approaches provide high-quality\n",
      "synthesized images. However, it is still challenging to accurately adjust image\n",
      "contents such as the positions and orientations of objects, and synthesized\n",
      "images often have geometrically invalid contents. To provide users with rich\n",
      "controllability on synthesized images in the aspect of 3D geometry, we propose\n",
      "a novel approach to realistic-looking image synthesis based on a configurable\n",
      "3D scene layout. Our approach takes a 3D scene with semantic class labels as\n",
      "input and trains a 3D scene painting network that synthesizes color values for\n",
      "the input 3D scene. With the trained painting network, realistic-looking images\n",
      "for the input 3D scene can be rendered and manipulated. To train the painting\n",
      "network without 3D color supervision, we exploit an off-the-shelf 2D semantic\n",
      "image synthesis method. In experiments, we show that our approach produces\n",
      "images with geometrically correct structures and supports geometric\n",
      "manipulation such as the change of the viewpoint and object poses as well as\n",
      "manipulation of the painting style. \n",
      "\n",
      "\n",
      "Contrast resolution beyond the limits of conventional cone-beam CT (CBCT)\n",
      "systems is essential to high-quality imaging of the brain. We present a deep\n",
      "learning reconstruction method (dubbed DL-Recon) that integrates physically\n",
      "principled reconstruction models with DL-based image synthesis based on the\n",
      "statistical uncertainty in the synthesis image. A synthesis network was\n",
      "developed to generate a synthesized CBCT image (DL-Synthesis) from an\n",
      "uncorrected filtered back-projection (FBP) image. To improve generalizability\n",
      "(including accurate representation of lesions not seen in training), voxel-wise\n",
      "epistemic uncertainty of DL-Synthesis was computed using a Bayesian inference\n",
      "technique (Monte-Carlo dropout). In regions of high uncertainty, the DL-Recon\n",
      "method incorporates information from a physics-based reconstruction model and\n",
      "artifact-corrected projection data. Two forms of the DL-Recon method are\n",
      "proposed: (i) image-domain fusion of DL-Synthesis and FBP (DL-FBP) weighted by\n",
      "DL uncertainty; and (ii) a model-based iterative image reconstruction (MBIR)\n",
      "optimization using DL-Synthesis to compute a spatially varying regularization\n",
      "term based on DL uncertainty (DL-MBIR). The error in DL-Synthesis images was\n",
      "correlated with the uncertainty in the synthesis estimate. Compared to FBP and\n",
      "PWLS, the DL-Recon methods (both DL-FBP and DL-MBIR) showed ~50% reduction in\n",
      "noise (at matched spatial resolution) and ~40-70% improvement in image\n",
      "uniformity. Conventional DL-Synthesis alone exhibited ~10-60% under-estimation\n",
      "of lesion contrast and ~5-40% reduction in lesion segmentation accuracy (Dice\n",
      "coefficient) in simulated and real brain lesions, suggesting a lack of\n",
      "reliability / generalizability for structures unseen in the training data.\n",
      "DL-FBP and DL-MBIR improved the accuracy of reconstruction by directly\n",
      "incorporating information from the measurements in regions of high uncertainty. \n",
      "\n",
      "\n",
      "We present a novel approach to automatic image colorization by imitating the\n",
      "imagination process of human experts. Our imagination module is designed to\n",
      "generate color images that are context-correlated with black-and-white photos.\n",
      "Given a black-and-white image, our imagination module firstly extracts the\n",
      "context information, which is then used to synthesize colorful and diverse\n",
      "images using a conditional image synthesis network (e.g., semantic image\n",
      "synthesis model). We then design a colorization module to colorize the\n",
      "black-and-white images with the guidance of imagination for photorealistic\n",
      "colorization. Experimental results show that our work produces more colorful\n",
      "and diverse results than state-of-the-art image colorization methods. Our\n",
      "source codes will be publicly available. \n",
      "\n",
      "\n",
      "Most deep models for underwater image enhancement resort to training on\n",
      "synthetic datasets based on underwater image formation models. Although\n",
      "promising performances have been achieved, they are still limited by two\n",
      "problems: (1) existing underwater image synthesis models have an intrinsic\n",
      "limitation, in which the homogeneous ambient light is usually randomly\n",
      "generated and many important dependencies are ignored, and thus the synthesized\n",
      "training data cannot adequately express characteristics of real underwater\n",
      "environments; (2) most of deep models disregard lots of favorable underwater\n",
      "priors and heavily rely on training data, which extensively limits their\n",
      "application ranges. To address these limitations, a new underwater synthetic\n",
      "dataset is first established, in which a revised ambient light synthesis\n",
      "equation is embedded. The revised equation explicitly defines the complex\n",
      "mathematical relationship among intensity values of the ambient light in RGB\n",
      "channels and many dependencies such as surface-object depth, water types, etc,\n",
      "which helps to better simulate real underwater scene appearances. Secondly, a\n",
      "unified framework is proposed, named ANA-SYN, which can effectively enhance\n",
      "underwater images under collaborations of priors (underwater domain knowledge)\n",
      "and data information (underwater distortion distribution). The proposed\n",
      "framework includes an analysis network and a synthesis network, one for priors\n",
      "exploration and another for priors integration. To exploit more accurate\n",
      "priors, the significance of each prior for the input image is explored in the\n",
      "analysis network and an adaptive weighting module is designed to dynamically\n",
      "recalibrate them. Meanwhile, a novel prior guidance module is introduced in the\n",
      "synthesis network, which effectively aggregates the prior and data features and\n",
      "thus provides better hybrid information to perform the more reasonable image\n",
      "enhancement. \n",
      "\n",
      "\n",
      "Autoregressive models and their sequential factorization of the data\n",
      "likelihood have recently demonstrated great potential for image representation\n",
      "and synthesis. Nevertheless, they incorporate image context in a linear 1D\n",
      "order by attending only to previously synthesized image patches above or to the\n",
      "left. Not only is this unidirectional, sequential bias of attention unnatural\n",
      "for images as it disregards large parts of a scene until synthesis is almost\n",
      "complete. It also processes the entire image on a single scale, thus ignoring\n",
      "more global contextual information up to the gist of the entire scene. As a\n",
      "remedy we incorporate a coarse-to-fine hierarchy of context by combining the\n",
      "autoregressive formulation with a multinomial diffusion process: Whereas a\n",
      "multistage diffusion process successively removes information to coarsen an\n",
      "image, we train a (short) Markov chain to invert this process. In each stage,\n",
      "the resulting autoregressive ImageBART model progressively incorporates context\n",
      "from previous stages in a coarse-to-fine manner. Experiments show greatly\n",
      "improved image modification capabilities over autoregressive models while also\n",
      "providing high-fidelity image generation, both of which are enabled through\n",
      "efficient training in a compressed latent space. Specifically, our approach can\n",
      "take unrestricted, user-provided masks into account to perform local image\n",
      "editing. Thus, in contrast to pure autoregressive models, it can solve\n",
      "free-form image inpainting and, in the case of conditional models, local,\n",
      "text-guided image modification without requiring mask-specific training. \n",
      "\n",
      "\n",
      "In this chapter, we review biomedical applications and breakthroughs via\n",
      "leveraging algorithm unrolling, an important technique that bridges between\n",
      "traditional iterative algorithms and modern deep learning techniques. To\n",
      "provide context, we start by tracing the origin of algorithm unrolling and\n",
      "providing a comprehensive tutorial on how to unroll iterative algorithms into\n",
      "deep networks. We then extensively cover algorithm unrolling in a wide variety\n",
      "of biomedical imaging modalities and delve into several representative recent\n",
      "works in detail. Indeed, there is a rich history of iterative algorithms for\n",
      "biomedical image synthesis, which makes the field ripe for unrolling\n",
      "techniques. In addition, we put algorithm unrolling into a broad perspective,\n",
      "in order to understand why it is particularly effective and discuss recent\n",
      "trends. Finally, we conclude the chapter by discussing open challenges, and\n",
      "suggesting future research directions. \n",
      "\n",
      "\n",
      "Automated segmentation in medical image analysis is a challenging task that\n",
      "requires a large amount of manually labeled data. However, most existing\n",
      "learning-based approaches usually suffer from limited manually annotated\n",
      "medical data, which poses a major practical problem for accurate and robust\n",
      "medical image segmentation. In addition, most existing semi-supervised\n",
      "approaches are usually not robust compared with the supervised counterparts,\n",
      "and also lack explicit modeling of geometric structure and semantic\n",
      "information, both of which limit the segmentation accuracy. In this work, we\n",
      "present SimCVD, a simple contrastive distillation framework that significantly\n",
      "advances state-of-the-art voxel-wise representation learning. We first describe\n",
      "an unsupervised training strategy, which takes two views of an input volume and\n",
      "predicts their signed distance maps of object boundaries in a contrastive\n",
      "objective, with only two independent dropout as mask. This simple approach\n",
      "works surprisingly well, performing on the same level as previous fully\n",
      "supervised methods with much less labeled data. We hypothesize that dropout can\n",
      "be viewed as a minimal form of data augmentation and makes the network robust\n",
      "to representation collapse. Then, we propose to perform structural distillation\n",
      "by distilling pair-wise similarities. We evaluate SimCVD on two popular\n",
      "datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\n",
      "dataset. The results on the LA dataset demonstrate that, in two types of\n",
      "labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n",
      "90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\n",
      "previous best results. Our method can be trained in an end-to-end fashion,\n",
      "showing the promise of utilizing SimCVD as a general framework for downstream\n",
      "tasks, such as medical image synthesis, enhancement, and registration. \n",
      "\n",
      "\n",
      "Despite recent advancements in single-domain or single-object image\n",
      "generation, it is still challenging to generate complex scenes containing\n",
      "diverse, multiple objects and their interactions. Scene graphs, composed of\n",
      "nodes as objects and directed-edges as relationships among objects, offer an\n",
      "alternative representation of a scene that is more semantically grounded than\n",
      "images. We hypothesize that a generative model for scene graphs might be able\n",
      "to learn the underlying semantic structure of real-world scenes more\n",
      "effectively than images, and hence, generate realistic novel scenes in the form\n",
      "of scene graphs. In this work, we explore a new task for the unconditional\n",
      "generation of semantic scene graphs. We develop a deep auto-regressive model\n",
      "called SceneGraphGen which can directly learn the probability distribution over\n",
      "labelled and directed graphs using a hierarchical recurrent architecture. The\n",
      "model takes a seed object as input and generates a scene graph in a sequence of\n",
      "steps, each step generating an object node, followed by a sequence of\n",
      "relationship edges connecting to the previous nodes. We show that the scene\n",
      "graphs generated by SceneGraphGen are diverse and follow the semantic patterns\n",
      "of real-world scenes. Additionally, we demonstrate the application of the\n",
      "generated graphs in image synthesis, anomaly detection and scene graph\n",
      "completion. \n",
      "\n",
      "\n",
      "Medical images, especially volumetric images, are of high resolution and\n",
      "often exceed the capacity of standard desktop GPUs. As a result, most deep\n",
      "learning-based medical image analysis tasks require the input images to be\n",
      "downsampled, often substantially, before these can be fed to a neural network.\n",
      "However, downsampling can lead to a loss of image quality, which is undesirable\n",
      "especially in reconstruction tasks, where the fine geometric details need to be\n",
      "preserved. In this paper, we propose that high-resolution images can be\n",
      "reconstructed in a coarse-to-fine fashion, where a deep learning algorithm is\n",
      "only responsible for generating a coarse representation of the image, which\n",
      "consumes moderate GPU memory. For producing the high-resolution outcome, we\n",
      "propose two novel methods: learned voxel rearrangement of the coarse output and\n",
      "hierarchical image synthesis. Compared to the coarse output, the\n",
      "high-resolution counterpart allows for smooth surface triangulation, which can\n",
      "be 3D-printed in the highest possible quality. Experiments of this paper are\n",
      "carried out on the dataset of AutoImplant 2021\n",
      "(https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial\n",
      "implant design. The dataset contains high-resolution skulls that can be viewed\n",
      "as 2D manifolds embedded in a 3D space. Codes associated with this study can be\n",
      "accessed at https://github.com/Jianningli/voxel_rearrangement. \n",
      "\n",
      "\n",
      "The interest of the machine learning community in image synthesis has grown\n",
      "significantly in recent years, with the introduction of a wide range of deep\n",
      "generative models and means for training them. In this work, we propose a\n",
      "general model-agnostic technique for improving the image quality and the\n",
      "distribution fidelity of generated images, obtained by any generative model.\n",
      "Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier),\n",
      "is based on a post-processing procedure via the guidance of a given robust\n",
      "classifier and without a need for additional training of the generative model.\n",
      "Given a synthesized image, we propose to update it through projected gradient\n",
      "steps over the robust classifier, in an attempt to refine its recognition. We\n",
      "demonstrate this post-processing algorithm on various image synthesis methods\n",
      "and show a significant improvement of the generated images, both quantitatively\n",
      "and qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the\n",
      "image synthesis state of the art on ImageNet 128x128 by 14.81%, attaining an\n",
      "FID score of 2.53 and on 256x256 by 7.87%, achieving an FID of 3.63. \n",
      "\n",
      "\n",
      "Pose-guided person image synthesis aims to synthesize person images by\n",
      "transforming reference images into target poses. In this paper, we observe that\n",
      "the commonly used spatial transformation blocks have complementary advantages.\n",
      "We propose a novel model by combining the attention operation with the\n",
      "flow-based operation. Our model not only takes the advantage of the attention\n",
      "operation to generate accurate target structures but also uses the flow-based\n",
      "operation to sample realistic source textures. Both objective and subjective\n",
      "experiments demonstrate the superiority of our model. Meanwhile, comprehensive\n",
      "ablation studies verify our hypotheses and show the efficacy of the proposed\n",
      "modules. Besides, additional experiments on the portrait image editing task\n",
      "demonstrate the versatility of the proposed combination. \n",
      "\n",
      "\n",
      "Furnishing and rendering indoor scenes has been a long-standing task for\n",
      "interior design, where artists create a conceptual design for the space, build\n",
      "a 3D model of the space, decorate, and then perform rendering. Although the\n",
      "task is important, it is tedious and requires tremendous effort. In this paper,\n",
      "we introduce a new problem of domain-specific indoor scene image synthesis,\n",
      "namely neural scene decoration. Given a photograph of an empty indoor space and\n",
      "a list of decorations with layout determined by user, we aim to synthesize a\n",
      "new image of the same space with desired furnishing and decorations. Neural\n",
      "scene decoration can be applied to create conceptual interior designs in a\n",
      "simple yet effective manner. Our attempt to this research problem is a novel\n",
      "scene generation architecture that transforms an empty scene and an object\n",
      "layout into a realistic furnished scene photograph. We demonstrate the\n",
      "performance of our proposed method by comparing it with conditional image\n",
      "synthesis baselines built upon prevailing image translation approaches both\n",
      "qualitatively and quantitatively. We conduct extensive experiments to further\n",
      "validate the plausibility and aesthetics of our generated scenes. Our\n",
      "implementation is available at\n",
      "\\url{https://github.com/hkust-vgd/neural_scene_decoration}. \n",
      "\n",
      "\n",
      "Brain age estimation based on magnetic resonance imaging (MRI) is an active\n",
      "research area in early diagnosis of some neurodegenerative diseases (e.g.\n",
      "Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain\n",
      "underdevelopment for the young group. Deep learning methods have achieved the\n",
      "state-of-the-art performance in many medical image analysis tasks, including\n",
      "brain age estimation. However, the performance and generalisability of the deep\n",
      "learning model are highly dependent on the quantity and quality of the training\n",
      "data set. Both collecting and annotating brain MRI data are extremely\n",
      "time-consuming. In this paper, to overcome the data scarcity problem, we\n",
      "propose a generative adversarial network (GAN) based image synthesis method.\n",
      "Different from the existing GAN-based methods, we integrate a task-guided\n",
      "branch (a regression model for age estimation) to the end of the generator in\n",
      "GAN. By adding a task-guided loss to the conventional GAN loss, the learned\n",
      "low-dimensional latent space and the synthesised images are more task-specific.\n",
      "It helps to boost the performance of the down-stream task by combining the\n",
      "synthesised images and real images for model training. The proposed method was\n",
      "evaluated on a public brain MRI data set for age estimation. Our proposed\n",
      "method outperformed (statistically significant) a deep convolutional neural\n",
      "network based regression model and the GAN-based image synthesis method without\n",
      "the task-guided branch. More importantly, it enables the identification of\n",
      "age-related brain regions in the image space. The code is available on GitHub\n",
      "(https://github.com/ruizhe-l/tgb-gan). \n",
      "\n",
      "\n",
      "This paper investigates an open research task of text-to-image synthesis for\n",
      "automatically generating or manipulating images from text descriptions.\n",
      "Prevailing methods mainly use the text as conditions for GAN generation, and\n",
      "train different models for the text-guided image generation and manipulation\n",
      "tasks. In this paper, we propose a novel unified framework of Cycle-consistent\n",
      "Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image\n",
      "manipulation tasks. Specifically, we first train a GAN model without text\n",
      "input, aiming to generate images with high diversity and quality. Then we learn\n",
      "a GAN inversion model to convert the images back to the GAN latent space and\n",
      "obtain the inverted latent codes for each image, where we introduce the\n",
      "cycle-consistency training to learn more robust and consistent inverted latent\n",
      "codes. We further uncover the latent space semantics of the trained GAN model,\n",
      "by learning a similarity model between text representations and the latent\n",
      "codes. In the text-guided optimization module, we generate images with the\n",
      "desired semantic attributes by optimizing the inverted latent codes. Extensive\n",
      "experiments on the Recipe1M and CUB datasets validate the efficacy of our\n",
      "proposed framework. \n",
      "\n",
      "\n",
      "Guided image synthesis enables everyday users to create and edit\n",
      "photo-realistic images with minimum effort. The key challenge is balancing\n",
      "faithfulness to the user input (e.g., hand-drawn colored strokes) and realism\n",
      "of the synthesized image. Existing GAN-based methods attempt to achieve such\n",
      "balance using either conditional GANs or GAN inversions, which are challenging\n",
      "and often require additional training data or loss functions for individual\n",
      "applications. To address these issues, we introduce a new image synthesis and\n",
      "editing method, Stochastic Differential Editing (SDEdit), based on a diffusion\n",
      "model generative prior, which synthesizes realistic images by iteratively\n",
      "denoising through a stochastic differential equation (SDE). Given an input\n",
      "image with user guide of any type, SDEdit first adds noise to the input, then\n",
      "subsequently denoises the resulting image through the SDE prior to increase its\n",
      "realism. SDEdit does not require task-specific training or inversions and can\n",
      "naturally achieve the balance between realism and faithfulness. SDEdit\n",
      "significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on\n",
      "realism and 91.72% on overall satisfaction scores, according to a human\n",
      "perception study, on multiple tasks, including stroke-based image synthesis and\n",
      "editing as well as image compositing. \n",
      "\n",
      "\n",
      "We describe a method for unpaired realistic depth synthesis that learns\n",
      "diverse variations from the real-world depth scans and ensures geometric\n",
      "consistency between the synthetic and synthesized depth. The synthesized\n",
      "realistic depth can then be used to train task-specific networks facilitating\n",
      "label transfer from the synthetic domain. Unlike existing image synthesis\n",
      "pipelines, where geometries are mostly ignored, we treat geometries carried by\n",
      "the depth scans based on their own existence. We propose differential\n",
      "contrastive learning that explicitly enforces the underlying geometric\n",
      "properties to be invariant regarding the real variations been learned. The\n",
      "resulting depth synthesis method is task-agnostic, and we demonstrate the\n",
      "effectiveness of the proposed synthesis method by extensive evaluations on\n",
      "real-world geometric reasoning tasks. The networks trained with the depth\n",
      "synthesized by our method consistently achieve better performance across a wide\n",
      "range of tasks than state of the art, and can even surpass the networks\n",
      "supervised with full real-world annotations when slightly fine-tuned, showing\n",
      "good transferability. \n",
      "\n",
      "\n",
      "With the success of deep learning-based methods applied in medical image\n",
      "analysis, convolutional neural networks (CNNs) have been investigated for\n",
      "classifying liver disease from ultrasound (US) data. However, the scarcity of\n",
      "available large-scale labeled US data has hindered the success of CNNs for\n",
      "classifying liver disease from US data. In this work, we propose a novel\n",
      "generative adversarial network (GAN) architecture for realistic diseased and\n",
      "healthy liver US image synthesis. We adopt the concept of stacking to\n",
      "synthesize realistic liver US data. Quantitative and qualitative evaluation is\n",
      "performed on 550 in-vivo B-mode liver US images collected from 55 subjects. We\n",
      "also show that the synthesized images, together with real in vivo data, can be\n",
      "used to significantly improve the performance of traditional CNN architectures\n",
      "for Nonalcoholic fatty liver disease (NAFLD) classification. \n",
      "\n",
      "\n",
      "Image synthesis via Generative Adversarial Networks (GANs) of\n",
      "three-dimensional (3D) medical images has great potential that can be extended\n",
      "to many medical applications, such as, image enhancement and disease\n",
      "progression modeling. However, current GAN technologies for 3D medical image\n",
      "synthesis need to be significantly improved to be readily adapted to real-world\n",
      "medical problems. In this paper, we extend the state-of-the-art StyleGAN2\n",
      "model, which natively works with two-dimensional images, to enable 3D image\n",
      "synthesis. In addition to the image synthesis, we investigate the\n",
      "controllability and interpretability of the 3D-StyleGAN via style vectors\n",
      "inherited form the original StyleGAN2 that are highly suitable for medical\n",
      "applications: (i) the latent space projection and reconstruction of unseen real\n",
      "images, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and\n",
      "feasibility with ~12,000 three-dimensional full brain MR T1 images, although it\n",
      "can be applied to any 3D volumetric images. Furthermore, we explore different\n",
      "configurations of hyperparameters to investigate potential improvement of the\n",
      "image synthesis with larger networks. The codes and pre-trained networks are\n",
      "available online: https://github.com/sh4174/3DStyleGAN. \n",
      "\n",
      "\n",
      "For successful scene text recognition (STR) models, synthetic text image\n",
      "generators have alleviated the lack of annotated text images from the real\n",
      "world. Specifically, they generate multiple text images with diverse\n",
      "backgrounds, font styles, and text shapes and enable STR models to learn visual\n",
      "patterns that might not be accessible from manually annotated data. In this\n",
      "paper, we introduce a new synthetic text image generator, SynthTIGER, by\n",
      "analyzing techniques used for text image synthesis and integrating effective\n",
      "ones under a single algorithm. Moreover, we propose two techniques that\n",
      "alleviate the long-tail problem in length and character distributions of\n",
      "training data. In our experiments, SynthTIGER achieves better STR performance\n",
      "than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).\n",
      "Our ablation study demonstrates the benefits of using sub-components of\n",
      "SynthTIGER and the guideline on generating synthetic text images for STR\n",
      "models. Our implementation is publicly available at\n",
      "https://github.com/clovaai/synthtiger. \n",
      "\n",
      "\n",
      "This paper strives to generate a synthetic computed tomography (CT) image\n",
      "from a magnetic resonance (MR) image. The synthetic CT image is valuable for\n",
      "radiotherapy planning when only an MR image is available. Recent approaches\n",
      "have made large strides in solving this challenging synthesis problem with\n",
      "convolutional neural networks that learn a mapping from MR inputs to CT\n",
      "outputs. In this paper, we find that all existing approaches share a common\n",
      "limitation: reconstruction breaks down in and around the high-frequency parts\n",
      "of CT images. To address this common limitation, we introduce\n",
      "frequency-supervised deep networks to explicitly enhance high-frequency\n",
      "MR-to-CT image reconstruction. We propose a frequency decomposition layer that\n",
      "learns to decompose predicted CT outputs into low- and high-frequency\n",
      "components, and we introduce a refinement module to improve high-frequency\n",
      "reconstruction through high-frequency adversarial learning. Experimental\n",
      "results on a new dataset with 45 pairs of 3D MR-CT brain images show the\n",
      "effectiveness and potential of the proposed approach. Code is available at\n",
      "\\url{https://github.com/shizenglin/Frequency-Supervised-MR-to-CT-Image-Synthesis}. \n",
      "\n",
      "\n",
      "Despite the numerous successes of machine learning over the past decade\n",
      "(image recognition, decision-making, NLP, image synthesis), self-driving\n",
      "technology has not yet followed the same trend. In this paper, we study the\n",
      "history, composition, and development bottlenecks of the modern self-driving\n",
      "stack. We argue that the slow progress is caused by approaches that require too\n",
      "much hand-engineering, an over-reliance on road testing, and high fleet\n",
      "deployment costs. We observe that the classical stack has several bottlenecks\n",
      "that preclude the necessary scale needed to capture the long tail of rare\n",
      "events. To resolve these problems, we outline the principles of Autonomy 2.0,\n",
      "an ML-first approach to self-driving, as a viable alternative to the currently\n",
      "adopted state-of-the-art. This approach is based on (i) a fully differentiable\n",
      "AV stack trainable from human demonstrations, (ii) closed-loop data-driven\n",
      "reactive simulation, and (iii) large-scale, low-cost data collections as\n",
      "critical solutions towards scalability issues. We outline the general\n",
      "architecture, survey promising works in this direction and propose key\n",
      "challenges to be addressed by the community in the future. \n",
      "\n",
      "\n",
      "The CycleGAN framework allows for unsupervised image-to-image translation of\n",
      "unpaired data. In a scenario of surgical training on a physical surgical\n",
      "simulator, this method can be used to transform endoscopic images of phantoms\n",
      "into images which more closely resemble the intra-operative appearance of the\n",
      "same surgical target structure. This can be viewed as a novel augmented reality\n",
      "approach, which we coined Hyperrealism in previous work. In this use case, it\n",
      "is of paramount importance to display objects like needles, sutures or\n",
      "instruments consistent in both domains while altering the style to a more\n",
      "tissue-like appearance. Segmentation of these objects would allow for a direct\n",
      "transfer, however, contouring of these, partly tiny and thin foreground objects\n",
      "is cumbersome and perhaps inaccurate. Instead, we propose to use landmark\n",
      "detection on the points when sutures pass into the tissue. This objective is\n",
      "directly incorporated into a CycleGAN framework by treating the performance of\n",
      "pre-trained detector models as an additional optimization goal. We show that a\n",
      "task defined on these sparse landmark labels improves consistency of synthesis\n",
      "by the generator network in both domains. Comparing a baseline CycleGAN\n",
      "architecture to our proposed extension (DetCycleGAN), mean precision (PPV)\n",
      "improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n",
      "+0.4743. Furthermore, it could be shown that by dataset fusion, generated\n",
      "intra-operative images can be leveraged as additional training data for the\n",
      "detection network itself. The data is released within the scope of the AdaptOR\n",
      "MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\n",
      "https://github.com/Cardio-AI/detcyclegan_pytorch. \n",
      "\n",
      "\n",
      "With the rapid development of data-driven techniques, data has played an\n",
      "essential role in various computer vision tasks. Many realistic and synthetic\n",
      "datasets have been proposed to address different problems. However, there are\n",
      "lots of unresolved challenges: (1) the creation of dataset is usually a tedious\n",
      "process with manual annotations, (2) most datasets are only designed for a\n",
      "single specific task, (3) the modification or randomization of the 3D scene is\n",
      "difficult, and (4) the release of commercial 3D data may encounter copyright\n",
      "issue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl\n",
      "Synthesis system, to facilitate the 3D scene modification and the 2D image\n",
      "synthesis for various vision tasks. In particular, we design a programmable\n",
      "pipeline with Domain-Specific Language, allowing users to (1) select scenes\n",
      "from the commercial indoor scene database, (2) synthesize scenes for different\n",
      "tasks with customized rules, and (3) render various imagery data, such as\n",
      "visual color, geometric structures, semantic label. Our system eases the\n",
      "difficulty of customizing massive numbers of scenes for different tasks and\n",
      "relieves users from manipulating fine-grained scene configurations by providing\n",
      "user-controllable randomness using multi-level samplers. Most importantly, it\n",
      "empowers users to access commercial scene databases with millions of indoor\n",
      "scenes and protects the copyright of core data assets, e.g., 3D CAD models. We\n",
      "demonstrate the validity and flexibility of our system by using our synthesized\n",
      "data to improve the performance on different kinds of computer vision tasks. \n",
      "\n",
      "\n",
      "Positron Emission Tomography (PET) is an important tool for studying\n",
      "Alzheimer's disease (AD). PET scans can be used as diagnostics tools, and to\n",
      "provide molecular characterization of patients with cognitive disorders.\n",
      "However, multiple tracers are needed to measure glucose metabolism (18F-FDG),\n",
      "synaptic vesicle protein (11C-UCB-J), and $\\beta$-amyloid (11C-PiB).\n",
      "Administering multiple tracers to patient will lead to high radiation dose and\n",
      "cost. In addition, access to PET scans using new or less-available tracers with\n",
      "sophisticated production methods and short half-life isotopes may be very\n",
      "limited. Thus, it is desirable to develop an efficient multi-tracer PET\n",
      "synthesis model that can generate multi-tracer PET from single-tracer PET.\n",
      "Previous works on medical image synthesis focus on one-to-one fixed domain\n",
      "translations, and cannot simultaneously learn the feature from multi-tracer\n",
      "domains. Given 3 or more tracers, relying on previous methods will also create\n",
      "a heavy burden on the number of models to be trained. To tackle these issues,\n",
      "we propose a 3D unified anatomy-aware cyclic adversarial network (UCAN) for\n",
      "translating multi-tracer PET volumes with one unified generative model, where\n",
      "MR with anatomical information is incorporated. Evaluations on a multi-tracer\n",
      "PET dataset demonstrate the feasibility that our UCAN can generate high-quality\n",
      "multi-tracer PET volumes, with NMSE less than 15% for all PET tracers. \n",
      "\n",
      "\n",
      "In many applications of computer graphics, art and design, it is desirable\n",
      "for a user to provide intuitive non-image input, such as text, sketch, stroke,\n",
      "graph or layout, and have a computer system automatically generate\n",
      "photo-realistic images that adhere to the input content. While classic works\n",
      "that allow such automatic image content generation have followed a framework of\n",
      "image retrieval and composition, recent advances in deep generative models such\n",
      "as generative adversarial networks (GANs), variational autoencoders (VAEs), and\n",
      "flow-based methods have enabled more powerful and versatile image generation\n",
      "tasks. This paper reviews recent works for image synthesis given intuitive user\n",
      "input, covering advances in input versatility, image generation methodology,\n",
      "benchmark datasets, and evaluation metrics. This motivates new perspectives on\n",
      "input representation and interactivity, cross pollination between major image\n",
      "generation paradigms, and evaluation and comparison of generation methods. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have become the de-facto standard in\n",
      "image synthesis. However, without considering the foreground-background\n",
      "decomposition, existing GANs tend to capture excessive content correlation\n",
      "between foreground and background, thus constraining the diversity in image\n",
      "generation. This paper presents a novel Foreground-Background Composition GAN\n",
      "(FBC-GAN) that performs image generation by generating foreground objects and\n",
      "background scenes concurrently and independently, followed by composing them\n",
      "with style and geometrical consistency. With this explicit design, FBC-GAN can\n",
      "generate images with foregrounds and backgrounds that are mutually independent\n",
      "in contents, thus lifting the undesirably learned content correlation\n",
      "constraint and achieving superior diversity. It also provides excellent\n",
      "flexibility by allowing the same foreground object with different background\n",
      "scenes, the same background scene with varying foreground objects, or the same\n",
      "foreground object and background scene with different object positions, sizes\n",
      "and poses. It can compose foreground objects and background scenes sampled from\n",
      "different datasets as well. Extensive experiments over multiple datasets show\n",
      "that FBC-GAN achieves competitive visual realism and superior diversity as\n",
      "compared with state-of-the-art methods. \n",
      "\n",
      "\n",
      "Despite significant progress on current state-of-the-art image generation\n",
      "models, synthesis of document images containing multiple and complex object\n",
      "layouts is a challenging task. This paper presents a novel approach, called\n",
      "DocSynth, to automatically synthesize document images based on a given layout.\n",
      "In this work, given a spatial layout (bounding boxes with object categories) as\n",
      "a reference by the user, our proposed DocSynth model learns to generate a set\n",
      "of realistic document images consistent with the defined layout. Also, this\n",
      "framework has been adapted to this work as a superior baseline model for\n",
      "creating synthetic document image datasets for augmenting real data during\n",
      "training for document layout analysis tasks. Different sets of learning\n",
      "objectives have been also used to improve the model performance.\n",
      "Quantitatively, we also compare the generated results of our model with real\n",
      "data using standard evaluation metrics. The results highlight that our model\n",
      "can successfully generate realistic and diverse document images with multiple\n",
      "objects. We also present a comprehensive qualitative analysis summary of the\n",
      "different scopes of synthetic image generation tasks. Lastly, to our knowledge\n",
      "this is the first work of its kind. \n",
      "\n",
      "\n",
      "The goal of text-to-image synthesis is to generate a visually realistic image\n",
      "that matches a given text description. In practice, the captions annotated by\n",
      "humans for the same image have large variance in terms of contents and the\n",
      "choice of words. The linguistic discrepancy between the captions of the\n",
      "identical image leads to the synthetic images deviating from the ground truth.\n",
      "To address this issue, we propose a contrastive learning approach to improve\n",
      "the quality and enhance the semantic consistency of synthetic images. In the\n",
      "pretraining stage, we utilize the contrastive learning approach to learn the\n",
      "consistent textual representations for the captions corresponding to the same\n",
      "image. Furthermore, in the following stage of GAN training, we employ the\n",
      "contrastive learning method to enhance the consistency between the generated\n",
      "images from the captions related to the same image. We evaluate our approach\n",
      "over two popular text-to-image synthesis models, AttnGAN and DM-GAN, on\n",
      "datasets CUB and COCO, respectively. Experimental results have shown that our\n",
      "approach can effectively improve the quality of synthetic images in terms of\n",
      "three metrics: IS, FID and R-precision. Especially, on the challenging COCO\n",
      "dataset, our approach boosts the FID signifcantly by 29.60% over AttnGAN and by\n",
      "21.96% over DM-GAN. \n",
      "\n",
      "\n",
      "This paper proposes an approach that generates multiple 3D human meshes from\n",
      "text. The human shapes are represented by 3D meshes based on the SMPL model.\n",
      "The model's performance is evaluated on the COCO dataset, which contains\n",
      "challenging human shapes and intricate interactions between individuals. The\n",
      "model is able to capture the dynamics of the scene and the interactions between\n",
      "individuals based on text. We further show how using such a shape as input to\n",
      "image synthesis frameworks helps to constrain the network to synthesize humans\n",
      "with realistic human shapes. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have been extremely successful in\n",
      "various application domains. Adversarial image synthesis has drawn increasing\n",
      "attention and made tremendous progress in recent years because of its wide\n",
      "range of applications in many computer vision and image processing problems.\n",
      "Among the many applications of GAN, image synthesis is the most well-studied\n",
      "one, and research in this area has already demonstrated the great potential of\n",
      "using GAN in image synthesis. In this paper, we provide a taxonomy of methods\n",
      "used in image synthesis, review different models for text-to-image synthesis\n",
      "and image-to-image translation, and discuss some evaluation metrics as well as\n",
      "possible future research directions in image synthesis with GAN. \n",
      "\n",
      "\n",
      "Generative adversarial models with convolutional neural network (CNN)\n",
      "backbones have recently been established as state-of-the-art in numerous\n",
      "medical image synthesis tasks. However, CNNs are designed to perform local\n",
      "processing with compact filters, and this inductive bias compromises learning\n",
      "of contextual features. Here, we propose a novel generative adversarial\n",
      "approach for medical image synthesis, ResViT, that leverages the contextual\n",
      "sensitivity of vision transformers along with the precision of convolution\n",
      "operators and realism of adversarial learning.} ResViT's generator employs a\n",
      "central bottleneck comprising novel aggregated residual transformer (ART)\n",
      "blocks that synergistically combine residual convolutional and transformer\n",
      "modules. Residual connections in ART blocks promote diversity in captured\n",
      "representations, while a channel compression module distills task-relevant\n",
      "information. A weight sharing strategy is introduced among ART blocks to\n",
      "mitigate computational burden. A unified implementation is introduced to avoid\n",
      "the need to rebuild separate synthesis models for varying source-target\n",
      "modality configurations. Comprehensive demonstrations are performed for\n",
      "synthesizing missing sequences in multi-contrast MRI, and CT images from MRI.\n",
      "Our results indicate superiority of ResViT against competing CNN- and\n",
      "transformer-based methods in terms of qualitative observations and quantitative\n",
      "metrics. \n",
      "\n",
      "\n",
      "This paper proposes two important contributions for conditional Generative\n",
      "Adversarial Networks (cGANs) to improve the wide variety of applications that\n",
      "exploit this architecture. The first main contribution is an analysis of cGANs\n",
      "to show that they are not explicitly conditional. In particular, it will be\n",
      "shown that the discriminator and subsequently the cGAN does not automatically\n",
      "learn the conditionality between inputs. The second contribution is a new\n",
      "method, called a contrario cGAN, that explicitly models conditionality for both\n",
      "parts of the adversarial architecture via a novel a contrario loss that\n",
      "involves training the discriminator to learn unconditional (adverse) examples.\n",
      "This leads to a novel type of data augmentation approach for GANs (a contrario\n",
      "learning) which allows to restrict the search space of the generator to\n",
      "conditional outputs using adverse examples. Extensive experimentation is\n",
      "carried out to evaluate the conditionality of the discriminator by proposing a\n",
      "probability distribution analysis. Comparisons with the cGAN architecture for\n",
      "different applications show significant improvements in performance on well\n",
      "known datasets including, semantic image synthesis, image segmentation,\n",
      "monocular depth prediction and \"single label\"-to-image using different metrics\n",
      "including Fr\\'echet Inception Distance (FID), mean Intersection over Union\n",
      "(mIoU), Root Mean Square Error log (RMSE log) and Number of\n",
      "statistically-Different Bins (NDB). \n",
      "\n",
      "\n",
      "Recent advances in image synthesis enables one to translate images by\n",
      "learning the mapping between a source domain and a target domain. Existing\n",
      "methods tend to learn the distributions by training a model on a variety of\n",
      "datasets, with results evaluated largely in a subjective manner. Relatively few\n",
      "works in this area, however, study the potential use of semantic image\n",
      "translation methods for image recognition tasks. In this paper, we explore the\n",
      "use of Single Image Texture Translation (SITT) for data augmentation. We first\n",
      "propose a lightweight model for translating texture to images based on a single\n",
      "input of source texture, allowing for fast training and testing. Based on SITT,\n",
      "we then explore the use of augmented data in long-tailed and few-shot image\n",
      "classification tasks. We find the proposed method is capable of translating\n",
      "input data into a target domain, leading to consistent improved image\n",
      "recognition performance. Finally, we examine how SITT and related image\n",
      "translation methods can provide a basis for a data-efficient, augmentation\n",
      "engineering approach to model training. \n",
      "\n",
      "\n",
      "Semantic image synthesis is a process for generating photorealistic images\n",
      "from a single semantic mask. To enrich the diversity of multimodal image\n",
      "synthesis, previous methods have controlled the global appearance of an output\n",
      "image by learning a single latent space. However, a single latent code is often\n",
      "insufficient for capturing various object styles because object appearance\n",
      "depends on multiple factors. To handle individual factors that determine object\n",
      "styles, we propose a class- and layer-wise extension to the variational\n",
      "autoencoder (VAE) framework that allows flexible control over each object class\n",
      "at the local to global levels by learning multiple latent spaces. Furthermore,\n",
      "we demonstrate that our method generates images that are both plausible and\n",
      "more diverse compared to state-of-the-art methods via extensive experiments\n",
      "with real and synthetic datasets inthree different domains. We also show that\n",
      "our method enables a wide range of applications in image synthesis and editing\n",
      "tasks. \n",
      "\n",
      "\n",
      "We present an algorithm that learns a coarse 3D representation of objects\n",
      "from unposed multi-view 2D mask supervision, then uses it to generate detailed\n",
      "mask and image texture. In contrast to existing voxel-based methods for unposed\n",
      "object reconstruction, our approach learns to represent the generated shape and\n",
      "pose with a set of self-supervised canonical 3D anisotropic Gaussians via a\n",
      "perspective camera, and a set of per-image transforms. We show that this\n",
      "approach can robustly estimate a 3D space for the camera and object, while\n",
      "recent baselines sometimes struggle to reconstruct coherent 3D spaces in this\n",
      "setting. We show results on synthetic datasets with realistic lighting, and\n",
      "demonstrate object insertion with interactive posing. With our work, we help\n",
      "move towards structured representations that handle more real-world variation\n",
      "in learning-based object reconstruction. \n",
      "\n",
      "\n",
      "Pseudo-normality synthesis, which computationally generates a pseudo-normal\n",
      "image from an abnormal one (e.g., with lesions), is critical in many\n",
      "perspectives, from lesion detection, data augmentation to clinical surgery\n",
      "suggestion. However, it is challenging to generate high-quality pseudo-normal\n",
      "images in the absence of the lesion information. Thus, expensive lesion\n",
      "segmentation data have been introduced to provide lesion information for the\n",
      "generative models and improve the quality of the synthetic images. In this\n",
      "paper, we aim to alleviate the need of a large amount of lesion segmentation\n",
      "data when generating pseudo-normal images. We propose a Semi-supervised Medical\n",
      "Image generative LEarning network (SMILE) which not only utilizes limited\n",
      "medical images with segmentation masks, but also leverages massive medical\n",
      "images without segmentation masks to generate realistic pseudo-normal images.\n",
      "Extensive experiments show that our model outperforms the best state-of-the-art\n",
      "model by up to 6% for data augmentation task and 3% in generating high-quality\n",
      "images. Moreover, the proposed semi-supervised learning achieves comparable\n",
      "medical image synthesis quality with supervised learning model, using only 50\n",
      "of segmentation data. \n",
      "\n",
      "\n",
      "Self-training based unsupervised domain adaptation (UDA) has shown great\n",
      "potential to address the problem of domain shift, when applying a trained deep\n",
      "learning model in a source domain to unlabeled target domains. However, while\n",
      "the self-training UDA has demonstrated its effectiveness on discriminative\n",
      "tasks, such as classification and segmentation, via the reliable pseudo-label\n",
      "selection based on the softmax discrete histogram, the self-training UDA for\n",
      "generative tasks, such as image synthesis, is not fully investigated. In this\n",
      "work, we propose a novel generative self-training (GST) UDA framework with\n",
      "continuous value prediction and regression objective for cross-domain image\n",
      "synthesis. Specifically, we propose to filter the pseudo-label with an\n",
      "uncertainty mask, and quantify the predictive confidence of generated images\n",
      "with practical variational Bayes learning. The fast test-time adaptation is\n",
      "achieved by a round-based alternative optimization scheme. We validated our\n",
      "framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\n",
      "problem, where datasets in the source and target domains were acquired from\n",
      "different scanners or centers. Extensive validations were carried out to verify\n",
      "our framework against popular adversarial training UDA methods. Results show\n",
      "that our GST, with tagged MRI of test subjects in new target domains, improved\n",
      "the synthesis quality by a large margin, compared with the adversarial training\n",
      "UDA methods. \n",
      "\n",
      "\n",
      "Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n",
      "(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\n",
      "of well-exposed areas must be expanded and any missing information due to\n",
      "over/under-exposure must be recovered (hallucinated). The majority of methods\n",
      "focus on the former and are relatively successful, while most attempts on the\n",
      "latter are not of sufficient quality, even ones based on Convolutional Neural\n",
      "Networks (CNNs). A major factor for the reduced inpainting quality in some\n",
      "works is the choice of loss function. Work based on Generative Adversarial\n",
      "Networks (GANs) shows promising results for image synthesis and LDR inpainting,\n",
      "suggesting that GAN losses can improve inverse tone mapping results. This work\n",
      "presents a GAN-based method that hallucinates missing information from badly\n",
      "exposed areas in LDR images and compares its efficacy with alternative\n",
      "variations. The proposed method is quantitatively competitive with\n",
      "state-of-the-art inverse tone mapping methods, providing good dynamic range\n",
      "expansion for well-exposed areas and plausible hallucinations for saturated and\n",
      "under-exposed areas. A density-based normalisation method, targeted for HDR\n",
      "content, is also proposed, as well as an HDR data augmentation method targeted\n",
      "for HDR hallucination. \n",
      "\n",
      "\n",
      "Deep generative models such as GANs have driven impressive advances in\n",
      "conditional image synthesis in recent years. A persistent challenge has been to\n",
      "generate diverse versions of output images from the same input image, due to\n",
      "the problem of mode collapse: because only one ground truth output image is\n",
      "given per input image, only one mode of the conditional distribution is\n",
      "modelled. In this paper, we focus on this problem of multimodal conditional\n",
      "image synthesis and build on the recently proposed technique of Implicit\n",
      "Maximum Likelihood Estimation (IMLE). Prior IMLE-based methods required\n",
      "different architectures for different tasks, which limit their applicability,\n",
      "and were lacking in fine details in the generated images. We propose CAM-Net, a\n",
      "unified architecture that can be applied to a broad range of tasks.\n",
      "Additionally, it is capable of generating convincing high frequency details,\n",
      "achieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%\n",
      "compared to the baseline. \n",
      "\n",
      "\n",
      "While medical image segmentation is an important task for computer aided\n",
      "diagnosis, the high expertise requirement for pixelwise manual annotations\n",
      "makes it a challenging and time consuming task. Since conventional data\n",
      "augmentations do not fully represent the underlying distribution of the\n",
      "training set, the trained models have varying performance when tested on images\n",
      "captured from different sources. Most prior work on image synthesis for data\n",
      "augmentation ignore the interleaved geometric relationship between different\n",
      "anatomical labels. We propose improvements over previous GAN-based medical\n",
      "image synthesis methods by learning the relationship between different\n",
      "anatomical labels. We use a weakly supervised segmentation method to obtain\n",
      "pixel level semantic label map of images which is used learn the intrinsic\n",
      "relationship of geometry and shape across semantic labels. Latent space\n",
      "variable sampling results in diverse generated images from a base image and\n",
      "improves robustness. We use the synthetic images from our method to train\n",
      "networks for segmenting COVID-19 infected areas from lung CT images. The\n",
      "proposed method outperforms state-of-the-art segmentation methods on a public\n",
      "dataset. Ablation studies also demonstrate benefits of integrating geometry and\n",
      "diversity. \n",
      "\n",
      "\n",
      "Despite unconditional feature inversion being the foundation of many image\n",
      "synthesis applications, training an inverter demands a high computational\n",
      "budget, large decoding capacity and imposing conditions such as autoregressive\n",
      "priors. To address these limitations, we propose the use of adversarially\n",
      "robust representations as a perceptual primitive for feature inversion. We\n",
      "train an adversarially robust encoder to extract disentangled and\n",
      "perceptually-aligned image representations, making them easily invertible. By\n",
      "training a simple generator with the mirror architecture of the encoder, we\n",
      "achieve superior reconstruction quality and generalization over standard\n",
      "models. Based on this, we propose an adversarially robust autoencoder and\n",
      "demonstrate its improved performance on style transfer, image denoising and\n",
      "anomaly detection tasks. Compared to recent ImageNet feature inversion methods,\n",
      "our model attains improved performance with significantly less complexity. \n",
      "\n",
      "\n",
      "Many machine learning applications can benefit from simulated data for\n",
      "systematic validation - in particular if real-life data is difficult to obtain\n",
      "or annotate. However, since simulations are prone to domain shift w.r.t.\n",
      "real-life data, it is crucial to verify the transferability of the obtained\n",
      "results. We propose a novel framework consisting of a generative label-to-image\n",
      "synthesis model together with different transferability measures to inspect to\n",
      "what extent we can transfer testing results of semantic segmentation models\n",
      "from synthetic data to equivalent real-life data. With slight modifications,\n",
      "our approach is extendable to, e.g., general multi-class classification tasks.\n",
      "Grounded on the transferability analysis, our approach additionally allows for\n",
      "extensive testing by incorporating controlled simulations. We validate our\n",
      "approach empirically on a semantic segmentation task on driving scenes.\n",
      "Transferability is tested using correlation analysis of IoU and a learned\n",
      "discriminator. Although the latter can distinguish between real-life and\n",
      "synthetic tests, in the former we observe surprisingly strong correlations of\n",
      "0.7 for both cars and pedestrians. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have significantly advanced image\n",
      "synthesis, however, the synthesis quality drops significantly given a limited\n",
      "amount of training data. To improve the data efficiency of GAN training, prior\n",
      "work typically employs data augmentation to mitigate the overfitting of the\n",
      "discriminator yet still learn the discriminator with a bi-classification (i.e.,\n",
      "real vs. fake) task. In this work, we propose a data-efficient Instance\n",
      "Generation (InsGen) method based on instance discrimination. Concretely,\n",
      "besides differentiating the real domain from the fake domain, the discriminator\n",
      "is required to distinguish every individual image, no matter it comes from the\n",
      "training set or from the generator. In this way, the discriminator can benefit\n",
      "from the infinite synthesized samples for training, alleviating the overfitting\n",
      "problem caused by insufficient training data. A noise perturbation strategy is\n",
      "further introduced to improve its discriminative power. Meanwhile, the learned\n",
      "instance discrimination capability from the discriminator is in turn exploited\n",
      "to encourage the generator for diverse generation. Extensive experiments\n",
      "demonstrate the effectiveness of our method on a variety of datasets and\n",
      "training settings. Noticeably, on the setting of 2K training images from the\n",
      "FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID\n",
      "improvement. \n",
      "\n",
      "\n",
      "Flow-based generative models have shown an excellent ability to explicitly\n",
      "learn the probability density function of data via a sequence of invertible\n",
      "transformations. Yet, learning attentions in generative flows remains\n",
      "understudied, while it has made breakthroughs in other domains. To fill the\n",
      "gap, this paper introduces two types of invertible attention mechanisms, i.e.,\n",
      "map-based and transformer-based attentions, for both unconditional and\n",
      "conditional generative flows. The key idea is to exploit a masked scheme of\n",
      "these two attentions to learn long-range data dependencies in the context of\n",
      "generative flows. The masked scheme allows for invertible attention modules\n",
      "with tractable Jacobian determinants, enabling its seamless integration at any\n",
      "positions of the flow-based models. The proposed attention mechanisms lead to\n",
      "more efficient generative flows, due to their capability of modeling the\n",
      "long-term data dependencies. Evaluation on multiple image synthesis tasks shows\n",
      "that the proposed attention flows result in efficient models and compare\n",
      "favorably against the state-of-the-art unconditional and conditional generative\n",
      "flows. \n",
      "\n",
      "\n",
      "We introduce CAFLOW, a new diverse image-to-image translation model that\n",
      "simultaneously leverages the power of auto-regressive modeling and the modeling\n",
      "efficiency of conditional normalizing flows. We transform the conditioning\n",
      "image into a sequence of latent encodings using a multi-scale normalizing flow\n",
      "and repeat the process for the conditioned image. We model the conditional\n",
      "distribution of the latent encodings by modeling the auto-regressive\n",
      "distributions with an efficient multi-scale normalizing flow, where each\n",
      "conditioning factor affects image synthesis at its respective resolution scale.\n",
      "Our proposed framework performs well on a range of image-to-image translation\n",
      "tasks. It outperforms former designs of conditional flows because of its\n",
      "expressive auto-regressive structure. \n",
      "\n",
      "\n",
      "Recently, AutoRegressive (AR) models for the whole image generation empowered\n",
      "by transformers have achieved comparable or even better performance to\n",
      "Generative Adversarial Networks (GANs). Unfortunately, directly applying such\n",
      "AR models to edit/change local image regions, may suffer from the problems of\n",
      "missing global information, slow inference speed, and information leakage of\n",
      "local guidance. To address these limitations, we propose a novel model -- image\n",
      "Local Autoregressive Transformer (iLAT), to better facilitate the locally\n",
      "guided image synthesis. Our iLAT learns the novel local discrete\n",
      "representations, by the newly proposed local autoregressive (LA) transformer of\n",
      "the attention mask and convolution mechanism. Thus iLAT can efficiently\n",
      "synthesize the local image regions by key guidance information. Our iLAT is\n",
      "evaluated on various locally guided image syntheses, such as pose-guided person\n",
      "image synthesis and face editing. Both the quantitative and qualitative results\n",
      "show the efficacy of our model. \n",
      "\n",
      "\n",
      "Evaluating the performance of generative models in image synthesis is a\n",
      "challenging task. Although the Fr\\'echet Inception Distance is a widely\n",
      "accepted evaluation metric, it integrates different aspects (e.g., fidelity and\n",
      "diversity) of synthesized images into a single score and assumes the normality\n",
      "of embedded vectors. Recent methods such as precision-and-recall and its\n",
      "variants such as density-and-coverage have been developed to separate fidelity\n",
      "and diversity based on k-nearest neighborhood methods. In this study, we\n",
      "propose an algorithm named barcode, which is inspired by the topological data\n",
      "analysis and is almost free of assumption and hyperparameter selections. In\n",
      "extensive experiments on real-world datasets as well as theoretical approach on\n",
      "high-dimensional normal samples, it was found that the 'usual' normality\n",
      "assumption of embedded vectors has several drawbacks. The experimental results\n",
      "demonstrate that barcode outperforms other methods in evaluating fidelity and\n",
      "diversity of GAN outputs. Official codes can be found in\n",
      "https://github.com/minjeekim00/Barcode. \n",
      "\n",
      "\n",
      "Despite the recent progress of generative adversarial networks (GANs) at\n",
      "synthesizing photo-realistic images, producing complex urban scenes remains a\n",
      "challenging problem. Previous works break down scene generation into two\n",
      "consecutive phases: unconditional semantic layout synthesis and image synthesis\n",
      "conditioned on layouts. In this work, we propose to condition layout generation\n",
      "as well for higher semantic control: given a vector of class proportions, we\n",
      "generate layouts with matching composition. To this end, we introduce a\n",
      "conditional framework with novel architecture designs and learning objectives,\n",
      "which effectively accommodates class proportions to guide the scene generation\n",
      "process. The proposed architecture also allows partial layout editing with\n",
      "interesting applications. Thanks to the semantic control, we can produce\n",
      "layouts close to the real distribution, helping enhance the whole scene\n",
      "generation process. On different metrics and urban scene benchmarks, our models\n",
      "outperform existing baselines. Moreover, we demonstrate the merit of our\n",
      "approach for data augmentation: semantic segmenters trained on real\n",
      "layout-image pairs along with additional ones generated by our approach\n",
      "outperform models only trained on real pairs. \n",
      "\n",
      "\n",
      "Controllable person image generation aims to produce realistic human images\n",
      "with desirable attributes (e.g., the given pose, cloth textures or hair style).\n",
      "However, the large spatial misalignment between the source and target images\n",
      "makes the standard architectures for image-to-image translation not suitable\n",
      "for this task. Most of the state-of-the-art architectures avoid the alignment\n",
      "step during the generation, which causes many artifacts, especially for person\n",
      "images with complex textures. To solve this problem, we introduce a novel\n",
      "Spatially-Adaptive Warped Normalization (SAWN), which integrates a learned\n",
      "flow-field to warp modulation parameters. This allows us to align person\n",
      "spatial-adaptive styles with pose features efficiently. Moreover, we propose a\n",
      "novel self-training part replacement strategy to refine the pretrained model\n",
      "for the texture-transfer task, significantly improving the quality of the\n",
      "generated cloth and the preservation ability of irrelevant regions. Our\n",
      "experimental results on the widely used DeepFashion dataset demonstrate a\n",
      "significant improvement of the proposed method over the state-of-the-art\n",
      "methods on both pose-transfer and texture-transfer tasks. The source code is\n",
      "available at https://github.com/zhangqianhui/Sawn. \n",
      "\n",
      "\n",
      "Conditional image synthesis aims to create an image according to some\n",
      "multi-modal guidance in the forms of textual descriptions, reference images,\n",
      "and image blocks to preserve, as well as their combinations. In this paper,\n",
      "instead of investigating these control signals separately, we propose a new\n",
      "two-stage architecture, M6-UFC, to unify any number of multi-modal controls. In\n",
      "M6-UFC, both the diverse control signals and the synthesized image are\n",
      "uniformly represented as a sequence of discrete tokens to be processed by\n",
      "Transformer. Different from existing two-stage autoregressive approaches such\n",
      "as DALL-E and VQGAN, M6-UFC adopts non-autoregressive generation (NAR) at the\n",
      "second stage to enhance the holistic consistency of the synthesized image, to\n",
      "support preserving specified image blocks, and to improve the synthesis speed.\n",
      "Further, we design a progressive algorithm that iteratively improves the\n",
      "non-autoregressively generated image, with the help of two estimators developed\n",
      "for evaluating the compliance with the controls and evaluating the fidelity of\n",
      "the synthesized image, respectively. Extensive experiments on a newly collected\n",
      "large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal\n",
      "CelebA-HQ verify that M6-UFC can synthesize high-fidelity images that comply\n",
      "with flexible multi-modal controls. \n",
      "\n",
      "\n",
      "Recently, Conditional Generative Adversarial Network (Conditional GAN) have\n",
      "shown very promising performance in several image-to-image translation\n",
      "applications. However, the uses of these conditional GANs are quite limited to\n",
      "low-resolution images, such as 256X256.The Pix2Pix-HD is a recent attempt to\n",
      "utilize the conditional GAN for high-resolution image synthesis. In this paper,\n",
      "we propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for\n",
      "high-resolution image-to-image translation up to 2048X1024 resolution. The\n",
      "proposed model is trained by allowing the flow of gradients from\n",
      "multiple-discriminators to a single generator at multiple scales. The proposed\n",
      "MSG U-Net architecture leads to photo-realistic high-resolution image-to-image\n",
      "translation. Moreover, the proposed model is computationally efficient as\n",
      "com-pared to the Pix2Pix-HD with an improvement in the inference time nearly by\n",
      "2.5 times. We provide the code of MSG U-Net model at\n",
      "https://github.com/laxmaniron/MSG-U-Net. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have recently achieved unprecedented\n",
      "success in photo-realistic image synthesis from low-dimensional random noise.\n",
      "The ability to synthesize high-quality content at a large scale brings\n",
      "potential risks as the generated samples may lead to misinformation that can\n",
      "create severe social, political, health, and business hazards. We propose\n",
      "SubsetGAN to identify generated content by detecting a subset of anomalous\n",
      "node-activations in the inner layers of pre-trained neural networks. These\n",
      "nodes, as a group, maximize a non-parametric measure of divergence away from\n",
      "the expected distribution of activations created from real data. This enable us\n",
      "to identify synthesised images without prior knowledge of their distribution.\n",
      "SubsetGAN efficiently scores subsets of nodes and returns the group of nodes\n",
      "within the pre-trained classifier that contributed to the maximum score. The\n",
      "classifier can be a general fake classifier trained over samples from multiple\n",
      "sources or the discriminator network from different GANs. Our approach shows\n",
      "consistently higher detection power than existing detection methods across\n",
      "several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\n",
      "proportions of generated content. \n",
      "\n",
      "\n",
      "We provide a new model for texture synthesis based on a multiscale,\n",
      "multilayer feature extractor. Within the model, textures are represented by a\n",
      "set of statistics computed from ReLU wavelet coefficients at different layers,\n",
      "scales and orientations. A new image is synthesized by matching the target\n",
      "statistics via an iterative projection algorithm. We explain the necessity of\n",
      "the different types of pre-defined wavelet filters used in our model and the\n",
      "advantages of multilayer structures for image synthesis. We demonstrate the\n",
      "power of our model by generating samples of high quality textures and providing\n",
      "insights into deep representations for texture images. \n",
      "\n",
      "\n",
      "Transformer models have recently attracted much interest from computer vision\n",
      "researchers and have since been successfully employed for several problems\n",
      "traditionally addressed with convolutional neural networks. At the same time,\n",
      "image synthesis using generative adversarial networks (GANs) has drastically\n",
      "improved over the last few years. The recently proposed TransGAN is the first\n",
      "GAN using only transformer-based architectures and achieves competitive results\n",
      "when compared to convolutional GANs. However, since transformers are\n",
      "data-hungry architectures, TransGAN requires data augmentation, an auxiliary\n",
      "super-resolution task during training, and a masking prior to guide the\n",
      "self-attention mechanism. In this paper, we study the combination of a\n",
      "transformer-based generator and convolutional discriminator and successfully\n",
      "remove the need of the aforementioned required design choices. We evaluate our\n",
      "approach by conducting a benchmark of well-known CNN discriminators, ablate the\n",
      "size of the transformer-based generator, and show that combining both\n",
      "architectural elements into a hybrid model leads to better results.\n",
      "Furthermore, we investigate the frequency spectrum properties of generated\n",
      "images and observe that our model retains the benefits of an attention based\n",
      "generator. \n",
      "\n",
      "\n",
      "Story visualization is an under-explored task that falls at the intersection\n",
      "of many important research directions in both computer vision and natural\n",
      "language processing. In this task, given a series of natural language captions\n",
      "which compose a story, an agent must generate a sequence of images that\n",
      "correspond to the captions. Prior work has introduced recurrent generative\n",
      "models which outperform text-to-image synthesis models on this task. However,\n",
      "there is room for improvement of generated images in terms of visual quality,\n",
      "coherence and relevance. We present a number of improvements to prior modeling\n",
      "approaches, including (1) the addition of a dual learning framework that\n",
      "utilizes video captioning to reinforce the semantic alignment between the story\n",
      "and generated images, (2) a copy-transform mechanism for\n",
      "sequentially-consistent story visualization, and (3) MART-based transformers to\n",
      "model complex interactions between frames. We present ablation studies to\n",
      "demonstrate the effect of each of these techniques on the generative power of\n",
      "the model for both individual images as well as the entire narrative.\n",
      "Furthermore, due to the complexity and generative nature of the task, standard\n",
      "evaluation metrics do not accurately reflect performance. Therefore, we also\n",
      "provide an exploration of evaluation metrics for the model, focused on aspects\n",
      "of the generated frames such as the presence/quality of generated characters,\n",
      "the relevance to captions, and the diversity of the generated images. We also\n",
      "present correlation experiments of our proposed automated metrics with human\n",
      "evaluations. Code and data available at:\n",
      "https://github.com/adymaharana/StoryViz \n",
      "\n",
      "\n",
      "Recent facial image synthesis methods have been mainly based on conditional\n",
      "generative models. Sketch-based conditions can effectively describe the\n",
      "geometry of faces, including the contours of facial components, hair\n",
      "structures, as well as salient edges (e.g., wrinkles) on face surfaces but lack\n",
      "effective control of appearance, which is influenced by color, material,\n",
      "lighting condition, etc. To have more control of generated results, one\n",
      "possible approach is to apply existing disentangling works to disentangle face\n",
      "images into geometry and appearance representations. However, existing\n",
      "disentangling methods are not optimized for human face editing, and cannot\n",
      "achieve fine control of facial details such as wrinkles. To address this issue,\n",
      "we propose DeepFaceEditing, a structured disentanglement framework specifically\n",
      "designed for face images to support face generation and editing with\n",
      "disentangled control of geometry and appearance. We adopt a local-to-global\n",
      "approach to incorporate the face domain knowledge: local component images are\n",
      "decomposed into geometry and appearance representations, which are fused\n",
      "consistently using a global fusion module to improve generation quality. We\n",
      "exploit sketches to assist in extracting a better geometry representation,\n",
      "which also supports intuitive geometry editing via sketching. The resulting\n",
      "method can either extract the geometry and appearance representations from face\n",
      "images, or directly extract the geometry representation from face sketches.\n",
      "Such representations allow users to easily edit and synthesize face images,\n",
      "with decoupled control of their geometry and appearance. Both qualitative and\n",
      "quantitative evaluations show the superior detail and appearance control\n",
      "abilities of our method compared to state-of-the-art methods. \n",
      "\n",
      "\n",
      "Accurate segmentation of brain tumors from multi-modal Magnetic Resonance\n",
      "(MR) images is essential in brain tumor diagnosis and treatment. However, due\n",
      "to the existence of domain shifts among different modalities, the performance\n",
      "of networks decreases dramatically when training on one modality and performing\n",
      "on another, e.g., train on T1 image while performing on T2 image, which is\n",
      "often required in clinical applications. This also prohibits a network from\n",
      "being trained on labeled data and then transferred to unlabeled data from a\n",
      "different domain. To overcome this, unsupervised domain adaptation (UDA)\n",
      "methods provide effective solutions to alleviate the domain shift between\n",
      "labeled source data and unlabeled target data. In this paper, we propose a\n",
      "novel Bidirectional Global-to-Local (BiGL) adaptation framework under a UDA\n",
      "scheme. Specifically, a bidirectional image synthesis and segmentation module\n",
      "is proposed to segment the brain tumor using the intermediate data\n",
      "distributions generated for the two domains, which includes an image-to-image\n",
      "translator and a shared-weighted segmentation network. Further, a\n",
      "global-to-local consistency learning module is proposed to build robust\n",
      "representation alignments in an integrated way. Extensive experiments on a\n",
      "multi-modal brain MR benchmark dataset demonstrate that the proposed method\n",
      "outperforms several state-of-the-art unsupervised domain adaptation methods by\n",
      "a large margin, while a comprehensive ablation study validates the\n",
      "effectiveness of each key component. The implementation code of our method will\n",
      "be released at \\url{https://github.com/KeleiHe/BiGL}. \n",
      "\n",
      "\n",
      "Generating images from a single sample, as a newly developing branch of image\n",
      "synthesis, has attracted extensive attention. In this paper, we formulate this\n",
      "problem as sampling from the conditional distribution of a single image, and\n",
      "propose a hierarchical framework that simplifies the learning of the intricate\n",
      "conditional distributions through the successive learning of the distributions\n",
      "about structure, semantics and texture, making the process of learning and\n",
      "generation comprehensible. On this basis, we design ExSinGAN composed of three\n",
      "cascaded GANs for learning an explainable generative model from a given image,\n",
      "where the cascaded GANs model the distributions about structure, semantics and\n",
      "texture successively. ExSinGAN is learned not only from the internal patches of\n",
      "the given image as the previous works did, but also from the external prior\n",
      "obtained by the GAN inversion technique. Benefiting from the appropriate\n",
      "combination of internal and external information, ExSinGAN has a more powerful\n",
      "capability of generation and competitive generalization ability for the image\n",
      "manipulation tasks compared with prior works. \n",
      "\n",
      "\n",
      "Functional MRI (fMRI) is a powerful technique that has allowed us to\n",
      "characterize visual cortex responses to stimuli, yet such experiments are by\n",
      "nature constructed based on a priori hypotheses, limited to the set of images\n",
      "presented to the individual while they are in the scanner, are subject to noise\n",
      "in the observed brain responses, and may vary widely across individuals. In\n",
      "this work, we propose a novel computational strategy, which we call NeuroGen,\n",
      "to overcome these limitations and develop a powerful tool for human vision\n",
      "neuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model\n",
      "of human vision with a deep generative network to synthesize images predicted\n",
      "to achieve a target pattern of macro-scale brain activation. We demonstrate\n",
      "that the reduction of noise that the encoding model provides, coupled with the\n",
      "generative network's ability to produce images of high fidelity, results in a\n",
      "robust discovery architecture for visual neuroscience. By using only a small\n",
      "number of synthetic images created by NeuroGen, we demonstrate that we can\n",
      "detect and amplify differences in regional and individual human brain response\n",
      "patterns to visual stimuli. We then verify that these discoveries are reflected\n",
      "in the several thousand observed image responses measured with fMRI. We further\n",
      "demonstrate that NeuroGen can create synthetic images predicted to achieve\n",
      "regional response patterns not achievable by the best-matching natural images.\n",
      "The NeuroGen framework extends the utility of brain encoding models and opens\n",
      "up a new avenue for exploring, and possibly precisely controlling, the human\n",
      "visual system. \n",
      "\n",
      "\n",
      "In medical image synthesis, model training could be challenging due to the\n",
      "inconsistencies between images of different modalities even with the same\n",
      "patient, typically caused by internal status/tissue changes as different\n",
      "modalities are usually obtained at a different time. This paper proposes a\n",
      "novel deep learning method, Structure-aware Generative Adversarial Network\n",
      "(SA-GAN), that preserves the shapes and locations of in-consistent structures\n",
      "when generating medical images. SA-GAN is employed to generate synthetic\n",
      "computed tomography (synCT) images from magnetic resonance imaging (MRI) with\n",
      "two parallel streams: the global stream translates the input from the MRI to\n",
      "the CT domain while the local stream automatically segments the inconsistent\n",
      "organs, maintains their locations and shapes in MRI, and translates the organ\n",
      "intensities to CT. Through extensive experiments on a pelvic dataset, we\n",
      "demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\n",
      "and organ segmentation and supports MR-only treatment planning in disease sites\n",
      "with internal organ status changes. \n",
      "\n",
      "\n",
      "We propose a Multiscale Invertible Generative Network (MsIGN) and associated\n",
      "training algorithm that leverages multiscale structure to solve\n",
      "high-dimensional Bayesian inference. To address the curse of dimensionality,\n",
      "MsIGN exploits the low-dimensional nature of the posterior, and generates\n",
      "samples from coarse to fine scale (low to high dimension) by iteratively\n",
      "upsampling and refining samples. MsIGN is trained in a multi-stage manner to\n",
      "minimize the Jeffreys divergence, which avoids mode dropping in\n",
      "high-dimensional cases. On two high-dimensional Bayesian inverse problems, we\n",
      "show superior performance of MsIGN over previous approaches in posterior\n",
      "approximation and multiple mode capture. On the natural image synthesis task,\n",
      "MsIGN achieves superior performance in bits-per-dimension over baseline models\n",
      "and yields great interpret-ability of its neurons in intermediate layers. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have become increasingly powerful,\n",
      "generating mind-blowing photorealistic images that mimic the content of\n",
      "datasets they were trained to replicate. One recurrent theme in medical imaging\n",
      "is whether GANs can also be effective at generating workable medical data as\n",
      "they are for generating realistic RGB images. In this paper, we perform a\n",
      "multi-GAN and multi-application study to gauge the benefits of GANs in medical\n",
      "imaging. We tested various GAN architectures from basic DCGAN to more\n",
      "sophisticated style-based GANs on three medical imaging modalities and organs\n",
      "namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\n",
      "well-known and widely utilized datasets from which their FID score were\n",
      "computed to measure the visual acuity of their generated images. We further\n",
      "tested their usefulness by measuring the segmentation accuracy of a U-Net\n",
      "trained on these generated images.\n",
      "  Results reveal that GANs are far from being equal as some are ill-suited for\n",
      "medical imaging applications while others are much better off. The\n",
      "top-performing GANs are capable of generating realistic-looking medical images\n",
      "by FID standards that can fool trained experts in a visual Turing test and\n",
      "comply to some metrics. However, segmentation results suggests that no GAN is\n",
      "capable of reproducing the full richness of a medical datasets. \n",
      "\n",
      "\n",
      "We show that diffusion models can achieve image sample quality superior to\n",
      "the current state-of-the-art generative models. We achieve this on\n",
      "unconditional image synthesis by finding a better architecture through a series\n",
      "of ablations. For conditional image synthesis, we further improve sample\n",
      "quality with classifier guidance: a simple, compute-efficient method for\n",
      "trading off diversity for fidelity using gradients from a classifier. We\n",
      "achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet\n",
      "256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep\n",
      "even with as few as 25 forward passes per sample, all while maintaining better\n",
      "coverage of the distribution. Finally, we find that classifier guidance\n",
      "combines well with upsampling diffusion models, further improving FID to 3.94\n",
      "on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our\n",
      "code at https://github.com/openai/guided-diffusion \n",
      "\n",
      "\n",
      "Purpose: Different Magnetic resonance imaging (MRI) modalities of the same\n",
      "anatomical structure are required to present different pathological information\n",
      "from the physical level for diagnostic needs. However, it is often difficult to\n",
      "obtain full-sequence MRI images of patients owing to limitations such as time\n",
      "consumption and high cost. The purpose of this work is to develop an algorithm\n",
      "for target MRI sequences prediction with high accuracy, and provide more\n",
      "information for clinical diagnosis. Methods: We propose a deep learning based\n",
      "multi-modal computing model for MRI synthesis with feature disentanglement\n",
      "strategy. To take full advantage of the complementary information provided by\n",
      "different modalities, multi-modal MRI sequences are utilized as input. Notably,\n",
      "the proposed approach decomposes each input modality into modality-invariant\n",
      "space with shared information and modality-specific space with specific\n",
      "information, so that features are extracted separately to effectively process\n",
      "the input data. Subsequently, both of them are fused through the adaptive\n",
      "instance normalization (AdaIN) layer in the decoder. In addition, to address\n",
      "the lack of specific information of the target modality in the test phase, a\n",
      "local adaptive fusion (LAF) module is adopted to generate a modality-like\n",
      "pseudo-target with specific information similar to the ground truth. Results:\n",
      "To evaluate the synthesis performance, we verify our method on the BRATS2015\n",
      "dataset of 164 subjects. The experimental results demonstrate our approach\n",
      "significantly outperforms the benchmark method and other state-of-the-art\n",
      "medical image synthesis methods in both quantitative and qualitative measures.\n",
      "Compared with the pix2pixGANs method, the PSNR improves from 23.68 to 24.8.\n",
      "Conclusion: The proposed method could be effective in prediction of target MRI\n",
      "sequences, and useful for clinical diagnosis and treatment. \n",
      "\n",
      "\n",
      "Regional facial image synthesis conditioned on semantic mask has achieved\n",
      "great success using generative adversarial networks. However, the appearance of\n",
      "different regions may be inconsistent with each other when conducting regional\n",
      "image editing. In this paper, we focus on the problem of harmonized regional\n",
      "style transfer and manipulation for facial images. The proposed approach\n",
      "supports regional style transfer and manipulation at the same time. A\n",
      "multi-scale encoder and style mapping networks are proposed in our work. The\n",
      "encoder is responsible for extracting regional styles of real faces. Style\n",
      "mapping networks generate styles from random samples for all facial regions. As\n",
      "the key part of our work, we propose a multi-region style attention module to\n",
      "adapt the multiple regional style embeddings from a reference image to a target\n",
      "image for generating harmonious and plausible results. Furthermore, we propose\n",
      "a new metric \"harmony score\" and conduct experiments in a challenging setting:\n",
      "three widely used face datasets are involved and we test the model by\n",
      "transferring the regional facial appearance between datasets. Images in\n",
      "different datasets are usually quite different, which makes the inconsistency\n",
      "between target and reference regions more obvious. Results show that our model\n",
      "can generate reliable style transfer and multi-modal manipulation results\n",
      "compared with SOTAs. Furthermore, we show two face editing applications using\n",
      "the proposed approach. \n",
      "\n",
      "\n",
      "Anomaly detection in visual data refers to the problem of differentiating\n",
      "abnormal appearances from normal cases. Supervised approaches have been\n",
      "successfully applied to different domains, but require an abundance of labeled\n",
      "data. Due to the nature of how anomalies occur and their underlying generating\n",
      "processes, it is hard to characterize and label them. Recent advances in deep\n",
      "generative-based models have sparked interest in applying such methods for\n",
      "unsupervised anomaly detection and have shown promising results in medical and\n",
      "industrial inspection domains. In this work we evaluate a crucial part of the\n",
      "unsupervised visual anomaly detection pipeline, that is needed for normal\n",
      "appearance modeling, as well as the ability to reconstruct closest looking\n",
      "normal and tumor samples. We adapt and evaluate different high-resolution\n",
      "state-of-the-art generative models from the face synthesis domain and\n",
      "demonstrate their superiority over currently used approaches on a challenging\n",
      "domain of digital pathology. Multifold improvement in image synthesis is\n",
      "demonstrated in terms of the quality and resolution of the generated images,\n",
      "validated also against the supervised model. \n",
      "\n",
      "\n",
      "This work focuses on the analysis that whether 3D face models can be learned\n",
      "from only the speech inputs of speakers. Previous works for cross-modal face\n",
      "synthesis study image generation from voices. However, image synthesis includes\n",
      "variations such as hairstyles, backgrounds, and facial textures, that are\n",
      "arguably irrelevant to voice or without direct studies to show correlations. We\n",
      "instead investigate the ability to reconstruct 3D faces to concentrate on only\n",
      "geometry, which is more physiologically grounded. We propose both the\n",
      "supervised learning and unsupervised learning frameworks. Especially we\n",
      "demonstrate how unsupervised learning is possible in the absence of a direct\n",
      "voice-to-3D-face dataset under limited availability of 3D face scans when the\n",
      "model is equipped with knowledge distillation. To evaluate the performance, we\n",
      "also propose several metrics to measure the geometric fitness of two 3D faces\n",
      "based on points, lines, and regions. We find that 3D face shapes can be\n",
      "reconstructed from voices. Experimental results suggest that 3D faces can be\n",
      "reconstructed from voices, and our method can improve the performance over the\n",
      "baseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio\n",
      "metric (ER) coincides with the intuition that one can roughly envision whether\n",
      "a speaker's face is overall wider or thinner only from a person's voice. See\n",
      "our project page for codes and data. \n",
      "\n",
      "\n",
      "The existing text-guided image synthesis methods can only produce limited\n",
      "quality results with at most \\mbox{$\\text{256}^2$} resolution and the textual\n",
      "instructions are constrained in a small Corpus. In this work, we propose a\n",
      "unified framework for both face image generation and manipulation that produces\n",
      "diverse and high-quality images with an unprecedented resolution at 1024 from\n",
      "multimodal inputs. More importantly, our method supports open-world scenarios,\n",
      "including both image and text, without any re-training, fine-tuning, or\n",
      "post-processing. To be specific, we propose a brand new paradigm of text-guided\n",
      "image generation and manipulation based on the superior characteristics of a\n",
      "pretrained GAN model. Our proposed paradigm includes two novel strategies. The\n",
      "first strategy is to train a text encoder to obtain latent codes that align\n",
      "with the hierarchically semantic of the aforementioned pretrained GAN model.\n",
      "The second strategy is to directly optimize the latent codes in the latent\n",
      "space of the pretrained GAN model with guidance from a pretrained language\n",
      "model. The latent codes can be randomly sampled from a prior distribution or\n",
      "inverted from a given image, which provides inherent supports for both image\n",
      "generation and manipulation from multi-modal inputs, such as sketches or\n",
      "semantic labels, with textual guidance. To facilitate text-guided multi-modal\n",
      "synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset\n",
      "consisting of real face images and corresponding semantic segmentation map,\n",
      "sketch, and textual descriptions. Extensive experiments on the introduced\n",
      "dataset demonstrate the superior performance of our proposed method. Code and\n",
      "data are available at https://github.com/weihaox/TediGAN. \n",
      "\n",
      "\n",
      "While commodity GPUs provide a continuously growing range of features and\n",
      "sophisticated methods for accelerating compute jobs, many state-of-the-art\n",
      "solutions for point cloud rendering still rely on the provided point primitives\n",
      "(GL_POINTS, POINTLIST, ...) of graphics APIs for image synthesis. In this\n",
      "paper, we present several compute-based point cloud rendering approaches that\n",
      "outperform the hardware pipeline by up to an order of magnitude and achieve\n",
      "significantly better frame times than previous compute-based methods. Beyond\n",
      "basic closest-point rendering, we also introduce a fast, high-quality variant\n",
      "to reduce aliasing. We present and evaluate several variants of our proposed\n",
      "methods with different flavors of optimization, in order to ensure their\n",
      "applicability and achieve optimal performance on a range of platforms and\n",
      "architectures with varying support for novel GPU hardware features. During our\n",
      "experiments, the observed peak performance was reached rendering 796 million\n",
      "points (12.7GB) at rates of 62 to 64 frames per second (50 billion points per\n",
      "second, 802GB/s) on an RTX 3090 without the use of level-of-detail structures.\n",
      "  We further introduce an optimized vertex order for point clouds to boost the\n",
      "efficiency of GL_POINTS by a factor of 5x in cases where hardware rendering is\n",
      "compulsory. We compare different orderings and show that Morton sorted buffers\n",
      "are faster for some viewpoints, while shuffled vertex buffers are faster in\n",
      "others. In contrast, combining both approaches by first sorting according to\n",
      "Morton-code and shuffling the resulting sequence in batches of 128 points leads\n",
      "to a vertex buffer layout with high rendering performance and low sensitivity\n",
      "to viewpoint changes. \n",
      "\n",
      "\n",
      "We propose a novel approach for multi-modal Image-to-image (I2I) translation.\n",
      "To tackle the one-to-many relationship between input and output domains,\n",
      "previous works use complex training objectives to learn a latent embedding,\n",
      "jointly with the generator, that models the variability of the output domain.\n",
      "In contrast, we directly model the style variability of images, independent of\n",
      "the image synthesis task. Specifically, we pre-train a generic style encoder\n",
      "using a novel proxy task to learn an embedding of images, from arbitrary\n",
      "domains, into a low-dimensional style latent space. The learned latent space\n",
      "introduces several advantages over previous traditional approaches to\n",
      "multi-modal I2I translation. First, it is not dependent on the target dataset,\n",
      "and generalizes well across multiple domains. Second, it learns a more powerful\n",
      "and expressive latent space, which improves the fidelity of style capture and\n",
      "transfer. The proposed style pre-training also simplifies the training\n",
      "objective and speeds up the training significantly. Furthermore, we provide a\n",
      "detailed study of the contribution of different loss terms to the task of\n",
      "multi-modal I2I translation, and propose a simple alternative to VAEs to enable\n",
      "sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art\n",
      "results on six challenging benchmarks with a simple training objective that\n",
      "includes only a GAN loss and a reconstruction loss. \n",
      "\n",
      "\n",
      "In Fluorescein Angiography (FA), an exogenous dye is injected in the\n",
      "bloodstream to image the vascular structure of the retina. The injected dye can\n",
      "cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\n",
      "death. In contrast, color fundus imaging is a non-invasive technique used for\n",
      "photographing the retina but does not have sufficient fidelity for capturing\n",
      "its vascular structure. The only non-invasive method for capturing retinal\n",
      "vasculature is optical coherence tomography-angiography (OCTA). However, OCTA\n",
      "equipment is quite expensive, and stable imaging is limited to small areas on\n",
      "the retina. In this paper, we propose a novel conditional generative\n",
      "adversarial network (GAN) capable of simultaneously synthesizing FA images from\n",
      "fundus photographs while predicting retinal degeneration. The proposed system\n",
      "has the benefit of addressing the problem of imaging retinal vasculature in a\n",
      "non-invasive manner as well as predicting the existence of retinal\n",
      "abnormalities. We use a semi-supervised approach to train our GAN using\n",
      "multiple weighted losses on different modalities of data. Our experiments\n",
      "validate that the proposed architecture exceeds recent state-of-the-art\n",
      "generative networks for fundus-to-angiography synthesis. Moreover, our vision\n",
      "transformer-based discriminators generalize quite well on out-of-distribution\n",
      "data sets for retinal disease prediction. \n",
      "\n",
      "\n",
      "We introduce an inversion based method, denoted as IMAge-Guided model\n",
      "INvErsion (IMAGINE), to generate high-quality and diverse images from only a\n",
      "single training sample. We leverage the knowledge of image semantics from a\n",
      "pre-trained classifier to achieve plausible generations via matching\n",
      "multi-level feature representations in the classifier, associated with\n",
      "adversarial training with an external discriminator. IMAGINE enables the\n",
      "synthesis procedure to simultaneously 1) enforce semantic specificity\n",
      "constraints during the synthesis, 2) produce realistic images without generator\n",
      "training, and 3) give users intuitive control over the generation process. With\n",
      "extensive experimental results, we demonstrate qualitatively and quantitatively\n",
      "that IMAGINE performs favorably against state-of-the-art GAN-based and\n",
      "inversion-based methods, across three different image domains (i.e., objects,\n",
      "scenes, and textures). \n",
      "\n",
      "\n",
      "In recent years, the use of Generative Adversarial Networks (GANs) has become\n",
      "very popular in generative image modeling. While style-based GAN architectures\n",
      "yield state-of-the-art results in high-fidelity image synthesis,\n",
      "computationally, they are highly complex. In our work, we focus on the\n",
      "performance optimization of style-based generative models. We analyze the most\n",
      "computationally hard parts of StyleGAN2, and propose changes in the generator\n",
      "network to make it possible to deploy style-based generative networks in the\n",
      "edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer\n",
      "parameters and is x9.5 less computationally complex than StyleGAN2, while\n",
      "providing comparable quality. \n",
      "\n",
      "\n",
      "This paper introduces a conditional generative adversarial network to\n",
      "redesign a street-level image of urban scenes by generating 1) an urban\n",
      "intervention policy, 2) an attention map that localises where intervention is\n",
      "needed, 3) a high-resolution street-level image (1024 X 1024 or 1536 X1536)\n",
      "after implementing the intervention. We also introduce a new dataset that\n",
      "comprises aligned street-level images of before and after urban interventions\n",
      "from real-life scenarios that make this research possible. The introduced\n",
      "method has been trained on different ranges of urban interventions applied to\n",
      "realistic images. The trained model shows strong performance in re-modelling\n",
      "cities, outperforming existing methods that apply image-to-image translation in\n",
      "other domains that is computed in a single GPU. This research opens the door\n",
      "for machine intelligence to play a role in re-thinking and re-designing the\n",
      "different attributes of cities based on adversarial learning, going beyond the\n",
      "mainstream of facial landmarks manipulation or image synthesis from semantic\n",
      "segmentation. \n",
      "\n",
      "\n",
      "We present a novel framework, InfinityGAN, for arbitrary-sized image\n",
      "generation. The task is associated with several key challenges. First, scaling\n",
      "existing models to an arbitrarily large image size is resource-constrained, in\n",
      "terms of both computation and availability of large-field-of-view training\n",
      "data. InfinityGAN trains and infers in a seamless patch-by-patch manner with\n",
      "low computational resources. Second, large images should be locally and\n",
      "globally consistent, avoid repetitive patterns, and look realistic. To address\n",
      "these, InfinityGAN disentangles global appearances, local structures, and\n",
      "textures. With this formulation, we can generate images with spatial size and\n",
      "level of details not attainable before. Experimental evaluation validates that\n",
      "InfinityGAN generates images with superior realism compared to baselines and\n",
      "features parallelizable inference. Finally, we show several applications\n",
      "unlocked by our approach, such as spatial style fusion, multi-modal\n",
      "outpainting, and image inbetweening. All applications can be operated with\n",
      "arbitrary input and output sizes. Please find the full version of the paper at\n",
      "https://openreview.net/forum?id=ufGMqIM0a4b . \n",
      "\n",
      "\n",
      "Recently, the power of unconditional image synthesis has significantly\n",
      "advanced through the use of Generative Adversarial Networks (GANs). The task of\n",
      "inverting an image into its corresponding latent code of the trained GAN is of\n",
      "utmost importance as it allows for the manipulation of real images, leveraging\n",
      "the rich semantics learned by the network. Recognizing the limitations of\n",
      "current inversion approaches, in this work we present a novel inversion scheme\n",
      "that extends current encoder-based inversion methods by introducing an\n",
      "iterative refinement mechanism. Instead of directly predicting the latent code\n",
      "of a given real image using a single pass, the encoder is tasked with\n",
      "predicting a residual with respect to the current estimate of the inverted\n",
      "latent code in a self-correcting manner. Our residual-based encoder, named\n",
      "ReStyle, attains improved accuracy compared to current state-of-the-art\n",
      "encoder-based methods with a negligible increase in inference time. We analyze\n",
      "the behavior of ReStyle to gain valuable insights into its iterative nature. We\n",
      "then evaluate the performance of our residual encoder and analyze its\n",
      "robustness compared to optimization-based inversion and state-of-the-art\n",
      "encoders. \n",
      "\n",
      "\n",
      "Text-to-image synthesis (T2I) aims to generate photo-realistic images which\n",
      "are semantically consistent with the text descriptions. Existing methods are\n",
      "usually built upon conditional generative adversarial networks (GANs) and\n",
      "initialize an image from noise with sentence embedding, and then refine the\n",
      "features with fine-grained word embedding iteratively. A close inspection of\n",
      "their generated images reveals a major limitation: even though the generated\n",
      "image holistically matches the description, individual image regions or parts\n",
      "of somethings are often not recognizable or consistent with words in the\n",
      "sentence, e.g. \"a white crown\". To address this problem, we propose a novel\n",
      "framework Semantic-Spatial Aware GAN for synthesizing images from input text.\n",
      "Concretely, we introduce a simple and effective Semantic-Spatial Aware block,\n",
      "which (1) learns semantic-adaptive transformation conditioned on text to\n",
      "effectively fuse text features and image features, and (2) learns a semantic\n",
      "mask in a weakly-supervised way that depends on the current text-image fusion\n",
      "process in order to guide the transformation spatially. Experiments on the\n",
      "challenging COCO and CUB bird datasets demonstrate the advantage of our method\n",
      "over the recent state-of-the-art approaches, regarding both visual fidelity and\n",
      "alignment with input text description. \n",
      "\n",
      "\n",
      "The interest of the deep learning community in image synthesis has grown\n",
      "massively in recent years. Nowadays, deep generative methods, and especially\n",
      "Generative Adversarial Networks (GANs), are leading to state-of-the-art\n",
      "performance, capable of synthesizing images that appear realistic. While the\n",
      "efforts for improving the quality of the generated images are extensive, most\n",
      "attempts still consider the generator part as an uncorroborated \"black-box\". In\n",
      "this paper, we aim to provide a better understanding and design of the image\n",
      "generation process. We interpret existing generators as implicitly relying on\n",
      "sparsity-inspired models. More specifically, we show that generators can be\n",
      "viewed as manifestations of the Convolutional Sparse Coding (CSC) and its\n",
      "Multi-Layered version (ML-CSC) synthesis processes. We leverage this\n",
      "observation by explicitly enforcing a sparsifying regularization on\n",
      "appropriately chosen activation layers in the generator, and demonstrate that\n",
      "this leads to improved image synthesis. Furthermore, we show that the same\n",
      "rationale and benefits apply to generators serving inverse problems,\n",
      "demonstrated on the Deep Image Prior (DIP) method. \n",
      "\n",
      "\n",
      "Tremendous progress in deep generative models has led to photorealistic image\n",
      "synthesis. While achieving compelling results, most approaches operate in the\n",
      "two-dimensional image domain, ignoring the three-dimensional nature of our\n",
      "world. Several recent works therefore propose generative models which are\n",
      "3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to\n",
      "the image plane. This leads to impressive 3D consistency, but incorporating\n",
      "such a bias comes at a price: the camera needs to be modeled as well. Current\n",
      "approaches assume fixed intrinsics and a predefined prior over camera pose\n",
      "ranges. As a result, parameter tuning is typically required for real-world\n",
      "data, and results degrade if the data distribution is not matched. Our key\n",
      "hypothesis is that learning a camera generator jointly with the image generator\n",
      "leads to a more principled approach to 3D-aware image synthesis. Further, we\n",
      "propose to decompose the scene into a background and foreground model, leading\n",
      "to more efficient and disentangled scene representations. While training from\n",
      "raw, unposed image collections, we learn a 3D- and camera-aware generative\n",
      "model which faithfully recovers not only the image but also the camera data\n",
      "distribution. At test time, our model generates images with explicit control\n",
      "over the camera as well as the shape and appearance of the scene. \n",
      "\n",
      "\n",
      "Face photo-sketch synthesis and recognition has many applications in digital\n",
      "entertainment and law enforcement. Recently, generative adversarial networks\n",
      "(GANs) based methods have significantly improved the quality of image\n",
      "synthesis, but they have not explicitly considered the purpose of recognition.\n",
      "In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model\n",
      "that applies a new perceptual loss to supervise the image generation network.\n",
      "It improves CycleGAN on photo-sketch synthesis by paying more attention to the\n",
      "synthesis of key facial regions, such as eyes and nose, which are important for\n",
      "identity recognition. Furthermore, we develop a mutual optimization procedure\n",
      "between the synthesis model and the recognition model, which iteratively\n",
      "synthesizes better images by IACycleGAN and enhances the recognition model by\n",
      "the triplet loss of the generated and real samples. Extensive experiments are\n",
      "performed on both photo-tosketch and sketch-to-photo tasks using the widely\n",
      "used CUFS and CUFSF databases. The results show that the proposed method\n",
      "performs better than several state-of-the-art methods in terms of both\n",
      "synthetic image quality and photo-sketch recognition accuracy. \n",
      "\n",
      "\n",
      "Photoacoustic tomography (PAT) has the potential to recover morphological and\n",
      "functional tissue properties such as blood oxygenation with high spatial\n",
      "resolution and in an interventional setting. However, decades of research\n",
      "invested in solving the inverse problem of recovering clinically relevant\n",
      "tissue properties from spectral measurements have failed to produce solutions\n",
      "that can quantify tissue parameters robustly in a clinical setting. Previous\n",
      "attempts to address the limitations of model-based approaches with machine\n",
      "learning were hampered by the absence of labeled reference data needed for\n",
      "supervised algorithm training. While this bottleneck has been tackled by\n",
      "simulating training data, the domain gap between real and simulated images\n",
      "remains a huge unsolved challenge. As a first step to address this bottleneck,\n",
      "we propose a novel approach to PAT data simulation, which we refer to as\n",
      "\"learning to simulate\". Our approach involves subdividing the challenge of\n",
      "generating plausible simulations into two disjoint problems: (1) Probabilistic\n",
      "generation of realistic tissue morphology, represented by semantic segmentation\n",
      "maps and (2) pixel-wise assignment of corresponding optical and acoustic\n",
      "properties. In the present work, we focus on the first challenge. Specifically,\n",
      "we leverage the concept of Generative Adversarial Networks (GANs) trained on\n",
      "semantically annotated medical imaging data to generate plausible tissue\n",
      "geometries. According to an initial in silico feasibility study our approach is\n",
      "well-suited for contributing to realistic PAT image synthesis and could thus\n",
      "become a fundamental step for deep learning-based quantitative PAT. \n",
      "\n",
      "\n",
      "This paper tackles a challenging problem of generating photorealistic images\n",
      "from semantic layouts in few-shot scenarios where annotated training pairs are\n",
      "hardly available but pixel-wise annotation is quite costly. We present a\n",
      "training strategy that performs pseudo labeling of semantic masks using the\n",
      "StyleGAN prior. Our key idea is to construct a simple mapping between the\n",
      "StyleGAN feature and each semantic class from a few examples of semantic masks.\n",
      "With such mappings, we can generate an unlimited number of pseudo semantic\n",
      "masks from random noise to train an encoder for controlling a pre-trained\n",
      "StyleGAN generator. Although the pseudo semantic masks might be too coarse for\n",
      "previous approaches that require pixel-aligned masks, our framework can\n",
      "synthesize high-quality images from not only dense semantic masks but also\n",
      "sparse inputs such as landmarks and scribbles. Qualitative and quantitative\n",
      "results with various datasets demonstrate improvement over previous approaches\n",
      "with respect to layout fidelity and visual quality in as few as one- or\n",
      "five-shot settings. \n",
      "\n",
      "\n",
      "Conditional image synthesis from layout has recently attracted much interest.\n",
      "Previous approaches condition the generator on object locations as well as\n",
      "class labels but lack fine-grained control over the diverse appearance aspects\n",
      "of individual objects. Gaining control over the image generation process is\n",
      "fundamental to build practical applications with a user-friendly interface. In\n",
      "this paper, we propose a method for attribute controlled image synthesis from\n",
      "layout which allows to specify the appearance of individual objects without\n",
      "affecting the rest of the image. We extend a state-of-the-art approach for\n",
      "layout-to-image generation to additionally condition individual objects on\n",
      "attributes. We create and experiment on a synthetic, as well as the challenging\n",
      "Visual Genome dataset. Our qualitative and quantitative results show that our\n",
      "method can successfully control the fine-grained details of individual objects\n",
      "when modelling complex scenes with multiple objects. Source code, dataset and\n",
      "pre-trained models are publicly available\n",
      "(https://github.com/stanifrolov/AttrLostGAN). \n",
      "\n",
      "\n",
      "Given a large dataset for training, GANs can achieve remarkable performance\n",
      "for the image synthesis task. However, training GANs in extremely low data\n",
      "regimes remains a challenge, as overfitting often occurs, leading to\n",
      "memorization or training divergence. In this work, we introduce SIV-GAN, an\n",
      "unconditional generative model that can generate new scene compositions from a\n",
      "single training image or a single video clip. We propose a two-branch\n",
      "discriminator architecture, with content and layout branches designed to judge\n",
      "internal content and scene layout realism separately from each other. This\n",
      "discriminator design enables synthesis of visually plausible, novel\n",
      "compositions of a scene, with varying content and layout, while preserving the\n",
      "context of the original sample. Compared to previous single-image GANs, our\n",
      "model generates more diverse, higher quality images, while not being restricted\n",
      "to a single image setting. We show that SIV-GAN successfully deals with a new\n",
      "challenging task of learning from a single video, for which prior GAN models\n",
      "fail to achieve synthesis of both high quality and diversity. \n",
      "\n",
      "\n",
      "Recent advances in neuroscience have highlighted the effectiveness of\n",
      "multi-modal medical data for investigating certain pathologies and\n",
      "understanding human cognition. However, obtaining full sets of different\n",
      "modalities is limited by various factors, such as long acquisition times, high\n",
      "examination costs and artifact suppression. In addition, the complexity, high\n",
      "dimensionality and heterogeneity of neuroimaging data remains another key\n",
      "challenge in leveraging existing randomized scans effectively, as data of the\n",
      "same modality is often measured differently by different machines. There is a\n",
      "clear need to go beyond the traditional imaging-dependent process and\n",
      "synthesize anatomically specific target-modality data from a source input. In\n",
      "this paper, we propose to learn dedicated features that cross both intre- and\n",
      "intra-modal variations using a novel CSC$\\ell_4$Net. Through an initial\n",
      "unification of intra-modal data in the feature maps and multivariate canonical\n",
      "adaptation, CSC$\\ell_4$Net facilitates feature-level mutual transformation. The\n",
      "positive definite Riemannian manifold-penalized data fidelity term further\n",
      "enables CSC$\\ell_4$Net to reconstruct missing measurements according to\n",
      "transformed features. Finally, the maximization $\\ell_4$-norm boils down to a\n",
      "computationally efficient optimization problem. Extensive experiments validate\n",
      "the ability and robustness of our CSC$\\ell_4$Net compared to the\n",
      "state-of-the-art methods on multiple datasets. \n",
      "\n",
      "\n",
      "One major impediment in rapidly deploying object detection models for\n",
      "industrial applications is the lack of large annotated datasets. We currently\n",
      "have presented the Sacked Carton Dataset(SCD) that contains carton images from\n",
      "three scenarios, such as comprehensive pharmaceutical logistics company(CPLC),\n",
      "e-commerce logistics company(ECLC), fruit market(FM). However, due to domain\n",
      "shift, the model trained with one of the three scenarios in SCD has poor\n",
      "generalization ability when applied to the rest scenarios. To solve this\n",
      "problem, a novel image synthesis method is proposed to replace the foreground\n",
      "texture of the source datasets with the texture of the target datasets. Our\n",
      "method can keep the context relationship of foreground objects and backgrounds\n",
      "unchanged and greatly augment the target datasets. We firstly propose a surface\n",
      "segmentation algorithm to achieve texture decoupling of each instance.\n",
      "Secondly, a contour reconstruction algorithm is proposed to keep the occlusion\n",
      "and truncation relationship of the instance unchanged. Finally, the Gaussian\n",
      "fusion algorithm is used to replace the foreground texture from the source\n",
      "datasets with the texture from the target datasets. The novel image synthesis\n",
      "method can largely boost AP by at least 4.3%~6.5% on RetinaNet and 3.4%~6.8% on\n",
      "Faster R-CNN for the target domain. Code is available at\n",
      "https://github.com/hustgetlijun/RCAN. \n",
      "\n",
      "\n",
      "The perceptual loss has been widely used as an effective loss term in image\n",
      "synthesis tasks including image super-resolution, and style transfer. It was\n",
      "believed that the success lies in the high-level perceptual feature\n",
      "representations extracted from CNNs pretrained with a large set of images. Here\n",
      "we reveal that, what matters is the network structure instead of the trained\n",
      "weights. Without any learning, the structure of a deep network is sufficient to\n",
      "capture the dependencies between multiple levels of variable statistics using\n",
      "multiple layers of CNNs. This insight removes the requirements of pre-training\n",
      "and a particular network structure (commonly, VGG) that are previously assumed\n",
      "for the perceptual loss, thus enabling a significantly wider range of\n",
      "applications. To this end, we demonstrate that a randomly-weighted deep CNN can\n",
      "be used to model the structured dependencies of outputs. On a few dense\n",
      "per-pixel prediction tasks such as semantic segmentation, depth estimation and\n",
      "instance segmentation, we show improved results of using the extended\n",
      "randomized perceptual loss, compared to the baselines using pixel-wise loss\n",
      "alone. We hope that this simple, extended perceptual loss may serve as a\n",
      "generic structured-output loss that is applicable to most structured output\n",
      "learning tasks. \n",
      "\n",
      "\n",
      "Deep learning has shown great promise for CT image reconstruction, in\n",
      "particular to enable low dose imaging and integrated diagnostics. These merits,\n",
      "however, stand at great odds with the low availability of diverse image data\n",
      "which are needed to train these neural networks. We propose to overcome this\n",
      "bottleneck via a deep reinforcement learning (DRL) approach that is integrated\n",
      "with a style-transfer (ST) methodology, where the DRL generates the anatomical\n",
      "shapes and the ST synthesizes the texture detail. We show that our method bears\n",
      "high promise for generating novel and anatomically accurate high resolution CT\n",
      "images at large and diverse quantities. Our approach is specifically designed\n",
      "to work with even small image datasets which is desirable given the often low\n",
      "amount of image data many researchers have available to them. \n",
      "\n",
      "\n",
      "Federated Generative Adversarial Network (FedGAN) is a\n",
      "communication-efficient approach to train a GAN across distributed clients\n",
      "without clients having to share their sensitive training data. In this paper,\n",
      "we experimentally show that FedGAN generates biased data points under\n",
      "non-independent-and-identically-distributed (non-iid) settings. Also, we\n",
      "propose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets\n",
      "using FedGAN. Our approach generates metadata at the aggregator using the\n",
      "models received from clients and retrains the federated model to achieve\n",
      "bias-free results for image synthesis. Bias-Free FedGAN has the same\n",
      "communication cost as that of FedGAN. Experimental results on image datasets\n",
      "(MNIST and FashionMNIST) validate our claims. \n",
      "\n",
      "\n",
      "As weak lensing surveys become deeper, they reveal more non-Gaussian aspects\n",
      "of the convergence field which can only be extracted using statistics beyond\n",
      "the power spectrum. In Cheng et al. (2020) we showed that the scattering\n",
      "transform, a novel statistic borrowing mathematical concepts from convolutional\n",
      "neural networks, is a powerful tool for cosmological parameter estimation in\n",
      "the non-Gaussian regime. Here, we extend that analysis to explore its\n",
      "sensitivity to dark energy and neutrino mass parameters with weak lensing\n",
      "surveys. We first use image synthesis to show visually that, compared to the\n",
      "power spectrum and bispectrum, the scattering transform provides a better\n",
      "statistical vocabulary to characterize the perceptual properties of lensing\n",
      "mass maps. We then show that it is also better suited for parameter inference:\n",
      "(i) it provides higher sensitivity in the noiseless regime, and (ii) at the\n",
      "noise level of Rubin-like surveys, though the constraints are not significantly\n",
      "tighter than those of the bispectrum, the scattering coefficients have a more\n",
      "Gaussian sampling distribution, which is an important property for likelihood\n",
      "parametrization and accurate cosmological inference. We argue that the\n",
      "scattering coefficients are preferred statistics considering both constraining\n",
      "power and likelihood properties. \n",
      "\n",
      "\n",
      "Both generative learning and discriminative learning have recently witnessed\n",
      "remarkable progress using Deep Neural Networks (DNNs). For structured input\n",
      "synthesis and structured output prediction problems (e.g., layout-to-image\n",
      "synthesis and image semantic segmentation respectively), they often are studied\n",
      "separately. This paper proposes deep consensus learning (DCL) for joint\n",
      "layout-to-image synthesis and weakly-supervised image semantic segmentation.\n",
      "The former is realized by a recently proposed LostGAN approach, and the latter\n",
      "by introducing an inference network as the third player joining the two-player\n",
      "game of LostGAN. Two deep consensus mappings are exploited to facilitate\n",
      "training the three networks end-to-end: Given an input layout (a list of object\n",
      "bounding boxes), the generator generates a mask (label map) and then use it to\n",
      "help synthesize an image. The inference network infers the mask for the\n",
      "synthesized image. Then, the latent consensus is measured between the mask\n",
      "generated by the generator and the one inferred by the inference network. For\n",
      "the real image corresponding to the input layout, its mask also is computed by\n",
      "the inference network, and then used by the generator to reconstruct the real\n",
      "image. Then, the data consensus is measured between the real image and its\n",
      "reconstructed image. The discriminator still plays the role of an adversary by\n",
      "computing the realness scores for a real image, its reconstructed image and a\n",
      "synthesized image. In experiments, our DCL is tested in the COCO-Stuff dataset.\n",
      "It obtains compelling layout-to-image synthesis results and weakly-supervised\n",
      "image semantic segmentation results. \n",
      "\n",
      "\n",
      "Conditional generative adversarial networks (cGANs) target at synthesizing\n",
      "diverse images given the input conditions and latent codes, but unfortunately,\n",
      "they usually suffer from the issue of mode collapse. To solve this issue,\n",
      "previous works mainly focused on encouraging the correlation between the latent\n",
      "codes and their generated images, while ignoring the relations between images\n",
      "generated from various latent codes. The recent MSGAN tried to encourage the\n",
      "diversity of the generated image but only considers \"negative\" relations\n",
      "between the image pairs. In this paper, we propose a novel DivCo framework to\n",
      "properly constrain both \"positive\" and \"negative\" relations between the\n",
      "generated images specified in the latent space. To the best of our knowledge,\n",
      "this is the first attempt to use contrastive learning for diverse conditional\n",
      "image synthesis. A novel latent-augmented contrastive loss is introduced, which\n",
      "encourages images generated from adjacent latent codes to be similar and those\n",
      "generated from distinct latent codes to be dissimilar. The proposed\n",
      "latent-augmented contrastive loss is well compatible with various cGAN\n",
      "architectures. Extensive experiments demonstrate that the proposed DivCo can\n",
      "produce more diverse images than state-of-the-art methods without sacrificing\n",
      "visual quality in multiple unpaired and paired image generation tasks. \n",
      "\n",
      "\n",
      "Generative adversarial networks achieve great performance in photorealistic\n",
      "image synthesis in various domains, including human images. However, they\n",
      "usually employ latent vectors that encode the sampled outputs globally. This\n",
      "does not allow convenient control of semantically-relevant individual parts of\n",
      "the image, and is not able to draw samples that only differ in partial aspects,\n",
      "such as clothing style. We address these limitations and present a generative\n",
      "model for images of dressed humans offering control over pose, local body part\n",
      "appearance and garment style. This is the first method to solve various aspects\n",
      "of human image generation such as global appearance sampling, pose transfer,\n",
      "parts and garment transfer, and parts sampling jointly in a unified framework.\n",
      "As our model encodes part-based latent appearance vectors in a normalized\n",
      "pose-independent space and warps them to different poses, it preserves body and\n",
      "clothing appearance under varying posture. Experiments show that our flexible\n",
      "and general generative method outperforms task-specific baselines for\n",
      "pose-conditioned image generation, pose transfer and part sampling in terms of\n",
      "realism and output resolution. \n",
      "\n",
      "\n",
      "Semantic image synthesis, translating semantic layouts to photo-realistic\n",
      "images, is a one-to-many mapping problem. Though impressive progress has been\n",
      "recently made, diverse semantic synthesis that can efficiently produce\n",
      "semantic-level multimodal results, still remains a challenge. In this paper, we\n",
      "propose a novel diverse semantic image synthesis framework from the perspective\n",
      "of semantic class distributions, which naturally supports diverse generation at\n",
      "semantic or even instance level. We achieve this by modeling class-level\n",
      "conditional modulation parameters as continuous probability distributions\n",
      "instead of discrete values, and sampling per-instance modulation parameters\n",
      "through instance-adaptive stochastic sampling that is consistent across the\n",
      "network. Moreover, we propose prior noise remapping, through linear\n",
      "perturbation parameters encoded from paired references, to facilitate\n",
      "supervised training and exemplar-based instance style control at test time.\n",
      "Extensive experiments on multiple datasets show that our method can achieve\n",
      "superior diversity and comparable quality compared to state-of-the-art methods.\n",
      "Code will be available at \\url{https://github.com/tzt101/INADE.git} \n",
      "\n",
      "\n",
      "The goal of cross-view image based geo-localization is to determine the\n",
      "location of a given street view image by matching it against a collection of\n",
      "geo-tagged satellite images. This task is notoriously challenging due to the\n",
      "drastic viewpoint and appearance differences between the two domains. We show\n",
      "that we can address this discrepancy explicitly by learning to synthesize\n",
      "realistic street views from satellite inputs. Following this observation, we\n",
      "propose a novel multi-task architecture in which image synthesis and retrieval\n",
      "are considered jointly. The rationale behind this is that we can bias our\n",
      "network to learn latent feature representations that are useful for retrieval\n",
      "if we utilize them to generate images across the two input domains. To the best\n",
      "of our knowledge, ours is the first approach that creates realistic street\n",
      "views from satellite images and localizes the corresponding query street-view\n",
      "simultaneously in an end-to-end manner. In our experiments, we obtain\n",
      "state-of-the-art performance on the CVUSA and CVACT benchmarks. Finally, we\n",
      "show compelling qualitative results for satellite-to-street view synthesis. \n",
      "\n",
      "\n",
      "The key procedure of haze image translation through adversarial training lies\n",
      "in the disentanglement between the feature only involved in haze synthesis,\n",
      "i.e.style feature, and the feature representing the invariant semantic content,\n",
      "i.e. content feature. Previous methods separate content feature apart by\n",
      "utilizing it to classify haze image during the training process. However, in\n",
      "this paper we recognize the incompleteness of the content-style disentanglement\n",
      "in such technical routine. The flawed style feature entangled with content\n",
      "information inevitably leads the ill-rendering of the haze images. To address,\n",
      "we propose a self-supervised style regression via stochastic linear\n",
      "interpolation to reduce the content information in style feature. The ablative\n",
      "experiments demonstrate the disentangling completeness and its superiority in\n",
      "level-aware haze image synthesis. Moreover, the generated haze data are applied\n",
      "in the testing generalization of vehicle detectors. Further study between\n",
      "haze-level and detection performance shows that haze has obvious impact on the\n",
      "generalization of the vehicle detectors and such performance degrading level is\n",
      "linearly correlated to the haze-level, which, in turn, validates the\n",
      "effectiveness of the proposed method. \n",
      "\n",
      "\n",
      "The surge in the internet of things (IoT) devices seriously threatens the\n",
      "current IoT security landscape, which requires a robust network intrusion\n",
      "detection system (NIDS). Despite superior detection accuracy, existing machine\n",
      "learning or deep learning based NIDS are vulnerable to adversarial examples.\n",
      "Recently, generative adversarial networks (GANs) have become a prevailing\n",
      "method in adversarial examples crafting. However, the nature of discrete\n",
      "network traffic at the packet level makes it hard for GAN to craft adversarial\n",
      "traffic as GAN is efficient in generating continuous data like image synthesis.\n",
      "Unlike previous methods that convert discrete network traffic into a grayscale\n",
      "image, this paper gains inspiration from SeqGAN in sequence generation with\n",
      "policy gradient. Based on the structure of SeqGAN, we propose Attack-GAN to\n",
      "generate adversarial network traffic at packet level that complies with domain\n",
      "constraints. Specifically, the adversarial packet generation is formulated into\n",
      "a sequential decision making process. In this case, each byte in a packet is\n",
      "regarded as a token in a sequence. The objective of the generator is to select\n",
      "a token to maximize its expected end reward. To bypass the detection of NIDS,\n",
      "the generated network traffic and benign traffic are classified by a black-box\n",
      "NIDS. The prediction results returned by the NIDS are fed into the\n",
      "discriminator to guide the update of the generator. We generate malicious\n",
      "adversarial traffic based on a real public available dataset with attack\n",
      "functionality unchanged. The experimental results validate that the generated\n",
      "adversarial samples are able to deceive many existing black-box NIDS. \n",
      "\n",
      "\n",
      "Person image synthesis, e.g., pose transfer, is a challenging problem due to\n",
      "large variation and occlusion. Existing methods have difficulties predicting\n",
      "reasonable invisible regions and fail to decouple the shape and style of\n",
      "clothing, which limits their applications on person image editing. In this\n",
      "paper, we propose PISE, a novel two-stage generative model for Person Image\n",
      "Synthesis and Editing, which is able to generate realistic person images with\n",
      "desired poses, textures, or semantic layouts. For human pose transfer, we first\n",
      "synthesize a human parsing map aligned with the target pose to represent the\n",
      "shape of clothing by a parsing generator, and then generate the final image by\n",
      "an image generator. To decouple the shape and style of clothing, we propose\n",
      "joint global and local per-region encoding and normalization to predict the\n",
      "reasonable style of clothing for invisible regions. We also propose\n",
      "spatial-aware normalization to retain the spatial context relationship in the\n",
      "source image. The results of qualitative and quantitative experiments\n",
      "demonstrate the superiority of our model on human pose transfer. Besides, the\n",
      "results of texture transfer and region editing show that our model can be\n",
      "applied to person image editing. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) have enabled photorealistic image\n",
      "synthesis and editing. However, due to the high computational cost of\n",
      "large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the\n",
      "results of a single edit on edge devices, prohibiting interactive user\n",
      "experience. In this paper, we take inspirations from modern rendering software\n",
      "and propose Anycost GAN for interactive natural image editing. We train the\n",
      "Anycost GAN to support elastic resolutions and channels for faster image\n",
      "generation at versatile speeds. Running subsets of the full generator produce\n",
      "outputs that are perceptually similar to the full generator, making them a good\n",
      "proxy for preview. By using sampling-based multi-resolution training,\n",
      "adaptive-channel training, and a generator-conditioned discriminator, the\n",
      "anycost generator can be evaluated at various configurations while achieving\n",
      "better image quality compared to separately trained models. Furthermore, we\n",
      "develop new encoder training and latent code optimization techniques to\n",
      "encourage consistency between the different sub-generators during image\n",
      "projection. Anycost GAN can be executed at various cost budgets (up to 10x\n",
      "computation reduction) and adapt to a wide range of hardware and latency\n",
      "requirements. When deployed on desktop CPUs and edge devices, our model can\n",
      "provide perceptually similar previews at 6-12x speedup, enabling interactive\n",
      "image editing. The code and demo are publicly available:\n",
      "https://github.com/mit-han-lab/anycost-gan. \n",
      "\n",
      "\n",
      "Synthesising photo-realistic images from natural language is one of the\n",
      "challenging problems in computer vision. Over the past decade, a number of\n",
      "approaches have been proposed, of which the improved Stacked Generative\n",
      "Adversarial Network (StackGAN-v2) has proven capable of generating high\n",
      "resolution images that reflect the details specified in the input text\n",
      "descriptions. In this paper, we aim to assess the robustness and\n",
      "fault-tolerance capability of the StackGAN-v2 model by introducing variations\n",
      "in the training data. However, due to the working principle of Generative\n",
      "Adversarial Network (GAN), it is difficult to predict the output of the model\n",
      "when the training data are modified. Hence, in this work, we adopt Metamorphic\n",
      "Testing technique to evaluate the robustness of the model with a variety of\n",
      "unexpected training dataset. As such, we first implement StackGAN-v2 algorithm\n",
      "and test the pre-trained model provided by the original authors to establish a\n",
      "ground truth for our experiments. We then identify a metamorphic relation, from\n",
      "which test cases are generated. Further, metamorphic relations were derived\n",
      "successively based on the observations of prior test results. Finally, we\n",
      "synthesise the results from our experiment of all the metamorphic relations and\n",
      "found that StackGAN-v2 algorithm is susceptible to input images with obtrusive\n",
      "objects, even if it overlaps with the main object minimally, which was not\n",
      "reported by the authors and users of StackGAN-v2 model. The proposed\n",
      "metamorphic relations can be applied to other text-to-image synthesis models to\n",
      "not only verify the robustness but also to help researchers understand and\n",
      "interpret the results made by the machine learning models. \n",
      "\n",
      "\n",
      "Photo-realistic re-rendering of a human from a single image with explicit\n",
      "control over body pose, shape and appearance enables a wide range of\n",
      "applications, such as human appearance transfer, virtual try-on, motion\n",
      "imitation, and novel view synthesis. While significant progress has been made\n",
      "in this direction using learning-based image generation tools, such as GANs,\n",
      "existing approaches yield noticeable artefacts such as blurring of fine\n",
      "details, unrealistic distortions of the body parts and garments as well as\n",
      "severe changes of the textures. We, therefore, propose a new method for\n",
      "synthesising photo-realistic human images with explicit control over pose and\n",
      "part-based appearance, i.e., StylePoseGAN, where we extend a non-controllable\n",
      "generator to accept conditioning of pose and appearance separately. Our network\n",
      "can be trained in a fully supervised way with human images to disentangle pose,\n",
      "appearance and body parts, and it significantly outperforms existing single\n",
      "image re-rendering methods. Our disentangled representation opens up further\n",
      "applications such as garment transfer, motion transfer, virtual try-on, head\n",
      "(identity) swap and appearance interpolation. StylePoseGAN achieves\n",
      "state-of-the-art image generation fidelity on common perceptual metrics\n",
      "compared to the current best-performing methods and convinces in a\n",
      "comprehensive user study. \n",
      "\n",
      "\n",
      "Devising domain- and model-agnostic evaluation metrics for generative models\n",
      "is an important and as yet unresolved problem. Most existing metrics, which\n",
      "were tailored solely to the image synthesis setup, exhibit a limited capacity\n",
      "for diagnosing the different modes of failure of generative models across\n",
      "broader application domains. In this paper, we introduce a 3-dimensional\n",
      "evaluation metric, ($\\alpha$-Precision, $\\beta$-Recall, Authenticity), that\n",
      "characterizes the fidelity, diversity and generalization performance of any\n",
      "generative model in a domain-agnostic fashion. Our metric unifies statistical\n",
      "divergence measures with precision-recall analysis, enabling sample- and\n",
      "distribution-level diagnoses of model fidelity and diversity. We introduce\n",
      "generalization as an additional, independent dimension (to the\n",
      "fidelity-diversity trade-off) that quantifies the extent to which a model\n",
      "copies training data -- a crucial performance indicator when modeling sensitive\n",
      "data with requirements on privacy. The three metric components correspond to\n",
      "(interpretable) probabilistic quantities, and are estimated via sample-level\n",
      "binary classification. The sample-level nature of our metric inspires a novel\n",
      "use case which we call model auditing, wherein we judge the quality of\n",
      "individual samples generated by a (black-box) model, discarding low-quality\n",
      "samples and hence improving the overall model performance in a post-hoc manner. \n",
      "\n",
      "\n",
      "A Magnetic Resonance Imaging (MRI) exam typically consists of the acquisition\n",
      "of multiple MR pulse sequences, which are required for a reliable diagnosis.\n",
      "Each sequence can be parameterized through multiple acquisition parameters\n",
      "affecting MR image contrast, signal-to-noise ratio, resolution, or scan time.\n",
      "With the rise of generative deep learning models, approaches for the synthesis\n",
      "of MR images are developed to either synthesize additional MR contrasts,\n",
      "generate synthetic data, or augment existing data for AI training. However,\n",
      "current generative approaches for the synthesis of MR images are only trained\n",
      "on images with a specific set of acquisition parameter values, limiting the\n",
      "clinical value of these methods as various sets of acquisition parameter\n",
      "settings are used in clinical practice. Therefore, we trained a generative\n",
      "adversarial network (GAN) to generate synthetic MR knee images conditioned on\n",
      "various acquisition parameters (repetition time, echo time, image orientation).\n",
      "This approach enables us to synthesize MR images with adjustable image\n",
      "contrast. In a visual Turing test, two experts mislabeled 40.5% of real and\n",
      "synthetic MR images, demonstrating that the image quality of the generated\n",
      "synthetic and real MR images is comparable. This work can support radiologists\n",
      "and technologists during the parameterization of MR sequences by previewing the\n",
      "yielded MR contrast, can serve as a valuable tool for radiology training, and\n",
      "can be used for customized data generation to support AI training. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have extended deep learning to complex\n",
      "generation and translation tasks across different data modalities. However,\n",
      "GANs are notoriously difficult to train: Mode collapse and other instabilities\n",
      "in the training process often degrade the quality of the generated results,\n",
      "such as images. This paper presents a new technique called TaylorGAN for\n",
      "improving GANs by discovering customized loss functions for each of its two\n",
      "networks. The loss functions are parameterized as Taylor expansions and\n",
      "optimized through multiobjective evolution. On an image-to-image translation\n",
      "benchmark task, this approach qualitatively improves generated image quality\n",
      "and quantitatively improves two independent GAN performance metrics. It\n",
      "therefore forms a promising approach for applying GANs to more challenging\n",
      "tasks in the future. \n",
      "\n",
      "\n",
      "The virtual try-on task is so attractive that it has drawn considerable\n",
      "attention in the field of computer vision. However, presenting the\n",
      "three-dimensional (3D) physical characteristic (e.g., pleat and shadow) based\n",
      "on a 2D image is very challenging. Although there have been several previous\n",
      "studies on 2D-based virtual try-on work, most 1) required user-specified target\n",
      "poses that are not user-friendly and may not be the best for the target\n",
      "clothing, and 2) failed to address some problematic cases, including facial\n",
      "details, clothing wrinkles and body occlusions. To address these two\n",
      "challenges, in this paper, we propose an innovative template-free try-on image\n",
      "synthesis (TF-TIS) network. The TF-TIS first synthesizes the target pose\n",
      "according to the user-specified in-shop clothing. Afterward, given an in-shop\n",
      "clothing image, a user image, and a synthesized pose, we propose a novel model\n",
      "for synthesizing a human try-on image with the target clothing in the best\n",
      "fitting pose. The qualitative and quantitative experiments both indicate that\n",
      "the proposed TF-TIS outperforms the state-of-the-art methods, especially for\n",
      "difficult cases. \n",
      "\n",
      "\n",
      "Medical image segmentation is routinely performed to isolate regions of\n",
      "interest, such as organs and lesions. Currently, deep learning is the state of\n",
      "the art for automatic segmentation, but is usually limited by the need for\n",
      "supervised training with large datasets that have been manually segmented by\n",
      "trained clinicians. The goal of semi-superised and unsupervised image\n",
      "segmentation is to greatly reduce, or even eliminate, the need for training\n",
      "data and therefore to minimze the burden on clinicians when training\n",
      "segmentation models. To this end we introduce a novel network architecture for\n",
      "capable of unsupervised and semi-supervised image segmentation called\n",
      "TricycleGAN. This approach uses three generative models to learn translations\n",
      "between medical images and segmentation maps using edge maps as an intermediate\n",
      "step. Distinct from other approaches based on generative networks, TricycleGAN\n",
      "relies on shape priors rather than colour and texture priors. As such, it is\n",
      "particularly well-suited for several domains of medical imaging, such as\n",
      "ultrasound imaging, where commonly used visual cues may be absent. We present\n",
      "experiments with TricycleGAN on a clinical dataset of kidney ultrasound images\n",
      "and the benchmark ISIC 2018 skin lesion dataset. \n",
      "\n",
      "\n",
      "With the advent of generative adversarial networks, synthesizing images from\n",
      "textual descriptions has recently become an active research area. It is a\n",
      "flexible and intuitive way for conditional image generation with significant\n",
      "progress in the last years regarding visual realism, diversity, and semantic\n",
      "alignment. However, the field still faces several challenges that require\n",
      "further research efforts such as enabling the generation of high-resolution\n",
      "images with multiple objects, and developing suitable and reliable evaluation\n",
      "metrics that correlate with human judgement. In this review, we contextualize\n",
      "the state of the art of adversarial text-to-image synthesis models, their\n",
      "development since their inception five years ago, and propose a taxonomy based\n",
      "on the level of supervision. We critically examine current strategies to\n",
      "evaluate text-to-image synthesis models, highlight shortcomings, and identify\n",
      "new areas of research, ranging from the development of better datasets and\n",
      "evaluation metrics to possible improvements in architectural design and model\n",
      "training. This review complements previous surveys on generative adversarial\n",
      "networks with a focus on text-to-image synthesis which we believe will help\n",
      "researchers to further advance the field. \n",
      "\n",
      "\n",
      "The use of fundus images for the early screening of eye diseases is of great\n",
      "clinical importance. Due to its powerful performance, deep learning is becoming\n",
      "more and more popular in related applications, such as lesion segmentation,\n",
      "biomarkers segmentation, disease diagnosis and image synthesis. Therefore, it\n",
      "is very necessary to summarize the recent developments in deep learning for\n",
      "fundus images with a review paper. In this review, we introduce 143 application\n",
      "papers with a carefully designed hierarchy. Moreover, 33 publicly available\n",
      "datasets are presented. Summaries and analyses are provided for each task.\n",
      "Finally, limitations common to all tasks are revealed and possible solutions\n",
      "are given. We will also release and regularly update the state-of-the-art\n",
      "results and newly-released datasets at https://github.com/nkicsl/Fundus Review\n",
      "to adapt to the rapid development of this field. \n",
      "\n",
      "\n",
      "Image-to-image translation (I2I) aims to transfer images from a source domain\n",
      "to a target domain while preserving the content representations. I2I has drawn\n",
      "increasing attention and made tremendous progress in recent years because of\n",
      "its wide range of applications in many computer vision and image processing\n",
      "problems, such as image synthesis, segmentation, style transfer, restoration,\n",
      "and pose estimation. In this paper, we provide an overview of the I2I works\n",
      "developed in recent years. We will analyze the key techniques of the existing\n",
      "I2I works and clarify the main progress the community has made. Additionally,\n",
      "we will elaborate on the effect of I2I on the research and industry community\n",
      "and point out remaining challenges in related fields. \n",
      "\n",
      "\n",
      "We propose a novel framework for controllable pathological image synthesis\n",
      "for data augmentation. Inspired by CycleGAN, we perform cycle-consistent\n",
      "image-to-image translation between two domains: healthy and pathological.\n",
      "Guided by a semantic mask, an adversarially trained generator synthesizes\n",
      "pathology on a healthy image in the specified location. We demonstrate our\n",
      "approach on an institutional dataset of cerebral microbleeds in traumatic brain\n",
      "injury patients. We utilize synthetic images generated with our method for data\n",
      "augmentation in cerebral microbleeds detection. Enriching the training dataset\n",
      "with synthetic images exhibits the potential to increase detection performance\n",
      "for cerebral microbleeds in traumatic brain injury patients. \n",
      "\n",
      "\n",
      "Tagged magnetic resonance imaging (MRI) is a widely used imaging technique\n",
      "for measuring tissue deformation in moving organs. Due to tagged MRI's\n",
      "intrinsic low anatomical resolution, another matching set of cine MRI with\n",
      "higher resolution is sometimes acquired in the same scanning session to\n",
      "facilitate tissue segmentation, thus adding extra time and cost. To mitigate\n",
      "this, in this work, we propose a novel dual-cycle constrained bijective VAE-GAN\n",
      "approach to carry out tagged-to-cine MR image synthesis. Our method is based on\n",
      "a variational autoencoder backbone with cycle reconstruction constrained\n",
      "adversarial training to yield accurate and realistic cine MR images given\n",
      "tagged MR images. Our framework has been trained, validated, and tested using\n",
      "1,768, 416, and 1,560 subject-independent paired slices of tagged and cine MRI\n",
      "from twenty healthy subjects, respectively, demonstrating superior performance\n",
      "over the comparison methods. Our method can potentially be used to reduce the\n",
      "extra acquisition time and cost, while maintaining the same workflow for\n",
      "further motion analyses. \n",
      "\n",
      "\n",
      "Training Generative Adversarial Networks (GAN) on high-fidelity images\n",
      "usually requires large-scale GPU-clusters and a vast number of training images.\n",
      "In this paper, we study the few-shot image synthesis task for GAN with minimum\n",
      "computing cost. We propose a light-weight GAN structure that gains superior\n",
      "quality on 1024*1024 resolution. Notably, the model converges from scratch with\n",
      "just a few hours of training on a single RTX-2080 GPU, and has a consistent\n",
      "performance, even with less than 100 training samples. Two technique designs\n",
      "constitute our work, a skip-layer channel-wise excitation module and a\n",
      "self-supervised discriminator trained as a feature-encoder. With thirteen\n",
      "datasets covering a wide variety of image domains (The datasets and code are\n",
      "available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our\n",
      "model's superior performance compared to the state-of-the-art StyleGAN2, when\n",
      "data and computing budget are limited. \n",
      "\n",
      "\n",
      "The output of text-to-image synthesis systems should be coherent, clear,\n",
      "photo-realistic scenes with high semantic fidelity to their conditioned text\n",
      "descriptions. Our Cross-Modal Contrastive Generative Adversarial Network\n",
      "(XMC-GAN) addresses this challenge by maximizing the mutual information between\n",
      "image and text. It does this via multiple contrastive losses which capture\n",
      "inter-modality and intra-modality correspondences. XMC-GAN uses an attentional\n",
      "self-modulation generator, which enforces strong text-image correspondence, and\n",
      "a contrastive discriminator, which acts as a critic as well as a feature\n",
      "encoder for contrastive learning. The quality of XMC-GAN's output is a major\n",
      "step up from previous models, as we show on three challenging datasets. On\n",
      "MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,\n",
      "but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1\n",
      "for image-text alignment, compared to three other recent models. XMC-GAN also\n",
      "generalizes to the challenging Localized Narratives dataset (which has longer,\n",
      "more detailed descriptions), improving state-of-the-art FID from 48.70 to\n",
      "14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images\n",
      "data, establishing a strong benchmark FID score of 26.91. \n",
      "\n",
      "\n",
      "In this paper, we propose a multi-stage and high-resolution model for image\n",
      "synthesis that uses fine-grained attributes and masks as input. With a\n",
      "fine-grained attribute, the proposed model can detailedly constrain the\n",
      "features of the generated image through rich and fine-grained semantic\n",
      "information in the attribute. With mask as prior, the model in this paper is\n",
      "constrained so that the generated images conform to visual senses, which will\n",
      "reduce the unexpected diversity of samples generated from the generative\n",
      "adversarial network. This paper also proposes a scheme to improve the\n",
      "discriminator of the generative adversarial network by simultaneously\n",
      "discriminating the total image and sub-regions of the image. In addition, we\n",
      "propose a method for optimizing the labeled attribute in datasets, which\n",
      "reduces the manual labeling noise. Extensive quantitative results show that our\n",
      "image synthesis model generates more realistic images. \n",
      "\n",
      "\n",
      "This chapter reviews recent developments of generative adversarial networks\n",
      "(GAN)-based methods for medical and biomedical image synthesis tasks. These\n",
      "methods are classified into conditional GAN and Cycle-GAN according to the\n",
      "network architecture designs. For each category, a literature survey is given,\n",
      "which covers discussions of the network architecture designs, highlights\n",
      "important contributions and identifies specific challenges. \n",
      "\n",
      "\n",
      "While Positron emission tomography (PET) imaging has been widely used in\n",
      "diagnosis of number of diseases, it has costly acquisition process which\n",
      "involves radiation exposure to patients. However, magnetic resonance imaging\n",
      "(MRI) is a safer imaging modality that does not involve patient's exposure to\n",
      "radiation. Therefore, a need exists for an efficient and automated PET image\n",
      "generation from MRI data. In this paper, we propose a new frequency-aware\n",
      "attention U-net for generating synthetic PET images. Specifically, we\n",
      "incorporate attention mechanism into different U-net layers responsible for\n",
      "estimating low/high frequency scales of the image. Our frequency-aware\n",
      "attention Unet computes the attention scores for feature maps in low/high\n",
      "frequency layers and use it to help the model focus more on the most important\n",
      "regions, leading to more realistic output images. Experimental results on 30\n",
      "subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset\n",
      "demonstrate good performance of the proposed model in PET image synthesis that\n",
      "achieved superior performance, both qualitative and quantitative, over current\n",
      "state-of-the-arts. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have been extremely successful in\n",
      "various application domains such as computer vision, medicine, and natural\n",
      "language processing. Moreover, transforming an object or person to a desired\n",
      "shape become a well-studied research in the GANs. GANs are powerful models for\n",
      "learning complex distributions to synthesize semantically meaningful samples.\n",
      "However, there is a lack of comprehensive review in this field, especially lack\n",
      "of a collection of GANs loss-variant, evaluation metrics, remedies for diverse\n",
      "image generation, and stable training. Given the current fast GANs development,\n",
      "in this survey, we provide a comprehensive review of adversarial models for\n",
      "image synthesis. We summarize the synthetic image generation methods, and\n",
      "discuss the categories including image-to-image translation, fusion image\n",
      "generation, label-to-image mapping, and text-to-image translation. We organize\n",
      "the literature based on their base models, developed ideas related to\n",
      "architectures, constraints, loss functions, evaluation metrics, and training\n",
      "datasets. We present milestones of adversarial models, review an extensive\n",
      "selection of previous works in various categories, and present insights on the\n",
      "development route from the model-based to data-driven methods. Further, we\n",
      "highlight a range of potential future research directions. One of the unique\n",
      "features of this review is that all software implementations of these GAN\n",
      "methods and datasets have been collected and made available in one place at\n",
      "https://github.com/pshams55/GAN-Case-Study. \n",
      "\n",
      "\n",
      "Given an \"in-the-wild\" video of a person, we reconstruct an animatable model\n",
      "of the person in the video. The output model can be rendered in any body pose\n",
      "to any camera view, via the learned controls, without explicit 3D mesh\n",
      "reconstruction. At the core of our method is a volumetric 3D human\n",
      "representation reconstructed with a deep network trained on input video,\n",
      "enabling novel pose/view synthesis. Our method is an advance over GAN-based\n",
      "image-to-image translation since it allows image synthesis for any pose and\n",
      "camera via the internal 3D representation, while at the same time it does not\n",
      "require a pre-rigged model or ground truth meshes for training, as in\n",
      "mesh-based learning. Experiments validate the design choices and yield results\n",
      "on synthetic data and on real videos of diverse people performing unconstrained\n",
      "activities (e.g. dancing or playing tennis). Finally, we demonstrate motion\n",
      "re-targeting and bullet-time rendering with the learned models. \n",
      "\n",
      "\n",
      "We present a generative model for controllable person image synthesis,as\n",
      "shown in Figure , which can be applied to pose-guided person image synthesis,\n",
      "$i.e.$, converting the pose of a source person image to the target pose while\n",
      "preserving the texture of that source person image, and clothing-guided person\n",
      "image synthesis, $i.e.$, changing the clothing texture of a source person image\n",
      "to the desired clothing texture. By explicitly establishing the dense\n",
      "correspondence between the target pose and the source image, we can effectively\n",
      "address the misalignment introduced by pose tranfer and generate high-quality\n",
      "images. Specifically, we first generate the target semantic map under the\n",
      "guidence of the target pose, which can provide more accurate pose\n",
      "representation and structural constraints during the generation process. Then,\n",
      "decomposed attribute encoder is used to extract the component features, which\n",
      "not only helps to establish a more accurate dense correspondence, but also\n",
      "realizes the clothing-guided person generation. After that, we will establish a\n",
      "dense correspondence between the target pose and the source image within the\n",
      "sharded domain. The source image feature is warped according to the dense\n",
      "correspondence to flexibly account for deformations. Finally, the network\n",
      "renders image based on the warped source image feature and the target pose.\n",
      "Experimental results show that our method is superior to state-of-the-art\n",
      "methods in pose-guided person generation and its effectiveness in\n",
      "clothing-guided person generation. \n",
      "\n",
      "\n",
      "Mainstream deep models for three-dimensional MRI synthesis are either\n",
      "cross-sectional or volumetric depending on the input. Cross-sectional models\n",
      "can decrease the model complexity, but they may lead to discontinuity\n",
      "artifacts. On the other hand, volumetric models can alleviate the discontinuity\n",
      "artifacts, but they might suffer from loss of spatial resolution due to\n",
      "increased model complexity coupled with scarce training data. To mitigate the\n",
      "limitations of both approaches, we propose a novel model that progressively\n",
      "recovers the target volume via simpler synthesis tasks across individual\n",
      "orientations. \n",
      "\n",
      "\n",
      "We introduce the problem of perpetual view generation - long-range generation\n",
      "of novel views corresponding to an arbitrarily long camera trajectory given a\n",
      "single image. This is a challenging problem that goes far beyond the\n",
      "capabilities of current view synthesis methods, which quickly degenerate when\n",
      "presented with large camera motions. Methods for video generation also have\n",
      "limited ability to produce long sequences and are often agnostic to scene\n",
      "geometry. We take a hybrid approach that integrates both geometry and image\n",
      "synthesis in an iterative `\\emph{render}, \\emph{refine} and \\emph{repeat}'\n",
      "framework, allowing for long-range generation that cover large distances after\n",
      "hundreds of frames. Our approach can be trained from a set of monocular video\n",
      "sequences. We propose a dataset of aerial footage of coastal scenes, and\n",
      "compare our method with recent view synthesis and conditional video generation\n",
      "baselines, showing that it can generate plausible scenes for much longer time\n",
      "horizons over large camera trajectories compared to existing methods. Project\n",
      "page at https://infinite-nature.github.io/. \n",
      "\n",
      "\n",
      "Designed to learn long-range interactions on sequential data, transformers\n",
      "continue to show state-of-the-art results on a wide variety of tasks. In\n",
      "contrast to CNNs, they contain no inductive bias that prioritizes local\n",
      "interactions. This makes them expressive, but also computationally infeasible\n",
      "for long sequences, such as high-resolution images. We demonstrate how\n",
      "combining the effectiveness of the inductive bias of CNNs with the expressivity\n",
      "of transformers enables them to model and thereby synthesize high-resolution\n",
      "images. We show how to (i) use CNNs to learn a context-rich vocabulary of image\n",
      "constituents, and in turn (ii) utilize transformers to efficiently model their\n",
      "composition within high-resolution images. Our approach is readily applied to\n",
      "conditional synthesis tasks, where both non-spatial information, such as object\n",
      "classes, and spatial information, such as segmentations, can control the\n",
      "generated image. In particular, we present the first results on\n",
      "semantically-guided synthesis of megapixel images with transformers and obtain\n",
      "the state of the art among autoregressive models on class-conditional ImageNet.\n",
      "Code and pretrained models can be found at\n",
      "https://github.com/CompVis/taming-transformers . \n",
      "\n",
      "\n",
      "Imagining a colored realistic image from an arbitrarily drawn sketch is one\n",
      "of the human capabilities that we eager machines to mimic. Unlike previous\n",
      "methods that either requires the sketch-image pairs or utilize low-quantity\n",
      "detected edges as sketches, we study the exemplar-based sketch-to-image (s2i)\n",
      "synthesis task in a self-supervised learning manner, eliminating the necessity\n",
      "of the paired sketch data. To this end, we first propose an unsupervised method\n",
      "to efficiently synthesize line-sketches for general RGB-only datasets. With the\n",
      "synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to\n",
      "decouple the content/style features from sketches and RGB-images, and\n",
      "synthesize images that are both content-faithful to the sketches and\n",
      "style-consistent to the RGB-images. While prior works employ either the\n",
      "cycle-consistence loss or dedicated attentional modules to enforce the\n",
      "content/style fidelity, we show AE's superior performance with pure\n",
      "self-supervisions. To further improve the synthesis quality in high resolution,\n",
      "we also leverage an adversarial network to refine the details of synthetic\n",
      "images. Extensive experiments on 1024*1024 resolution demonstrate a new\n",
      "state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art\n",
      "datasets. Moreover, with the proposed sketch generator, the model shows a\n",
      "promising performance on style mixing and style transfer, which require\n",
      "synthesized images to be both style-consistent and semantically meaningful. Our\n",
      "code is available on\n",
      "https://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,\n",
      "and please visit https://create.playform.io/my-projects?mode=sketch for an\n",
      "online demo of our model. \n",
      "\n",
      "\n",
      "As a generic modeling tool, Convolutional Neural Networks (CNNs) have been\n",
      "widely employed in image generation and translation tasks. However, when fed\n",
      "with a flat input, current CNN models may fail to generate vivid results due to\n",
      "the spatially shared convolution kernels. We call it the flatness degradation\n",
      "of CNNs. Unfortunately, such degradation is the greatest obstacles to generate\n",
      "a spatially-variant output from a flat input, which has been barely discussed\n",
      "in the previous literature. To tackle this problem, we propose a model agnostic\n",
      "solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in\n",
      "for any CNN generation model. The key idea is to break the flat input condition\n",
      "while keeping the intactness of the original information. Specifically, the NIB\n",
      "perturbs the input data symmetrically with a noise map and reassembles them in\n",
      "the feature domain as driven by the objective function. Extensive experiments\n",
      "show that existing CNN models equipped with NIB survive from the flatness\n",
      "degradation and are able to generate visually better results with richer\n",
      "details in some specific image generation tasks given flat inputs, e.g.\n",
      "semantic image synthesis, data-hidden image generation, and deep neural\n",
      "dithering. \n",
      "\n",
      "\n",
      "Despite their recent successes, GAN models for semantic image synthesis still\n",
      "suffer from poor image quality when trained with only adversarial supervision.\n",
      "Historically, additionally employing the VGG-based perceptual loss has helped\n",
      "to overcome this issue, significantly improving the synthesis quality, but at\n",
      "the same time limiting the progress of GAN models for semantic image synthesis.\n",
      "In this work, we propose a novel, simplified GAN model, which needs only\n",
      "adversarial supervision to achieve high quality results. We re-design the\n",
      "discriminator as a semantic segmentation network, directly using the given\n",
      "semantic label maps as the ground truth for training. By providing stronger\n",
      "supervision to the discriminator as well as to the generator through spatially-\n",
      "and semantically-aware discriminator feedback, we are able to synthesize images\n",
      "of higher fidelity with better alignment to their input label maps, making the\n",
      "use of the perceptual loss superfluous. Moreover, we enable high-quality\n",
      "multi-modal image synthesis through global and local sampling of a 3D noise\n",
      "tensor injected into the generator, which allows complete or partial image\n",
      "change. We show that images synthesized by our model are more diverse and\n",
      "follow the color and texture distributions of real images more closely. We\n",
      "achieve an average improvement of $6$ FID and $5$ mIoU points over the state of\n",
      "the art across different datasets using only adversarial supervision. \n",
      "\n",
      "\n",
      "Spatially-adaptive normalization (SPADE) is remarkably successful recently in\n",
      "conditional semantic image synthesis \\cite{park2019semantic}, which modulates\n",
      "the normalized activation with spatially-varying transformations learned from\n",
      "semantic layouts, to prevent the semantic information from being washed away.\n",
      "Despite its impressive performance, a more thorough understanding of the\n",
      "advantages inside the box is still highly demanded to help reduce the\n",
      "significant computation and parameter overhead introduced by this novel\n",
      "structure. In this paper, from a return-on-investment point of view, we conduct\n",
      "an in-depth analysis of the effectiveness of this spatially-adaptive\n",
      "normalization and observe that its modulation parameters benefit more from\n",
      "semantic-awareness rather than spatial-adaptiveness, especially for\n",
      "high-resolution input masks. Inspired by this observation, we propose\n",
      "class-adaptive normalization (CLADE), a lightweight but equally-effective\n",
      "variant that is only adaptive to semantic class. In order to further improve\n",
      "spatial-adaptiveness, we introduce intra-class positional map encoding\n",
      "calculated from semantic layouts to modulate the normalization parameters of\n",
      "CLADE and propose a truly spatially-adaptive variant of CLADE, namely\n",
      "CLADE-ICPE.Through extensive experiments on multiple challenging datasets, we\n",
      "demonstrate that the proposed CLADE can be generalized to different SPADE-based\n",
      "methods while achieving comparable generation quality compared to SPADE, but it\n",
      "is much more efficient with fewer extra parameters and lower computational\n",
      "cost. The code and pretrained models are available at\n",
      "\\url{https://github.com/tzt101/CLADE.git}. \n",
      "\n",
      "\n",
      "The main difficulty of person re-identification (ReID) lies in collecting\n",
      "annotated data and transferring the model across different domains. This paper\n",
      "presents UnrealPerson, a novel pipeline that makes full use of unreal image\n",
      "data to decrease the costs in both the training and deployment stages. Its\n",
      "fundamental part is a system that can generate synthesized images of\n",
      "high-quality and from controllable distributions. Instance-level annotation\n",
      "goes with the synthesized data and is almost free. We point out some details in\n",
      "image synthesis that largely impact the data quality. With 3,000 IDs and\n",
      "120,000 instances, our method achieves a 38.5% rank-1 accuracy when being\n",
      "directly transferred to MSMT17. It almost doubles the former record using\n",
      "synthesized data and even surpasses previous direct transfer records using real\n",
      "data. This offers a good basis for unsupervised domain adaption, where our\n",
      "pre-trained model is easily plugged into the state-of-the-art algorithms\n",
      "towards higher accuracy. In addition, the data distribution can be flexibly\n",
      "adjusted to fit some corner ReID scenarios, which widens the application of our\n",
      "pipeline. We will publish our data synthesis toolkit and synthesized data in\n",
      "https://github.com/FlyHighest/UnrealPerson. \n",
      "\n",
      "\n",
      "In this work, we propose TediGAN, a novel framework for multi-modal image\n",
      "generation and manipulation with textual descriptions. The proposed method\n",
      "consists of three components: StyleGAN inversion module, visual-linguistic\n",
      "similarity learning, and instance-level optimization. The inversion module maps\n",
      "real images to the latent space of a well-trained StyleGAN. The\n",
      "visual-linguistic similarity learns the text-image matching by mapping the\n",
      "image and text into a common embedding space. The instance-level optimization\n",
      "is for identity preservation in manipulation. Our model can produce diverse and\n",
      "high-quality images with an unprecedented resolution at 1024. Using a control\n",
      "mechanism based on style-mixing, our TediGAN inherently supports image\n",
      "synthesis with multi-modal inputs, such as sketches or semantic labels, with or\n",
      "without instance guidance. To facilitate text-guided multi-modal synthesis, we\n",
      "propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real\n",
      "face images and corresponding semantic segmentation map, sketch, and textual\n",
      "descriptions. Extensive experiments on the introduced dataset demonstrate the\n",
      "superior performance of our proposed method. Code and data are available at\n",
      "https://github.com/weihaox/TediGAN. \n",
      "\n",
      "\n",
      "This paper tackles automated categorization of Age-related Macular\n",
      "Degeneration (AMD), a common macular disease among people over 50. Previous\n",
      "research efforts mainly focus on AMD categorization with a single-modal input,\n",
      "let it be a color fundus photograph (CFP) or an OCT B-scan image. By contrast,\n",
      "we consider AMD categorization given a multi-modal input, a direction that is\n",
      "clinically meaningful yet mostly unexplored. Contrary to the prior art that\n",
      "takes a traditional approach of feature extraction plus classifier training\n",
      "that cannot be jointly optimized, we opt for end-to-end multi-modal\n",
      "Convolutional Neural Networks (MM-CNN). Our MM-CNN is instantiated by a\n",
      "two-stream CNN, with spatially-invariant fusion to combine information from the\n",
      "CFP and OCT streams. In order to visually interpret the contribution of the\n",
      "individual modalities to the final prediction, we extend the class activation\n",
      "mapping (CAM) technique to the multi-modal scenario. For effective training of\n",
      "MM-CNN, we develop two data augmentation methods. One is GAN-based CFP/OCT\n",
      "image synthesis, with our novel use of CAMs as conditional input of a\n",
      "high-resolution image-to-image translation GAN. The other method is Loose\n",
      "Pairing, which pairs a CFP image and an OCT image on the basis of their classes\n",
      "instead of eye identities. Experiments on a clinical dataset consisting of\n",
      "1,094 CFP images and 1,289 OCT images acquired from 1,093 distinct eyes show\n",
      "that the proposed solution obtains better F1 and Accuracy than multiple\n",
      "baselines for multi-modal AMD categorization. Code and data are available at\n",
      "https://github.com/li-xirong/mmc-amd. \n",
      "\n",
      "\n",
      "Image synthesis from corrupted contrasts increases the diversity of\n",
      "diagnostic information available for many neurological diseases. Recently the\n",
      "image-to-image translation has experienced significant levels of interest\n",
      "within medical research, beginning with the successful use of the Generative\n",
      "Adversarial Network (GAN) to the introduction of cyclic constraint extended to\n",
      "multiple domains. However, in current approaches, there is no guarantee that\n",
      "the mapping between the two image domains would be unique or one-to-one. In\n",
      "this paper, we introduce a novel approach to unpaired image-to-image\n",
      "translation based on the invertible architecture. The invertible property of\n",
      "the flow-based architecture assures a cycle-consistency of image-to-image\n",
      "translation without additional loss functions. We utilize the temporal\n",
      "information between consecutive slices to provide more constraints to the\n",
      "optimization for transforming one domain to another in unpaired volumetric\n",
      "medical images. To capture temporal structures in the medical images, we\n",
      "explore the displacement between the consecutive slices using a deformation\n",
      "field. In our approach, the deformation field is used as a guidance to keep the\n",
      "translated slides realistic and consistent across the translation. The\n",
      "experimental results have shown that the synthesized images using our proposed\n",
      "approach are able to archive a competitive performance in terms of mean squared\n",
      "error, peak signal-to-noise ratio, and structural similarity index when\n",
      "compared with the existing deep learning-based methods on three standard\n",
      "datasets, i.e. HCP, MRBrainS13, and Brats2019. \n",
      "\n",
      "\n",
      "Computational food analysis (CFA) naturally requires multi-modal evidence of\n",
      "a particular food, e.g., images, recipe text, etc. A key to making CFA possible\n",
      "is multi-modal shared representation learning, which aims to create a joint\n",
      "representation of the multiple views (text and image) of the data. In this work\n",
      "we propose a method for food domain cross-modal shared representation learning\n",
      "that preserves the vast semantic richness present in the food data. Our\n",
      "proposed method employs an effective transformer-based multilingual recipe\n",
      "encoder coupled with a traditional image embedding architecture. Here, we\n",
      "propose the use of imperfect multilingual translations to effectively\n",
      "regularize the model while at the same time adding functionality across\n",
      "multiple languages and alphabets. Experimental analysis on the public Recipe1M\n",
      "dataset shows that the representation learned via the proposed method\n",
      "significantly outperforms the current state-of-the-arts (SOTA) on retrieval\n",
      "tasks. Furthermore, the representational power of the learned representation is\n",
      "demonstrated through a generative food image synthesis model conditioned on\n",
      "recipe embeddings. Synthesized images can effectively reproduce the visual\n",
      "appearance of paired samples, indicating that the learned representation\n",
      "captures the joint semantics of both the textual recipe and its visual content,\n",
      "thus narrowing the modality gap. \n",
      "\n",
      "\n",
      "We have witnessed rapid progress on 3D-aware image synthesis, leveraging\n",
      "recent advances in generative visual models and neural rendering. Existing\n",
      "approaches however fall short in two ways: first, they may lack an underlying\n",
      "3D representation or rely on view-inconsistent rendering, hence synthesizing\n",
      "images that are not multi-view consistent; second, they often depend upon\n",
      "representation network architectures that are not expressive enough, and their\n",
      "results thus lack in image quality. We propose a novel generative model, named\n",
      "Periodic Implicit Generative Adversarial Networks ($\\pi$-GAN or pi-GAN), for\n",
      "high-quality 3D-aware image synthesis. $\\pi$-GAN leverages neural\n",
      "representations with periodic activation functions and volumetric rendering to\n",
      "represent scenes as view-consistent 3D representations with fine detail. The\n",
      "proposed approach obtains state-of-the-art results for 3D-aware image synthesis\n",
      "with multiple real and synthetic datasets. \n",
      "\n",
      "\n",
      "In this paper, we propose a novel CycleGAN without checkerboard artifacts for\n",
      "counter-forensics of fake-image detection. Recent rapid advances in image\n",
      "manipulation tools and deep image synthesis techniques, such as Generative\n",
      "Adversarial Networks (GANs) have easily generated fake images, so detecting\n",
      "manipulated images has become an urgent issue. Most state-of-the-art forgery\n",
      "detection methods assume that images include checkerboard artifacts which are\n",
      "generated by using DNNs. Accordingly, we propose a novel CycleGAN without any\n",
      "checkerboard artifacts for counter-forensics of fake-mage detection methods for\n",
      "the first time, as an example of GANs without checkerboard artifacts. \n",
      "\n",
      "\n",
      "The generation of synthetic images is currently being dominated by Generative\n",
      "Adversarial Networks (GANs). Despite their outstanding success in generating\n",
      "realistic looking images, they still suffer from major drawbacks, including an\n",
      "unstable and highly sensitive training procedure, mode-collapse and\n",
      "mode-mixture, and dependency on large training sets. In this work we present a\n",
      "novel non-adversarial generative method - Clustered Optimization of LAtent\n",
      "space (COLA), which overcomes some of the limitations of GANs, and outperforms\n",
      "GANs when training data is scarce. In the full data regime, our method is\n",
      "capable of generating diverse multi-class images with no supervision,\n",
      "surpassing previous non-adversarial methods in terms of image quality and\n",
      "diversity. In the small-data regime, where only a small sample of labeled\n",
      "images is available for training with no access to additional unlabeled data,\n",
      "our results surpass state-of-the-art GAN models trained on the same amount of\n",
      "data. Finally, when utilizing our model to augment small datasets, we surpass\n",
      "the state-of-the-art performance in small-sample classification tasks on\n",
      "challenging datasets, including CIFAR-10, CIFAR-100, STL-10 and Tiny-ImageNet.\n",
      "A theoretical analysis supporting the essence of the method is presented. \n",
      "\n",
      "\n",
      "Despite data augmentation being a de facto technique for boosting the\n",
      "performance of deep neural networks, little attention has been paid to\n",
      "developing augmentation strategies for generative adversarial networks (GANs).\n",
      "To this end, we introduce a novel augmentation scheme designed specifically for\n",
      "GAN-based semantic image synthesis models. We propose to randomly warp object\n",
      "shapes in the semantic label maps used as an input to the generator. The local\n",
      "shape discrepancies between the warped and non-warped label maps and images\n",
      "enable the GAN to learn better the structural and geometric details of the\n",
      "scene and thus to improve the quality of generated images. While benchmarking\n",
      "the augmented GAN models against their vanilla counterparts, we discover that\n",
      "the quantification metrics reported in the previous semantic image synthesis\n",
      "studies are strongly biased towards specific semantic classes as they are\n",
      "derived via an external pre-trained segmentation network. We therefore propose\n",
      "to improve the established semantic image synthesis evaluation scheme by\n",
      "analyzing separately the performance of generated images on the biased and\n",
      "unbiased classes for the given segmentation network. Finally, we show strong\n",
      "quantitative and qualitative improvements obtained with our augmentation\n",
      "scheme, on both class splits, using state-of-the-art semantic image synthesis\n",
      "models across three different datasets. On average across COCO-Stuff, ADE20K\n",
      "and Cityscapes datasets, the augmented models outperform their vanilla\n",
      "counterparts by ~3 mIoU and ~10 FID points. \n",
      "\n",
      "\n",
      "Deep generative models allow for photorealistic image synthesis at high\n",
      "resolutions. But for many applications, this is not enough: content creation\n",
      "also needs to be controllable. While several recent works investigate how to\n",
      "disentangle underlying factors of variation in the data, most of them operate\n",
      "in 2D and hence ignore that our world is three-dimensional. Further, only few\n",
      "works consider the compositional nature of scenes. Our key hypothesis is that\n",
      "incorporating a compositional 3D scene representation into the generative model\n",
      "leads to more controllable image synthesis. Representing scenes as\n",
      "compositional generative neural feature fields allows us to disentangle one or\n",
      "multiple objects from the background as well as individual objects' shapes and\n",
      "appearances while learning from unstructured and unposed image collections\n",
      "without any additional supervision. Combining this scene representation with a\n",
      "neural rendering pipeline yields a fast and realistic image synthesis model. As\n",
      "evidenced by our experiments, our model is able to disentangle individual\n",
      "objects and allows for translating and rotating them in the scene as well as\n",
      "changing the camera pose. \n",
      "\n",
      "\n",
      "We present a new generative autoencoder model with dual contradistinctive\n",
      "losses to improve generative autoencoder that performs simultaneous inference\n",
      "(reconstruction) and synthesis (sampling). Our model, named dual\n",
      "contradistinctive generative autoencoder (DC-VAE), integrates an instance-level\n",
      "discriminative loss (maintaining the instance-level fidelity for the\n",
      "reconstruction/synthesis) with a set-level adversarial loss (encouraging the\n",
      "set-level fidelity for there construction/synthesis), both being\n",
      "contradistinctive. Extensive experimental results by DC-VAE across different\n",
      "resolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two\n",
      "contradistinctive losses in VAE work harmoniously in DC-VAE leading to a\n",
      "significant qualitative and quantitative performance enhancement over the\n",
      "baseline VAEs without architectural changes. State-of-the-art or competitive\n",
      "results among generative autoencoders for image reconstruction, image\n",
      "synthesis, image interpolation, and representation learning are observed.\n",
      "DC-VAE is a general-purpose VAE model, applicable to a wide variety of\n",
      "downstream tasks in computer vision and machine learning. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) with style-based generators (e.g.\n",
      "StyleGAN) successfully enable semantic control over image synthesis, and recent\n",
      "studies have also revealed that interpretable image translations could be\n",
      "obtained by modifying the latent code. However, in terms of the low-level image\n",
      "content, traveling in the latent space would lead to `spatially entangled\n",
      "changes' in corresponding images, which is undesirable in many real-world\n",
      "applications where local editing is required. To solve this problem, we analyze\n",
      "properties of the 'style space' and explore the possibility of controlling the\n",
      "local translation with pre-trained style-based generators. Concretely, we\n",
      "propose 'Style Intervention', a lightweight optimization-based algorithm which\n",
      "could adapt to arbitrary input images and render natural translation effects\n",
      "under flexible objectives. We verify the performance of the proposed framework\n",
      "in facial attribute editing on high-resolution images, where both photo-realism\n",
      "and consistency are required. Extensive qualitative results demonstrate the\n",
      "effectiveness of our method, and quantitative measurements also show that the\n",
      "proposed algorithm outperforms state-of-the-art benchmarks in various aspects. \n",
      "\n",
      "\n",
      "We tackle human image synthesis, including human motion imitation, appearance\n",
      "transfer, and novel view synthesis, within a unified framework. It means that\n",
      "the model, once being trained, can be used to handle all these tasks. The\n",
      "existing task-specific methods mainly use 2D keypoints to estimate the human\n",
      "body structure. However, they only express the position information with no\n",
      "abilities to characterize the personalized shape of the person and model the\n",
      "limb rotations. In this paper, we propose to use a 3D body mesh recovery module\n",
      "to disentangle the pose and shape. It can not only model the joint location and\n",
      "rotation but also characterize the personalized body shape. To preserve the\n",
      "source information, such as texture, style, color, and face identity, we\n",
      "propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block\n",
      "(AttLWB) that propagates the source information in both image and feature\n",
      "spaces to the synthesized reference. Specifically, the source features are\n",
      "extracted by a denoising convolutional auto-encoder for characterizing the\n",
      "source identity well. Furthermore, our proposed method can support a more\n",
      "flexible warping from multiple sources. To further improve the generalization\n",
      "ability of the unseen source images, a one/few-shot adversarial learning is\n",
      "applied. In detail, it firstly trains a model in an extensive training set.\n",
      "Then, it finetunes the model by one/few-shot unseen image(s) in a\n",
      "self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024)\n",
      "results. Also, we build a new dataset, namely iPER dataset, for the evaluation\n",
      "of human motion imitation, appearance transfer, and novel view synthesis.\n",
      "Extensive experiments demonstrate the effectiveness of our methods in terms of\n",
      "preserving face identity, shape consistency, and clothes details. All codes and\n",
      "dataset are available on\n",
      "https://impersonator.org/work/impersonator-plus-plus.html. \n",
      "\n",
      "\n",
      "Recent work about synthetic indoor datasets from perspective views has shown\n",
      "significant improvements of object detection results with Convolutional Neural\n",
      "Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale\n",
      "indoor dataset containing 100,000 high-resolution diversified fisheye images\n",
      "with 14 classes. To this end, we create 3D virtual environments of living\n",
      "rooms, different human characters and interior textures. Beside capturing\n",
      "fisheye images from virtual environments we create annotations for semantic\n",
      "segmentation, instance masks and bounding boxes for object detection tasks. We\n",
      "compare our synthetic dataset to state of the art real-world datasets for\n",
      "omnidirectional images. Based on MS COCO weights, we show that our dataset is\n",
      "well suited for fine-tuning CNNs for object detection. Through a high\n",
      "generalization of our models by means of image synthesis and domain\n",
      "randomization, we reach an AP up to 0.84 for class person on High-Definition\n",
      "Analytics dataset. \n",
      "\n",
      "\n",
      "Recently, there is a vast interest in developing image feature learning\n",
      "methods that are independent of the training data, such as deep image prior,\n",
      "InGAN, SinGAN, and DCIL. These methods are unsupervised and are used to perform\n",
      "low-level vision tasks such as image restoration, image editing, and image\n",
      "synthesis. In this work, we proposed a new training data-independent framework,\n",
      "called Deep Contextual Features Learning (DeepCFL), to perform image synthesis\n",
      "and image restoration based on the semantics of the input image. The contextual\n",
      "features are simply the high dimensional vectors representing the semantics of\n",
      "the given image. DeepCFL is a single image GAN framework that learns the\n",
      "distribution of the context vectors from the input image. We show the\n",
      "performance of contextual learning in various challenging scenarios:\n",
      "outpainting, inpainting, and restoration of randomly removed pixels. DeepCFL is\n",
      "applicable when the input source image and the generated target image are not\n",
      "aligned. We illustrate image synthesis using DeepCFL for the task of image\n",
      "resizing. \n",
      "\n",
      "\n",
      "Blind motion deblurring involves reconstructing a sharp image from an\n",
      "observation that is blurry. It is a problem that is ill-posed and lies in the\n",
      "categories of image restoration problems. The training data-based methods for\n",
      "image deblurring mostly involve training models that take a lot of time. These\n",
      "models are data-hungry i.e., they require a lot of training data to generate\n",
      "satisfactory results. Recently, there are various image feature learning\n",
      "methods developed which relieve us of the need for training data and perform\n",
      "image restoration and image synthesis, e.g., DIP, InGAN, and SinGAN. SinGAN is\n",
      "a generative model that is unconditional and could be learned from a single\n",
      "natural image. This model primarily captures the internal distribution of the\n",
      "patches which are present in the image and is capable of generating samples of\n",
      "varied diversity while preserving the visual content of the image. Images\n",
      "generated from the model are very much like real natural images. In this paper,\n",
      "we focus on blind motion deblurring through SinGAN architecture. \n",
      "\n",
      "\n",
      "Facial attributes in StyleGAN generated images are entangled in the latent\n",
      "space which makes it very difficult to independently control a specific\n",
      "attribute without affecting the others. Supervised attribute editing requires\n",
      "annotated training data which is difficult to obtain and limits the editable\n",
      "attributes to those with labels. Therefore, unsupervised attribute editing in\n",
      "an disentangled latent space is key to performing neat and versatile semantic\n",
      "face editing. In this paper, we present a new technique termed\n",
      "Structure-Texture Independent Architecture with Weight Decomposition and\n",
      "Orthogonal Regularization (STIA-WO) to disentangle the latent space for\n",
      "unsupervised semantic face editing. By applying STIA-WO to GAN, we have\n",
      "developed a StyleGAN termed STGAN-WO which performs weight decomposition\n",
      "through utilizing the style vector to construct a fully controllable weight\n",
      "matrix to regulate image synthesis, and employs orthogonal regularization to\n",
      "ensure each entry of the style vector only controls one independent feature\n",
      "matrix. To further disentangle the facial attributes, STGAN-WO introduces a\n",
      "structure-texture independent architecture which utilizes two independently and\n",
      "identically distributed (i.i.d.) latent vectors to control the synthesis of the\n",
      "texture and structure components in a disentangled way. Unsupervised semantic\n",
      "editing is achieved by moving the latent code in the coarse layers along its\n",
      "orthogonal directions to change texture related attributes or changing the\n",
      "latent code in the fine layers to manipulate structure related ones. We present\n",
      "experimental results which show that our new STGAN-WO can achieve better\n",
      "attribute editing than state of the art methods. \n",
      "\n",
      "\n",
      "Generating images from textual descriptions has recently attracted a lot of\n",
      "interest. While current models can generate photo-realistic images of\n",
      "individual objects such as birds and human faces, synthesising images with\n",
      "multiple objects is still very difficult. In this paper, we propose an\n",
      "effective way to combine Text-to-Image (T2I) synthesis with Visual Question\n",
      "Answering (VQA) to improve the image quality and image-text alignment of\n",
      "generated images by leveraging the VQA 2.0 dataset. We create additional\n",
      "training samples by concatenating question and answer (QA) pairs and employ a\n",
      "standard VQA model to provide the T2I model with an auxiliary learning signal.\n",
      "We encourage images generated from QA pairs to look realistic and additionally\n",
      "minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38\n",
      "and increases the R-prec. from 83.82% to 84.79% when compared to the baseline,\n",
      "which indicates that T2I synthesis can successfully be improved using a\n",
      "standard VQA model. \n",
      "\n",
      "\n",
      "Microscopic images from multiple modalities can produce plentiful\n",
      "experimental information. In practice, biological or physical constraints under\n",
      "a given observation period may prevent researchers from acquiring enough\n",
      "microscopic scanning. Recent studies demonstrate that image synthesis is one of\n",
      "the popular approaches to release such constraints. Nonetheless, most existing\n",
      "synthesis approaches only translate images from the source domain to the target\n",
      "domain without solid geometric associations. To embrace this challenge, we\n",
      "propose an innovative model architecture, BANIS, to synthesize diversified\n",
      "microscopic images from multi-source domains with distinct geometric features.\n",
      "The experimental outcomes indicate that BANIS successfully synthesizes\n",
      "favorable image pairs on C. elegans microscopy embryonic images. To the best of\n",
      "our knowledge, BANIS is the first application to synthesize microscopic images\n",
      "that associate distinct spatial geometric features from multi-source domains. \n",
      "\n",
      "\n",
      "Automatic analysis of spatio-temporal microscopy images is inevitable for\n",
      "state-of-the-art research in the life sciences. Recent developments in deep\n",
      "learning provide powerful tools for automatic analyses of such image data, but\n",
      "heavily depend on the amount and quality of provided training data to perform\n",
      "well. To this end, we developed a new method for realistic generation of\n",
      "synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei.\n",
      "The method combines spatiotemporal statistical shape models of different cell\n",
      "cycle stages with a conditional GAN to generate time series of cell populations\n",
      "and provides instance-level control of cell cycle stage and the fluorescence\n",
      "intensity of generated cells. We show the effect of the GAN conditioning and\n",
      "create a set of synthetic images that can be readily used for training and\n",
      "benchmarking of cell segmentation and tracking approaches. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have shown remarkable performance in\n",
      "image synthesis tasks, but typically require a large number of training samples\n",
      "to achieve high-quality synthesis. This paper proposes a simple and effective\n",
      "method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than\n",
      "100 images). FSGAN repurposes component analysis techniques and learns to adapt\n",
      "the singular values of the pre-trained weights while freezing the corresponding\n",
      "singular vectors. This provides a highly expressive parameter space for\n",
      "adaptation while constraining changes to the pretrained weights. We validate\n",
      "our method in a challenging few-shot setting of 5-100 images in the target\n",
      "domain. We show that our method has significant visual quality gains compared\n",
      "with existing GAN adaptation methods. We report qualitative and quantitative\n",
      "results showing the effectiveness of our method. We additionally highlight a\n",
      "problem for few-shot synthesis in the standard quantitative metric used by\n",
      "data-efficient image synthesis works. Code and additional results are available\n",
      "at http://e-271.github.io/few-shot-gan. \n",
      "\n",
      "\n",
      "Motivated by the lack of publicly available datasets of chest radiographs of\n",
      "positive patients with Coronavirus disease 2019 (COVID-19), we build the\n",
      "first-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high\n",
      "fidelity using an unsupervised domain adaptation approach by leveraging class\n",
      "conditioning and adversarial training. Our contributions are twofold. First, we\n",
      "show considerable performance improvements on COVID-19 detection using various\n",
      "deep learning architectures when employing synthetic images as additional\n",
      "training set. Second, we show how our image synthesis method can serve as a\n",
      "data anonymization tool by achieving comparable detection performance when\n",
      "trained only on synthetic data. In addition, the proposed data generation\n",
      "framework offers a viable solution to the COVID-19 detection in particular, and\n",
      "to medical image classification tasks in general. Our publicly available\n",
      "benchmark dataset consists of 21,295 synthetic COVID-19 chest X-ray images. The\n",
      "insights gleaned from this dataset can be used for preventive actions in the\n",
      "fight against the COVID-19 pandemic. \n",
      "\n",
      "\n",
      "Synthetic data used for scene text detection and recognition tasks have\n",
      "proven effective. However, there are still two problems: First, the color\n",
      "schemes used for text coloring in the existing methods are relatively fixed\n",
      "color key-value pairs learned from real datasets. The dirty data in real\n",
      "datasets may cause the problem that the colors of text and background are too\n",
      "similar to be distinguished from each other. Second, the generated texts are\n",
      "uniformly limited to the same depth of a picture, while there are special cases\n",
      "in the real world that text may appear across depths. To address these\n",
      "problems, in this paper we design a novel method to generate color schemes,\n",
      "which are consistent with the characteristics of human eyes to observe things.\n",
      "The advantages of our method are as follows: (1) overcomes the color confusion\n",
      "problem between text and background caused by dirty data; (2) the texts\n",
      "generated are allowed to appear in most locations of any image, even across\n",
      "depths; (3) avoids analyzing the depth of background, such that the performance\n",
      "of our method exceeds the state-of-the-art methods; (4) the speed of generating\n",
      "images is fast, nearly one picture generated per three milliseconds. The\n",
      "effectiveness of our method is verified on several public datasets. \n",
      "\n",
      "\n",
      "Image-to-Image (I2I) translation is a heated topic in academia, and it also\n",
      "has been applied in real-world industry for tasks like image synthesis,\n",
      "super-resolution, and colorization. However, traditional I2I translation\n",
      "methods train data in two or more domains together. This requires lots of\n",
      "computation resources. Moreover, the results are of lower quality, and they\n",
      "contain many more artifacts. The training process could be unstable when the\n",
      "data in different domains are not balanced, and modal collapse is more likely\n",
      "to happen. We proposed a new I2I translation method that generates a new model\n",
      "in the target domain via a series of model transformations on a pre-trained\n",
      "StyleGAN2 model in the source domain. After that, we proposed an inversion\n",
      "method to achieve the conversion between an image and its latent vector. By\n",
      "feeding the latent vector into the generated model, we can perform I2I\n",
      "translation between the source domain and target domain. Both qualitative and\n",
      "quantitative evaluations were conducted to prove that the proposed method can\n",
      "achieve outstanding performance in terms of image quality, diversity and\n",
      "semantic similarity to the input and reference images compared to\n",
      "state-of-the-art works. \n",
      "\n",
      "\n",
      "GANs can generate photo-realistic images from the domain of their training\n",
      "data. However, those wanting to use them for creative purposes often want to\n",
      "generate imagery from a truly novel domain, a task which GANs are inherently\n",
      "unable to do. It is also desirable to have a level of control so that there is\n",
      "a degree of artistic direction rather than purely curation of random results.\n",
      "Here we present a method for interpolating between generative models of the\n",
      "StyleGAN architecture in a resolution dependent manner. This allows us to\n",
      "generate images from an entirely novel domain and do this with a degree of\n",
      "control over the nature of the output. \n",
      "\n",
      "\n",
      "Style transfer is the image synthesis task, which applies a style of one\n",
      "image to another while preserving the content. In statistical methods, the\n",
      "adaptive instance normalization (AdaIN) whitens the source images and applies\n",
      "the style of target images through normalizing the mean and variance of\n",
      "features. However, computing feature statistics for each instance would neglect\n",
      "the inherent relationship between features, so it is hard to learn global\n",
      "styles while fitting to the individual training dataset. In this paper, we\n",
      "present a novel learnable normalization technique for style transfer using\n",
      "graph convolutional networks, termed Graph Instance Normalization (GrIN). This\n",
      "algorithm makes the style transfer approach more robust by taking into account\n",
      "similar information shared between instances. Besides, this simple module is\n",
      "also applicable to other tasks like image-to-image translation or domain\n",
      "adaptation. \n",
      "\n",
      "\n",
      "Cm-wavelength radio continuum emission in excess of free-free, synchrotron\n",
      "and Rayleigh-Jeans dust emission (excess microwave emission, EME), and often\n",
      "called `anomalous microwave emission', is bright in molecular cloud regions\n",
      "exposed to UV radiation, i.e. in photo-dissociation regions (PDRs). The EME\n",
      "correlates with IR dust emission on degree angular scales. Resolved\n",
      "observations of well-studied PDRs are needed to compare the spectral variations\n",
      "of the cm-continuum with tracers of physical conditions and of the dust grain\n",
      "population. The EME is particularly bright in the regions of the rho Ophiuchi\n",
      "molecular cloud (rho Oph) that surround the earliest type star in the complex,\n",
      "HD 147889, where the peak signal stems from the filament known as the rho Oph-W\n",
      "PDR. Here we report on ATCA observations of rho Oph-W that resolve the width of\n",
      "the filament. We recover extended emission using a variant of non-parametric\n",
      "image synthesis performed in the sky plane. The multi-frequency 17 GHz to 39\n",
      "GHz mosaics reveal spectral variations in the cm-wavelength continuum. At ~30\n",
      "arcsec resolutions, the 17-20 GHz intensities follow tightly the mid-IR, Icm\n",
      "propto I(8 um), despite the breakdown of this correlation on larger scales.\n",
      "However, while the 33-39 GHz filament is parallel to IRAC 8 mum, it is offset\n",
      "by 15-20 arcsec towards the UV source. Such morphological differences in\n",
      "frequency reflect spectral variations, which we quantify spectroscopically as a\n",
      "sharp and steepening high-frequency cutoff, interpreted in terms of the\n",
      "spinning dust emission mechanism as a minimum grain size a_cutoff ~ 6 +- 1A\n",
      "that increases deeper into the PDR. \n",
      "\n",
      "\n",
      "Advances on signal, image and video generation underly major breakthroughs on\n",
      "generative medical imaging tasks, including Brain Image Synthesis. Still, the\n",
      "extent to which functional Magnetic Ressonance Imaging (fMRI) can be mapped\n",
      "from the brain electrophysiology remains largely unexplored. This work provides\n",
      "the first comprehensive view on how to use state-of-the-art principles from\n",
      "Neural Processing to synthesize fMRI data from electroencephalographic (EEG)\n",
      "data. Given the distinct spatiotemporal nature of haemodynamic and\n",
      "electrophysiological signals, this problem is formulated as the task of\n",
      "learning a mapping function between multivariate time series with highly\n",
      "dissimilar structures. A comparison of state-of-the-art synthesis approaches,\n",
      "including Autoencoders, Generative Adversarial Networks and Pairwise Learning,\n",
      "is undertaken. Results highlight the feasibility of EEG to fMRI brain image\n",
      "mappings, pinpointing the role of current advances in Machine Learning and\n",
      "showing the relevance of upcoming contributions to further improve performance.\n",
      "EEG to fMRI synthesis offers a way to enhance and augment brain image data, and\n",
      "guarantee access to more affordable, portable and long-lasting protocols of\n",
      "brain activity monitoring. The code used in this manuscript is available in\n",
      "Github and the datasets are open source. \n",
      "\n",
      "\n",
      "Rain often poses inevitable threats to deep neural network (DNN) based\n",
      "perception systems, and a comprehensive investigation of the potential risks of\n",
      "the rain to DNNs is of great importance. However, it is rather difficult to\n",
      "collect or synthesize rainy images that can represent all rain situations that\n",
      "would possibly occur in the real world. To this end, in this paper, we start\n",
      "from a new perspective and propose to combine two totally different studies,\n",
      "i.e., rainy image synthesis and adversarial attack. We first present an\n",
      "adversarial rain attack, with which we could simulate various rain situations\n",
      "with the guidance of deployed DNNs and reveal the potential threat factors that\n",
      "can be brought by rain. In particular, we design a factor-aware rain generation\n",
      "that synthesizes rain streaks according to the camera exposure process and\n",
      "models the learnable rain factors for adversarial attack. With this generator,\n",
      "we perform the adversarial rain attack against the image classification and\n",
      "object detection. To defend the DNNs from the negative rain effect, we also\n",
      "present a defensive deraining strategy, for which we design an adversarial rain\n",
      "augmentation that uses mixed adversarial rain layers to enhance deraining\n",
      "models for downstream DNN perception. Our large-scale evaluation on various\n",
      "datasets demonstrates that our synthesized rainy images with realistic\n",
      "appearances not only exhibit strong adversarial capability against DNNs, but\n",
      "also boost the deraining models for defensive purposes, building the foundation\n",
      "for further rain-robust perception studies. \n",
      "\n",
      "\n",
      "Recently, learning-based image synthesis has enabled to generate\n",
      "high-resolution images, either applying popular adversarial training or a\n",
      "powerful perceptual loss. However, it remains challenging to successfully\n",
      "leverage synthetic data for improving semantic segmentation with additional\n",
      "synthetic images. Therefore, we suggest to generate intermediate convolutional\n",
      "features and propose the first synthesis approach that is catered to such\n",
      "intermediate convolutional features. This allows us to generate new features\n",
      "from label masks and include them successfully into the training procedure in\n",
      "order to improve the performance of semantic segmentation. Experimental results\n",
      "and analysis on two challenging datasets Cityscapes and ADE20K show that our\n",
      "generated feature improves performance on segmentation tasks. \n",
      "\n",
      "\n",
      "An effective perception system is a fundamental component for farming robots,\n",
      "as it enables them to properly perceive the surrounding environment and to\n",
      "carry out targeted operations. The most recent methods make use of\n",
      "state-of-the-art machine learning techniques to learn a valid model for the\n",
      "target task. However, those techniques need a large amount of labeled data for\n",
      "training. A recent approach to deal with this issue is data augmentation\n",
      "through Generative Adversarial Networks (GANs), where entire synthetic scenes\n",
      "are added to the training data, thus enlarging and diversifying their\n",
      "informative content. In this work, we propose an alternative solution with\n",
      "respect to the common data augmentation methods, applying it to the fundamental\n",
      "problem of crop/weed segmentation in precision farming. Starting from real\n",
      "images, we create semi-artificial samples by replacing the most relevant object\n",
      "classes (i.e., crop and weeds) with their synthesized counterparts. To do that,\n",
      "we employ a conditional GAN (cGAN), where the generative model is trained by\n",
      "conditioning the shape of the generated object. Moreover, in addition to RGB\n",
      "data, we take into account also near-infrared (NIR) information, generating\n",
      "four channel multi-spectral synthetic images. Quantitative experiments, carried\n",
      "out on three publicly available datasets, show that (i) our model is capable of\n",
      "generating realistic multi-spectral images of plants and (ii) the usage of such\n",
      "synthetic images in the training process improves the segmentation performance\n",
      "of state-of-the-art semantic segmentation convolutional networks. \n",
      "\n",
      "\n",
      "In this paper, we focus on the semantic image synthesis task that aims at\n",
      "transferring semantic label maps to photo-realistic images. Existing methods\n",
      "lack effective semantic constraints to preserve the semantic information and\n",
      "ignore the structural correlations in both spatial and channel dimensions,\n",
      "leading to unsatisfactory blurry and artifact-prone results. To address these\n",
      "limitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize\n",
      "photo-realistic and semantically-consistent images with fine details from the\n",
      "input layouts without imposing extra training overhead or modifying the network\n",
      "architectures of existing methods. We also propose two novel modules, i.e.,\n",
      "position-wise Spatial Attention Module (SAM) and scale-wise Channel Attention\n",
      "Module (CAM), to capture semantic structure attention in spatial and channel\n",
      "dimensions, respectively. Specifically, SAM selectively correlates the pixels\n",
      "at each position by a spatial attention map, leading to pixels with the same\n",
      "semantic label being related to each other regardless of their spatial\n",
      "distances. Meanwhile, CAM selectively emphasizes the scale-wise features at\n",
      "each channel by a channel attention map, which integrates associated features\n",
      "among all channel maps regardless of their scales. We finally sum the outputs\n",
      "of SAM and CAM to further improve feature representation. Extensive experiments\n",
      "on four challenging datasets show that DAGAN achieves remarkably better results\n",
      "than state-of-the-art methods, while using fewer model parameters. The source\n",
      "code and trained models are available at https://github.com/Ha0Tang/DAGAN. \n",
      "\n",
      "\n",
      "We tackle a new problem of semantic view synthesis -- generating\n",
      "free-viewpoint rendering of a synthesized scene using a semantic label map as\n",
      "input. We build upon recent advances in semantic image synthesis and view\n",
      "synthesis for handling photographic image content generation and view\n",
      "extrapolation. Direct application of existing image/view synthesis methods,\n",
      "however, results in severe ghosting/blurry artifacts. To address the drawbacks,\n",
      "we propose a two-step approach. First, we focus on synthesizing the color and\n",
      "depth of the visible surface of the 3D scene. We then use the synthesized color\n",
      "and depth to impose explicit constraints on the multiple-plane image (MPI)\n",
      "representation prediction process. Our method produces sharp contents at the\n",
      "original view and geometrically consistent renderings across novel viewpoints.\n",
      "The experiments on numerous indoor and outdoor images show favorable results\n",
      "against several strong baselines and validate the effectiveness of our\n",
      "approach. \n",
      "\n",
      "\n",
      "While existing makeup style transfer models perform an image synthesis whose\n",
      "results cannot be explicitly controlled, the ability to modify makeup color\n",
      "continuously is a desirable property for virtual try-on applications. We\n",
      "propose a new formulation for the makeup style transfer task, with the\n",
      "objective to learn a color controllable makeup style synthesis. We introduce\n",
      "CA-GAN, a generative model that learns to modify the color of specific objects\n",
      "(e.g. lips or eyes) in the image to an arbitrary target color while preserving\n",
      "background. Since color labels are rare and costly to acquire, our method\n",
      "leverages weakly supervised learning for conditional GANs. This enables to\n",
      "learn a controllable synthesis of complex objects, and only requires a weak\n",
      "proxy of the image attribute that we desire to modify. Finally, we present for\n",
      "the first time a quantitative analysis of makeup style transfer and color\n",
      "control performance. \n",
      "\n",
      "\n",
      "Underwater image enhancement, as a pre-processing step to improve the\n",
      "accuracy of the following object detection task, has drawn considerable\n",
      "attention in the field of underwater navigation and ocean exploration. However,\n",
      "most of the existing underwater image enhancement strategies tend to consider\n",
      "enhancement and detection as two independent modules with no interaction, and\n",
      "the practice of separate optimization does not always help the underwater\n",
      "object detection task. In this paper, we propose two perceptual enhancement\n",
      "models, each of which uses a deep enhancement model with a detection perceptor.\n",
      "The detection perceptor provides coherent information in the form of gradients\n",
      "to the enhancement model, guiding the enhancement model to generate patch level\n",
      "visually pleasing images or detection favourable images. In addition, due to\n",
      "the lack of training data, a hybrid underwater image synthesis model, which\n",
      "fuses physical priors and data-driven cues, is proposed to synthesize training\n",
      "data and generalise our enhancement model for real-world underwater images.\n",
      "Experimental results show the superiority of our proposed method over several\n",
      "state-of-the-art methods on both real-world and synthetic underwater datasets. \n",
      "\n",
      "\n",
      "Estimating future events is a difficult task. Unlike humans, machine learning\n",
      "approaches are not regularized by a natural understanding of physics. In the\n",
      "wild, a plausible succession of events is governed by the rules of causality,\n",
      "which cannot easily be derived from a finite training set. In this paper we\n",
      "propose a novel theoretical framework to perform causal future prediction by\n",
      "embedding spatiotemporal information on a Minkowski space-time. We utilize the\n",
      "concept of a light cone from special relativity to restrict and traverse the\n",
      "latent space of an arbitrary model. We demonstrate successful applications in\n",
      "causal image synthesis and future video frame prediction on a dataset of\n",
      "images. Our framework is architecture- and task-independent and comes with\n",
      "strong theoretical guarantees of causal capabilities. \n",
      "\n",
      "\n",
      "Content creation, central to applications such as virtual reality, can be a\n",
      "tedious and time-consuming. Recent image synthesis methods simplify this task\n",
      "by offering tools to generate new views from as little as a single input image,\n",
      "or by converting a semantic map into a photorealistic image. We propose to push\n",
      "the envelope further, and introduce Generative View Synthesis (GVS), which can\n",
      "synthesize multiple photorealistic views of a scene given a single semantic\n",
      "map. We show that the sequential application of existing techniques, e.g.,\n",
      "semantics-to-image translation followed by monocular view synthesis, fail at\n",
      "capturing the scene's structure. In contrast, we solve the semantics-to-image\n",
      "translation in concert with the estimation of the 3D layout of the scene, thus\n",
      "producing geometrically consistent novel views that preserve semantic\n",
      "structures. We first lift the input 2D semantic map onto a 3D layered\n",
      "representation of the scene in feature space, thereby preserving the semantic\n",
      "labels of 3D geometric structures. We then project the layered features onto\n",
      "the target views to generate the final novel-view images. We verify the\n",
      "strengths of our method and compare it with several advanced baselines on three\n",
      "different datasets. Our approach also allows for style manipulation and image\n",
      "editing operations, such as the addition or removal of objects, with simple\n",
      "manipulations of the input style images and semantic maps respectively. Visit\n",
      "the project page at https://gvsnet.github.io. \n",
      "\n",
      "\n",
      "Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the\n",
      "developing brain but is not suitable for second-trimester anomaly screening,\n",
      "for which ultrasound (US) is employed. Although expert sonographers are adept\n",
      "at reading US images, MR images which closely resemble anatomical images are\n",
      "much easier for non-experts to interpret. Thus in this paper we propose to\n",
      "generate MR-like images directly from clinical US images. In medical image\n",
      "analysis such a capability is potentially useful as well, for instance for\n",
      "automatic US-MRI registration and fusion. The proposed model is end-to-end\n",
      "trainable and self-supervised without any external annotations. Specifically,\n",
      "based on an assumption that the US and MRI data share a similar anatomical\n",
      "latent space, we first utilise a network to extract the shared latent features,\n",
      "which are then used for MRI synthesis. Since paired data is unavailable for our\n",
      "study (and rare in practice), pixel-level constraints are infeasible to apply.\n",
      "We instead propose to enforce the distributions to be statistically\n",
      "indistinguishable, by adversarial learning in both the image domain and feature\n",
      "space. To regularise the anatomical structures between US and MRI during\n",
      "synthesis, we further propose an adversarial structural constraint. A new\n",
      "cross-modal attention technique is proposed to utilise non-local spatial\n",
      "information, by encouraging multi-modal knowledge fusion and propagation. We\n",
      "extend the approach to consider the case where 3D auxiliary information (e.g.,\n",
      "3D neighbours and a 3D location index) from volumetric data is also available,\n",
      "and show that this improves image synthesis. The proposed approach is evaluated\n",
      "quantitatively and qualitatively with comparison to real fetal MR images and\n",
      "other approaches to synthesis, demonstrating its feasibility of synthesising\n",
      "realistic MR images. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have long been used to understand the\n",
      "semantic relationship between the text and image. However, there are problems\n",
      "with mode collapsing in the image generation that causes some preferred output\n",
      "modes. Our aim is to improve the training of the network by using a specialized\n",
      "mode-seeking loss function to avoid this issue. In the text to image synthesis,\n",
      "our loss function differentiates two points in latent space for the generation\n",
      "of distinct images. We validate our model on the Caltech Birds (CUB) dataset\n",
      "and the Microsoft COCO dataset by changing the intensity of the loss function\n",
      "during the training. Experimental results demonstrate that our model works very\n",
      "well compared to some state-of-the-art approaches. \n",
      "\n",
      "\n",
      "Many object pose estimation algorithms rely on the analysis-by-synthesis\n",
      "framework which requires explicit representations of individual object\n",
      "instances. In this paper we combine a gradient-based fitting procedure with a\n",
      "parametric neural image synthesis module that is capable of implicitly\n",
      "representing the appearance, shape and pose of entire object categories, thus\n",
      "rendering the need for explicit CAD models per object instance unnecessary. The\n",
      "image synthesis network is designed to efficiently span the pose configuration\n",
      "space so that model capacity can be used to capture the shape and local\n",
      "appearance (i.e., texture) variations jointly. At inference time the\n",
      "synthesized images are compared to the target via an appearance based loss and\n",
      "the error signal is backpropagated through the network to the input parameters.\n",
      "Keeping the network parameters fixed, this allows for iterative optimization of\n",
      "the object pose, shape and appearance in a joint manner and we experimentally\n",
      "show that the method can recover orientation of objects with high accuracy from\n",
      "2D images alone. When provided with depth measurements, to overcome scale\n",
      "ambiguities, the method can accurately recover the full 6DOF pose successfully. \n",
      "\n",
      "\n",
      "Despite recent advances in deep learning-based face frontalization methods,\n",
      "photo-realistic and illumination preserving frontal face synthesis is still\n",
      "challenging due to large pose and illumination discrepancy during training. We\n",
      "propose a novel Flow-based Feature Warping Model (FFWM) which can learn to\n",
      "synthesize photo-realistic and illumination preserving frontal images with\n",
      "illumination inconsistent supervision. Specifically, an Illumination Preserving\n",
      "Module (IPM) is proposed to learn illumination preserving image synthesis from\n",
      "illumination inconsistent image pairs. IPM includes two pathways which\n",
      "collaborate to ensure the synthesized frontal images are illumination\n",
      "preserving and with fine details. Moreover, a Warp Attention Module (WAM) is\n",
      "introduced to reduce the pose discrepancy in the feature level, and hence to\n",
      "synthesize frontal images more effectively and preserve more details of profile\n",
      "images. The attention mechanism in WAM helps reduce the artifacts caused by the\n",
      "displacements between the profile and the frontal images. Quantitative and\n",
      "qualitative experimental results show that our FFWM can synthesize\n",
      "photo-realistic and illumination preserving frontal images and performs\n",
      "favorably against the state-of-the-art results. \n",
      "\n",
      "\n",
      "Synthesizing high-quality realistic images from text descriptions is a\n",
      "challenging task. Existing text-to-image Generative Adversarial Networks\n",
      "generally employ a stacked architecture as the backbone yet still remain three\n",
      "flaws. First, the stacked architecture introduces the entanglements between\n",
      "generators of different image scales. Second, existing studies prefer to apply\n",
      "and fix extra networks in adversarial learning for text-image semantic\n",
      "consistency, which limits the supervision capability of these networks. Third,\n",
      "the cross-modal attention-based text-image fusion that widely adopted by\n",
      "previous works is limited on several special image scales because of the\n",
      "computational cost. To these ends, we propose a simpler but more effective Deep\n",
      "Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose:\n",
      "(i) a novel one-stage text-to-image backbone that directly synthesizes\n",
      "high-resolution images without entanglements between different generators, (ii)\n",
      "a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty\n",
      "and One-Way Output, which enhances the text-image semantic consistency without\n",
      "introducing extra networks, (iii) a novel deep text-image fusion block, which\n",
      "deepens the fusion process to make a full fusion between text and visual\n",
      "features. Compared with current state-of-the-art methods, our proposed DF-GAN\n",
      "is simpler but more efficient to synthesize realistic and text-matching images\n",
      "and achieves better performance on widely used datasets. \n",
      "\n",
      "\n",
      "Deep learning motivated by convolutional neural networks has been highly\n",
      "successful in a range of medical imaging problems like image classification,\n",
      "image segmentation, image synthesis etc. However for validation and\n",
      "interpretability, not only do we need the predictions made by the model but\n",
      "also how confident it is while making those predictions. This is important in\n",
      "safety critical applications for the people to accept it. In this work, we used\n",
      "an encoder decoder architecture based on variational inference techniques for\n",
      "segmenting brain tumour images. We evaluate our work on the publicly available\n",
      "BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\n",
      "Union (IOU) as the evaluation metrics. Our model is able to segment brain\n",
      "tumours while taking into account both aleatoric uncertainty and epistemic\n",
      "uncertainty in a principled bayesian manner. \n",
      "\n",
      "\n",
      "The success of supervised lesion segmentation algorithms using Computed\n",
      "Tomography (CT) exams depends significantly on the quantity and variability of\n",
      "samples available for training. While annotating such data constitutes a\n",
      "challenge itself, the variability of lesions in the dataset also depends on the\n",
      "prevalence of different types of lesions. This phenomenon adds an inherent bias\n",
      "to lesion segmentation algorithms that can be diminished, among different\n",
      "possibilities, using aggressive data augmentation methods. In this paper, we\n",
      "present a method for implanting realistic lesions in CT slices to provide a\n",
      "rich and controllable set of training samples and ultimately improving semantic\n",
      "segmentation network performances for delineating lesions in CT exams. Our\n",
      "results show that implanting synthetic lesions not only improves (up to around\n",
      "12\\%) the segmentation performance considering different architectures but also\n",
      "that this improvement is consistent among different image synthesis networks.\n",
      "We conclude that increasing the variability of lesions synthetically in terms\n",
      "of size, density, shape, and position seems to improve the performance of\n",
      "segmentation models for liver lesion segmentation in CT slices. \n",
      "\n",
      "\n",
      "We propose a novel framework to produce cartoon videos by fetching the color\n",
      "information from two input keyframes while following the animated motion guided\n",
      "by a user sketch. The key idea of the proposed approach is to estimate the\n",
      "dense cross-domain correspondence between the sketch and cartoon video frames,\n",
      "and employ a blending module with occlusion estimation to synthesize the middle\n",
      "frame guided by the sketch. After that, the input frames and the synthetic\n",
      "frame equipped with established correspondence are fed into an arbitrary-time\n",
      "frame interpolation pipeline to generate and refine additional inbetween\n",
      "frames. Finally, a module to preserve temporal consistency is employed.\n",
      "Compared to common frame interpolation methods, our approach can address frames\n",
      "with relatively large motion and also has the flexibility to enable users to\n",
      "control the generated video sequences by editing the sketch guidance. By\n",
      "explicitly considering the correspondence between frames and the sketch, we can\n",
      "achieve higher quality results than other image synthesis methods. Our results\n",
      "show that our system generalizes well to different movie frames, achieving\n",
      "better results than existing solutions. \n",
      "\n",
      "\n",
      "Fast Style Transfer is a series of Neural Style Transfer algorithms that use\n",
      "feed-forward neural networks to render input images. Because of the high\n",
      "dimension of the output layer, these networks require much memory for\n",
      "computation. Therefore, for high-resolution images, most mobile devices and\n",
      "personal computers cannot stylize them, which greatly limits the application\n",
      "scenarios of Fast Style Transfer. At present, the two existing solutions are\n",
      "purchasing more memory and using the feathering-based method, but the former\n",
      "requires additional cost, and the latter has poor image quality. To solve this\n",
      "problem, we propose a novel image synthesis method named \\emph{block shuffle},\n",
      "which converts a single task with high memory consumption to multiple subtasks\n",
      "with low memory consumption. This method can act as a plug-in for Fast Style\n",
      "Transfer without any modification to the network architecture. We use the most\n",
      "popular Fast Style Transfer repository on GitHub as the baseline. Experiments\n",
      "show that the quality of high-resolution images generated by our method is\n",
      "better than that of the feathering-based method. Although our method is an\n",
      "order of magnitude slower than the baseline, it can stylize high-resolution\n",
      "images with limited memory, which is impossible with the baseline. The code and\n",
      "models will be made available on \\url{https://github.com/czczup/block-shuffle}. \n",
      "\n",
      "\n",
      "Self-supervised depth estimators have recently shown results comparable to\n",
      "the supervised methods on the challenging single image depth estimation (SIDE)\n",
      "task, by exploiting the geometrical relations between target and reference\n",
      "views in the training data. However, previous methods usually learn forward or\n",
      "backward image synthesis, but not depth estimation, as they cannot effectively\n",
      "neglect occlusions between the target and the reference images. Previous works\n",
      "rely on rigid photometric assumptions or the SIDE network to infer depth and\n",
      "occlusions, resulting in limited performance. On the other hand, we propose a\n",
      "method to \"Forget About the LiDAR\" (FAL), for the training of depth estimators,\n",
      "with Mirrored Exponential Disparity (MED) probability volumes, from which we\n",
      "obtain geometrically inspired occlusion maps with our novel Mirrored Occlusion\n",
      "Module (MOM). Our MOM does not impose a burden on our FAL-net. Contrary to the\n",
      "previous methods that learn SIDE from stereo pairs by regressing disparity in\n",
      "the linear space, our FAL-net regresses disparity by binning it into the\n",
      "exponential space, which allows for better detection of distant and nearby\n",
      "objects. We define a two-step training strategy for our FAL-net: It is first\n",
      "trained for view synthesis and then fine-tuned for depth estimation with our\n",
      "MOM. Our FAL-net is remarkably light-weight and outperforms the previous\n",
      "state-of-the-art methods with 8x fewer parameters and 3x faster inference\n",
      "speeds on the challenging KITTI dataset. We present extensive experimental\n",
      "results on the KITTI, CityScapes, and Make3D datasets to verify our method's\n",
      "effectiveness. To the authors' best knowledge, the presented method performs\n",
      "the best among all the previous self-supervised methods until now. \n",
      "\n",
      "\n",
      "Multimodal image-to-image translation (I2IT) aims to learn a conditional\n",
      "distribution that explores multiple possible images in the target domain given\n",
      "an input image in the source domain. Conditional generative adversarial\n",
      "networks (cGANs) are often adopted for modeling such a conditional\n",
      "distribution. However, cGANs are prone to ignore the latent code and learn a\n",
      "unimodal distribution in conditional image synthesis, which is also known as\n",
      "the mode collapse issue of GANs. To solve the problem, we propose a simple yet\n",
      "effective method that explicitly estimates and maximizes the mutual information\n",
      "between the latent code and the output image in cGANs by using a deep mutual\n",
      "information neural estimator in this paper. Maximizing the mutual information\n",
      "strengthens the statistical dependency between the latent code and the output\n",
      "image, which prevents the generator from ignoring the latent code and\n",
      "encourages cGANs to fully utilize the latent code for synthesizing diverse\n",
      "results. Our method not only provides a new perspective from information theory\n",
      "to improve diversity for I2IT but also achieves disentanglement between the\n",
      "source domain content and the target domain style for free. \n",
      "\n",
      "\n",
      "A core problem in cognitive science and machine learning is to understand how\n",
      "humans derive semantic representations from perceptual objects, such as color\n",
      "from an apple, pleasantness from a musical chord, or seriousness from a face.\n",
      "Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying\n",
      "such representations, in which participants are presented with binary choice\n",
      "trials constructed such that the decisions follow a Markov Chain Monte Carlo\n",
      "acceptance rule. However, while MCMCP has strong asymptotic properties, its\n",
      "binary choice paradigm generates relatively little information per trial, and\n",
      "its local proposal function makes it slow to explore the parameter space and\n",
      "find the modes of the distribution. Here we therefore generalize MCMCP to a\n",
      "continuous-sampling paradigm, where in each iteration the participant uses a\n",
      "slider to continuously manipulate a single stimulus dimension to optimize a\n",
      "given criterion such as 'pleasantness'. We formulate both methods from a\n",
      "utility-theory perspective, and show that the new method can be interpreted as\n",
      "'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation\n",
      "parameter to the transition step, and show that this parameter can be\n",
      "manipulated to flexibly shift between Gibbs sampling and deterministic\n",
      "optimization. In an initial study, we show GSP clearly outperforming MCMCP; we\n",
      "then show that GSP provides novel and interpretable results in three other\n",
      "domains, namely musical chords, vocal emotions, and faces. We validate these\n",
      "results through large-scale perceptual rating experiments. The final\n",
      "experiments use GSP to navigate the latent space of a state-of-the-art image\n",
      "synthesis network (StyleGAN), a promising approach for applying GSP to\n",
      "high-dimensional perceptual spaces. We conclude by discussing future cognitive\n",
      "applications and ethical implications. \n",
      "\n",
      "\n",
      "Driving simulators play a large role in developing and testing new\n",
      "intelligent vehicle systems. The visual fidelity of the simulation is critical\n",
      "for building vision-based algorithms and conducting human driver experiments.\n",
      "Low visual fidelity breaks immersion for human-in-the-loop driving experiments.\n",
      "Conventional computer graphics pipelines use detailed 3D models, meshes,\n",
      "textures, and rendering engines to generate 2D images from 3D scenes. These\n",
      "processes are labor-intensive, and they do not generate photorealistic imagery.\n",
      "Here we introduce a hybrid generative neural graphics pipeline for improving\n",
      "the visual fidelity of driving simulations. Given a 3D scene, we partially\n",
      "render only important objects of interest, such as vehicles, and use generative\n",
      "adversarial processes to synthesize the background and the rest of the image.\n",
      "To this end, we propose a novel image formation strategy to form 2D semantic\n",
      "images from 3D scenery consisting of simple object models without textures.\n",
      "These semantic images are then converted into photorealistic RGB images with a\n",
      "state-of-the-art Generative Adversarial Network (GAN) trained on real-world\n",
      "driving scenes. This replaces repetitiveness with randomly generated but\n",
      "photorealistic surfaces. Finally, the partially-rendered and GAN synthesized\n",
      "images are blended with a blending GAN. We show that the photorealism of images\n",
      "generated with the proposed method is more similar to real-world driving\n",
      "datasets such as Cityscapes and KITTI than conventional approaches. This\n",
      "comparison is made using semantic retention analysis and Frechet Inception\n",
      "Distance (FID) measurements. \n",
      "\n",
      "\n",
      "In image-to-image translation, each patch in the output should reflect the\n",
      "content of the corresponding patch in the input, independent of domain. We\n",
      "propose a straightforward method for doing so -- maximizing mutual information\n",
      "between the two, using a framework based on contrastive learning. The method\n",
      "encourages two elements (corresponding patches) to map to a similar point in a\n",
      "learned feature space, relative to other elements (other patches) in the\n",
      "dataset, referred to as negatives. We explore several critical design choices\n",
      "for making contrastive learning effective in the image synthesis setting.\n",
      "Notably, we use a multilayer, patch-based approach, rather than operate on\n",
      "entire images. Furthermore, we draw negatives from within the input image\n",
      "itself, rather than from the rest of the dataset. We demonstrate that our\n",
      "framework enables one-sided translation in the unpaired image-to-image\n",
      "translation setting, while improving quality and reducing training time. In\n",
      "addition, our method can even be extended to the training setting where each\n",
      "\"domain\" is only a single image. \n",
      "\n",
      "\n",
      "We present a novel high-fidelity real-time neural vocoder called VocGAN. A\n",
      "recently developed GAN-based vocoder, MelGAN, produces speech waveforms in\n",
      "real-time. However, it often produces a waveform that is insufficient in\n",
      "quality or inconsistent with acoustic characteristics of the input mel\n",
      "spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves\n",
      "the quality and consistency of the output waveform. VocGAN applies a\n",
      "multi-scale waveform generator and a hierarchically-nested discriminator to\n",
      "learn multiple levels of acoustic properties in a balanced way. It also applies\n",
      "the joint conditional and unconditional objective, which has shown successful\n",
      "results in high-resolution image synthesis. In experiments, VocGAN synthesizes\n",
      "speech waveforms 416.7x faster on a GTX 1080Ti GPU and 3.24x faster on a CPU\n",
      "than real-time. Compared with MelGAN, it also exhibits significantly improved\n",
      "quality in multiple evaluation metrics including mean opinion score (MOS) with\n",
      "minimal additional overhead. Additionally, compared with Parallel WaveGAN,\n",
      "another recently developed high-fidelity vocoder, VocGAN is 6.98x faster on a\n",
      "CPU and exhibits higher MOS. \n",
      "\n",
      "\n",
      "Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has\n",
      "spread rapidly since December 2019. Real-time reverse transcription polymerase\n",
      "chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play\n",
      "an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits\n",
      "of quick reporting, a low cost, and high sensitivity for the detection of\n",
      "pulmonary infection. Recently, deep-learning-based computer vision methods have\n",
      "demonstrated great promise for use in medical imaging applications, including\n",
      "X-rays, magnetic resonance imaging, and CT imaging. However, training a\n",
      "deep-learning model requires large volumes of data, and medical staff faces a\n",
      "high risk when collecting COVID-19 CT data due to the high infectivity of the\n",
      "disease. Another issue is the lack of experts available for data labeling. In\n",
      "order to meet the data requirements for COVID-19 CT imaging, we propose a CT\n",
      "image synthesis approach based on a conditional generative adversarial network\n",
      "that can effectively generate high-quality and realistic COVID-19 CT images for\n",
      "use in deep-learning-based medical imaging tasks. Experimental results show\n",
      "that the proposed method outperforms other state-of-the-art image synthesis\n",
      "methods with the generated COVID-19 CT images and indicates promising for\n",
      "various machine learning applications including semantic segmentation and\n",
      "classification. \n",
      "\n",
      "\n",
      "Though GAN (Generative Adversarial Networks) based technique has greatly\n",
      "advanced the performance of image synthesis and face translation, only few\n",
      "works available in literature provide region based style encoding and\n",
      "translation. We propose in this paper a region-wise normalization framework,\n",
      "for region level face translation. While per-region style is encoded using\n",
      "available approach, we build a so called RIN (region-wise normalization) block\n",
      "to individually inject the styles into per-region feature maps and then fuse\n",
      "them for following convolution and upsampling. Both shape and texture of\n",
      "different regions can thus be translated to various target styles. A region\n",
      "matching loss has also been proposed to significantly reduce the inference\n",
      "between regions during the translation process. Extensive experiments on three\n",
      "publicly available datasets, i.e. Morph, RaFD and CelebAMask-HQ, suggest that\n",
      "our approach demonstrate a large improvement over state-of-the-art methods like\n",
      "StarGAN, SEAN and FUNIT. Our approach has further advantages in precise control\n",
      "of the regions to be translated. As a result, region level expression changes\n",
      "and step by step make up can be achieved. The video demo is available at\n",
      "https://youtu.be/ceRqsbzXAfk. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) have provided promising data\n",
      "enrichment solutions by synthesizing high-fidelity images. However, generating\n",
      "large sets of labeled images with new anatomical variations remains unexplored.\n",
      "We propose a novel method for synthesizing cardiac magnetic resonance (CMR)\n",
      "images on a population of virtual subjects with a large anatomical variation,\n",
      "introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human\n",
      "phantom. We investigate two conditional image synthesis approaches grounded on\n",
      "a semantically-consistent mask-guided image generation technique: 4-class and\n",
      "8-class XCAT-GANs. The 4-class technique relies on only the annotations of the\n",
      "heart; while the 8-class technique employs a predicted multi-tissue label map\n",
      "of the heart-surrounding organs and provides better guidance for our\n",
      "conditional image synthesis. For both techniques, we train our conditional\n",
      "XCAT-GAN with real images paired with corresponding labels and subsequently at\n",
      "the inference time, we substitute the labels with the XCAT derived ones.\n",
      "Therefore, the trained network accurately transfers the tissue-specific\n",
      "textures to the new label maps. By creating 33 virtual subjects of synthetic\n",
      "CMR images at the end-diastolic and end-systolic phases, we evaluate the\n",
      "usefulness of such data in the downstream cardiac cavity segmentation task\n",
      "under different augmentation strategies. Results demonstrate that even with\n",
      "only 20% of real images (40 volumes) seen during training, segmentation\n",
      "performance is retained with the addition of synthetic CMR images. Moreover,\n",
      "the improvement in utilizing synthetic images for augmenting the real data is\n",
      "evident through the reduction of Hausdorff distance up to 28% and an increase\n",
      "in the Dice score up to 5%, indicating a higher similarity to the ground truth\n",
      "in all dimensions. \n",
      "\n",
      "\n",
      "We introduce a simple and versatile framework for image-to-image translation.\n",
      "We unearth the importance of normalization layers, and provide a carefully\n",
      "designed two-stream generative model with newly proposed feature\n",
      "transformations in a coarse-to-fine fashion. This allows multi-scale semantic\n",
      "structure information and style representation to be effectively captured and\n",
      "fused by the network, permitting our method to scale to various tasks in both\n",
      "unsupervised and supervised settings. No additional constraints (e.g., cycle\n",
      "consistency) are needed, contributing to a very clean and simple method.\n",
      "Multi-modal image synthesis with arbitrary style control is made possible. A\n",
      "systematic study compares the proposed method with several state-of-the-art\n",
      "task-specific baselines, verifying its effectiveness in both perceptual quality\n",
      "and quantitative evaluations. \n",
      "\n",
      "\n",
      "In this paper, we aim to address the problem of heterogeneous or\n",
      "cross-spectral face recognition using machine learning to synthesize visual\n",
      "spectrum face from infrared images. The synthesis of visual-band face images\n",
      "allows for more optimal extraction of facial features to be used for face\n",
      "identification and/or verification. We explore the ability to use Generative\n",
      "Adversarial Networks (GANs) for face image synthesis, and examine the\n",
      "performance of these images using pre-trained Convolutional Neural Networks\n",
      "(CNNs). The features extracted using CNNs are applied in face identification\n",
      "and verification. We explore the performance in terms of acceptance rate when\n",
      "using various similarity measures for face verification. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have recently advanced image synthesis\n",
      "by learning the underlying distribution of the observed data. However, how the\n",
      "features learned from solving the task of image generation are applicable to\n",
      "other vision tasks remains seldom explored. In this work, we show that learning\n",
      "to synthesize images can bring remarkable hierarchical visual features that are\n",
      "generalizable across a wide range of applications. Specifically, we consider\n",
      "the pre-trained StyleGAN generator as a learned loss function and utilize its\n",
      "layer-wise representation to train a novel hierarchical encoder. The visual\n",
      "feature produced by our encoder, termed as Generative Hierarchical Feature\n",
      "(GH-Feat), has strong transferability to both generative and discriminative\n",
      "tasks, including image editing, image harmonization, image classification, face\n",
      "verification, landmark detection, and layout prediction. Extensive qualitative\n",
      "and quantitative experimental results demonstrate the appealing performance of\n",
      "GH-Feat. \n",
      "\n",
      "\n",
      "With the growing technology of photon-counting detectors (PCD), spectral CT\n",
      "is a widely concerned topic which has the potential of material\n",
      "differentiation. However, due to some non-ideal factors such as cross talk and\n",
      "pulse pile-up of the detectors, direct reconstruction from detected spectrum\n",
      "without any corrections will get a wrong result. Conventional methods try to\n",
      "model these factors using calibration and make corrections accordingly, but\n",
      "depend on the preciseness of the model. To solve this problem, in this paper,\n",
      "we proposed a novel deep learning-based monochromatic image synthesis method\n",
      "working in sinogram domain. Different from previous deep learning-based methods\n",
      "aimed at this problem, we designed a novel network architecture according to\n",
      "the physical model of cross talk, and it can solve this problem better in an\n",
      "ingenious way. Our method was tested on a cone-beam CT (CBCT) system equipped\n",
      "with a PCD. After using FDK algorithm on the corrected projection, we got quite\n",
      "more accurate results with less noise, which showed the feasibility of\n",
      "monochromatic image synthesis by our method. \n",
      "\n",
      "\n",
      "Learning to generate natural scenes has always been a daunting task in\n",
      "computer vision. This is even more laborious when generating images with very\n",
      "different views. When the views are very different, the view fields have little\n",
      "overlap or objects are occluded, leading the task very challenging. In this\n",
      "paper, we propose to use Generative Adversarial Networks(GANs) based on a\n",
      "deformable convolution and attention mechanism to solve the problem of\n",
      "cross-view image synthesis (see Fig.1). It is difficult to understand and\n",
      "transform scenes appearance and semantic information from another view, thus we\n",
      "use deformed convolution in the U-net network to improve the network's ability\n",
      "to extract features of objects at different scales. Moreover, to better learn\n",
      "the correspondence between images from different views, we apply an attention\n",
      "mechanism to refine the intermediate feature map thus generating more realistic\n",
      "images. A large number of experiments on different size images on the Dayton\n",
      "dataset[1] show that our model can produce better results than state-of-the-art\n",
      "methods. \n",
      "\n",
      "\n",
      "Image generation from scene description is a cornerstone technique for the\n",
      "controlled generation, which is beneficial to applications such as content\n",
      "creation and image editing. In this work, we aim to synthesize images from\n",
      "scene description with retrieved patches as reference. We propose a\n",
      "differentiable retrieval module. With the differentiable retrieval module, we\n",
      "can (1) make the entire pipeline end-to-end trainable, enabling the learning of\n",
      "better feature embedding for retrieval; (2) encourage the selection of mutually\n",
      "compatible patches with additional objective functions. We conduct extensive\n",
      "quantitative and qualitative experiments to demonstrate that the proposed\n",
      "method can generate realistic and diverse images, where the retrieved patches\n",
      "are reasonable and mutually compatible. \n",
      "\n",
      "\n",
      "Flexible user controls are desirable for content creation and image editing.\n",
      "A semantic map is commonly used intermediate representation for conditional\n",
      "image generation. Compared to the operation on raw RGB pixels, the semantic map\n",
      "enables simpler user modification. In this work, we specifically target at\n",
      "generating semantic maps given a label-set consisting of desired categories.\n",
      "The proposed framework, SegVAE, synthesizes semantic maps in an iterative\n",
      "manner using conditional variational autoencoder. Quantitative and qualitative\n",
      "experiments demonstrate that the proposed model can generate realistic and\n",
      "diverse semantic maps. We also apply an off-the-shelf image-to-image\n",
      "translation model to generate realistic RGB images to better understand the\n",
      "quality of the synthesized semantic maps. Furthermore, we showcase several\n",
      "real-world image-editing applications including object removal, object\n",
      "insertion, and object replacement. \n",
      "\n",
      "\n",
      "It is an innate ability for humans to imagine something only according to\n",
      "their impression, without having to memorize all the details of what they have\n",
      "seen. In this work, we would like to demonstrate that a trained convolutional\n",
      "neural network also has the capability to \"remember\" its input images. To\n",
      "achieve this, we propose a simple but powerful framework to establish an\n",
      "{\\emph{Impression Space}} upon an off-the-shelf pretrained network. This\n",
      "network is referred to as the {\\emph{Template Network}} because its filters\n",
      "will be used as templates to reconstruct images from the impression. In our\n",
      "framework, the impression space and image space are bridged by a layer-wise\n",
      "encoding and iterative decoding process. It turns out that the impression space\n",
      "indeed captures the salient features from images, and it can be directly\n",
      "applied to tasks such as unpaired image translation and image synthesis through\n",
      "impression matching without further network training. Furthermore, the\n",
      "impression naturally constructs a high-level common space for different data.\n",
      "Based on this, we propose a mechanism to model the data relations inside the\n",
      "impression space, which is able to reveal the feature similarity between\n",
      "images. Our code will be released. \n",
      "\n",
      "\n",
      "While Generative Adversarial Networks (GANs) are fundamental to many\n",
      "generative modelling applications, they suffer from numerous issues. In this\n",
      "work, we propose a principled framework to simultaneously mitigate two\n",
      "fundamental issues in GANs: catastrophic forgetting of the discriminator and\n",
      "mode collapse of the generator. We achieve this by employing for GANs a\n",
      "contrastive learning and mutual information maximization approach, and perform\n",
      "extensive analyses to understand sources of improvements. Our approach\n",
      "significantly stabilizes GAN training and improves GAN performance for image\n",
      "synthesis across five datasets under the same training and evaluation\n",
      "conditions against state-of-the-art works. In particular, compared to the\n",
      "state-of-the-art SSGAN, our approach does not suffer from poorer performance on\n",
      "image domains such as faces, and instead improves performance significantly.\n",
      "Our approach is simple to implement and practical: it involves only one\n",
      "auxiliary objective, has a low computational cost, and performs robustly across\n",
      "a wide range of training settings and datasets without any hyperparameter\n",
      "tuning. For reproducibility, our code is available in Mimicry:\n",
      "https://github.com/kwotsin/mimicry. \n",
      "\n",
      "\n",
      "Ischemic stroke lesion segmentation from Computed Tomography Perfusion (CTP)\n",
      "images is important for accurate diagnosis of stroke in acute care units.\n",
      "However, it is challenged by low image contrast and resolution of the perfusion\n",
      "parameter maps, in addition to the complex appearance of the lesion. To deal\n",
      "with this problem, we propose a novel framework based on synthesized pseudo\n",
      "Diffusion-Weighted Imaging (DWI) from perfusion parameter maps to obtain better\n",
      "image quality for more accurate segmentation. Our framework consists of three\n",
      "components based on Convolutional Neural Networks (CNNs) and is trained\n",
      "end-to-end. First, a feature extractor is used to obtain both a low-level and\n",
      "high-level compact representation of the raw spatiotemporal Computed Tomography\n",
      "Angiography (CTA) images. Second, a pseudo DWI generator takes as input the\n",
      "concatenation of CTP perfusion parameter maps and our extracted features to\n",
      "obtain the synthesized pseudo DWI. To achieve better synthesis quality, we\n",
      "propose a hybrid loss function that pays more attention to lesion regions and\n",
      "encourages high-level contextual consistency. Finally, we segment the lesion\n",
      "region from the synthesized pseudo DWI, where the segmentation network is based\n",
      "on switchable normalization and channel calibration for better performance.\n",
      "Experimental results showed that our framework achieved the top performance on\n",
      "ISLES 2018 challenge and: 1) our method using synthesized pseudo DWI\n",
      "outperformed methods segmenting the lesion from perfusion parameter maps\n",
      "directly; 2) the feature extractor exploiting additional spatiotemporal CTA\n",
      "images led to better synthesized pseudo DWI quality and higher segmentation\n",
      "accuracy; and 3) the proposed loss functions and network structure improved the\n",
      "pseudo DWI synthesis and lesion segmentation performance. \n",
      "\n",
      "\n",
      "Generative models are undoubtedly a hot topic in Artificial Intelligence,\n",
      "among which the most common type is Generative Adversarial Networks (GANs).\n",
      "These architectures let one synthesise artificial datasets by implicitly\n",
      "modelling the underlying probability distribution of a real-world training\n",
      "dataset. With the introduction of Conditional GANs and their variants, these\n",
      "methods were extended to generating samples conditioned on ancillary\n",
      "information available for each sample within the dataset. From a practical\n",
      "standpoint, however, one might desire to generate data conditioned on partial\n",
      "information. That is, only a subset of the ancillary conditioning variables\n",
      "might be of interest when synthesising data. In this work, we argue that\n",
      "standard Conditional GANs are not suitable for such a task and propose a new\n",
      "Adversarial Network architecture and training strategy to deal with the ensuing\n",
      "problems. Experiments illustrating the value of the proposed approach in digit\n",
      "and face image synthesis under partial conditioning information are presented,\n",
      "showing that the proposed method can effectively outperform the standard\n",
      "approach under these circumstances. \n",
      "\n",
      "\n",
      "While 2D generative adversarial networks have enabled high-resolution image\n",
      "synthesis, they largely lack an understanding of the 3D world and the image\n",
      "formation process. Thus, they do not provide precise control over camera\n",
      "viewpoint or object pose. To address this problem, several recent approaches\n",
      "leverage intermediate voxel-based representations in combination with\n",
      "differentiable rendering. However, existing methods either produce low image\n",
      "resolution or fall short in disentangling camera and scene properties, e.g.,\n",
      "the object identity may vary with the viewpoint. In this paper, we propose a\n",
      "generative model for radiance fields which have recently proven successful for\n",
      "novel view synthesis of a single scene. In contrast to voxel-based\n",
      "representations, radiance fields are not confined to a coarse discretization of\n",
      "the 3D space, yet allow for disentangling camera and scene properties while\n",
      "degrading gracefully in the presence of reconstruction ambiguity. By\n",
      "introducing a multi-scale patch-based discriminator, we demonstrate synthesis\n",
      "of high-resolution images while training our model from unposed 2D images\n",
      "alone. We systematically analyze our approach on several challenging synthetic\n",
      "and real-world datasets. Our experiments reveal that radiance fields are a\n",
      "powerful representation for generative image synthesis, leading to 3D\n",
      "consistent models that render with high fidelity. \n",
      "\n",
      "\n",
      "The 2D virtual try-on task has recently attracted a great interest from the\n",
      "research community, for its direct potential applications in online shopping as\n",
      "well as for its inherent and non-addressed scientific challenges. This task\n",
      "requires fitting an in-shop cloth image on the image of a person, which is\n",
      "highly challenging because it involves cloth warping, image compositing, and\n",
      "synthesizing. Casting virtual try-on into a supervised task faces a difficulty:\n",
      "available datasets are composed of pairs of pictures (cloth, person wearing the\n",
      "cloth). Thus, we have no access to ground-truth when the cloth on the person\n",
      "changes. State-of-the-art models solve this by masking the cloth information on\n",
      "the person with both a human parser and a pose estimator. Then, image synthesis\n",
      "modules are trained to reconstruct the person image from the masked person\n",
      "image and the cloth image. This procedure has several caveats: firstly, human\n",
      "parsers are prone to errors; secondly, it is a costly pre-processing step,\n",
      "which also has to be applied at inference time; finally, it makes the task\n",
      "harder than it is since the mask covers information that should be kept such as\n",
      "hands or accessories. In this paper, we propose a novel student-teacher\n",
      "paradigm where the teacher is trained in the standard way (reconstruction)\n",
      "before guiding the student to focus on the initial task (changing the cloth).\n",
      "The student additionally learns from an adversarial loss, which pushes it to\n",
      "follow the distribution of the real images. Consequently, the student exploits\n",
      "information that is masked to the teacher. A student trained without the\n",
      "adversarial loss would not use this information. Also, getting rid of both\n",
      "human parser and pose estimator at inference time allows obtaining a real-time\n",
      "virtual try-on. \n",
      "\n",
      "\n",
      "Neural rendering techniques promise efficient photo-realistic image synthesis\n",
      "while at the same time providing rich control over scene parameters by learning\n",
      "the physical image formation process. While several supervised methods have\n",
      "been proposed for this task, acquiring a dataset of images with accurately\n",
      "aligned 3D models is very difficult. The main contribution of this work is to\n",
      "lift this restriction by training a neural rendering algorithm from unpaired\n",
      "data. More specifically, we propose an autoencoder for joint generation of\n",
      "realistic images from synthetic 3D models while simultaneously decomposing real\n",
      "images into their intrinsic shape and appearance properties. In contrast to a\n",
      "traditional graphics pipeline, our approach does not require to specify all\n",
      "scene properties, such as material parameters and lighting by hand. Instead, we\n",
      "learn photo-realistic deferred rendering from a small set of 3D models and a\n",
      "larger set of unaligned real images, both of which are easy to acquire in\n",
      "practice. Simultaneously, we obtain accurate intrinsic decompositions of real\n",
      "images while not requiring paired ground truth. Our experiments confirm that a\n",
      "joint treatment of rendering and decomposition is indeed beneficial and that\n",
      "our approach outperforms state-of-the-art image-to-image translation baselines\n",
      "both qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "Recently, high dynamic range (HDR) image reconstruction based on the multiple\n",
      "exposure stack from a given single exposure utilizes a deep learning framework\n",
      "to generate high-quality HDR images. These conventional networks focus on the\n",
      "exposure transfer task to reconstruct the multi-exposure stack. Therefore, they\n",
      "often fail to fuse the multi-exposure stack into a perceptually pleasant HDR\n",
      "image as the inversion artifacts occur. We tackle the problem in stack\n",
      "reconstruction-based methods by proposing a novel framework with a fully\n",
      "differentiable high dynamic range imaging (HDRI) process. By explicitly using\n",
      "the loss, which compares the network's output with the ground truth HDR image,\n",
      "our framework enables a neural network that generates the multiple exposure\n",
      "stack for HDRI to train stably. In other words, our differentiable HDR\n",
      "synthesis layer helps the deep neural network to train to create multi-exposure\n",
      "stacks while reflecting the precise correlations between multi-exposure images\n",
      "in the HDRI process. In addition, our network uses the image decomposition and\n",
      "the recursive process to facilitate the exposure transfer task and to\n",
      "adaptively respond to recursion frequency. The experimental results show that\n",
      "the proposed network outperforms the state-of-the-art quantitative and\n",
      "qualitative results in terms of both the exposure transfer tasks and the whole\n",
      "HDRI process. \n",
      "\n",
      "\n",
      "Invertible neural networks based on coupling flows (CF-INNs) have various\n",
      "machine learning applications such as image synthesis and representation\n",
      "learning. However, their desirable characteristics such as analytic\n",
      "invertibility come at the cost of restricting the functional forms. This poses\n",
      "a question on their representation power: are CF-INNs universal approximators\n",
      "for invertible functions? Without a universality, there could be a well-behaved\n",
      "invertible transformation that the CF-INN can never approximate, hence it would\n",
      "render the model class unreliable. We answer this question by showing a\n",
      "convenient criterion: a CF-INN is universal if its layers contain affine\n",
      "coupling and invertible linear functions as special cases. As its corollary, we\n",
      "can affirmatively resolve a previously unsolved problem: whether normalizing\n",
      "flow models based on affine coupling can be universal distributional\n",
      "approximators. In the course of proving the universality, we prove a general\n",
      "theorem to show the equivalence of the universality for certain diffeomorphism\n",
      "classes, a theoretical insight that is of interest by itself. \n",
      "\n",
      "\n",
      "We present high quality image synthesis results using diffusion probabilistic\n",
      "models, a class of latent variable models inspired by considerations from\n",
      "nonequilibrium thermodynamics. Our best results are obtained by training on a\n",
      "weighted variational bound designed according to a novel connection between\n",
      "diffusion probabilistic models and denoising score matching with Langevin\n",
      "dynamics, and our models naturally admit a progressive lossy decompression\n",
      "scheme that can be interpreted as a generalization of autoregressive decoding.\n",
      "On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and\n",
      "a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality\n",
      "similar to ProgressiveGAN. Our implementation is available at\n",
      "https://github.com/hojonathanho/diffusion \n",
      "\n",
      "\n",
      "Sketch-to-image (S2I) translation plays an important role in image synthesis\n",
      "and manipulation tasks, such as photo editing and colorization. Some specific\n",
      "S2I translation including sketch-to-photo and sketch-to-painting can be used as\n",
      "powerful tools in the art design industry. However, previous methods only\n",
      "support S2I translation with a single level of density, which gives less\n",
      "flexibility to users for controlling the input sketches. In this work, we\n",
      "propose the first multi-level density sketch-to-image translation framework,\n",
      "which allows the input sketch to cover a wide range from rough object outlines\n",
      "to micro structures. Moreover, to tackle the problem of noncontinuous\n",
      "representation of multi-level density input sketches, we project the density\n",
      "level into a continuous latent space, which can then be linearly controlled by\n",
      "a parameter. This allows users to conveniently control the densities of input\n",
      "sketches and generation of images. Moreover, our method has been successfully\n",
      "verified on various datasets for different applications including face editing,\n",
      "multi-modal sketch-to-photo translation, and anime colorization, providing\n",
      "coarse-to-fine levels of controls to these applications. \n",
      "\n",
      "\n",
      "Increasingly more similarities between human vision and convolutional neural\n",
      "networks (CNNs) have been revealed in the past few years. Yet, vanilla CNNs\n",
      "often fall short in generalizing to adversarial or out-of-distribution (OOD)\n",
      "examples which humans demonstrate superior performance. Adversarial training is\n",
      "a leading learning algorithm for improving the robustness of CNNs on\n",
      "adversarial and OOD data; however, little is known about the properties,\n",
      "specifically the shape bias and internal features learned inside\n",
      "adversarially-robust CNNs. In this paper, we perform a thorough, systematic\n",
      "study to understand the shape bias and some internal mechanisms that enable the\n",
      "generalizability of AlexNet, GoogLeNet, and ResNet-50 models trained via\n",
      "adversarial training. We find that while standard ImageNet classifiers have a\n",
      "strong texture bias, their R counterparts rely heavily on shapes. Remarkably,\n",
      "adversarial training induces three simplicity biases into hidden neurons in the\n",
      "process of \"robustifying\" CNNs. That is, each convolutional neuron in R\n",
      "networks often changes to detecting (1) pixel-wise smoother patterns, i.e., a\n",
      "mechanism that blocks high-frequency noise from passing through the network;\n",
      "(2) more lower-level features i.e. textures and colors (instead of objects);and\n",
      "(3) fewer types of inputs. Our findings reveal the interesting mechanisms that\n",
      "made networks more adversarially robust and also explain some recent findings\n",
      "e.g., why R networks benefit from a much larger capacity (Xie et al. 2020) and\n",
      "can act as a strong image prior in image synthesis (Santurkar et al. 2019). \n",
      "\n",
      "\n",
      "In this paper, we aim to synthesize cell microscopy images under different\n",
      "molecular interventions, motivated by practical applications to drug\n",
      "development. Building on the recent success of graph neural networks for\n",
      "learning molecular embeddings and flow-based models for image generation, we\n",
      "propose Mol2Image: a flow-based generative model for molecule to cell image\n",
      "synthesis. To generate cell features at different resolutions and scale to\n",
      "high-resolution images, we develop a novel multi-scale flow architecture based\n",
      "on a Haar wavelet image pyramid. To maximize the mutual information between the\n",
      "generated images and the molecular interventions, we devise a training strategy\n",
      "based on contrastive learning. To evaluate our model, we propose a new set of\n",
      "metrics for biological image generation that are robust, interpretable, and\n",
      "relevant to practitioners. We show quantitatively that our method learns a\n",
      "meaningful embedding of the molecular intervention, which is translated into an\n",
      "image representation reflecting the biological effects of the intervention. \n",
      "\n",
      "\n",
      "Magnetic Resonance (MR) Imaging and Computed Tomography (CT) are the primary\n",
      "diagnostic imaging modalities quite frequently used for surgical planning and\n",
      "analysis. A general problem with medical imaging is that the acquisition\n",
      "process is quite expensive and time-consuming. Deep learning techniques like\n",
      "generative adversarial networks (GANs) can help us to leverage the possibility\n",
      "of an image to image translation between multiple imaging modalities, which in\n",
      "turn helps in saving time and cost. These techniques will help to conduct\n",
      "surgical planning under CT with the feedback of MRI information. While previous\n",
      "studies have shown paired and unpaired image synthesis from MR to CT, image\n",
      "synthesis from CT to MR still remains a challenge, since it involves the\n",
      "addition of extra tissue information. In this manuscript, we have implemented\n",
      "two different variations of Generative Adversarial Networks exploiting the\n",
      "cycling consistency and structural similarity between both CT and MR image\n",
      "modalities on a pelvis dataset, thus facilitating a bidirectional exchange of\n",
      "content and style between these image modalities. The proposed GANs translate\n",
      "the input medical images by different mechanisms, and hence generated images\n",
      "not only appears realistic but also performs well across various comparison\n",
      "metrics, and these images have also been cross verified with a radiologist. The\n",
      "radiologist verification has shown that slight variations in generated MR and\n",
      "CT images may not be exactly the same as their true counterpart but it can be\n",
      "used for medical purposes. \n",
      "\n",
      "\n",
      "Data augmentations have been widely studied to improve the accuracy and\n",
      "robustness of classifiers. However, the potential of image augmentation in\n",
      "improving GAN models for image synthesis has not been thoroughly investigated\n",
      "in previous studies. In this work, we systematically study the effectiveness of\n",
      "various existing augmentation techniques for GAN training in a variety of\n",
      "settings. We provide insights and guidelines on how to augment images for both\n",
      "vanilla GANs and GANs with regularizations, improving the fidelity of the\n",
      "generated images substantially. Surprisingly, we find that vanilla GANs attain\n",
      "generation quality on par with recent state-of-the-art results if we use\n",
      "augmentations on both real and generated images. When this GAN training is\n",
      "combined with other augmentation-based regularization techniques, such as\n",
      "contrastive loss and consistency regularization, the augmentations further\n",
      "improve the quality of generated images. We provide new state-of-the-art\n",
      "results for conditional generation on CIFAR-10 with both consistency loss and\n",
      "contrastive loss as additional regularizations. \n",
      "\n",
      "\n",
      "We propose an image synthesis approach that provides stratified navigation in\n",
      "the latent code space. With a tiny amount of partial or very low-resolution\n",
      "image, our approach can consistently out-perform state-of-the-art counterparts\n",
      "in terms of generating the closest sampled image to the ground truth. We\n",
      "achieve this through scale-independent editing while expanding scale-specific\n",
      "diversity. Scale-independence is achieved with a nested scale disentanglement\n",
      "loss. Scale-specific diversity is created by incorporating a progressive\n",
      "diversification constraint. We introduce semantic persistency across the scales\n",
      "by sharing common latent codes. Together they provide better control of the\n",
      "image synthesis process. We evaluate the effectiveness of our proposed approach\n",
      "through various tasks, including image outpainting, image superresolution, and\n",
      "cross-domain image translation. \n",
      "\n",
      "\n",
      "Image synthesis has witnessed substantial progress due to the increasing\n",
      "power of generative model. This paper we propose a novel generative approach\n",
      "for exemplar based facial editing in the form of the region inpainting. Our\n",
      "method first masks the facial editing region to eliminates the pixel\n",
      "constraints of the original image, then exemplar based facial editing can be\n",
      "achieved by learning the corresponding information from the reference image to\n",
      "complete the masked region. In additional, we impose the attribute labels\n",
      "constraint to model disentangled encodings in order to avoid undesired\n",
      "information being transferred from the exemplar to the original image editing\n",
      "region. Experimental results demonstrate our method can produce diverse and\n",
      "personalized face editing results and provide far more user control flexibility\n",
      "than nearly all existing methods. \n",
      "\n",
      "\n",
      "Given the ever-increasing computational costs of modern machine learning\n",
      "models, we need to find new ways to reuse such expert models and thus tap into\n",
      "the resources that have been invested in their creation. Recent work suggests\n",
      "that the power of these massive models is captured by the representations they\n",
      "learn. Therefore, we seek a model that can relate between different existing\n",
      "representations and propose to solve this task with a conditionally invertible\n",
      "network. This network demonstrates its capability by (i) providing generic\n",
      "transfer between diverse domains, (ii) enabling controlled content synthesis by\n",
      "allowing modification in other domains, and (iii) facilitating diagnosis of\n",
      "existing representations by translating them into interpretable domains such as\n",
      "images. Our domain transfer network can translate between fixed representations\n",
      "without having to learn or finetune them. This allows users to utilize various\n",
      "existing domain-specific expert models from the literature that had been\n",
      "trained with extensive computational resources. Experiments on diverse\n",
      "conditional image synthesis tasks, competitive image modification results and\n",
      "experiments on image-to-image and text-to-image generation demonstrate the\n",
      "generic applicability of our approach. For example, we translate between BERT\n",
      "and BigGAN, state-of-the-art text and image models to provide text-to-image\n",
      "generation, which neither of both experts can perform on their own. \n",
      "\n",
      "\n",
      "The ability to produce convincing textural details is essential for the\n",
      "fidelity of synthesized person images. However, existing methods typically\n",
      "follow a ``warping-based'' strategy that propagates appearance features through\n",
      "the same pathway used for pose transfer. However, most fine-grained features\n",
      "would be lost due to down-sampling, leading to over-smoothed clothes and\n",
      "missing details in the output images. In this paper we presents RATE-Net, a\n",
      "novel framework for synthesizing person images with sharp texture details. The\n",
      "proposed framework leverages an additional texture enhancing module to extract\n",
      "appearance information from the source image and estimate a fine-grained\n",
      "residual texture map, which helps to refine the coarse estimation from the pose\n",
      "transfer module. In addition, we design an effective alternate updating\n",
      "strategy to promote mutual guidance between two modules for better shape and\n",
      "appearance consistency. Experiments conducted on DeepFashion benchmark dataset\n",
      "have demonstrated the superiority of our framework compared with existing\n",
      "networks. \n",
      "\n",
      "\n",
      "In this paper, we propose a novel generative network (SegAttnGAN) that\n",
      "utilizes additional segmentation information for the text-to-image synthesis\n",
      "task. As the segmentation data introduced to the model provides useful guidance\n",
      "on the generator training, the proposed model can generate images with better\n",
      "realism quality and higher quantitative measures compared with the previous\n",
      "state-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset\n",
      "and 3.52 on the Oxford-102 dataset. Besides, we tested the self-attention\n",
      "SegAttnGAN which uses generated segmentation data instead of masks from\n",
      "datasets for attention and achieved similar high-quality results, suggesting\n",
      "that our model can be adapted for the text-to-image synthesis task. \n",
      "\n",
      "\n",
      "As a powerful technique in medical imaging, image synthesis is widely used in\n",
      "applications such as denoising, super resolution and modality transformation\n",
      "etc. Recently, the revival of deep neural networks made immense progress in the\n",
      "field of medical imaging. Although many deep leaning based models have been\n",
      "proposed to improve the image synthesis accuracy, the evaluation of the model\n",
      "uncertainty, which is highly important for medical applications, has been a\n",
      "missing part. In this work, we propose to use Bayesian conditional generative\n",
      "adversarial network (GAN) with concrete dropout to improve image synthesis\n",
      "accuracy. Meanwhile, an uncertainty calibration approach is involved in the\n",
      "whole pipeline to make the uncertainty generated by Bayesian network\n",
      "interpretable. The method is validated with the T1w to T2w MR image translation\n",
      "with a brain tumor dataset of 102 subjects. Compared with the conventional\n",
      "Bayesian neural network with Monte Carlo dropout, results of the proposed\n",
      "method reach a significant lower RMSE with a p-value of 0.0186. Improvement of\n",
      "the calibration of the generated uncertainty by the uncertainty recalibration\n",
      "method is also illustrated. \n",
      "\n",
      "\n",
      "Motivated by the current demand of clinical governance, surgical simulation\n",
      "is now a well-established modality for basic skills training and assessment.\n",
      "The practical deployment of the technique is a multi-disciplinary venture\n",
      "encompassing areas in engineering, medicine and psychology. This paper provides\n",
      "an overview of the key topics involved in surgical simulation and associated\n",
      "technical challenges. The paper discusses the clinical motivation for surgical\n",
      "simulation, the use of virtual environments for surgical training, model\n",
      "acquisition and simplification, deformable models, collision detection, tissue\n",
      "property measurement, haptic rendering and image synthesis. Additional topics\n",
      "include surgical skill training and assessment metrics as well as challenges\n",
      "facing the incorporation of surgical simulation into medical education\n",
      "curricula. \n",
      "\n",
      "\n",
      "Magnetic Resonance (MR) images of different modalities can provide\n",
      "complementary information for clinical diagnosis, but whole modalities are\n",
      "often costly to access. Most existing methods only focus on synthesizing\n",
      "missing images between two modalities, which limits their robustness and\n",
      "efficiency when multiple modalities are missing. To address this problem, we\n",
      "propose a multi-modality generative adversarial network (MGAN) to synthesize\n",
      "three high-quality MR modalities (FLAIR, T1 and T1ce) from one MR modality T2\n",
      "simultaneously. The experimental results show that the quality of the\n",
      "synthesized images by our proposed methods is better than the one synthesized\n",
      "by the baseline model, pix2pix. Besides, for MR brain image synthesis, it is\n",
      "important to preserve the critical tumor information in the generated\n",
      "modalities, so we further introduce a multi-modality tumor consistency loss to\n",
      "MGAN, called TC-MGAN. We use the synthesized modalities by TC-MGAN to boost the\n",
      "tumor segmentation accuracy, and the results demonstrate its effectiveness. \n",
      "\n",
      "\n",
      "While the quality of GAN image synthesis has improved tremendously in recent\n",
      "years, our ability to control and condition the output is still limited.\n",
      "Focusing on StyleGAN, we introduce a simple and effective method for making\n",
      "local, semantically-aware edits to a target output image. This is accomplished\n",
      "by borrowing elements from a source image, also a GAN output, via a novel\n",
      "manipulation of style vectors. Our method requires neither supervision from an\n",
      "external model, nor involves complex spatial morphing operations. Instead, it\n",
      "relies on the emergent disentanglement of semantic objects that is learned by\n",
      "StyleGAN during its training. Semantic editing is demonstrated on GANs\n",
      "producing human faces, indoor scenes, cats, and cars. We measure the locality\n",
      "and photorealism of the edits produced by our method, and find that it\n",
      "accomplishes both. \n",
      "\n",
      "\n",
      "In this paper, we propose a fast transient hydrostatic stress analysis for\n",
      "electromigration (EM) failure assessment for multi-segment interconnects using\n",
      "generative adversarial networks (GANs). Our work leverages the image synthesis\n",
      "feature of GAN-based generative deep neural networks. The stress evaluation of\n",
      "multi-segment interconnects, modeled by partial differential equations, can be\n",
      "viewed as time-varying 2D-images-to-image problem where the input is the\n",
      "multi-segment interconnects topology with current densities and the output is\n",
      "the EM stress distribution in those wire segments at the given aging time.\n",
      "Based on this observation, we train conditional GAN model using the images of\n",
      "many self-generated multi-segment wires and wire current densities and aging\n",
      "time (as conditions) against the COMSOL simulation results. Different\n",
      "hyperparameters of GAN were studied and compared. The proposed algorithm,\n",
      "called {\\it EM-GAN}, can quickly give accurate stress distribution of a general\n",
      "multi-segment wire tree for a given aging time, which is important for\n",
      "full-chip fast EM failure assessment. Our experimental results show that the\n",
      "EM-GAN shows 6.6\\% averaged error compared to COMSOL simulation results with\n",
      "orders of magnitude speedup. It also delivers 8.3X speedup over\n",
      "state-of-the-art analytic based EM analysis solver. \n",
      "\n",
      "\n",
      "We propose DiscoFaceGAN, an approach for face image generation of virtual\n",
      "people with disentangled, precisely-controllable latent representations for\n",
      "identity of non-existing people, expression, pose, and illumination. We embed\n",
      "3D priors into adversarial learning and train the network to imitate the image\n",
      "formation of an analytic 3D face deformation and rendering process. To deal\n",
      "with the generation freedom induced by the domain gap between real and rendered\n",
      "faces, we further introduce contrastive learning to promote disentanglement by\n",
      "comparing pairs of generated images. Experiments show that through our\n",
      "imitative-contrastive learning, the factor variations are very well\n",
      "disentangled and the properties of a generated face can be precisely\n",
      "controlled. We also analyze the learned latent space and present several\n",
      "meaningful properties supporting factor disentanglement. Our method can also be\n",
      "used to embed real images into the disentangled latent space. We hope our\n",
      "method could provide new understandings of the relationship between physical\n",
      "properties and deep image synthesis. \n",
      "\n",
      "\n",
      "Text-to-image synthesis is the task of generating images from text\n",
      "descriptions. Image generation, by itself, is a challenging task. When we\n",
      "combine image generation and text, we bring complexity to a new level: we need\n",
      "to combine data from two different modalities. Most of recent works in\n",
      "text-to-image synthesis follow a similar approach when it comes to neural\n",
      "architectures. Due to aforementioned difficulties, plus the inherent difficulty\n",
      "of training GANs at high resolutions, most methods have adopted a multi-stage\n",
      "training strategy. In this paper we shift the architectural paradigm currently\n",
      "used in text-to-image methods and show that an effective neural architecture\n",
      "can achieve state-of-the-art performance using a single stage training with a\n",
      "single generator and a single discriminator. We do so by applying deep residual\n",
      "networks along with a novel sentence interpolation strategy that enables\n",
      "learning a smooth conditional space. Finally, our work points a new direction\n",
      "for text-to-image research, which has not experimented with novel neural\n",
      "architectures recently. \n",
      "\n",
      "\n",
      "Exploiting learning algorithms under scarce data regimes is a limitation and\n",
      "a reality of the medical imaging field. In an attempt to mitigate the problem,\n",
      "we propose a data augmentation protocol based on generative adversarial\n",
      "networks. We condition the networks at a pixel-level (segmentation mask) and at\n",
      "a global-level information (acquisition environment or lesion type). Such\n",
      "conditioning provides immediate access to the image-label pairs while\n",
      "controlling global class specific appearance of the synthesized images. To\n",
      "stimulate synthesis of the features relevant for the segmentation task, an\n",
      "additional passive player in a form of segmentor is introduced into the\n",
      "adversarial game. We validate the approach on two medical datasets: BraTS,\n",
      "ISIC. By controlling the class distribution through injection of synthetic\n",
      "images into the training set we achieve control over the accuracy levels of the\n",
      "datasets' classes. \n",
      "\n",
      "\n",
      "This paper reviewed the deep learning-based studies for medical imaging\n",
      "synthesis and its clinical application. Specifically, we summarized the recent\n",
      "developments of deep learning-based methods in inter- and intra-modality image\n",
      "synthesis by listing and highlighting the proposed methods, study designs and\n",
      "reported performances with related clinical applications on representative\n",
      "studies. The challenges among the reviewed studies were summarized in the\n",
      "discussion part. \n",
      "\n",
      "\n",
      "Conditional image synthesis for generating photorealistic images serves\n",
      "various applications for content editing to content generation. Previous\n",
      "conditional image synthesis algorithms mostly rely on semantic maps, and often\n",
      "fail in complex environments where multiple instances occlude each other. We\n",
      "propose a panoptic aware image synthesis network to generate high fidelity and\n",
      "photorealistic images conditioned on panoptic maps which unify semantic and\n",
      "instance information. To achieve this, we efficiently use panoptic maps in\n",
      "convolution and upsampling layers. We show that with the proposed changes to\n",
      "the generator, we can improve on the previous state-of-the-art methods by\n",
      "generating images in complex instance interaction environments in higher\n",
      "fidelity and tiny objects in more details. Furthermore, our proposed method\n",
      "also outperforms the previous state-of-the-art methods in metrics of mean IoU\n",
      "(Intersection over Union), and detAP (Detection Average Precision). \n",
      "\n",
      "\n",
      "Facial sketches drawn by artists are widely used for visual identification\n",
      "applications and mostly by law enforcement agencies, but the quality of these\n",
      "sketches depend on the ability of the artist to clearly replicate all the key\n",
      "facial features that could aid in capturing the true identity of a subject.\n",
      "Recent works have attempted to synthesize these sketches into plausible visual\n",
      "images to improve visual recognition and identification. However, synthesizing\n",
      "photo-realistic images from sketches proves to be an even more challenging\n",
      "task, especially for sensitive applications such as suspect identification. In\n",
      "this work, we propose a novel approach that adopts a generative adversarial\n",
      "network that synthesizes a single sketch into multiple synthetic images with\n",
      "unique attributes like hair color, sex, etc. We incorporate a hybrid\n",
      "discriminator which performs attribute classification of multiple target\n",
      "attributes, a quality guided encoder that minimizes the perceptual\n",
      "dissimilarity of the latent space embedding of the synthesized and real image\n",
      "at different layers in the network and an identity preserving network that\n",
      "maintains the identity of the synthesised image throughout the training\n",
      "process. Our approach is aimed at improving the visual appeal of the\n",
      "synthesised images while incorporating multiple attribute assignment to the\n",
      "generator without compromising the identity of the synthesised image. We\n",
      "synthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and\n",
      "CelebA-HQ datasets and from an auxiliary generator trained on sketches from\n",
      "CUHK, IIT-D and FERET datasets. Our results are impressive compared to current\n",
      "state of the art. \n",
      "\n",
      "\n",
      "Example-guided image synthesis has recently been attempted to synthesize an\n",
      "image from a semantic label map and an exemplary image. In the task, the\n",
      "additional exemplar image provides the style guidance that controls the\n",
      "appearance of the synthesized output. Despite the controllability advantage,\n",
      "the existing models are designed on datasets with specific and roughly aligned\n",
      "objects. In this paper, we tackle a more challenging and general task, where\n",
      "the exemplar is an arbitrary scene image that is semantically different from\n",
      "the given label map. To this end, we first propose a Masked Spatial-Channel\n",
      "Attention (MSCA) module which models the correspondence between two arbitrary\n",
      "scenes via efficient decoupled attention. Next, we propose an end-to-end\n",
      "network for joint global and local feature alignment and synthesis. Finally, we\n",
      "propose a novel self-supervision task to enable training. Experiments on the\n",
      "large-scale and more diverse COCO-stuff dataset show significant improvements\n",
      "over the existing methods. Moreover, our approach provides interpretability and\n",
      "can be readily extended to other content manipulation tasks including style and\n",
      "spatial interpolation or extrapolation. \n",
      "\n",
      "\n",
      "Recently, there has been substantial progress in image synthesis from\n",
      "semantic labelmaps. However, methods used for this task assume the availability\n",
      "of complete and unambiguous labelmaps, with instance boundaries of objects, and\n",
      "class labels for each pixel. This reliance on heavily annotated inputs\n",
      "restricts the application of image synthesis techniques to real-world\n",
      "applications, especially under uncertainty due to weather, occlusion, or noise.\n",
      "On the other hand, algorithms that can synthesize images from sparse labelmaps\n",
      "or sketches are highly desirable as tools that can guide content creators and\n",
      "artists to quickly generate scenes by simply specifying locations of a few\n",
      "objects. In this paper, we address the problem of complex scene completion from\n",
      "sparse labelmaps. Under this setting, very few details about the scene (30\\% of\n",
      "object instances) are available as input for image synthesis. We propose a\n",
      "two-stage deep network based method, called `Halluci-Net', that learns\n",
      "co-occurence relationships between objects in scenes, and then exploits these\n",
      "relationships to produce a dense and complete labelmap. The generated dense\n",
      "labelmap can then be used as input by state-of-the-art image synthesis\n",
      "techniques like pix2pixHD to obtain the final image. The proposed method is\n",
      "evaluated on the Cityscapes dataset and it outperforms two baselines methods on\n",
      "performance metrics like Fr\\'echet Inception Distance (FID), semantic\n",
      "segmentation accuracy, and similarity in object co-occurrences. We also show\n",
      "qualitative results on a subset of ADE20K dataset that contains bedroom images. \n",
      "\n",
      "\n",
      "Skin lesion datasets consist predominantly of normal samples with only a\n",
      "small percentage of abnormal ones, giving rise to the class imbalance problem.\n",
      "Also, skin lesion images are largely similar in overall appearance owing to the\n",
      "low inter-class variability. In this paper, we propose a two-stage framework\n",
      "for automatic classification of skin lesion images using adversarial training\n",
      "and transfer learning toward melanoma detection. In the first stage, we\n",
      "leverage the inter-class variation of the data distribution for the task of\n",
      "conditional image synthesis by learning the inter-class mapping and\n",
      "synthesizing under-represented class samples from the over-represented ones\n",
      "using unpaired image-to-image translation. In the second stage, we train a deep\n",
      "convolutional neural network for skin lesion classification using the original\n",
      "training set combined with the newly synthesized under-represented class\n",
      "samples. The training of this classifier is carried out by minimizing the focal\n",
      "loss function, which assists the model in learning from hard examples, while\n",
      "down-weighting the easy ones. Experiments conducted on a dermatology image\n",
      "benchmark demonstrate the superiority of our proposed approach over several\n",
      "standard baseline methods, achieving significant performance improvements.\n",
      "Interestingly, we show through feature visualization and analysis that our\n",
      "method leads to context based lesion assessment that can reach an expert\n",
      "dermatologist level. \n",
      "\n",
      "\n",
      "Camera captured human pose is an outcome of several sources of variation.\n",
      "Performance of supervised 3D pose estimation approaches comes at the cost of\n",
      "dispensing with variations, such as shape and appearance, that may be useful\n",
      "for solving other related tasks. As a result, the learned model not only\n",
      "inculcates task-bias but also dataset-bias because of its strong reliance on\n",
      "the annotated samples, which also holds true for weakly-supervised models.\n",
      "Acknowledging this, we propose a self-supervised learning framework to\n",
      "disentangle such variations from unlabeled video frames. We leverage the prior\n",
      "knowledge on human skeleton and poses in the form of a single part-based 2D\n",
      "puppet model, human pose articulation constraints, and a set of unpaired 3D\n",
      "poses. Our differentiable formalization, bridging the representation gap\n",
      "between the 3D pose and spatial part maps, not only facilitates discovery of\n",
      "interpretable pose disentanglement but also allows us to operate on videos with\n",
      "diverse camera movements. Qualitative results on unseen in-the-wild datasets\n",
      "establish our superior generalization across multiple tasks beyond the primary\n",
      "tasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate\n",
      "state-of-the-art weakly-supervised 3D pose estimation performance on both\n",
      "Human3.6M and MPI-INF-3DHP datasets. \n",
      "\n",
      "\n",
      "Flow-based generative models are an important class of exact inference models\n",
      "that admit efficient inference and sampling for image synthesis. Owing to the\n",
      "efficiency constraints on the design of the flow layers, e.g. split coupling\n",
      "flow layers in which approximately half the pixels do not undergo further\n",
      "transformations, they have limited expressiveness for modeling long-range data\n",
      "dependencies compared to autoregressive models that rely on conditional\n",
      "pixel-wise generation. In this work, we improve the representational power of\n",
      "flow-based models by introducing channel-wise dependencies in their latent\n",
      "space through multi-scale autoregressive priors (mAR). Our mAR prior for models\n",
      "with split coupling flow layers (mAR-SCF) can better capture dependencies in\n",
      "complex multimodal data. The resulting model achieves state-of-the-art density\n",
      "estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that\n",
      "mAR-SCF allows for improved image generation quality, with gains in FID and\n",
      "Inception scores compared to state-of-the-art flow-based models. \n",
      "\n",
      "\n",
      "Efficient rendering of photo-realistic virtual worlds is a long standing\n",
      "effort of computer graphics. Modern graphics techniques have succeeded in\n",
      "synthesizing photo-realistic images from hand-crafted scene representations.\n",
      "However, the automatic generation of shape, materials, lighting, and other\n",
      "aspects of scenes remains a challenging problem that, if solved, would make\n",
      "photo-realistic computer graphics more widely accessible. Concurrently,\n",
      "progress in computer vision and machine learning have given rise to a new\n",
      "approach to image synthesis and editing, namely deep generative models. Neural\n",
      "rendering is a new and rapidly emerging field that combines generative machine\n",
      "learning techniques with physical knowledge from computer graphics, e.g., by\n",
      "the integration of differentiable rendering into network training. With a\n",
      "plethora of applications in computer graphics and vision, neural rendering is\n",
      "poised to become a new area in the graphics community, yet no survey of this\n",
      "emerging field exists. This state-of-the-art report summarizes the recent\n",
      "trends and applications of neural rendering. We focus on approaches that\n",
      "combine classic computer graphics techniques with deep generative models to\n",
      "obtain controllable and photo-realistic outputs. Starting with an overview of\n",
      "the underlying computer graphics and machine learning concepts, we discuss\n",
      "critical aspects of neural rendering approaches. This state-of-the-art report\n",
      "is focused on the many important use cases for the described algorithms such as\n",
      "novel view synthesis, semantic photo manipulation, facial and body reenactment,\n",
      "relighting, free-viewpoint video, and the creation of photo-realistic avatars\n",
      "for virtual and augmented reality telepresence. Finally, we conclude with a\n",
      "discussion of the social implications of such technology and investigate open\n",
      "research problems. \n",
      "\n",
      "\n",
      "We present AugurOne, a novel approach for training single image generative\n",
      "models. Our approach trains an upscaling neural network using non-affine\n",
      "augmentations of the (single) input image, particularly including non-rigid\n",
      "thin plate spline image warps. The extensive augmentations significantly\n",
      "increase the in-sample distribution for the upsampling network enabling the\n",
      "upscaling of highly variable inputs. A compact latent space is jointly learned\n",
      "allowing for controlled image synthesis. Differently from Single Image GAN, our\n",
      "approach does not require GAN training and takes place in an end-to-end fashion\n",
      "allowing fast and stable training. We experimentally evaluate our method and\n",
      "show that it obtains compelling novel animations of single-image, as well as,\n",
      "state-of-the-art performance on conditional generation tasks e.g.\n",
      "paint-to-image and edges-to-image. \n",
      "\n",
      "\n",
      "Many tasks in computer vision and graphics fall within the framework of\n",
      "conditional image synthesis. In recent years, generative adversarial nets\n",
      "(GANs) have delivered impressive advances in quality of synthesized images.\n",
      "However, it remains a challenge to generate both diverse and plausible images\n",
      "for the same input, due to the problem of mode collapse. In this paper, we\n",
      "develop a new generic multimodal conditional image synthesis method based on\n",
      "Implicit Maximum Likelihood Estimation (IMLE) and demonstrate improved\n",
      "multimodal image synthesis performance on two tasks, single image\n",
      "super-resolution and image synthesis from scene layouts. We make our\n",
      "implementation publicly available. \n",
      "\n",
      "\n",
      "Normalization layers and activation functions are fundamental components in\n",
      "deep networks and typically co-locate with each other. Here we propose to\n",
      "design them using an automated approach. Instead of designing them separately,\n",
      "we unify them into a single tensor-to-tensor computation graph, and evolve its\n",
      "structure starting from basic mathematical functions. Examples of such\n",
      "mathematical functions are addition, multiplication and statistical moments.\n",
      "The use of low-level mathematical functions, in contrast to the use of\n",
      "high-level modules in mainstream NAS, leads to a highly sparse and large search\n",
      "space which can be challenging for search methods. To address the challenge, we\n",
      "develop efficient rejection protocols to quickly filter out candidate layers\n",
      "that do not work well. We also use multi-objective evolution to optimize each\n",
      "layer's performance across many architectures to prevent overfitting. Our\n",
      "method leads to the discovery of EvoNorms, a set of new\n",
      "normalization-activation layers with novel, and sometimes surprising structures\n",
      "that go beyond existing design patterns. For example, some EvoNorms do not\n",
      "assume that normalization and activation functions must be applied\n",
      "sequentially, nor need to center the feature maps, nor require explicit\n",
      "activation functions. Our experiments show that EvoNorms work well on image\n",
      "classification models including ResNets, MobileNets and EfficientNets but also\n",
      "transfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to\n",
      "BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers\n",
      "in many cases. \n",
      "\n",
      "\n",
      "Spatially-adaptive normalization is remarkably successful recently in\n",
      "conditional semantic image synthesis, which modulates the normalized activation\n",
      "with spatially-varying transformations learned from semantic layouts, to\n",
      "preserve the semantic information from being washed away. Despite its\n",
      "impressive performance, a more thorough understanding of the true advantages\n",
      "inside the box is still highly demanded, to help reduce the significant\n",
      "computation and parameter overheads introduced by these new structures. In this\n",
      "paper, from a return-on-investment point of view, we present a deep analysis of\n",
      "the effectiveness of SPADE and observe that its advantages actually come mainly\n",
      "from its semantic-awareness rather than the spatial-adaptiveness. Inspired by\n",
      "this point, we propose class-adaptive normalization (CLADE), a lightweight\n",
      "variant that is not adaptive to spatial positions or layouts. Benefited from\n",
      "this design, CLADE greatly reduces the computation cost while still being able\n",
      "to preserve the semantic information during the generation. Extensive\n",
      "experiments on multiple challenging datasets demonstrate that while the\n",
      "resulting fidelity is on par with SPADE, its overhead is much cheaper than\n",
      "SPADE. Take the generator for ADE20k dataset as an example, the extra parameter\n",
      "and computation cost introduced by CLADE are only 4.57% and 0.07% while that of\n",
      "SPADE are 39.21% and 234.73% respectively. \n",
      "\n",
      "\n",
      "This paper describes a simple technique to analyze Generative Adversarial\n",
      "Networks (GANs) and create interpretable controls for image synthesis, such as\n",
      "change of viewpoint, aging, lighting, and time of day. We identify important\n",
      "latent directions based on Principal Components Analysis (PCA) applied either\n",
      "in latent space or feature space. Then, we show that a large number of\n",
      "interpretable controls can be defined by layer-wise perturbation along the\n",
      "principal directions. Moreover, we show that BigGAN can be controlled with\n",
      "layer-wise inputs in a StyleGAN-like manner. We show results on different GANs\n",
      "trained on various datasets, and demonstrate good qualitative matches to edit\n",
      "directions found through earlier supervised approaches. \n",
      "\n",
      "\n",
      "The task of unsupervised image-to-image translation has seen substantial\n",
      "advancements in recent years through the use of deep neural networks.\n",
      "Typically, the proposed solutions learn the characterizing distribution of two\n",
      "large, unpaired collections of images, and are able to alter the appearance of\n",
      "a given image, while keeping its geometry intact. In this paper, we explore the\n",
      "capabilities of neural networks to understand image structure given only a\n",
      "single pair of images, A and B. We seek to generate images that are\n",
      "structurally aligned: that is, to generate an image that keeps the appearance\n",
      "and style of B, but has a structural arrangement that corresponds to A. The key\n",
      "idea is to map between image patches at different scales. This enables\n",
      "controlling the granularity at which analogies are produced, which determines\n",
      "the conceptual distinction between style and content. In addition to structural\n",
      "alignment, our method can be used to generate high quality imagery in other\n",
      "conditional generation tasks utilizing images A and B only: guided image\n",
      "synthesis, style and texture transfer, text translation as well as video\n",
      "translation. Our code and additional results are available in\n",
      "https://github.com/rmokady/structural-analogy/. \n",
      "\n",
      "\n",
      "Ultrasound (US) is widely accepted in clinic for anatomical structure\n",
      "inspection. However, lacking in resources to practice US scan, novices often\n",
      "struggle to learn the operation skills. Also, in the deep learning era,\n",
      "automated US image analysis is limited by the lack of annotated samples.\n",
      "Efficiently synthesizing realistic, editable and high resolution US images can\n",
      "solve the problems. The task is challenging and previous methods can only\n",
      "partially complete it. In this paper, we devise a new framework for US image\n",
      "synthesis. Particularly, we firstly adopt a sketch generative adversarial\n",
      "networks (Sgan) to introduce background sketch upon object mask in a\n",
      "conditioned generative adversarial network. With enriched sketch cues, Sgan can\n",
      "generate realistic US images with editable and fine-grained structure details.\n",
      "Although effective, Sgan is hard to generate high resolution US images. To\n",
      "achieve this, we further implant the Sgan into a progressive growing scheme\n",
      "(PGSgan). By smoothly growing both generator and discriminator, PGSgan can\n",
      "gradually synthesize US images from low to high resolution. By synthesizing\n",
      "ovary and follicle US images, our extensive perceptual evaluation, user study\n",
      "and segmentation results prove the promising efficacy and efficiency of the\n",
      "proposed PGSgan. \n",
      "\n",
      "\n",
      "A key step in understanding the spatial organization of cells and tissues is\n",
      "the ability to construct generative models that accurately reflect that\n",
      "organization. In this paper, we focus on building generative models of electron\n",
      "microscope (EM) images in which the positions of cell membranes and\n",
      "mitochondria have been densely annotated, and propose a two-stage procedure\n",
      "that produces realistic images using Generative Adversarial Networks (or GANs)\n",
      "in a supervised way. In the first stage, we synthesize a label \"image\" given a\n",
      "noise \"image\" as input, which then provides supervision for EM image synthesis\n",
      "in the second stage. The full model naturally generates label-image pairs. We\n",
      "show that accurate synthetic EM images are produced using assessment via (1)\n",
      "shape features and global statistics, (2) segmentation accuracies, and (3) user\n",
      "studies. We also demonstrate further improvements by enforcing a reconstruction\n",
      "loss on intermediate synthetic labels and thus unifying the two stages into one\n",
      "single end-to-end framework. \n",
      "\n",
      "\n",
      "Medical image segmentation is an important task for computer aided diagnosis.\n",
      "Pixelwise manual annotations of large datasets require high expertise and is\n",
      "time consuming. Conventional data augmentations have limited benefit by not\n",
      "fully representing the underlying distribution of the training set, thus\n",
      "affecting model robustness when tested on images captured from different\n",
      "sources. Prior work leverages synthetic images for data augmentation ignoring\n",
      "the interleaved geometric relationship between different anatomical labels. We\n",
      "propose improvements over previous GAN-based medical image synthesis methods by\n",
      "jointly encoding the intrinsic relationship of geometry and shape. Latent space\n",
      "variable sampling results in diverse generated images from a base image and\n",
      "improves robustness. Given those augmented images generated by our method, we\n",
      "train the segmentation network to enhance the segmentation performance of\n",
      "retinal optical coherence tomography (OCT) images. The proposed method\n",
      "outperforms state-of-the-art segmentation methods on the public RETOUCH dataset\n",
      "having images captured from different acquisition procedures. Ablation studies\n",
      "and visual analysis also demonstrate benefits of integrating geometry and\n",
      "diversity. \n",
      "\n",
      "\n",
      "We propose a novel Edge guided Generative Adversarial Network (EdgeGAN) for\n",
      "photo-realistic image synthesis from semantic layouts. Although considerable\n",
      "improvement has been achieved, the quality of synthesized images is far from\n",
      "satisfactory due to two largely unresolved challenges. First, the semantic\n",
      "labels do not provide detailed structural information, making it difficult to\n",
      "synthesize local details and structures. Second, the widely adopted CNN\n",
      "operations such as convolution, down-sampling and normalization usually cause\n",
      "spatial resolution loss and thus are unable to fully preserve the original\n",
      "semantic information, leading to semantically inconsistent results (e.g.,\n",
      "missing small objects). To tackle the first challenge, we propose to use the\n",
      "edge as an intermediate representation which is further adopted to guide image\n",
      "generation via a proposed attention guided edge transfer module. Edge\n",
      "information is produced by a convolutional generator and introduces detailed\n",
      "structure information. Further, to preserve the semantic information, we design\n",
      "an effective module to selectively highlight class-dependent feature maps\n",
      "according to the original semantic layout. Extensive experiments on two\n",
      "challenging datasets show that the proposed EdgeGAN can generate significantly\n",
      "better results than state-of-the-art methods. The source code and trained\n",
      "models are available at https://github.com/Ha0Tang/EdgeGAN. \n",
      "\n",
      "\n",
      "In this paper, we focus on semantically multi-modal image synthesis (SMIS)\n",
      "task, namely, generating multi-modal images at the semantic level. Previous\n",
      "work seeks to use multiple class-specific generators, constraining its usage in\n",
      "datasets with a small number of classes. We instead propose a novel Group\n",
      "Decreasing Network (GroupDNet) that leverages group convolutions in the\n",
      "generator and progressively decreases the group numbers of the convolutions in\n",
      "the decoder. Consequently, GroupDNet is armed with much more controllability on\n",
      "translating semantic labels to natural images and has plausible high-quality\n",
      "yields for datasets with many classes. Experiments on several challenging\n",
      "datasets demonstrate the superiority of GroupDNet on performing the SMIS task.\n",
      "We also show that GroupDNet is capable of performing a wide range of\n",
      "interesting synthesis applications. Codes and models are available at:\n",
      "https://github.com/Seanseattle/SMIS. \n",
      "\n",
      "\n",
      "Implicit representations of 3D objects have recently achieved impressive\n",
      "results on learning-based 3D reconstruction tasks. While existing works use\n",
      "simple texture models to represent object appearance, photo-realistic image\n",
      "synthesis requires reasoning about the complex interplay of light, geometry and\n",
      "surface properties. In this work, we propose a novel implicit representation\n",
      "for capturing the visual appearance of an object in terms of its surface light\n",
      "field. In contrast to existing representations, our implicit model represents\n",
      "surface light fields in a continuous fashion and independent of the geometry.\n",
      "Moreover, we condition the surface light field with respect to the location and\n",
      "color of a small light source. Compared to traditional surface light field\n",
      "models, this allows us to manipulate the light source and relight the object\n",
      "using environment maps. We further demonstrate the capabilities of our model to\n",
      "predict the visual appearance of an unseen object from a single real RGB image\n",
      "and corresponding 3D shape information. As evidenced by our experiments, our\n",
      "model is able to infer rich visual appearance including shadows and specular\n",
      "reflections. Finally, we show that the proposed representation can be embedded\n",
      "into a variational auto-encoder for generating novel appearances that conform\n",
      "to the specified illumination conditions. \n",
      "\n",
      "\n",
      "This paper introduces the Attribute-Decomposed GAN, a novel generative model\n",
      "for controllable person image synthesis, which can produce realistic person\n",
      "images with desired human attributes (e.g., pose, head, upper clothes and\n",
      "pants) provided in various source inputs. The core idea of the proposed model\n",
      "is to embed human attributes into the latent space as independent codes and\n",
      "thus achieve flexible and continuous control of attributes via mixing and\n",
      "interpolation operations in explicit style representations. Specifically, a new\n",
      "architecture consisting of two encoding pathways with style block connections\n",
      "is proposed to decompose the original hard mapping into multiple more\n",
      "accessible subtasks. In source pathway, we further extract component layouts\n",
      "with an off-the-shelf human parser and feed them into a shared global texture\n",
      "encoder for decomposed latent codes. This strategy allows for the synthesis of\n",
      "more realistic output images and automatic separation of un-annotated\n",
      "attributes. Experimental results demonstrate the proposed method's superiority\n",
      "over the state of the art in pose transfer and its effectiveness in the\n",
      "brand-new task of component attribute transfer. \n",
      "\n",
      "\n",
      "We propose a new task towards more practical application for image generation\n",
      "- high-quality image synthesis from salient object layout. This new setting\n",
      "allows users to provide the layout of salient objects only (i.e., foreground\n",
      "bounding boxes and categories), and lets the model complete the drawing with an\n",
      "invented background and a matching foreground. Two main challenges spring from\n",
      "this new task: (i) how to generate fine-grained details and realistic textures\n",
      "without segmentation map input; and (ii) how to create a background and weave\n",
      "it seamlessly into standalone objects. To tackle this, we propose Background\n",
      "Hallucination Generative Adversarial Network (BachGAN), which first selects a\n",
      "set of segmentation maps from a large candidate pool via a background retrieval\n",
      "module, then encodes these candidate layouts via a background fusion module to\n",
      "hallucinate a suitable background for the given objects. By generating the\n",
      "hallucinated background representation dynamically, our model can synthesize\n",
      "high-resolution images with both photo-realistic foreground and integral\n",
      "background. Experiments on Cityscapes and ADE20K datasets demonstrate the\n",
      "advantage of BachGAN over existing methods, measured on both visual fidelity of\n",
      "generated images and visual alignment between output images and input layouts. \n",
      "\n",
      "\n",
      "With the remarkable recent progress on learning deep generative models, it\n",
      "becomes increasingly interesting to develop models for controllable image\n",
      "synthesis from reconfigurable inputs. This paper focuses on a recent emerged\n",
      "task, layout-to-image, to learn generative models that are capable of\n",
      "synthesizing photo-realistic images from spatial layout (i.e., object bounding\n",
      "boxes configured in an image lattice) and style (i.e., structural and\n",
      "appearance variations encoded by latent vectors). This paper first proposes an\n",
      "intuitive paradigm for the task, layout-to-mask-to-image, to learn to unfold\n",
      "object masks of given bounding boxes in an input layout to bridge the gap\n",
      "between the input layout and synthesized images. Then, this paper presents a\n",
      "method built on Generative Adversarial Networks for the proposed\n",
      "layout-to-mask-to-image with style control at both image and mask levels.\n",
      "Object masks are learned from the input layout and iteratively refined along\n",
      "stages in the generator network. Style control at the image level is the same\n",
      "as in vanilla GANs, while style control at the object mask level is realized by\n",
      "a proposed novel feature normalization scheme, Instance-Sensitive and\n",
      "Layout-Aware Normalization. In experiments, the proposed method is tested in\n",
      "the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art\n",
      "performance obtained. \n",
      "\n",
      "\n",
      "Synthetic data has been a critical tool for training scene text detection and\n",
      "recognition models. On the one hand, synthetic word images have proven to be a\n",
      "successful substitute for real images in training scene text recognizers. On\n",
      "the other hand, however, scene text detectors still heavily rely on a large\n",
      "amount of manually annotated real-world images, which are expensive. In this\n",
      "paper, we introduce UnrealText, an efficient image synthesis method that\n",
      "renders realistic images via a 3D graphics engine. 3D synthetic engine provides\n",
      "realistic appearance by rendering scene and text as a whole, and allows for\n",
      "better text region proposals with access to precise scene information, e.g.\n",
      "normal and even object meshes. The comprehensive experiments verify its\n",
      "effectiveness on both scene text detection and recognition. We also generate a\n",
      "multilingual version for future research into multilingual scene text detection\n",
      "and recognition. Additionally, we re-annotate scene text recognition datasets\n",
      "in a case-sensitive way and include punctuation marks for more comprehensive\n",
      "evaluations. The code and the generated datasets are released at:\n",
      "https://github.com/Jyouhou/UnrealText/ . \n",
      "\n",
      "\n",
      "Conditional Generative Adversarial Networks (cGANs) have enabled controllable\n",
      "image synthesis for many vision and graphics applications. However, recent\n",
      "cGANs are 1-2 orders of magnitude more compute-intensive than modern\n",
      "recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to\n",
      "0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In\n",
      "this work, we propose a general-purpose compression framework for reducing the\n",
      "inference time and model size of the generator in cGANs. Directly applying\n",
      "existing compression methods yields poor performance due to the difficulty of\n",
      "GAN training and the differences in generator architectures. We address these\n",
      "challenges in two ways. First, to stabilize GAN training, we transfer knowledge\n",
      "of multiple intermediate representations of the original model to its\n",
      "compressed model and unify unpaired and paired learning. Second, instead of\n",
      "reusing existing CNN designs, our method finds efficient architectures via\n",
      "neural architecture search. To accelerate the search process, we decouple the\n",
      "model training and search via weight sharing. Experiments demonstrate the\n",
      "effectiveness of our method across different supervision settings, network\n",
      "architectures, and learning methods. Without losing image quality, we reduce\n",
      "the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by\n",
      "9x, paving the way for interactive image synthesis. \n",
      "\n",
      "\n",
      "The ability to detect failures and anomalies are fundamental requirements for\n",
      "building reliable systems for computer vision applications, especially\n",
      "safety-critical applications of semantic segmentation, such as autonomous\n",
      "driving and medical image analysis. In this paper, we systematically study\n",
      "failure and anomaly detection for semantic segmentation and propose a unified\n",
      "framework, consisting of two modules, to address these two related problems.\n",
      "The first module is an image synthesis module, which generates a synthesized\n",
      "image from a segmentation layout map, and the second is a comparison module,\n",
      "which computes the difference between the synthesized image and the input\n",
      "image. We validate our framework on three challenging datasets and improve the\n",
      "state-of-the-arts by large margins, \\emph{i.e.}, 6% AUPR-Error on Cityscapes,\n",
      "7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on\n",
      "StreetHazards anomaly segmentation. \n",
      "\n",
      "\n",
      "Style transfer has recently received a lot of attention, since it allows to\n",
      "study fundamental challenges in image understanding and synthesis. Recent work\n",
      "has significantly improved the representation of color and texture and\n",
      "computational speed and image resolution. The explicit transformation of image\n",
      "content has, however, been mostly neglected: while artistic style affects\n",
      "formal characteristics of an image, such as color, shape or texture, it also\n",
      "deforms, adds or removes content details. This paper explicitly focuses on a\n",
      "content-and style-aware stylization of a content image. Therefore, we introduce\n",
      "a content transformation module between the encoder and decoder. Moreover, we\n",
      "utilize similar content appearing in photographs and style samples to learn how\n",
      "style alters content details and we generalize this to other class details.\n",
      "Additionally, this work presents a novel normalization layer critical for high\n",
      "resolution image synthesis. The robustness and speed of our model enables a\n",
      "video stylization in real-time and high definition. We perform extensive\n",
      "qualitative and quantitative evaluations to demonstrate the validity of our\n",
      "approach. \n",
      "\n",
      "\n",
      "Swapping text in scene images while preserving original fonts, colors, sizes\n",
      "and background textures is a challenging task due to the complex interplay\n",
      "between different factors. In this work, we present SwapText, a three-stage\n",
      "framework to transfer texts across scene images. First, a novel text swapping\n",
      "network is proposed to replace text labels only in the foreground image.\n",
      "Second, a background completion network is learned to reconstruct background\n",
      "images. Finally, the generated foreground image and background image are used\n",
      "to generate the word image by the fusion network. Using the proposing\n",
      "framework, we can manipulate the texts of the input images even with severe\n",
      "geometric distortion. Qualitative and quantitative results are presented on\n",
      "several scene text datasets, including regular and irregular text datasets. We\n",
      "conducted extensive experiments to prove the usefulness of our method such as\n",
      "image based text translation, text image synthesis, etc. \n",
      "\n",
      "\n",
      "We show that the sum of the implicit generator log-density $\\log p_g$ of a\n",
      "GAN with the logit score of the discriminator defines an energy function which\n",
      "yields the true data density when the generator is imperfect but the\n",
      "discriminator is optimal, thus making it possible to improve on the typical\n",
      "generator (with implicit density $p_g$). To make that practical, we show that\n",
      "sampling from this modified density can be achieved by sampling in latent space\n",
      "according to an energy-based model induced by the sum of the latent prior\n",
      "log-density and the discriminator output score. This can be achieved by running\n",
      "a Langevin MCMC in latent space and then applying the generator function, which\n",
      "we call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is\n",
      "highly efficient compared to previous methods which work in the\n",
      "high-dimensional pixel space and can be applied to improve on previously\n",
      "trained GANs of many types. We evaluate DDLS on both synthetic and real-world\n",
      "datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially\n",
      "improves the Inception Score of an off-the-shelf pre-trained\n",
      "SN-GAN~\\citep{sngan} from $8.22$ to $9.09$ which is even comparable to the\n",
      "class-conditional BigGAN~\\citep{biggan} model. This achieves a new\n",
      "state-of-the-art in unconditional image synthesis setting without introducing\n",
      "extra parameters or additional training. \n",
      "\n",
      "\n",
      "In conditional Generative Adversarial Networks (cGANs), when two different\n",
      "initial noises are concatenated with the same conditional information, the\n",
      "distance between their outputs is relatively smaller, which makes minor modes\n",
      "likely to collapse into large modes. To prevent this happen, we proposed a\n",
      "hierarchical mode exploring method to alleviate mode collapse in cGANs by\n",
      "introducing a diversity measurement into the objective function as the\n",
      "regularization term. We also introduced the Expected Ratios of Expansion (ERE)\n",
      "into the regularization term, by minimizing the sum of differences between the\n",
      "real change of distance and ERE, we can control the diversity of generated\n",
      "images w.r.t specific-level features. We validated the proposed algorithm on\n",
      "four conditional image synthesis tasks including categorical generation, paired\n",
      "and un-paired image translation and text-to-image generation. Both qualitative\n",
      "and quantitative results show that the proposed method is effective in\n",
      "alleviating the mode collapse problem in cGANs, and can control the diversity\n",
      "of output images w.r.t specific-level features. \n",
      "\n",
      "\n",
      "In this work we propose a new computational framework, based on generative\n",
      "deep models, for synthesis of photo-realistic food meal images from textual\n",
      "list of its ingredients. Previous works on synthesis of images from text\n",
      "typically rely on pre-trained text models to extract text features, followed by\n",
      "generative neural networks (GAN) aimed to generate realistic images conditioned\n",
      "on the text features. These works mainly focus on generating spatially compact\n",
      "and well-defined categories of objects, such as birds or flowers, but meal\n",
      "images are significantly more complex, consisting of multiple ingredients whose\n",
      "appearance and spatial qualities are further modified by cooking methods. To\n",
      "generate real-like meal images from ingredients, we propose Cook Generative\n",
      "Adversarial Networks (CookGAN), CookGAN first builds an attention-based\n",
      "ingredients-image association model, which is then used to condition a\n",
      "generative neural network tasked with synthesizing meal images. Furthermore, a\n",
      "cycle-consistent constraint is added to further improve image quality and\n",
      "control appearance. Experiments show our model is able to generate meal images\n",
      "corresponding to the ingredients. \n",
      "\n",
      "\n",
      "We propose a hybrid controllable image generation method to synthesize\n",
      "anatomically meaningful 3D+t labeled Cardiac Magnetic Resonance (CMR) images.\n",
      "Our hybrid method takes the mechanistic 4D eXtended CArdiac Torso (XCAT) heart\n",
      "model as the anatomical ground truth and synthesizes CMR images via a\n",
      "data-driven Generative Adversarial Network (GAN). We employ the\n",
      "state-of-the-art SPatially Adaptive De-normalization (SPADE) technique for\n",
      "conditional image synthesis to preserve the semantic spatial information of\n",
      "ground truth anatomy. Using the parameterized motion model of the XCAT heart,\n",
      "we generate labels for 25 time frames of the heart for one cardiac cycle at 18\n",
      "locations for the short axis view. Subsequently, realistic images are generated\n",
      "from these labels, with modality-specific features that are learned from real\n",
      "CMR image data. We demonstrate that style transfer from another cardiac image\n",
      "can be accomplished by using a style encoder network. Due to the flexibility of\n",
      "XCAT in creating new heart models, this approach can result in a realistic\n",
      "virtual population to address different challenges the medical image analysis\n",
      "research community is facing such as expensive data collection. Our proposed\n",
      "method has a great potential to synthesize 4D controllable CMR images with\n",
      "annotations and adaptable styles to be used in various supervised multi-site,\n",
      "multi-vendor applications in medical image analysis. \n",
      "\n",
      "\n",
      "Monte Carlo sampling techniques are used to estimate high-dimensional\n",
      "integrals that model the physics of light transport in virtual scenes for\n",
      "computer graphics applications. These methods rely on the law of large numbers\n",
      "to estimate expectations via simulation, typically resulting in slow\n",
      "convergence. Their errors usually manifest as undesirable grain in the pictures\n",
      "generated by image synthesis algorithms. It is well known that these errors\n",
      "diminish when the samples are chosen appropriately. A well known technique for\n",
      "reducing error operates by subdividing the integration domain, estimating\n",
      "integrals in each \\emph{stratum} and aggregating these values into a stratified\n",
      "sampling estimate. Na\\\"{i}ve methods for stratification, based on a lattice\n",
      "(grid) are known to improve the convergence rate of Monte Carlo, but require\n",
      "samples that grow exponentially with the dimensionality of the domain.\n",
      "  We propose a simple stratification scheme for $d$ dimensional hypercubes\n",
      "using the kd-tree data structure. Our scheme enables the generation of an\n",
      "arbitrary number of equal volume partitions of the rectangular domain, and $n$\n",
      "samples can be generated in $O(n)$ time. Since we do not always need to\n",
      "explicitly build a kd-tree, we provide a simple procedure that allows the\n",
      "sample set to be drawn fully in parallel without any precomputation or storage,\n",
      "speeding up sampling to $O(\\log n)$ time per sample when executed on $n$ cores.\n",
      "If the tree is implicitly precomputed ($O(n)$ storage) the parallelised run\n",
      "time reduces to $O(1)$ on $n$ cores. In addition to these benefits, we provide\n",
      "an upper bound on the worst case star-discrepancy for $n$ samples matching that\n",
      "of lattice-based sampling strategies, which occur as a special case of our\n",
      "proposed method. We use a number of quantitative and qualitative tests to\n",
      "compare our method against state of the art samplers for image synthesis. \n",
      "\n",
      "\n",
      "Learned joint representations of images and text form the backbone of several\n",
      "important cross-domain tasks such as image captioning. Prior work mostly maps\n",
      "both domains into a common latent representation in a purely supervised\n",
      "fashion. This is rather restrictive, however, as the two domains follow\n",
      "distinct generative processes. Therefore, we propose a novel semi-supervised\n",
      "framework, which models shared information between domains and domain-specific\n",
      "information separately. The information shared between the domains is aligned\n",
      "with an invertible neural network. Our model integrates normalizing flow-based\n",
      "priors for the domain-specific information, which allows us to learn diverse\n",
      "many-to-many mappings between the two domains. We demonstrate the effectiveness\n",
      "of our model on diverse tasks, including image captioning and text-to-image\n",
      "synthesis. \n",
      "\n",
      "\n",
      "In this paper, we demonstrated a practical application of realistic river\n",
      "image generation using deep learning. Specifically, we explored a generative\n",
      "adversarial network (GAN) model capable of generating high-resolution and\n",
      "realistic river images that can be used to support modeling and analysis in\n",
      "surface water estimation, river meandering, wetland loss, and other\n",
      "hydrological research studies. First, we have created an extensive repository\n",
      "of overhead river images to be used in training. Second, we incorporated the\n",
      "Progressive Growing GAN (PGGAN), a network architecture that iteratively trains\n",
      "smaller-resolution GANs to gradually build up to a very high resolution to\n",
      "generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\n",
      "GAN architectures, difficulties arose in terms of exponential increase of\n",
      "training time and vanishing/exploding gradient issues, which the PGGAN\n",
      "implementation seemed to significantly reduce. The results presented in this\n",
      "study show great promise in generating high-quality images and capturing the\n",
      "details of river structure and flow to support hydrological research, which\n",
      "often requires extensive imagery for model performance. \n",
      "\n",
      "\n",
      "Recent work has increased the performance of Generative Adversarial Networks\n",
      "(GANs) by enforcing a consistency cost on the discriminator. We improve on this\n",
      "technique in several ways. We first show that consistency regularization can\n",
      "introduce artifacts into the GAN samples and explain how to fix this issue. We\n",
      "then propose several modifications to the consistency regularization procedure\n",
      "designed to improve its performance. We carry out extensive experiments\n",
      "quantifying the benefit of our improvements. For unconditional image synthesis\n",
      "on CIFAR-10 and CelebA, our modifications yield the best known FID scores on\n",
      "various GAN architectures. For conditional image synthesis on CIFAR-10, we\n",
      "improve the state-of-the-art FID score from 11.48 to 9.21. Finally, on\n",
      "ImageNet-2012, we apply our technique to the original BigGAN model and improve\n",
      "the FID from 6.66 to 5.38, which is the best score at that model size. \n",
      "\n",
      "\n",
      "Magnetic resonance imaging (MRI) is a widely used neuroimaging technique that\n",
      "can provide images of different contrasts (i.e., modalities). Fusing this\n",
      "multi-modal data has proven particularly effective for boosting model\n",
      "performance in many tasks. However, due to poor data quality and frequent\n",
      "patient dropout, collecting all modalities for every patient remains a\n",
      "challenge. Medical image synthesis has been proposed as an effective solution\n",
      "to this, where any missing modalities are synthesized from the existing ones.\n",
      "In this paper, we propose a novel Hybrid-fusion Network (Hi-Net) for\n",
      "multi-modal MR image synthesis, which learns a mapping from multi-modal source\n",
      "images (i.e., existing modalities) to target images (i.e., missing modalities).\n",
      "In our Hi-Net, a modality-specific network is utilized to learn representations\n",
      "for each individual modality, and a fusion network is employed to learn the\n",
      "common latent representation of multi-modal data. Then, a multi-modal synthesis\n",
      "network is designed to densely combine the latent representation with\n",
      "hierarchical features from each modality, acting as a generator to synthesize\n",
      "the target images. Moreover, a layer-wise multi-modal fusion strategy is\n",
      "presented to effectively exploit the correlations among multiple modalities, in\n",
      "which a Mixed Fusion Block (MFB) is proposed to adaptively weight different\n",
      "fusion strategies (i.e., element-wise summation, product, and maximization).\n",
      "Extensive experiments demonstrate that the proposed model outperforms other\n",
      "state-of-the-art medical image synthesis methods. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have proven successful for\n",
      "unsupervised image generation. Several works have extended GANs to image\n",
      "inpainting by conditioning the generation with parts of the image to be\n",
      "reconstructed. Despite their success, these methods have limitations in\n",
      "settings where only a small subset of the image pixels is known beforehand. In\n",
      "this paper we investigate the effectiveness of conditioning GANs when very few\n",
      "pixel values are provided. We propose a modelling framework which results in\n",
      "adding an explicit cost term to the GAN objective function to enforce\n",
      "pixel-wise conditioning. We investigate the influence of this regularization\n",
      "term on the quality of the generated images and the fulfillment of the given\n",
      "pixel constraints. Using the recent PacGAN technique, we ensure that we keep\n",
      "diversity in the generated samples. Conducted experiments on FashionMNIST show\n",
      "that the regularization term effectively controls the trade-off between quality\n",
      "of the generated images and the conditioning. Experimental evaluation on the\n",
      "CIFAR-10 and CelebA datasets evidences that our method achieves accurate\n",
      "results both visually and quantitatively in term of Fr\\'echet Inception\n",
      "Distance, while still enforcing the pixel conditioning. We also evaluate our\n",
      "method on a texture image generation task using fully-convolutional networks.\n",
      "As a final contribution, we apply the method to a classical geological\n",
      "simulation application. \n",
      "\n",
      "\n",
      "We propose a novel model named Multi-Channel Attention Selection Generative\n",
      "Adversarial Network (SelectionGAN) for guided image-to-image translation, where\n",
      "we translate an input image into another while respecting an external semantic\n",
      "guidance. The proposed SelectionGAN explicitly utilizes the semantic guidance\n",
      "information and consists of two stages. In the first stage, the input image and\n",
      "the conditional semantic guidance are fed into a cycled semantic-guided\n",
      "generation network to produce initial coarse results. In the second stage, we\n",
      "refine the initial results by using the proposed multi-scale spatial pooling \\&\n",
      "channel selection module and the multi-channel attention selection module.\n",
      "Moreover, uncertainty maps automatically learned from attention maps are used\n",
      "to guide the pixel loss for better network optimization. Exhaustive experiments\n",
      "on four challenging guided image-to-image translation tasks (face, hand, body\n",
      "and street view) demonstrate that our SelectionGAN is able to generate\n",
      "significantly better results than the state-of-the-art methods. Meanwhile, the\n",
      "proposed framework and modules are unified solutions and can be applied to\n",
      "solve other generation tasks, such as semantic image synthesis. The code is\n",
      "available at https://github.com/Ha0Tang/SelectionGAN. \n",
      "\n",
      "\n",
      "We present a method for improving human design of chairs. The goal of the\n",
      "method is generating enormous chair candidates in order to facilitate human\n",
      "designer by creating sketches and 3d models accordingly based on the generated\n",
      "chair design. It consists of an image synthesis module, which learns the\n",
      "underlying distribution of training dataset, a super-resolution module, which\n",
      "improve quality of generated image and human involvements. Finally, we manually\n",
      "pick one of the generated candidates to create a real life chair for\n",
      "illustration. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are now widely used for\n",
      "photo-realistic image synthesis. In applications where a simulated image needs\n",
      "to be translated into a realistic image (sim-to-real), GANs trained on unpaired\n",
      "data from the two domains are susceptible to failure in semantic content\n",
      "retention as the image is translated from one domain to the other. This failure\n",
      "mode is more pronounced in cases where the real data lacks content diversity,\n",
      "resulting in a content \\emph{mismatch} between the two domains - a situation\n",
      "often encountered in real-world deployment. In this paper, we investigate the\n",
      "role of the discriminator's receptive field in GANs for unsupervised\n",
      "image-to-image translation with mismatched data, and study its effect on\n",
      "semantic content retention. Experiments with the discriminator architecture of\n",
      "a state-of-the-art coupled Variational Auto-Encoder (VAE) - GAN model on\n",
      "diverse, mismatched datasets show that the discriminator receptive field is\n",
      "directly correlated with semantic content discrepancy of the generated image. \n",
      "\n",
      "\n",
      "We present a robotic system for picking a target from a pile of objects that\n",
      "is capable of finding and grasping the target object by removing obstacles in\n",
      "the appropriate order. The fundamental idea is to segment instances with both\n",
      "visible and occluded masks, which we call `instance occlusion segmentation'. To\n",
      "achieve this, we extend an existing instance segmentation model with a novel\n",
      "`relook' architecture, in which the model explicitly learns the inter-instance\n",
      "relationship. Also, by using image synthesis, we make the system capable of\n",
      "handling new objects without human annotations. The experimental results show\n",
      "the effectiveness of the relook architecture when compared with a conventional\n",
      "model and of the image synthesis when compared to a human-annotated dataset. We\n",
      "also demonstrate the capability of our system to achieve picking a target in a\n",
      "cluttered environment with a real robot. \n",
      "\n",
      "\n",
      "Style transfer is a useful image synthesis technique that can re-render given\n",
      "image into another artistic style while preserving its content information.\n",
      "Generative Adversarial Network (GAN) is a widely adopted framework toward this\n",
      "task for its better representation ability on local style patterns than the\n",
      "traditional Gram-matrix based methods. However, most previous methods rely on\n",
      "sufficient amount of pre-collected style images to train the model. In this\n",
      "paper, a novel Patch Permutation GAN (P$^2$-GAN) network that can efficiently\n",
      "learn the stroke style from a single style image is proposed. We use patch\n",
      "permutation to generate multiple training samples from the given style image. A\n",
      "patch discriminator that can simultaneously process patch-wise images and\n",
      "natural images seamlessly is designed. We also propose a local texture\n",
      "descriptor based criterion to quantitatively evaluate the style transfer\n",
      "quality. Experimental results showed that our method can produce finer quality\n",
      "re-renderings from single style image with improved computational efficiency\n",
      "compared with many state-of-the-arts methods. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present Generative Adversarial Networks (GANs), in which the symmetric\n",
      "property of the generated images is controlled. This is obtained through the\n",
      "generator network's architecture, while the training procedure and the loss\n",
      "remain the same. The symmetric GANs are applied to face image synthesis in\n",
      "order to generate novel faces with a varying amount of symmetry. We also\n",
      "present an unsupervised face rotation capability, which is based on the novel\n",
      "notion of one-shot fine tuning. \n",
      "\n",
      "\n",
      "Learning disentangled representation of data without supervision is an\n",
      "important step towards improving the interpretability of generative models.\n",
      "Despite recent advances in disentangled representation learning, existing\n",
      "approaches often suffer from the trade-off between representation learning and\n",
      "generation performance i.e. improving generation quality sacrifices\n",
      "disentanglement performance). We propose an Information-Distillation Generative\n",
      "Adversarial Network (ID-GAN), a simple yet generic framework that easily\n",
      "incorporates the existing state-of-the-art models for both disentanglement\n",
      "learning and high-fidelity synthesis. Our method learns disentangled\n",
      "representation using VAE-based models, and distills the learned representation\n",
      "with an additional nuisance variable to the separate GAN-based generator for\n",
      "high-fidelity synthesis. To ensure that both generative models are aligned to\n",
      "render the same generative factors, we further constrain the GAN generator to\n",
      "maximize the mutual information between the learned latent code and the output.\n",
      "Despite the simplicity, we show that the proposed method is highly effective,\n",
      "achieving comparable image generation quality to the state-of-the-art methods\n",
      "using the disentangled representation. We also show that the proposed\n",
      "decomposition leads to an efficient and stable model design, and we demonstrate\n",
      "photo-realistic high-resolution image synthesis results (1024x1024 pixels) for\n",
      "the first time using the disentangled representations. \n",
      "\n",
      "\n",
      "Deep neural networks (DNNs) have achieved impressive performance on handling\n",
      "computer vision problems, however, it has been found that DNNs are vulnerable\n",
      "to adversarial examples. For such reason, adversarial perturbations have been\n",
      "recently studied in several respects. However, most previous works have focused\n",
      "on image classification tasks, and it has never been studied regarding\n",
      "adversarial perturbations on Image-to-image (Im2Im) translation tasks, showing\n",
      "great success in handling paired and/or unpaired mapping problems in the field\n",
      "of autonomous driving and robotics. This paper examines different types of\n",
      "adversarial perturbations that can fool Im2Im frameworks for autonomous driving\n",
      "purpose. We propose both quasi-physical and digital adversarial perturbations\n",
      "that can make Im2Im models yield unexpected results. We then empirically\n",
      "analyze these perturbations and show that they generalize well under both\n",
      "paired for image synthesis and unpaired settings for style transfer. We also\n",
      "validate that there exist some perturbation thresholds over which the Im2Im\n",
      "mapping is disrupted or impossible. The existence of these perturbations\n",
      "reveals that there exist crucial weaknesses in Im2Im models. Lastly, we show\n",
      "that our methods illustrate how perturbations affect the quality of outputs,\n",
      "pioneering the improvement of the robustness of current SOTA networks for\n",
      "autonomous driving. \n",
      "\n",
      "\n",
      "Generating photorealistic images of human subjects in any unseen pose have\n",
      "crucial applications in generating a complete appearance model of the subject.\n",
      "However, from a computer vision perspective, this task becomes significantly\n",
      "challenging due to the inability of modelling the data distribution conditioned\n",
      "on pose. Existing works use a complicated pose transformation model with\n",
      "various additional features such as foreground segmentation, human body parsing\n",
      "etc. to achieve robustness that leads to computational overhead. In this work,\n",
      "we propose a simple yet effective pose transformation GAN by utilizing the\n",
      "Residual Learning method without any additional feature learning to generate a\n",
      "given human image in any arbitrary pose. Using effective data augmentation\n",
      "techniques and cleverly tuning the model, we achieve robustness in terms of\n",
      "illumination, occlusion, distortion and scale. We present a detailed study,\n",
      "both qualitative and quantitative, to demonstrate the superiority of our model\n",
      "over the existing methods on two large datasets. \n",
      "\n",
      "\n",
      "Graphic design is essential for visual communication with layouts being\n",
      "fundamental to composing attractive designs. Layout generation differs from\n",
      "pixel-level image synthesis and is unique in terms of the requirement of mutual\n",
      "relations among the desired components. We propose a method for design layout\n",
      "generation that can satisfy user-specified constraints. The proposed neural\n",
      "design network (NDN) consists of three modules. The first module predicts a\n",
      "graph with complete relations from a graph with user-specified relations. The\n",
      "second module generates a layout from the predicted graph. Finally, the third\n",
      "module fine-tunes the predicted layout. Quantitative and qualitative\n",
      "experiments demonstrate that the generated layouts are visually similar to real\n",
      "design layouts. We also construct real designs based on predicted layouts for a\n",
      "better understanding of the visual quality. Finally, we demonstrate a practical\n",
      "application on layout recommendation. \n",
      "\n",
      "\n",
      "Typical methods for text-to-image synthesis seek to design effective\n",
      "generative architecture to model the text-to-image mapping directly. It is\n",
      "fairly arduous due to the cross-modality translation. In this paper we\n",
      "circumvent this problem by focusing on parsing the content of both the input\n",
      "text and the synthesized image thoroughly to model the text-to-image\n",
      "consistency in the semantic level. Particularly, we design a memory structure\n",
      "to parse the textual content by exploring semantic correspondence between each\n",
      "word in the vocabulary to its various visual contexts across relevant images\n",
      "during text encoding. Meanwhile, the synthesized image is parsed to learn its\n",
      "semantics in an object-aware manner. Moreover, we customize a conditional\n",
      "discriminator to model the fine-grained correlations between words and image\n",
      "sub-regions to push for the text-image semantic alignment. Extensive\n",
      "experiments on COCO dataset manifest that our model advances the\n",
      "state-of-the-art performance significantly (from 35.69 to 52.73 in Inception\n",
      "Score). \n",
      "\n",
      "\n",
      "Automatic segmentation of anatomical landmarks from ultrasound (US) plays an\n",
      "important role in the management of preterm neonates with a very low birth\n",
      "weight due to the increased risk of developing intraventricular hemorrhage\n",
      "(IVH) or other complications. One major problem in developing an automatic\n",
      "segmentation method for this task is the limited availability of annotated\n",
      "data. To tackle this issue, we propose a novel image synthesis method using\n",
      "multi-scale self attention generator to synthesize US images from various\n",
      "segmentation masks. We show that our method can synthesize high-quality US\n",
      "images for every manipulated segmentation label with qualitative and\n",
      "quantitative improvements over the recent state-of-the-art synthesis methods.\n",
      "Furthermore, for the segmentation task, we propose a novel method, called\n",
      "Confidence-guided Brain Anatomy Segmentation (CBAS) network, where segmentation\n",
      "and corresponding confidence maps are estimated at different scales. In\n",
      "addition, we introduce a technique which guides CBAS to learn the weights based\n",
      "on the confidence measure about the estimate. Extensive experiments demonstrate\n",
      "that the proposed method for both synthesis and segmentation tasks achieve\n",
      "significant improvements over the recent state-of-the-art methods. In\n",
      "particular, we show that the new synthesis framework can be used to generate\n",
      "realistic US images which can be used to improve the performance of a\n",
      "segmentation algorithm. \n",
      "\n",
      "\n",
      "Despite the success of Generative Adversarial Networks (GANs) in image\n",
      "synthesis, applying trained GAN models to real image processing remains\n",
      "challenging. Previous methods typically invert a target image back to the\n",
      "latent space either by back-propagation or by learning an additional encoder.\n",
      "However, the reconstructions from both of the methods are far from ideal. In\n",
      "this work, we propose a novel approach, called mGANprior, to incorporate the\n",
      "well-trained GANs as effective prior to a variety of image processing tasks. In\n",
      "particular, we employ multiple latent codes to generate multiple feature maps\n",
      "at some intermediate layer of the generator, then compose them with adaptive\n",
      "channel importance to recover the input image. Such an over-parameterization of\n",
      "the latent space significantly improves the image reconstruction quality,\n",
      "outperforming existing competitors. The resulting high-fidelity image\n",
      "reconstruction enables the trained GAN models as prior to many real-world\n",
      "applications, such as image colorization, super-resolution, image inpainting,\n",
      "and semantic manipulation. We further analyze the properties of the layer-wise\n",
      "representation learned by GAN models and shed light on what knowledge each\n",
      "layer is capable of representing. \n",
      "\n",
      "\n",
      "Image-to-image translation is significant to many computer vision and machine\n",
      "learning tasks such as image synthesis and video synthesis. It has primary\n",
      "applications in the graphics editing and animation industries. With the\n",
      "development of generative adversarial networks, a lot of attention has been\n",
      "drawn to image-to-image translation tasks. In this paper, we propose and\n",
      "investigate a novel task named as panoptic-level image-to-image translation and\n",
      "a naive baseline of solving this task. Panoptic-level image translation extends\n",
      "the current image translation task to two separate objectives of semantic style\n",
      "translation (adjust the style of objects to that of different domains) and\n",
      "instance transfiguration (swap between different types of objects). The\n",
      "proposed task generates an image from a complete and detailed panoptic\n",
      "perspective which can enrich the context of real-world vision synthesis. Our\n",
      "contribution consists of the proposal of a significant task worth investigating\n",
      "and a naive baseline of solving it. The proposed baseline consists of the\n",
      "multiple instances sequential translation and semantic-level translation with\n",
      "domain-invariant content code. \n",
      "\n",
      "\n",
      "Satellite images hold great promise for continuous environmental monitoring\n",
      "and earth observation. Occlusions cast by clouds, however, can severely limit\n",
      "coverage, making ground information extraction more difficult. Existing\n",
      "pipelines typically perform cloud removal with simple temporal composites and\n",
      "hand-crafted filters. In contrast, we cast the problem of cloud removal as a\n",
      "conditional image synthesis challenge, and we propose a trainable\n",
      "spatiotemporal generator network (STGAN) to remove clouds. We train our model\n",
      "on a new large-scale spatiotemporal dataset that we construct, containing 97640\n",
      "image pairs covering all continents. We demonstrate experimentally that the\n",
      "proposed STGAN model outperforms standard models and can generate realistic\n",
      "cloud-free images with high PSNR and SSIM values across a variety of\n",
      "atmospheric conditions, leading to improved performance in downstream tasks\n",
      "such as land cover classification. \n",
      "\n",
      "\n",
      "In recent years, Generative Adversarial Networks have achieved impressive\n",
      "results in photorealistic image synthesis. This progress nurtures hopes that\n",
      "one day the classical rendering pipeline can be replaced by efficient models\n",
      "that are learned directly from images. However, current image synthesis models\n",
      "operate in the 2D domain where disentangling 3D properties such as camera\n",
      "viewpoint or object pose is challenging. Furthermore, they lack an\n",
      "interpretable and controllable representation. Our key hypothesis is that the\n",
      "image generation process should be modeled in 3D space as the physical world\n",
      "surrounding us is intrinsically three-dimensional. We define the new task of 3D\n",
      "controllable image synthesis and propose an approach for solving it by\n",
      "reasoning both in 3D space and in the 2D image domain. We demonstrate that our\n",
      "model is able to disentangle latent 3D factors of simple multi-object scenes in\n",
      "an unsupervised fashion from raw images. Compared to pure 2D baselines, it\n",
      "allows for synthesizing scenes that are consistent wrt. changes in viewpoint or\n",
      "object pose. We further evaluate various 3D representations in terms of their\n",
      "usefulness for this challenging task. \n",
      "\n",
      "\n",
      "We present a neural rendering framework that maps a voxelized scene into a\n",
      "high quality image. Highly-textured objects and scene element interactions are\n",
      "realistically rendered by our method, despite having a rough representation as\n",
      "an input. Moreover, our approach allows controllable rendering: geometric and\n",
      "appearance modifications in the input are accurately propagated to the output.\n",
      "The user can move, rotate and scale an object, change its appearance and\n",
      "texture or modify the position of the light and all these edits are represented\n",
      "in the final rendering. We demonstrate the effectiveness of our approach by\n",
      "rendering scenes with varying appearance, from single color per object to\n",
      "complex, high-frequency textures. We show that our rerendering network can\n",
      "generate very detailed images that represent precisely the appearance of the\n",
      "input scene. Our experiments illustrate that our approach achieves more\n",
      "accurate image synthesis results compared to alternatives and can also handle\n",
      "low voxel grid resolutions. Finally, we show how our neural rendering framework\n",
      "can capture and faithfully render objects from real images and from a diverse\n",
      "set of classes. \n",
      "\n",
      "\n",
      "In architecture and computer-aided design, wireframes (i.e., line-based\n",
      "models) are widely used as basic 3D models for design evaluation and fast\n",
      "design iterations. However, unlike a full design file, a wireframe model lacks\n",
      "critical information, such as detailed shape, texture, and materials, needed by\n",
      "a conventional renderer to produce 2D renderings of the objects or scenes. In\n",
      "this paper, we bridge the information gap by generating photo-realistic\n",
      "rendering of indoor scenes from wireframe models in an image translation\n",
      "framework. While existing image synthesis methods can generate visually\n",
      "pleasing images for common objects such as faces and birds, these methods do\n",
      "not explicitly model and preserve essential structural constraints in a\n",
      "wireframe model, such as junctions, parallel lines, and planar surfaces. To\n",
      "this end, we propose a novel model based on a structure-appearance joint\n",
      "representation learned from both images and wireframes. In our model,\n",
      "structural constraints are explicitly enforced by learning a joint\n",
      "representation in a shared encoder network that must support the generation of\n",
      "both images and wireframes. Experiments on a wireframe-scene dataset show that\n",
      "our wireframe-to-image translation model significantly outperforms the\n",
      "state-of-the-art methods in both visual quality and structural integrity of\n",
      "generated images. \n",
      "\n",
      "\n",
      "Image synthesis is currently one of the most addressed image processing topic\n",
      "in computer vision and deep learning fields of study. Researchers have tackled\n",
      "this problem focusing their efforts on its several challenging problems, e.g.\n",
      "image quality and size, domain and pose changing, architecture of the networks,\n",
      "and so on. Above all, producing images belonging to different domains by using\n",
      "a single architecture is a very relevant goal for image generation. In fact, a\n",
      "single multi-domain network would allow greater flexibility and robustness in\n",
      "the image synthesis task than other approaches. This paper proposes a novel\n",
      "architecture and a training algorithm, which are able to produce multi-domain\n",
      "outputs using a single network. A small portion of a dataset is intentionally\n",
      "used, and there are no hard-coded labels (or classes). This is achieved by\n",
      "combining a conditional Generative Adversarial Network (cGAN) for image\n",
      "generation and a Meta-Learning algorithm for domain switch, and we called our\n",
      "approach MetalGAN. The approach has proved to be appropriate for solving the\n",
      "multi-domain problem and it is validated on facial attribute transfer, using\n",
      "CelebA dataset. \n",
      "\n",
      "\n",
      "A good image-to-image translation model should learn a mapping between\n",
      "different visual domains while satisfying the following properties: 1)\n",
      "diversity of generated images and 2) scalability over multiple domains.\n",
      "Existing methods address either of the issues, having limited diversity or\n",
      "multiple models for all domains. We propose StarGAN v2, a single framework that\n",
      "tackles both and shows significantly improved results over the baselines.\n",
      "Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our\n",
      "superiority in terms of visual quality, diversity, and scalability. To better\n",
      "assess image-to-image translation models, we release AFHQ, high-quality animal\n",
      "faces with large inter- and intra-domain differences. The code, pretrained\n",
      "models, and dataset can be found at https://github.com/clovaai/stargan-v2. \n",
      "\n",
      "\n",
      "Recent years have seen the rise of convolutional neural network techniques in\n",
      "exemplar-based image synthesis. These methods often rely on the minimization of\n",
      "some variational formulation on the image space for which the minimizers are\n",
      "assumed to be the solutions of the synthesis problem. In this paper we\n",
      "investigate, both theoretically and experimentally, another framework to deal\n",
      "with this problem using an alternate sampling/minimization scheme. First, we\n",
      "use results from information geometry to assess that our method yields a\n",
      "probability measure which has maximum entropy under some constraints in\n",
      "expectation. Then, we turn to the analysis of our method and we show, using\n",
      "recent results from the Markov chain literature, that its error can be\n",
      "explicitly bounded with constants which depend polynomially in the dimension\n",
      "even in the non-convex setting. This includes the case where the constraints\n",
      "are defined via a differentiable neural network. Finally, we present an\n",
      "extensive experimental study of the model, including a comparison with\n",
      "state-of-the-art methods and an extension to style transfer. \n",
      "\n",
      "\n",
      "We propose semantic region-adaptive normalization (SEAN), a simple but\n",
      "effective building block for Generative Adversarial Networks conditioned on\n",
      "segmentation masks that describe the semantic regions in the desired output\n",
      "image. Using SEAN normalization, we can build a network architecture that can\n",
      "control the style of each semantic region individually, e.g., we can specify\n",
      "one style reference image per region. SEAN is better suited to encode,\n",
      "transfer, and synthesize style than the best previous method in terms of\n",
      "reconstruction quality, variability, and visual quality. We evaluate SEAN on\n",
      "multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than\n",
      "the current state of the art. SEAN also pushes the frontier of interactive\n",
      "image editing. We can interactively edit images by changing segmentation masks\n",
      "or the style for any given region. We can also interpolate styles from two\n",
      "reference images per region. \n",
      "\n",
      "\n",
      "Medical image synthesis has gained a great focus recently, especially after\n",
      "the introduction of Generative Adversarial Networks (GANs). GANs have been used\n",
      "widely to provide anatomically-plausible and diverse samples for augmentation\n",
      "and other applications, including segmentation and super resolution. In our\n",
      "previous work, Deep Convolutional GANs were used to generate synthetic\n",
      "mammogram lesions, masses mainly, that could enhance the classification\n",
      "performance in imbalanced datasets. In this new work, a deeper investigation\n",
      "was carried out to explore other aspects of the generated images evaluation,\n",
      "i.e., realism, feature space distribution, and observers studies. t-Stochastic\n",
      "Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and\n",
      "fake images to enable 2D visualisations. Additionally, two expert radiologists\n",
      "performed a realism-evaluation study. Visualisations showed that the generated\n",
      "images have a similar feature distribution of the real ones, avoiding outliers.\n",
      "Moreover, Receiver Operating Characteristic (ROC) curve showed that the\n",
      "radiologists could not, in many cases, distinguish between synthetic and real\n",
      "lesions, giving 48% and 61% accuracies in a balanced sample set. \n",
      "\n",
      "\n",
      "Example-guided image synthesis has been recently attempted to synthesize an\n",
      "image from a semantic label map and an exemplary image. In the task, the\n",
      "additional exemplary image serves to provide style guidance that controls the\n",
      "appearance of the synthesized output. Despite the controllability advantage,\n",
      "the previous models are designed on datasets with specific and roughly aligned\n",
      "objects. In this paper, we tackle a more challenging and general task, where\n",
      "the exemplar is an arbitrary scene image that is semantically unaligned to the\n",
      "given label map. To this end, we first propose a new Masked Spatial-Channel\n",
      "Attention (MSCA) module which models the correspondence between two\n",
      "unstructured scenes via cross-attention. Next, we propose an end-to-end network\n",
      "for joint global and local feature alignment and synthesis. In addition, we\n",
      "propose a novel patch-based self-supervision scheme to enable training.\n",
      "Experiments on the large-scale CCOO-stuff dataset show significant improvements\n",
      "over existing methods. Moreover, our approach provides interpretability and can\n",
      "be readily extended to other tasks including style and spatial interpolation or\n",
      "extrapolation, as well as other content manipulation. \n",
      "\n",
      "\n",
      "Coupling the high-fidelity generation capabilities of label-conditional image\n",
      "synthesis methods with the flexibility of unconditional generative models, we\n",
      "propose a semantic bottleneck GAN model for unconditional synthesis of complex\n",
      "scenes. We assume pixel-wise segmentation labels are available during training\n",
      "and use them to learn the scene structure. During inference, our model first\n",
      "synthesizes a realistic segmentation layout from scratch, then synthesizes a\n",
      "realistic scene conditioned on that layout. For the former, we use an\n",
      "unconditional progressive segmentation generation network that captures the\n",
      "distribution of realistic semantic scene layouts. For the latter, we use a\n",
      "conditional segmentation-to-image synthesis network that captures the\n",
      "distribution of photo-realistic images conditioned on the semantic layout. When\n",
      "trained end-to-end, the resulting model outperforms state-of-the-art generative\n",
      "models in unsupervised image synthesis on two challenging domains in terms of\n",
      "the Frechet Inception Distance and user-study evaluations. Moreover, we\n",
      "demonstrate the generated segmentation maps can be used as additional training\n",
      "data to strongly improve recent segmentation-to-image synthesis networks. \n",
      "\n",
      "\n",
      "Contrast enhancement and noise removal are coupled problems for low-light\n",
      "image enhancement. The existing Retinex based methods do not take the coupling\n",
      "relation into consideration, resulting in under or over-smoothing of the\n",
      "enhanced images. To address this issue, this paper presents a novel progressive\n",
      "Retinex framework, in which illumination and noise of low-light image are\n",
      "perceived in a mutually reinforced manner, leading to noise reduction low-light\n",
      "enhancement results. Specifically, two fully pointwise convolutional neural\n",
      "networks are devised to model the statistical regularities of ambient light and\n",
      "image noise respectively, and to leverage them as constraints to facilitate the\n",
      "mutual learning process. The proposed method not only suppresses the\n",
      "interference caused by the ambiguity between tiny textures and image noises,\n",
      "but also greatly improves the computational efficiency. Moreover, to solve the\n",
      "problem of insufficient training data, we propose an image synthesis strategy\n",
      "based on camera imaging model, which generates color images corrupted by\n",
      "illumination-dependent noises. Experimental results on both synthetic and real\n",
      "low-light images demonstrate the superiority of our proposed approaches against\n",
      "the State-Of-The-Art (SOTA) low-light enhancement methods. \n",
      "\n",
      "\n",
      "Despite the success of Generative Adversarial Networks (GANs) in image\n",
      "synthesis, there lacks enough understanding on what generative models have\n",
      "learned inside the deep generative representations and how photo-realistic\n",
      "images are able to be composed of the layer-wise stochasticity introduced in\n",
      "recent GANs. In this work, we show that highly-structured semantic hierarchy\n",
      "emerges as variation factors from synthesizing scenes from the generative\n",
      "representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By\n",
      "probing the layer-wise representations with a broad set of semantics at\n",
      "different abstraction levels, we are able to quantify the causality between the\n",
      "activations and semantics occurring in the output image. Such a quantification\n",
      "identifies the human-understandable variation factors learned by GANs to\n",
      "compose scenes. The qualitative and quantitative results further suggest that\n",
      "the generative representations learned by the GANs with layer-wise latent codes\n",
      "are specialized to synthesize different hierarchical semantics: the early\n",
      "layers tend to determine the spatial layout and configuration, the middle\n",
      "layers control the categorical objects, and the later layers finally render the\n",
      "scene attributes as well as color scheme. Identifying such a set of\n",
      "manipulatable latent variation factors facilitates semantic scene manipulation. \n",
      "\n",
      "\n",
      "Over the past years, Generative Adversarial Networks (GANs) have shown a\n",
      "remarkable generation performance especially in image synthesis. Unfortunately,\n",
      "they are also known for having an unstable training process and might loose\n",
      "parts of the data distribution for heterogeneous input data. In this paper, we\n",
      "propose a novel GAN extension for multi-modal distribution learning (MMGAN). In\n",
      "our approach, we model the latent space as a Gaussian mixture model with a\n",
      "number of clusters referring to the number of disconnected data manifolds in\n",
      "the observation space, and include a clustering network, which relates each\n",
      "data manifold to one Gaussian cluster. Thus, the training gets more stable.\n",
      "Moreover, MMGAN allows for clustering real data according to the learned data\n",
      "manifold in the latent space. By a series of benchmark experiments, we\n",
      "illustrate that MMGAN outperforms competitive state-of-the-art models in terms\n",
      "of clustering performance. \n",
      "\n",
      "\n",
      "Recent advances in deep generative models have demonstrated impressive\n",
      "results in photo-realistic facial image synthesis and editing. Facial\n",
      "expressions are inherently the result of muscle movement. However, existing\n",
      "neural network-based approaches usually only rely on texture generation to edit\n",
      "expressions and largely neglect the motion information. In this work, we\n",
      "propose a novel end-to-end network that disentangles the task of facial editing\n",
      "into two steps: a \" \"motion-editing\" step and a \"texture-editing\" step. In the\n",
      "\"motion-editing\" step, we explicitly model facial movement through image\n",
      "deformation, warping the image into the desired expression. In the\n",
      "\"texture-editing\" step, we generate necessary textures, such as teeth and\n",
      "shading effects, for a photo-realistic result. Our physically-based\n",
      "task-disentanglement system design allows each step to learn a focused task,\n",
      "removing the need of generating texture to hallucinate motion. Our system is\n",
      "trained in a self-supervised manner, requiring no ground truth deformation\n",
      "annotation. Using Action Units [8] as the representation for facial expression,\n",
      "our method improves the state-of-the-art facial expression editing performance\n",
      "in both qualitative and quantitative evaluations. \n",
      "\n",
      "\n",
      "Image generation task has received increasing attention because of its wide\n",
      "application in security and entertainment. Sketch-based face generation brings\n",
      "more fun and better quality of image generation due to supervised interaction.\n",
      "However, When a sketch poorly aligned with the true face is given as input,\n",
      "existing supervised image-to-image translation methods often cannot generate\n",
      "acceptable photo-realistic face images. To address this problem, in this paper\n",
      "we propose Cali-Sketch, a poorly-drawn-sketch to photo-realistic-image\n",
      "generation method. Cali-Sketch explicitly models stroke calibration and image\n",
      "generation using two constituent networks: a Stroke Calibration Network (SCN),\n",
      "which calibrates strokes of facial features and enriches facial details while\n",
      "preserving the original intent features; and an Image Synthesis Network (ISN),\n",
      "which translates the calibrated and enriched sketches to photo-realistic face\n",
      "images. In this way, we manage to decouple a difficult cross-domain translation\n",
      "problem into two easier steps. Extensive experiments verify that the face\n",
      "photos generated by Cali-Sketch are both photo-realistic and faithful to the\n",
      "input sketches, compared with state-of-the-art methods \n",
      "\n",
      "\n",
      "A commonly used evaluation metric for text-to-image synthesis is the\n",
      "Inception score (IS) \\cite{inceptionscore}, which has been shown to be a\n",
      "quality metric that correlates well with human judgment. However, IS does not\n",
      "reveal properties of the generated images indicating the ability of a\n",
      "text-to-image synthesis method to correctly convey semantics of the input text\n",
      "descriptions. In this paper, we introduce an evaluation metric and a visual\n",
      "evaluation method allowing for the simultaneous estimation of the realism,\n",
      "variety and semantic accuracy of generated images. The proposed method uses a\n",
      "pre-trained Inception network \\cite{inceptionnet} to produce high dimensional\n",
      "representations for both real and generated images. These image representations\n",
      "are then visualized in a $2$-dimensional feature space defined by the\n",
      "t-distributed Stochastic Neighbor Embedding (t-SNE) \\cite{tsne}. Visual\n",
      "concepts are determined by clustering the real image representations, and are\n",
      "subsequently used to evaluate the similarity of the generated images to the\n",
      "real ones by classifying them to the closest visual concept. The resulting\n",
      "classification accuracy is shown to be a effective gauge for the semantic\n",
      "accuracy of text-to-image synthesis methods. \n",
      "\n",
      "\n",
      "Generative adversarial networks conditioned on textual image descriptions are\n",
      "capable of generating realistic-looking images. However, current methods still\n",
      "struggle to generate images based on complex image captions from a\n",
      "heterogeneous domain. Furthermore, quantitatively evaluating these\n",
      "text-to-image models is challenging, as most evaluation metrics only judge\n",
      "image quality but not the conformity between the image and its caption. To\n",
      "address these challenges we introduce a new model that explicitly models\n",
      "individual objects within an image and a new evaluation metric called Semantic\n",
      "Object Accuracy (SOA) that specifically evaluates images given an image\n",
      "caption. The SOA uses a pre-trained object detector to evaluate if a generated\n",
      "image contains objects that are mentioned in the image caption, e.g. whether an\n",
      "image generated from \"a car driving down the street\" contains a car. We perform\n",
      "a user study comparing several text-to-image models and show that our SOA\n",
      "metric ranks the models the same way as humans, whereas other metrics such as\n",
      "the Inception Score do not. Our evaluation also shows that models which\n",
      "explicitly model objects outperform models which only model global image\n",
      "characteristics. \n",
      "\n",
      "\n",
      "Deep generative models come with the promise to learn an explainable\n",
      "representation for visual objects that allows image sampling, synthesis, and\n",
      "selective modification. The main challenge is to learn to properly model the\n",
      "independent latent characteristics of an object, especially its appearance and\n",
      "pose. We present a novel approach that learns disentangled representations of\n",
      "these characteristics and explains them individually. Training requires only\n",
      "pairs of images depicting the same object appearance, but no pose annotations.\n",
      "We propose an additional classifier that estimates the minimal amount of\n",
      "regularization required to enforce disentanglement. Thus both representations\n",
      "together can completely explain an image while being independent of each other.\n",
      "Previous methods based on adversarial approaches fail to enforce this\n",
      "independence, while methods based on variational approaches lead to\n",
      "uninformative representations. In experiments on diverse object categories, the\n",
      "approach successfully recombines pose and appearance to reconstruct and\n",
      "retarget novel synthesized images. We achieve significant improvements over\n",
      "state-of-the-art methods which utilize the same level of supervision, and reach\n",
      "performances comparable to those of pose-supervised approaches. However, we can\n",
      "handle the vast body of articulated object classes for which no pose\n",
      "models/annotations are available. \n",
      "\n",
      "\n",
      "Purpose: To assess whether a generative adversarial network (GAN) could\n",
      "synthesize realistic optical coherence tomography (OCT) images that\n",
      "satisfactorily serve as the educational images for retinal specialists and the\n",
      "training datasets for the classification of various retinal disorders using\n",
      "deep learning (DL). Methods: The GANs architecture was adopted to synthesis\n",
      "high-resolution OCT images training on a publicly available OCT dataset\n",
      "including urgent referrals (choroidal neovascularization and diabetic macular\n",
      "edema) and non-urgent referrals (normal and drusen). 400 real and synthetic OCT\n",
      "images were evaluated by 2 retinal specialists to assess image quality. We\n",
      "further trained 2 DL models on either real or synthetic datasets and compared\n",
      "the performance of urgent vs nonurgent referrals diagnosis tested on a local\n",
      "(1000 images from the public dataset) and clinical validation dataset (278\n",
      "images from Shanghai Shibei Hospital). Results: The image quality of real vs\n",
      "synthetic OCT images was similar as assessed by 2 retinal specialists. The\n",
      "accuracy of discrimination as real vs synthetic OCT images was 59.50% for\n",
      "retinal specialist 1 and 53.67% for retinal specialist 2. For the local\n",
      "dataset, the DL model trained on real (DL_Model_R) and synthetic OCT images\n",
      "(DL_Model_S) had an area under the curve (AUC) of 0.99, and 0.98 respectively.\n",
      "For the clinical dataset, the AUC was 0.94 for DL_Model_R, 0.90 for DL_Model_S.\n",
      "Conclusions: The GAN-synthetic OCT images can be used by clinicians for\n",
      "educational purposes and developing DL algorithms. Translational Relevance: The\n",
      "medical image synthesis based on GANs is promising in human and machine to\n",
      "fulfill clinical tasks. \n",
      "\n",
      "\n",
      "Text-to-image synthesis refers to computational methods which translate human\n",
      "written textual descriptions, in the form of keywords or sentences, into images\n",
      "with similar semantic meaning to the text. In earlier research, image synthesis\n",
      "relied mainly on word to image correlation analysis combined with supervised\n",
      "methods to find best alignment of the visual content matching to the text.\n",
      "Recent progress in deep learning (DL) has brought a new set of unsupervised\n",
      "deep learning methods, particularly deep generative models which are able to\n",
      "generate realistic visual images using suitably trained neural network models.\n",
      "In this paper, we review the most recent development in the text-to-image\n",
      "synthesis research domain. Our survey first introduces image synthesis and its\n",
      "challenges, and then reviews key concepts such as generative adversarial\n",
      "networks (GANs) and deep convolutional encoder-decoder neural networks (DCNN).\n",
      "After that, we propose a taxonomy to summarize GAN based text-to-image\n",
      "synthesis into four major categories: Semantic Enhancement GANs, Resolution\n",
      "Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We\n",
      "elaborate the main objective of each group, and further review typical GAN\n",
      "architectures in each group. The taxonomy and the review outline the techniques\n",
      "and the evolution of different approaches, and eventually provide a clear\n",
      "roadmap to summarize the list of contemporaneous solutions that utilize GANs\n",
      "and DCNNs to generate enthralling results in categories such as human faces,\n",
      "birds, flowers, room interiors, object reconstruction from edge maps (games)\n",
      "etc. The survey will conclude with a comparison of the proposed solutions,\n",
      "challenges that remain unresolved, and future developments in the text-to-image\n",
      "synthesis domain. \n",
      "\n",
      "\n",
      "Semantic image synthesis aims at generating photorealistic images from\n",
      "semantic layouts. Previous approaches with conditional generative adversarial\n",
      "networks (GAN) show state-of-the-art performance on this task, which either\n",
      "feed the semantic label maps as inputs to the generator, or use them to\n",
      "modulate the activations in normalization layers via affine transformations. We\n",
      "argue that convolutional kernels in the generator should be aware of the\n",
      "distinct semantic labels at different locations when generating images. In\n",
      "order to better exploit the semantic layout for the image generator, we propose\n",
      "to predict convolutional kernels conditioned on the semantic label map to\n",
      "generate the intermediate feature maps from the noise maps and eventually\n",
      "generate the images. Moreover, we propose a feature pyramid semantics-embedding\n",
      "discriminator, which is more effective in enhancing fine details and semantic\n",
      "alignments between the generated images and the input semantic layouts than\n",
      "previous multi-scale discriminators. We achieve state-of-the-art results on\n",
      "both quantitative metrics and subjective evaluation on various semantic\n",
      "segmentation datasets, demonstrating the effectiveness of our approach. \n",
      "\n",
      "\n",
      "Automatic generation of artistic glyph images is a challenging task that\n",
      "attracts many research interests. Previous methods either are specifically\n",
      "designed for shape synthesis or focus on texture transfer. In this paper, we\n",
      "propose a novel model, AGIS-Net, to transfer both shape and texture styles in\n",
      "one-stage with only a few stylized samples. To achieve this goal, we first\n",
      "disentangle the representations for content and style by using two encoders,\n",
      "ensuring the multi-content and multi-style generation. Then we utilize two\n",
      "collaboratively working decoders to generate the glyph shape image and its\n",
      "texture image simultaneously. In addition, we introduce a local texture\n",
      "refinement loss to further improve the quality of the synthesized textures. In\n",
      "this manner, our one-stage model is much more efficient and effective than\n",
      "other multi-stage stacked methods. We also propose a large-scale dataset with\n",
      "Chinese glyph images in various shape and texture styles, rendered from 35\n",
      "professional-designed artistic fonts with 7,326 characters and 2,460 synthetic\n",
      "artistic fonts with 639 characters, to validate the effectiveness and\n",
      "extendability of our method. Extensive experiments on both English and Chinese\n",
      "artistic glyph image datasets demonstrate the superiority of our model in\n",
      "generating high-quality stylized glyph images against other state-of-the-art\n",
      "methods. \n",
      "\n",
      "\n",
      "This paper explores visual indeterminacy as a description for artwork created\n",
      "with Generative Adversarial Networks (GANs). Visual indeterminacy describes\n",
      "images which appear to depict real scenes, but, on closer examination, defy\n",
      "coherent spatial interpretation. GAN models seem to be predisposed to producing\n",
      "indeterminate images, and indeterminacy is a key feature of much modern\n",
      "representational art, as well as most GAN art. It is hypothesized that\n",
      "indeterminacy is a consequence of a powerful-but-imperfect image synthesis\n",
      "model that must combine general classes of objects, scenes, and textures. \n",
      "\n",
      "\n",
      "Text to Image Synthesis refers to the process of automatic generation of a\n",
      "photo-realistic image starting from a given text and is revolutionizing many\n",
      "real-world applications. In order to perform such process it is necessary to\n",
      "exploit datasets containing captioned images, meaning that each image is\n",
      "associated with one (or more) captions describing it. Despite the abundance of\n",
      "uncaptioned images datasets, the number of captioned datasets is limited. To\n",
      "address this issue, in this paper we propose an approach capable of generating\n",
      "images starting from a given text using conditional GANs trained on uncaptioned\n",
      "images dataset. In particular, uncaptioned images are fed to an Image\n",
      "Captioning Module to generate the descriptions. Then, the GAN Module is trained\n",
      "on both the input image and the machine-generated caption. To evaluate the\n",
      "results, the performance of our solution is compared with the results obtained\n",
      "by the unconditional GAN. For the experiments, we chose to use the uncaptioned\n",
      "dataset LSUN bedroom. The results obtained in our study are preliminary but\n",
      "still promising. \n",
      "\n",
      "\n",
      "Microstructures of a material form the bridge linking processing conditions -\n",
      "which can be controlled, to the material property - which is the primary\n",
      "interest in engineering applications. Thus a critical task in material design\n",
      "is establishing the processing-structure relationship, which requires domain\n",
      "expertise and techniques that can model the high-dimensional material\n",
      "microstructure. This work proposes a deep learning based approach that models\n",
      "the processing-structure relationship as a conditional image synthesis problem.\n",
      "In particular, we develop an auxiliary classifier Wasserstein GAN with gradient\n",
      "penalty (ACWGAN-GP) to synthesize microstructures under a given processing\n",
      "condition. This approach is free of feature engineering, requires modest domain\n",
      "knowledge and is applicable to a wide range of material systems. We demonstrate\n",
      "this approach using the ultra high carbon steel (UHCS) database, where each\n",
      "microstructure is annotated with a label describing the cooling method it was\n",
      "subjected to. Our results show that ACWGAN-GP can synthesize high-quality\n",
      "multiphase microstructures for a given cooling method. \n",
      "\n",
      "\n",
      "Understanding three-dimensional (3D) geometries from two-dimensional (2D)\n",
      "images without any labeled information is promising for understanding the real\n",
      "world without incurring annotation cost. We herein propose a novel generative\n",
      "model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D\n",
      "images. The proposed method enables camera parameter-conditional image\n",
      "generation and depth image generation without any 3D annotations, such as\n",
      "camera poses or depth. We use an explicit 3D consistency loss for two RGBD\n",
      "images generated from different camera parameters, in addition to the ordinal\n",
      "GAN objective. The loss is simple yet effective for any type of image generator\n",
      "such as DCGAN and StyleGAN to be conditioned on camera parameters. Through\n",
      "experiments, we demonstrated that the proposed method could learn 3D\n",
      "representations from 2D images with various generator architectures. \n",
      "\n",
      "\n",
      "Multi-contrast MRI protocols increase the level of morphological information\n",
      "available for diagnosis. Yet, the number and quality of contrasts is limited in\n",
      "practice by various factors including scan time and patient motion. Synthesis\n",
      "of missing or corrupted contrasts can alleviate this limitation to improve\n",
      "clinical utility. Common approaches for multi-contrast MRI involve either\n",
      "one-to-one and many-to-one synthesis methods. One-to-one methods take as input\n",
      "a single source contrast, and they learn a latent representation sensitive to\n",
      "unique features of the source. Meanwhile, many-to-one methods receive multiple\n",
      "distinct sources, and they learn a shared latent representation more sensitive\n",
      "to common features across sources. For enhanced image synthesis, here we\n",
      "propose a multi-stream approach that aggregates information across multiple\n",
      "source images via a mixture of multiple one-to-one streams and a joint\n",
      "many-to-one stream. The shared feature maps generated in the many-to-one stream\n",
      "and the complementary feature maps generated in the one-to-one streams are\n",
      "combined with a fusion block. The location of the fusion block is adaptively\n",
      "modified to maximize task-specific performance. Qualitative and quantitative\n",
      "assessments on T1-, T2-, PD-weighted and FLAIR images clearly demonstrate the\n",
      "superior performance of the proposed method compared to previous\n",
      "state-of-the-art one-to-one and many-to-one methods. \n",
      "\n",
      "\n",
      "Deep learning approaches based on convolutional neural networks (CNNs) have\n",
      "been successful in solving a number of problems in medical imaging, including\n",
      "image segmentation. In recent years, it has been shown that CNNs are vulnerable\n",
      "to attacks in which the input image is perturbed by relatively small amounts of\n",
      "noise so that the CNN is no longer able to perform a segmentation of the\n",
      "perturbed image with sufficient accuracy. Therefore, exploring methods on how\n",
      "to attack CNN-based models as well as how to defend models against attacks have\n",
      "become a popular topic as this also provides insights into the performance and\n",
      "generalization abilities of CNNs. However, most of the existing work assumes\n",
      "unrealistic attack models, i.e. the resulting attacks were specified in\n",
      "advance. In this paper, we propose a novel approach for generating adversarial\n",
      "examples to attack CNN-based segmentation models for medical images. Our\n",
      "approach has three key features: 1) The generated adversarial examples exhibit\n",
      "anatomical variations (in form of deformations) as well as appearance\n",
      "perturbations; 2) The adversarial examples attack segmentation models so that\n",
      "the Dice scores decrease by a pre-specified amount; 3) The attack is not\n",
      "required to be specified beforehand. We have evaluated our approach on\n",
      "CNN-based approaches for the multi-organ segmentation problem in 2D CT images.\n",
      "We show that the proposed approach can be used to attack different CNN-based\n",
      "segmentation models. \n",
      "\n",
      "\n",
      "Generating good quality and geometrically plausible synthetic images of\n",
      "humans with the ability to control appearance, pose and shape parameters, has\n",
      "become increasingly important for a variety of tasks ranging from photo\n",
      "editing, fashion virtual try-on, to special effects and image compression. In\n",
      "this paper, we propose HUSC, a HUman Synthesis and Scene Compositing framework\n",
      "for the realistic synthesis of humans with different appearance, in novel poses\n",
      "and scenes. Central to our formulation is 3d reasoning for both people and\n",
      "scenes, in order to produce realistic collages, by correctly modeling\n",
      "perspective effects and occlusion, by taking into account scene semantics and\n",
      "by adequately handling relative scales. Conceptually our framework consists of\n",
      "three components: (1) a human image synthesis model with controllable pose and\n",
      "appearance, based on a parametric representation, (2) a person insertion\n",
      "procedure that leverages the geometry and semantics of the 3d scene, and (3) an\n",
      "appearance compositing process to create a seamless blending between the colors\n",
      "of the scene and the generated human image, and avoid visual artifacts. The\n",
      "performance of our framework is supported by both qualitative and quantitative\n",
      "results, in particular state-of-the art synthesis scores for the DeepFashion\n",
      "dataset. \n",
      "\n",
      "\n",
      "Compositional Pattern Producing Networks (CPPNs) are differentiable networks\n",
      "that independently map (x, y) pixel coordinates to (r, g, b) colour values.\n",
      "Recently, CPPNs have been used for creating interesting imagery for creative\n",
      "purposes, e.g., neural art. However their architecture biases generated images\n",
      "to be overly smooth, lacking high-frequency detail. In this work, we extend\n",
      "CPPNs to explicitly model the frequency information for each pixel output,\n",
      "capturing frequencies beyond the DC component. We show that our Fourier-CPPNs\n",
      "(F-CPPNs) provide improved visual detail for image synthesis. \n",
      "\n",
      "\n",
      "In recent years, generative adversarial networks (GANs) and its variants have\n",
      "achieved unprecedented success in image synthesis. They are widely adopted in\n",
      "synthesizing facial images which brings potential security concerns to humans\n",
      "as the fakes spread and fuel the misinformation. However, robust detectors of\n",
      "these AI-synthesized fake faces are still in their infancy and are not ready to\n",
      "fully tackle this emerging challenge. In this work, we propose a novel\n",
      "approach, named FakeSpotter, based on monitoring neuron behaviors to spot\n",
      "AI-synthesized fake faces. The studies on neuron coverage and interactions have\n",
      "successfully shown that they can be served as testing criteria for deep\n",
      "learning systems, especially under the settings of being exposed to adversarial\n",
      "attacks. Here, we conjecture that monitoring neuron behavior can also serve as\n",
      "an asset in detecting fake faces since layer-by-layer neuron activation\n",
      "patterns may capture more subtle features that are important for the fake\n",
      "detector. Experimental results on detecting four types of fake faces\n",
      "synthesized with the state-of-the-art GANs and evading four perturbation\n",
      "attacks show the effectiveness and robustness of our approach. \n",
      "\n",
      "\n",
      "Most deep latent factor models choose simple priors for simplicity,\n",
      "tractability or not knowing what prior to use. Recent studies show that the\n",
      "choice of the prior may have a profound effect on the expressiveness of the\n",
      "model,especially when its generative network has limited capacity. In this\n",
      "paper, we propose to learn a proper prior from data for adversarial\n",
      "autoencoders(AAEs). We introduce the notion of code generators to transform\n",
      "manually selected simple priors into ones that can better characterize the data\n",
      "distribution. Experimental results show that the proposed model can generate\n",
      "better image quality and learn better disentangled representations than AAEs in\n",
      "both supervised and unsupervised settings. Lastly, we present its ability to do\n",
      "cross-domain translation in a text-to-image synthesis task. \n",
      "\n",
      "\n",
      "This paper proposes to learn hierarchical compositional AND-OR model for\n",
      "interpretable image synthesis by sparsifying the generator network. The\n",
      "proposed method adopts the scene-objects-parts-subparts-primitives hierarchy in\n",
      "image representation. A scene has different types (i.e., OR) each of which\n",
      "consists of a number of objects (i.e., AND). This can be recursively formulated\n",
      "across the scene-objects-parts-subparts hierarchy and is terminated at the\n",
      "primitive level (e.g., wavelets-like basis). To realize this AND-OR hierarchy\n",
      "in image synthesis, we learn a generator network that consists of the following\n",
      "two components: (i) Each layer of the hierarchy is represented by an\n",
      "over-complete set of convolutional basis functions. Off-the-shelf convolutional\n",
      "neural architectures are exploited to implement the hierarchy. (ii)\n",
      "Sparsity-inducing constraints are introduced in end-to-end training, which\n",
      "induces a sparsely activated and sparsely connected AND-OR model from the\n",
      "initially densely connected generator network. A straightforward\n",
      "sparsity-inducing constraint is utilized, that is to only allow the top-$k$\n",
      "basis functions to be activated at each layer (where $k$ is a hyper-parameter).\n",
      "The learned basis functions are also capable of image reconstruction to explain\n",
      "the input images. In experiments, the proposed method is tested on four\n",
      "benchmark datasets. The results show that meaningful and interpretable\n",
      "hierarchical representations are learned with better qualities of image\n",
      "synthesis and reconstruction obtained than baselines. \n",
      "\n",
      "\n",
      "Recently image-to-image translation has attracted significant interests in\n",
      "the literature, starting from the successful use of the generative adversarial\n",
      "network (GAN), to the introduction of cyclic constraint, to extensions to\n",
      "multiple domains. However, in existing approaches, there is no guarantee that\n",
      "the mapping between two image domains is unique or one-to-one. Here we propose\n",
      "a self-inverse network learning approach for unpaired image-to-image\n",
      "translation. Building on top of CycleGAN, we learn a self-inverse function by\n",
      "simply augmenting the training samples by swapping inputs and outputs during\n",
      "training and with separated cycle consistency loss for each mapping direction.\n",
      "The outcome of such learning is a proven one-to-one mapping function. Our\n",
      "extensive experiments on a variety of datasets, including cross-modal medical\n",
      "image synthesis, object transfiguration, and semantic labeling, consistently\n",
      "demonstrate clear improvement over the CycleGAN method both qualitatively and\n",
      "quantitatively. Especially our proposed method reaches the state-of-the-art\n",
      "result on the cityscapes benchmark dataset for the label to photo unpaired\n",
      "directional image translation. \n",
      "\n",
      "\n",
      "The one-to-one mapping is necessary for many bidirectional image-to-image\n",
      "translation applications, such as MRI image synthesis as MRI images are unique\n",
      "to the patient. State-of-the-art approaches for image synthesis from domain X\n",
      "to domain Y learn a convolutional neural network that meticulously maps between\n",
      "the domains. A different network is typically implemented to map along the\n",
      "opposite direction, from Y to X. In this paper, we explore the possibility of\n",
      "only wielding one network for bi-directional image synthesis. In other words,\n",
      "such an autonomous learning network implements a self-inverse function. A\n",
      "self-inverse network shares several distinct advantages: only one network\n",
      "instead of two, better generalization and more restricted parameter space. Most\n",
      "importantly, a self-inverse function guarantees a one-to-one mapping, a\n",
      "property that cannot be guaranteed by earlier approaches that are not\n",
      "self-inverse. The experiments on three datasets show that, compared with the\n",
      "baseline approaches that use two separate models for the image synthesis along\n",
      "two directions, our self-inverse network achieves better synthesis results in\n",
      "terms of standard metrics. Finally, our sensitivity analysis confirms the\n",
      "feasibility of learning a self-inverse function for the bidirectional image\n",
      "translation. \n",
      "\n",
      "\n",
      "Synthesis of high resolution images using Generative Adversarial Networks\n",
      "(GANs) is challenging, which usually requires numbers of high-end graphic cards\n",
      "with large memory and long time of training. In this paper, we propose a\n",
      "two-stage framework to accelerate the training process of synthesizing high\n",
      "resolution images. High resolution images are first transformed to small codes\n",
      "via the trained encoder and decoder networks. The code in latent space is times\n",
      "smaller than the original high resolution images. Then, we train a code\n",
      "generation network to learn the distribution of the latent codes. In this way,\n",
      "the generator only learns to generate small latent codes instead of large\n",
      "images. Finally, we decode the generated latent codes to image space via the\n",
      "decoder networks so as to output the synthesized high resolution images.\n",
      "Experimental results show that the proposed method accelerates the training\n",
      "process significantly and increases the quality of the generated samples. The\n",
      "proposed acceleration framework makes it possible to generate high resolution\n",
      "images using less training time with limited hardware resource. After using the\n",
      "proposed acceleration method, it takes only 3 days to train a 1024 *1024 image\n",
      "generator on Celeba-HQ dataset using just one NVIDIA P100 graphic card. \n",
      "\n",
      "\n",
      "The migration of planetary cores embedded in a protoplanetary disk is an\n",
      "important mechanism within planet-formation theory, relevant for the\n",
      "architecture of planetary systems. Consequently, planet migration is actively\n",
      "discussed, yet often results of independent theoretical or numerical studies\n",
      "are unconstrained due to the lack of observational diagnostics designed in\n",
      "light of planet migration. In this work we follow the idea of inferring the\n",
      "migration behavior of embedded planets by means of the characteristic radial\n",
      "structures that they imprint in the disk's dust density distribution. We run\n",
      "hydrodynamical multifluid simulations of gas and several dust species in a\n",
      "locally isothermal $\\alpha$-disk in the low-viscosity regime ($\\alpha=10^{-5}$)\n",
      "and investigate the obtained dust structures. In this framework, a planet of\n",
      "roughly Neptune mass can create three (or more) rings in which dust\n",
      "accumulates. We find that the relative spacing of these rings depends on the\n",
      "planet's migration speed and direction. By performing subsequent radiative\n",
      "transfer calculations and image synthesis we show that - always under the\n",
      "condition of a near-inviscid disk - different migration scenarios are, in\n",
      "principle, distinguishable by long-baseline, state-of-the-art ALMA\n",
      "observations. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks have been crucial in the developments made in\n",
      "unsupervised learning in recent times. Exemplars of image synthesis from text\n",
      "or other images, these networks have shown remarkable improvements over\n",
      "conventional methods in terms of performance. Trained on the adversarial\n",
      "training philosophy, these networks aim to estimate the potential distribution\n",
      "from the real data and then use this as input to generate the synthetic data.\n",
      "Based on this fundamental principle, several frameworks can be generated that\n",
      "are paragon implementations in several real-life applications such as art\n",
      "synthesis, generation of high resolution outputs and synthesis of images from\n",
      "human drawn sketches, to name a few. While theoretically GANs present better\n",
      "results and prove to be an improvement over conventional methods in many\n",
      "factors, the implementation of these frameworks for dedicated applications\n",
      "remains a challenge. This study explores and presents a taxonomy of these\n",
      "frameworks and their use in various image to image synthesis and text to image\n",
      "synthesis applications. The basic GANs, as well as a variety of different niche\n",
      "frameworks, are critically analyzed. The advantages of GANs for image\n",
      "generation over conventional methods as well their disadvantages amongst other\n",
      "frameworks are presented. The future applications of GANs in industries such as\n",
      "healthcare, art and entertainment are also discussed. \n",
      "\n",
      "\n",
      "Positron emission tomography (PET) imaging is an imaging modality for\n",
      "diagnosing a number of neurological diseases. In contrast to Magnetic Resonance\n",
      "Imaging (MRI), PET is costly and involves injecting a radioactive substance\n",
      "into the patient. Motivated by developments in modality transfer in vision, we\n",
      "study the generation of certain types of PET images from MRI data. We derive\n",
      "new flow-based generative models which we show perform well in this small\n",
      "sample size regime (much smaller than dataset sizes available in standard\n",
      "vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks\n",
      "and a relation network that maps the latent spaces to each other. We discuss\n",
      "how given the prior distribution, learning the conditional distribution of PET\n",
      "given the MRI image reduces to obtaining the conditional distribution between\n",
      "the two latent codes w.r.t. the two image types. We also extend our framework\n",
      "to leverage 'side' information (or attributes) when available. By controlling\n",
      "the PET generation through 'conditioning' on age, our model is also able to\n",
      "capture brain FDG-PET (hypometabolism) changes, as a function of age. We\n",
      "present experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI)\n",
      "dataset with 826 subjects, and obtain good performance in PET image synthesis,\n",
      "qualitatively and quantitatively better than recent works. \n",
      "\n",
      "\n",
      "Despite remarkable recent progress on both unconditional and conditional\n",
      "image synthesis, it remains a long-standing problem to learn generative models\n",
      "that are capable of synthesizing realistic and sharp images from reconfigurable\n",
      "spatial layout (i.e., bounding boxes + class labels in an image lattice) and\n",
      "style (i.e., structural and appearance variations encoded by latent vectors),\n",
      "especially at high resolution. By reconfigurable, it means that a model can\n",
      "preserve the intrinsic one-to-many mapping from a given layout to multiple\n",
      "plausible images with different styles, and is adaptive with respect to\n",
      "perturbations of a layout and style latent code. In this paper, we present a\n",
      "layout- and style-based architecture for generative adversarial networks\n",
      "(termed LostGANs) that can be trained end-to-end to generate images from\n",
      "reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed\n",
      "LostGAN consists of two new components: (i) learning fine-grained mask maps in\n",
      "a weakly-supervised manner to bridge the gap between layouts and images, and\n",
      "(ii) learning object instance-specific layout-aware feature normalization\n",
      "(ISLA-Norm) in the generator to realize multi-object style generation. In\n",
      "experiments, the proposed method is tested on the COCO-Stuff dataset and the\n",
      "Visual Genome dataset with state-of-the-art performance obtained. The code and\n",
      "pretrained models are available at \\url{https://github.com/iVMCL/LostGANs}. \n",
      "\n",
      "\n",
      "Deep generative models have led to significant advances in cross-modal\n",
      "generation such as text-to-image synthesis. Training these models typically\n",
      "requires paired data with direct correspondence between modalities. We\n",
      "introduce the novel problem of translating instances from one modality to\n",
      "another without paired data by leveraging an intermediate modality shared by\n",
      "the two other modalities. To demonstrate this, we take the problem of\n",
      "translating images to speech. In this case, one could leverage disjoint\n",
      "datasets with one shared modality, e.g., image-text pairs and text-speech\n",
      "pairs, with text as the shared modality. We call this problem \"skip-modal\n",
      "generation\" because the shared modality is skipped during the generation\n",
      "process. We propose a multimodal information bottleneck approach that learns\n",
      "the correspondence between modalities from unpaired data (image and speech) by\n",
      "leveraging the shared modality (text). We address fundamental challenges of\n",
      "skip-modal generation: 1) learning multimodal representations using a single\n",
      "model, 2) bridging the domain gap between two unrelated datasets, and 3)\n",
      "learning the correspondence between modalities from unpaired data. We show\n",
      "qualitative results on image-to-speech synthesis; this is the first time such\n",
      "results have been reported in the literature. We also show that our approach\n",
      "improves performance on traditional cross-modal generation, suggesting that it\n",
      "improves data efficiency in solving individual tasks. \n",
      "\n",
      "\n",
      "Synthesizing images from a given text description involves engaging two types\n",
      "of information: the content, which includes information explicitly described in\n",
      "the text (e.g., color, composition, etc.), and the style, which is usually not\n",
      "well described in the text (e.g., location, quantity, size, etc.). However, in\n",
      "previous works, it is typically treated as a process of generating images only\n",
      "from the content, i.e., without considering learning meaningful style\n",
      "representations. In this paper, we aim to learn two variables that are\n",
      "disentangled in the latent space, representing content and style respectively.\n",
      "We achieve this by augmenting current text-to-image synthesis frameworks with a\n",
      "dual adversarial inference mechanism. Through extensive experiments, we show\n",
      "that our model learns, in an unsupervised manner, style representations\n",
      "corresponding to certain meaningful information present in the image that are\n",
      "not well described in the text. The new framework also improves the quality of\n",
      "synthesized images when evaluated on Oxford-102, CUB and COCO datasets. \n",
      "\n",
      "\n",
      "In this paper, we are interested in editing text in natural images, which\n",
      "aims to replace or modify a word in the source image with another one while\n",
      "maintaining its realistic look. This task is challenging, as the styles of both\n",
      "background and text need to be preserved so that the edited image is visually\n",
      "indistinguishable from the source image. Specifically, we propose an end-to-end\n",
      "trainable style retention network (SRNet) that consists of three modules: text\n",
      "conversion module, background inpainting module and fusion module. The text\n",
      "conversion module changes the text content of the source image into the target\n",
      "text while keeping the original text style. The background inpainting module\n",
      "erases the original text, and fills the text region with appropriate texture.\n",
      "The fusion module combines the information from the two former modules, and\n",
      "generates the edited text images. To our knowledge, this work is the first\n",
      "attempt to edit text in natural images at the word level. Both visual effects\n",
      "and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully\n",
      "confirm the importance and necessity of modular decomposition. We also conduct\n",
      "extensive experiments to validate the usefulness of our method in various\n",
      "real-world applications such as text image synthesis, augmented reality (AR)\n",
      "translation, information hiding, etc. \n",
      "\n",
      "\n",
      "Relighting of human images has various applications in image synthesis. For\n",
      "relighting, we must infer albedo, shape, and illumination from a human\n",
      "portrait. Previous techniques rely on human faces for this inference, based on\n",
      "spherical harmonics (SH) lighting. However, because they often ignore light\n",
      "occlusion, inferred shapes are biased and relit images are unnaturally bright\n",
      "particularly at hollowed regions such as armpits, crotches, or garment\n",
      "wrinkles. This paper introduces the first attempt to infer light occlusion in\n",
      "the SH formulation directly. Based on supervised learning using convolutional\n",
      "neural networks (CNNs), we infer not only an albedo map, illumination but also\n",
      "a light transport map that encodes occlusion as nine SH coefficients per pixel.\n",
      "The main difficulty in this inference is the lack of training datasets compared\n",
      "to unlimited variations of human portraits. Surprisingly, geometric information\n",
      "including occlusion can be inferred plausibly even with a small dataset of\n",
      "synthesized human figures, by carefully preparing the dataset so that the CNNs\n",
      "can exploit the data coherency. Our method accomplishes more realistic\n",
      "relighting than the occlusion-ignored formulation. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have the capability of synthesizing\n",
      "images, which have been successfully applied to medical image synthesis tasks.\n",
      "However, most of existing methods merely consider the global contextual\n",
      "information and ignore the fine foreground structures, e.g., vessel, skeleton,\n",
      "which may contain diagnostic indicators for medical image analysis. Inspired by\n",
      "human painting procedure, which is composed of stroking and color rendering\n",
      "steps, we propose a Sketching-rendering Unconditional Generative Adversarial\n",
      "Network (SkrGAN) to introduce a sketch prior constraint to guide the medical\n",
      "image generation. In our SkrGAN, a sketch guidance module is utilized to\n",
      "generate a high quality structural sketch from random noise, then a color\n",
      "render mapping is used to embed the sketch-based representations and resemble\n",
      "the background appearances. Experimental results show that the proposed SkrGAN\n",
      "achieves the state-of-the-art results in synthesizing images for various image\n",
      "modalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and\n",
      "Magnetic Resonance Imaging (MRI). In addition, we also show that the\n",
      "performances of medical image segmentation method have been improved by using\n",
      "our synthesized images as data augmentation. \n",
      "\n",
      "\n",
      "We propose InSituNet, a deep learning based surrogate model to support\n",
      "parameter space exploration for ensemble simulations that are visualized in\n",
      "situ. In situ visualization, generating visualizations at simulation time, is\n",
      "becoming prevalent in handling large-scale simulations because of the I/O and\n",
      "storage constraints. However, in situ visualization approaches limit the\n",
      "flexibility of post-hoc exploration because the raw simulation data are no\n",
      "longer available. Although multiple image-based approaches have been proposed\n",
      "to mitigate this limitation, those approaches lack the ability to explore the\n",
      "simulation parameters. Our approach allows flexible exploration of parameter\n",
      "space for large-scale ensemble simulations by taking advantage of the recent\n",
      "advances in deep learning. Specifically, we design InSituNet as a convolutional\n",
      "regression model to learn the mapping from the simulation and visualization\n",
      "parameters to the visualization results. With the trained model, users can\n",
      "generate new images for different simulation parameters under various\n",
      "visualization settings, which enables in-depth analysis of the underlying\n",
      "ensemble simulations. We demonstrate the effectiveness of InSituNet in\n",
      "combustion, cosmology, and ocean simulations through quantitative and\n",
      "qualitative evaluations. \n",
      "\n",
      "\n",
      "Deep learning (DL) has shown great potential in medical image enhancement\n",
      "problems, such as super-resolution or image synthesis. However, to date, little\n",
      "consideration has been given to uncertainty quantification over the output\n",
      "image. Here we introduce methods to characterise different components of\n",
      "uncertainty in such problems and demonstrate the ideas using diffusion MRI\n",
      "super-resolution. Specifically, we propose to account for $intrinsic$\n",
      "uncertainty through a heteroscedastic noise model and for $parameter$\n",
      "uncertainty through approximate Bayesian inference, and integrate the two to\n",
      "quantify $predictive$ uncertainty over the output image. Moreover, we introduce\n",
      "a method to propagate the predictive uncertainty on a multi-channelled image to\n",
      "derived scalar parameters, and separately quantify the effects of intrinsic and\n",
      "parameter uncertainty therein. The methods are evaluated for super-resolution\n",
      "of two different signal representations of diffusion MR images---DTIs and Mean\n",
      "Apparent Propagator MRI---and their derived quantities such as MD and FA, on\n",
      "multiple datasets of both healthy and pathological human brains. Results\n",
      "highlight three key benefits of uncertainty modelling for improving the safety\n",
      "of DL-based image enhancement systems. Firstly, incorporating uncertainty\n",
      "improves the predictive performance even when test data departs from training\n",
      "data. Secondly, the predictive uncertainty highly correlates with errors, and\n",
      "is therefore capable of detecting predictive \"failures\". Results demonstrate\n",
      "that such an uncertainty measure enables subject-specific and voxel-wise risk\n",
      "assessment of the output images. Thirdly, we show that the method for\n",
      "decomposing predictive uncertainty into its independent sources provides\n",
      "high-level \"explanations\" for the performance by quantifying how much\n",
      "uncertainty arises from the inherent difficulty of the task or the limited\n",
      "training examples. \n",
      "\n",
      "\n",
      "Despite the recent advance of Generative Adversarial Networks (GANs) in\n",
      "high-fidelity image synthesis, there lacks enough understanding of how GANs are\n",
      "able to map a latent code sampled from a random distribution to a\n",
      "photo-realistic image. Previous work assumes the latent space learned by GANs\n",
      "follows a distributed representation but observes the vector arithmetic\n",
      "phenomenon. In this work, we propose a novel framework, called InterFaceGAN,\n",
      "for semantic face editing by interpreting the latent semantics learned by GANs.\n",
      "In this framework, we conduct a detailed study on how different semantics are\n",
      "encoded in the latent space of GANs for face synthesis. We find that the latent\n",
      "code of well-trained generative models actually learns a disentangled\n",
      "representation after linear transformations. We explore the disentanglement\n",
      "between various semantics and manage to decouple some entangled semantics with\n",
      "subspace projection, leading to more precise control of facial attributes.\n",
      "Besides manipulating gender, age, expression, and the presence of eyeglasses,\n",
      "we can even vary the face pose as well as fix the artifacts accidentally\n",
      "generated by GAN models. The proposed method is further applied to achieve real\n",
      "image manipulation when combined with GAN inversion methods or some\n",
      "encoder-involved models. Extensive results suggest that learning to synthesize\n",
      "faces spontaneously brings a disentangled and controllable facial attribute\n",
      "representation. \n",
      "\n",
      "\n",
      "With the development of deep neural networks, the demand for a significant\n",
      "amount of annotated training data becomes the performance bottlenecks in many\n",
      "fields of research and applications. Image synthesis can generate annotated\n",
      "images automatically and freely, which gains increasing attention recently. In\n",
      "this paper, we propose to synthesize scene text images from the 3D virtual\n",
      "worlds, where the precise descriptions of scenes, editable\n",
      "illumination/visibility, and realistic physics are provided. Different from the\n",
      "previous methods which paste the rendered text on static 2D images, our method\n",
      "can render the 3D virtual scene and text instances as an entirety. In this way,\n",
      "real-world variations, including complex perspective transformations, various\n",
      "illuminations, and occlusions, can be realized in our synthesized scene text\n",
      "images. Moreover, the same text instances with various viewpoints can be\n",
      "produced by randomly moving and rotating the virtual camera, which acts as\n",
      "human eyes. The experiments on the standard scene text detection benchmarks\n",
      "using the generated synthetic data demonstrate the effectiveness and\n",
      "superiority of the proposed method. The code and synthetic data is available\n",
      "at: https://github.com/MhLiao/SynthText3D \n",
      "\n",
      "\n",
      "Severe color casts, low contrast and blurriness of underwater images caused\n",
      "by light absorption and scattering result in a difficult task for exploring\n",
      "underwater environments. Different from most of previous underwater image\n",
      "enhancement methods that compute light attenuation along object-camera path\n",
      "through hazy image formation model, we propose a novel jointly wavelength\n",
      "compensation and dehazing network (JWCDN) that takes into account the\n",
      "wavelength attenuation along surface-object path and the scattering along\n",
      "object-camera path simultaneously. By embedding a simplified underwater\n",
      "formation model into generative adversarial network, we can jointly estimates\n",
      "the transmission map, wavelength attenuation and background light via different\n",
      "network modules, and uses the simplified underwater image formation model to\n",
      "recover degraded underwater images. Especially, a multi-scale densely connected\n",
      "encoder-decoder network is proposed to leverage features from multiple layers\n",
      "for estimating the transmission map. To further improve the recovered image, we\n",
      "use an edge preserving network module to enhance the detail of the recovered\n",
      "image. Moreover, to train the proposed network, we propose a novel underwater\n",
      "image synthesis method that generates underwater images with inherent optical\n",
      "properties of different water types. The synthesis method can simulate the\n",
      "color, contrast and blurriness appearance of real-world underwater environments\n",
      "simultaneously. Extensive experiments on synthetic and real-world underwater\n",
      "images demonstrate that the proposed method yields comparable or better results\n",
      "on both subjective and objective assessments, compared with several\n",
      "state-of-the-art methods. \n",
      "\n",
      "\n",
      "Generative adversarial networks have led to significant advances in\n",
      "cross-modal/domain translation. However, typically these networks are designed\n",
      "for a specific task (e.g., dialogue generation or image synthesis, but not\n",
      "both). We present a unified model, M3D-GAN, that can translate across a wide\n",
      "range of modalities (e.g., text, image, and speech) and domains (e.g.,\n",
      "attributes in images or emotions in speech). Our model consists of modality\n",
      "subnets that convert data from different modalities into unified\n",
      "representations, and a unified computing body where data from different\n",
      "modalities share the same network architecture. We introduce a universal\n",
      "attention module that is jointly trained with the whole network and learns to\n",
      "encode a large range of domain information into a highly structured latent\n",
      "space. We use this to control synthesis in novel ways, such as producing\n",
      "diverse realistic pictures from a sketch or varying the emotion of synthesized\n",
      "speech. We evaluate our approach on extensive benchmark tasks, including\n",
      "image-to-image, text-to-image, image captioning, text-to-speech, speech\n",
      "recognition, and machine translation. Our results show state-of-the-art\n",
      "performance on some of the tasks. \n",
      "\n",
      "\n",
      "Medical imaging plays a critical role in various clinical applications.\n",
      "However, due to multiple considerations such as cost and risk, the acquisition\n",
      "of certain image modalities could be limited. To address this issue, many\n",
      "cross-modality medical image synthesis methods have been proposed. However, the\n",
      "current methods cannot well model the hard-to-synthesis regions (e.g., tumor or\n",
      "lesion regions). To address this issue, we propose a simple but effective\n",
      "strategy, that is, we propose a dual-discriminator (dual-D) adversarial\n",
      "learning system, in which, a global-D is used to make an overall evaluation for\n",
      "the synthetic image, and a local-D is proposed to densely evaluate the local\n",
      "regions of the synthetic image. More importantly, we build an adversarial\n",
      "attention mechanism which targets at better modeling hard-to-synthesize regions\n",
      "(e.g., tumor or lesion regions) based on the local-D. Experimental results show\n",
      "the robustness and accuracy of our method in synthesizing fine-grained target\n",
      "images from the corresponding source images. In particular, we evaluate our\n",
      "method on two datasets, i.e., to address the tasks of generating T2 MRI from T1\n",
      "MRI for the brain tumor images and generating MRI from CT. Our method\n",
      "outperforms the state-of-the-art methods under comparison in all datasets and\n",
      "tasks. And the proposed difficult-region-aware attention mechanism is also\n",
      "proved to be able to help generate more realistic images, especially for the\n",
      "hard-to-synthesize regions. \n",
      "\n",
      "\n",
      "Adversarially trained generative models (GANs) have recently achieved\n",
      "compelling image synthesis results. But despite early successes in using GANs\n",
      "for unsupervised representation learning, they have since been superseded by\n",
      "approaches based on self-supervision. In this work we show that progress in\n",
      "image generation quality translates to substantially improved representation\n",
      "learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art\n",
      "BigGAN model, extending it to representation learning by adding an encoder and\n",
      "modifying the discriminator. We extensively evaluate the representation\n",
      "learning and generation capabilities of these BigBiGAN models, demonstrating\n",
      "that these generation-based models achieve the state of the art in unsupervised\n",
      "representation learning on ImageNet, as well as in unconditional image\n",
      "generation. Pretrained BigBiGAN models -- including image generators and\n",
      "encoders -- are available on TensorFlow Hub\n",
      "(https://tfhub.dev/s?publisher=deepmind&q=bigbigan). \n",
      "\n",
      "\n",
      "We present a novel method for generating robust adversarial image examples\n",
      "building upon the recent `deep image prior' (DIP) that exploits convolutional\n",
      "network architectures to enforce plausible texture in image synthesis.\n",
      "Adversarial images are commonly generated by perturbing images to introduce\n",
      "high frequency noise that induces image misclassification, but that is fragile\n",
      "to subsequent digital manipulation of the image. We show that using DIP to\n",
      "reconstruct an image under adversarial constraint induces perturbations that\n",
      "are more robust to affine deformation, whilst remaining visually imperceptible.\n",
      "Furthermore we show that our DIP approach can also be adapted to produce local\n",
      "adversarial patches (`adversarial stickers'). We demonstrate robust adversarial\n",
      "examples over a broad gamut of images and object classes drawn from the\n",
      "ImageNet dataset. \n",
      "\n",
      "\n",
      "Recent advancements in conditional Generative Adversarial Networks (cGANs)\n",
      "have shown promises in label guided image synthesis. Semantic masks, such as\n",
      "sketches and label maps, are another intuitive and effective form of guidance\n",
      "in image synthesis. Directly incorporating the semantic masks as constraints\n",
      "dramatically reduces the variability and quality of the synthesized results. We\n",
      "observe this is caused by the incompatibility of features from different inputs\n",
      "(such as mask image and latent vector) of the generator. To use semantic masks\n",
      "as guidance whilst providing realistic synthesized results with fine details,\n",
      "we propose to use mask embedding mechanism to allow for a more efficient\n",
      "initial feature projection in the generator. We validate the effectiveness of\n",
      "our approach by training a mask guided face generator using CELEBA-HQ dataset.\n",
      "We can generate realistic and high resolution facial images up to the\n",
      "resolution of 512*512 with a mask guidance. Our code is publicly available. \n",
      "\n",
      "\n",
      "Recently, with the prevalence of large-scale image dataset, the co-occurrence\n",
      "information among classes becomes rich, calling for a new way to exploit it to\n",
      "facilitate inference. In this paper, we propose Obj-GloVe, a generic\n",
      "scene-based contextual embedding for common visual objects, where we adopt the\n",
      "word embedding method GloVe to exploit the co-occurrence between entities. We\n",
      "train the embedding on pre-processed Open Images V4 dataset and provide\n",
      "extensive visualization and analysis by dimensionality reduction and projecting\n",
      "the vectors along a specific semantic axis, and showcasing the nearest\n",
      "neighbors of the most common objects. Furthermore, we reveal the potential\n",
      "applications of Obj-GloVe on object detection and text-to-image synthesis, then\n",
      "verify its effectiveness on these two applications respectively. \n",
      "\n",
      "\n",
      "Realistic image synthesis is to generate an image that is perceptually\n",
      "indistinguishable from an actual image. Generating realistic looking images\n",
      "with large variations (e.g., large spatial deformations and large pose change),\n",
      "however, is very challenging. Handing large variations as well as preserving\n",
      "appearance needs to be taken into account in the realistic looking image\n",
      "generation. In this paper, we propose a novel realistic looking image synthesis\n",
      "method, especially in large change demands. To do that, we devise generative\n",
      "guiding blocks. The proposed generative guiding block includes realistic\n",
      "appearance preserving discriminator and naturalistic variation transforming\n",
      "discriminator. By taking the proposed generative guiding blocks into generative\n",
      "model, the latent features at the layer of generative model are enhanced to\n",
      "synthesize both realistic looking- and target variation- image. With\n",
      "qualitative and quantitative evaluation in experiments, we demonstrated the\n",
      "effectiveness of the proposed generative guiding blocks, compared to the\n",
      "state-of-the-arts. \n",
      "\n",
      "\n",
      "Automatic synthesis of high quality 3D shapes is an ongoing and challenging\n",
      "area of research. While several data-driven methods have been proposed that\n",
      "make use of neural networks to generate 3D shapes, none of them reach the level\n",
      "of quality that deep learning synthesis approaches for images provide. In this\n",
      "work we present a method for a convolutional point cloud decoder/generator that\n",
      "makes use of recent advances in the domain of image synthesis. Namely, we use\n",
      "Adaptive Instance Normalization and offer an intuition on why it can improve\n",
      "training. Furthermore, we propose extensions to the minimization of the\n",
      "commonly used Chamfer distance for auto-encoding point clouds. In addition, we\n",
      "show that careful sampling is important both for the input geometry and in our\n",
      "point cloud generation process to improve results. The results are evaluated in\n",
      "an auto-encoding setup to offer both qualitative and quantitative analysis. The\n",
      "proposed decoder is validated by an extensive ablation study and is able to\n",
      "outperform current state of the art results in a number of experiments. We show\n",
      "the applicability of our method in the fields of point cloud upsampling, single\n",
      "view reconstruction, and shape synthesis. \n",
      "\n",
      "\n",
      "One of the major obstacles in automatic polyp detection during colonoscopy is\n",
      "the lack of labeled polyp training images. In this paper, we propose a\n",
      "framework of conditional adversarial networks to increase the number of\n",
      "training samples by generating synthetic polyp images. Using a normal binary\n",
      "form of polyp mask which represents only the polyp position as an input\n",
      "conditioned image, realistic polyp image generation is a difficult task in a\n",
      "generative adversarial networks approach. We propose an edge filtering-based\n",
      "combined input conditioned image to train our proposed networks. This enables\n",
      "realistic polyp image generations while maintaining the original structures of\n",
      "the colonoscopy image frames. More importantly, our proposed framework\n",
      "generates synthetic polyp images from normal colonoscopy images which have the\n",
      "advantage of being relatively easy to obtain. The network architecture is based\n",
      "on the use of multiple dilated convolutions in each encoding part of our\n",
      "generator network to consider large receptive fields and avoid many\n",
      "contractions of a feature map size. An image resizing with convolution for\n",
      "upsampling in the decoding layers is considered to prevent artifacts on\n",
      "generated images. We show that the generated polyp images are not only\n",
      "qualitatively realistic but also help to improve polyp detection performance. \n",
      "\n",
      "\n",
      "Phantoms for surgical training are able to mimic cutting and suturing\n",
      "properties and patient-individual shape of organs, but lack a realistic visual\n",
      "appearance that captures the heterogeneity of surgical scenes. In order to\n",
      "overcome this in endoscopic approaches, hyperrealistic concepts have been\n",
      "proposed to be used in an augmented reality-setting, which are based on deep\n",
      "image-to-image transformation methods. Such concepts are able to generate\n",
      "realistic representations of phantoms learned from real intraoperative\n",
      "endoscopic sequences. Conditioned on frames from the surgical training process,\n",
      "the learned models are able to generate impressive results by transforming\n",
      "unrealistic parts of the image (e.g.\\ the uniform phantom texture is replaced\n",
      "by the more heterogeneous texture of the tissue). Image-to-image synthesis\n",
      "usually learns a mapping $G:X~\\to~Y$ such that the distribution of images from\n",
      "$G(X)$ is indistinguishable from the distribution $Y$. However, it does not\n",
      "necessarily force the generated images to be consistent and without artifacts.\n",
      "In the endoscopic image domain this can affect depth cues and stereo\n",
      "consistency of a stereo image pair, which ultimately impairs surgical vision.\n",
      "We propose a cross-domain conditional generative adversarial network approach\n",
      "(GAN) that aims to generate more consistent stereo pairs. The results show\n",
      "substantial improvements in depth perception and realism evaluated by 3 domain\n",
      "experts and 3 medical students on a 3D monitor over the baseline method. In 84\n",
      "of 90 instances our proposed method was preferred or rated equal to the\n",
      "baseline. \n",
      "\n",
      "\n",
      "Generating a photorealistic image with intended human pose is a promising yet\n",
      "challenging research topic for many applications such as smart photo editing,\n",
      "movie making, virtual try-on, and fashion display. In this paper, we present a\n",
      "novel deep generative model to transfer an image of a person from a given pose\n",
      "to a new pose while keeping fashion item consistent. In order to formulate the\n",
      "framework, we employ one generator and two discriminators for image synthesis.\n",
      "The generator includes an image encoder, a pose encoder and a decoder. The two\n",
      "encoders provide good representation of visual and geometrical context which\n",
      "will be utilized by the decoder in order to generate a photorealistic image.\n",
      "Unlike existing pose-guided image generation models, we exploit two\n",
      "discriminators to guide the synthesis process where one discriminator\n",
      "differentiates between generated image and real images (training samples), and\n",
      "another discriminator verifies the consistency of appearance between a target\n",
      "pose and a generated image. We perform end-to-end training of the network to\n",
      "learn the parameters through back-propagation given ground-truth images. The\n",
      "proposed generative model is capable of synthesizing a photorealistic image of\n",
      "a person given a target pose. We have demonstrated our results by conducting\n",
      "rigorous experiments on two data sets, both quantitatively and qualitatively. \n",
      "\n",
      "\n",
      "Scene text magnifier aims to magnify text in natural scene images without\n",
      "recognition. It could help the special groups, who have myopia or dyslexia to\n",
      "better understand the scene. In this paper, we design the scene text magnifier\n",
      "through interacted four CNN-based networks: character erasing, character\n",
      "extraction, character magnify, and image synthesis. The architecture of the\n",
      "networks are extended based on the hourglass encoder-decoders. It inputs the\n",
      "original scene text image and outputs the text magnified image while keeps the\n",
      "background unchange. Intermediately, we can get the side-output results of text\n",
      "erasing and text extraction. The four sub-networks are first trained\n",
      "independently and fine-tuned in end-to-end mode. The training samples for each\n",
      "stage are processed through a flow with original image and text annotation in\n",
      "ICDAR2013 and Flickr dataset as input, and corresponding text erased image,\n",
      "magnified text annotation, and text magnified scene image as output. To\n",
      "evaluate the performance of text magnifier, the Structural Similarity is used\n",
      "to measure the regional changes in each character region. The experimental\n",
      "results demonstrate our method can magnify scene text effectively without\n",
      "effecting the background. \n",
      "\n",
      "\n",
      "Skin lesion segmentation is a vital task in skin cancer diagnosis and further\n",
      "treatment. Although deep learning based approaches have significantly improved\n",
      "the segmentation accuracy, these algorithms are still reliant on having a large\n",
      "enough dataset in order to achieve adequate results. Inspired by the immense\n",
      "success of generative adversarial networks (GANs), we propose a GAN-based\n",
      "augmentation of the original dataset in order to improve the segmentation\n",
      "performance. In particular, we use the segmentation masks available in the\n",
      "training dataset to train the Mask2Lesion model, and use the model to generate\n",
      "new lesion images given any arbitrary mask, which are then used to augment the\n",
      "original training dataset. We test Mask2Lesion augmentation on the ISBI ISIC\n",
      "2017 Skin Lesion Segmentation Challenge dataset and achieve an improvement of\n",
      "5.17% in the mean Dice score as compared to a model trained with only classical\n",
      "data augmentation techniques. \n",
      "\n",
      "\n",
      "We introduce a data-driven approach for interactively synthesizing\n",
      "in-the-wild images from semantic label maps. Our approach is dramatically\n",
      "different from recent work in this space, in that we make use of no learning.\n",
      "Instead, our approach uses simple but classic tools for matching scene context,\n",
      "shapes, and parts to a stored library of exemplars. Though simple, this\n",
      "approach has several notable advantages over recent work: (1) because nothing\n",
      "is learned, it is not limited to specific training data distributions (such as\n",
      "cityscapes, facades, or faces); (2) it can synthesize arbitrarily\n",
      "high-resolution images, limited only by the resolution of the exemplar library;\n",
      "(3) by appropriately composing shapes and parts, it can generate an\n",
      "exponentially large set of viable candidate output images (that can say, be\n",
      "interactively searched by a user). We present results on the diverse COCO\n",
      "dataset, significantly outperforming learning-based approaches on standard\n",
      "image synthesis metrics. Finally, we explore user-interaction and\n",
      "user-controllability, demonstrating that our system can be used as a platform\n",
      "for user-driven content creation. \n",
      "\n",
      "\n",
      "Computed tomography (CT) is a widely used imaging modality for medical\n",
      "diagnosis and treatment. In electroencephalography (EEG), CT imaging is\n",
      "necessary for co-registering with magnetic resonance imaging (MRI) and for\n",
      "creating more accurate head models for the brain electrical activity due to\n",
      "better representation of bone anatomy. Unfortunately, CT imaging exposes\n",
      "patients to potentially harmful sources of ionizing radiation. Image synthesis\n",
      "methods present a solution for avoiding extra radiation exposure. In this\n",
      "paper, we perform image synthesis to create a realistic, synthetic CT image\n",
      "from MRI of the same subject, and we present a comparison of different image\n",
      "synthesis techniques. Using a dataset of 30 paired MRI and CT image volumes,\n",
      "our results compare image synthesis using deep neural network regression,\n",
      "state-of-the-art adversarial deep learning, as well as atlas-based synthesis\n",
      "utilizing image registration. We also present a novel synthesis method that\n",
      "combines multi-atlas registration as a prior to deep learning algorithms, in\n",
      "which we perform a weighted addition of synthetic CT images, derived from\n",
      "atlases, to the output of a deep neural network to obtain a residual type of\n",
      "learning. In addition to evaluating the quality of the synthetic CT images, we\n",
      "also demonstrate that image synthesis methods allow for more accurate bone\n",
      "segmentation using the synthetic CT imaging than would otherwise be possible by\n",
      "segmenting the bone in the MRI directly. \n",
      "\n",
      "\n",
      "We show that the basic classification framework alone can be used to tackle\n",
      "some of the most challenging tasks in image synthesis. In contrast to other\n",
      "state-of-the-art approaches, the toolkit we develop is rather minimal: it uses\n",
      "a single, off-the-shelf classifier for all these tasks. The crux of our\n",
      "approach is that we train this classifier to be adversarially robust. It turns\n",
      "out that adversarial robustness is precisely what we need to directly\n",
      "manipulate salient features of the input. Overall, our findings demonstrate the\n",
      "utility of robustness in the broader machine learning context. Code and models\n",
      "for our experiments can be found at https://git.io/robust-apps. \n",
      "\n",
      "\n",
      "Example-guided image synthesis aims to synthesize an image from a semantic\n",
      "label map and an exemplary image indicating style. We use the term \"style\" in\n",
      "this problem to refer to implicit characteristics of images, for example: in\n",
      "portraits \"style\" includes gender, racial identity, age, hairstyle; in full\n",
      "body pictures it includes clothing; in street scenes, it refers to weather and\n",
      "time of day and such like. A semantic label map in these cases indicates facial\n",
      "expression, full body pose, or scene segmentation. We propose a solution to the\n",
      "example-guided image synthesis problem using conditional generative adversarial\n",
      "networks with style consistency. Our key contributions are (i) a novel style\n",
      "consistency discriminator to determine whether a pair of images are consistent\n",
      "in style; (ii) an adaptive semantic consistency loss; and (iii) a training data\n",
      "sampling strategy, for synthesizing style-consistent results to the exemplar. \n",
      "\n",
      "\n",
      "This paper investigates a novel a-posteriori variance reduction approach in\n",
      "Monte Carlo image synthesis. Unlike most established methods based on lateral\n",
      "filtering in the image space, our proposition is to produce the best possible\n",
      "estimate for each pixel separately, from all the samples drawn for it. To\n",
      "enable this, we systematically study the per-pixel sample distributions for\n",
      "diverse scene configurations. Noting that these are too complex to be\n",
      "characterized by standard statistical distributions (e.g. Gaussians), we\n",
      "identify patterns recurring in them and exploit those for training a\n",
      "variance-reduction model based on neural nets. In result, we obtain numerically\n",
      "better estimates compared to simple averaging of samples. This method is\n",
      "compatible with existing image-space denoising methods, as the improved\n",
      "estimates of our model can be used for further processing. We conclude by\n",
      "discussing how the proposed model could in future be extended for fully\n",
      "progressive rendering with constant memory footprint and scene-sensitive\n",
      "output. \n",
      "\n",
      "\n",
      "These days deep learning is the fastest-growing area in the field of Machine\n",
      "Learning. Convolutional Neural Networks are currently the main tool used for\n",
      "image analysis and classification purposes. Although great achievements and\n",
      "perspectives, deep neural networks and accompanying learning algorithms have\n",
      "some relevant challenges to tackle. In this paper, we have focused on the most\n",
      "frequently mentioned problem in the field of machine learning, that is\n",
      "relatively poor generalization abilities. Partial remedies for this are\n",
      "regularization techniques e.g. dropout, batch normalization, weight decay,\n",
      "transfer learning, early stopping and data augmentation. In this paper, we have\n",
      "focused on data augmentation. We propose to use a method based on a neural\n",
      "style transfer, which allows generating new unlabeled images of a high\n",
      "perceptual quality that combine the content of a base image with the appearance\n",
      "of another one. In a proposed approach, the newly created images are described\n",
      "with pseudo-labels, and then used as a training dataset. Real, labeled images\n",
      "are divided into the validation and test set. We validated the proposed method\n",
      "on a challenging skin lesion classification case study. Four representative\n",
      "neural architectures are examined. Obtained results show the strong potential\n",
      "of the proposed approach. \n",
      "\n",
      "\n",
      "Spiking neural networks (SNNs) offer a promising alternative to current\n",
      "artificial neural networks to enable low-power event-driven neuromorphic\n",
      "hardware. Spike-based neuromorphic applications require processing and\n",
      "extracting meaningful information from spatio-temporal data, represented as\n",
      "series of spike trains over time. In this paper, we propose a method to\n",
      "synthesize images from multiple modalities in a spike-based environment. We use\n",
      "spiking auto-encoders to convert image and audio inputs into compact\n",
      "spatio-temporal representations that is then decoded for image synthesis. For\n",
      "this, we use a direct training algorithm that computes loss on the membrane\n",
      "potential of the output layer and back-propagates it by using a sigmoid\n",
      "approximation of the neuron's activation function to enable differentiability.\n",
      "The spiking autoencoders are benchmarked on MNIST and Fashion-MNIST and achieve\n",
      "very low reconstruction loss, comparable to ANNs. Then, spiking autoencoders\n",
      "are trained to learn meaningful spatio-temporal representations of the data,\n",
      "across the two modalities - audio and visual. We synthesize images from audio\n",
      "in a spike-based environment by first generating, and then utilizing such\n",
      "shared multi-modal spatio-temporal representations. Our audio to image\n",
      "synthesis model is tested on the task of converting TI-46 digits audio samples\n",
      "to MNIST images. We are able to synthesize images with high fidelity and the\n",
      "model achieves competitive performance against ANNs. \n",
      "\n",
      "\n",
      "Since the advent of deep convolutional neural networks (DNNs), computer\n",
      "vision has seen an extremely rapid progress that has led to huge advances in\n",
      "medical imaging. This article does not aim to cover all aspects of the field\n",
      "but focuses on a particular topic, image-to-image translation. Although the\n",
      "topic may not sound familiar, it turns out that many seemingly irrelevant\n",
      "applications can be understood as instances of image-to-image translation. Such\n",
      "applications include (1) noise reduction, (2) super-resolution, (3) image\n",
      "synthesis, and (4) reconstruction. The same underlying principles and\n",
      "algorithms work for various tasks. Our aim is to introduce some of the key\n",
      "ideas on this topic from a uniform point of view. We introduce core ideas and\n",
      "jargon that are specific to image processing by use of DNNs. Having an\n",
      "intuitive grasp of the core ideas of and a knowledge of technical terms would\n",
      "be of great help to the reader for understanding the existing and future\n",
      "applications. Most of the recent applications which build on image-to-image\n",
      "translation are based on one of two fundamental architectures, called pix2pix\n",
      "and CycleGAN, depending on whether the available training data are paired or\n",
      "unpaired. We provide computer codes which implement these two architectures\n",
      "with various enhancements. Our codes are available online with use of the very\n",
      "permissive MIT license. We provide a hands-on tutorial for training a model for\n",
      "denoising based on our codes. We hope that this article, together with the\n",
      "codes, will provide both an overview and the details of the key algorithms, and\n",
      "that it will serve as a basis for the development of new applications. \n",
      "\n",
      "\n",
      "Cross-domain synthesizing realistic faces to learn deep models has attracted\n",
      "increasing attention for facial expression analysis as it helps to improve the\n",
      "performance of expression recognition accuracy despite having small number of\n",
      "real training images. However, learning from synthetic face images can be\n",
      "problematic due to the distribution discrepancy between low-quality synthetic\n",
      "images and real face images and may not achieve the desired performance when\n",
      "the learned model applies to real world scenarios. To this end, we propose a\n",
      "new attribute guided face image synthesis to perform a translation between\n",
      "multiple image domains using a single model. In addition, we adopt the proposed\n",
      "model to learn from synthetic faces by matching the feature distributions\n",
      "between different domains while preserving each domain's characteristics. We\n",
      "evaluate the effectiveness of the proposed approach on several face datasets on\n",
      "generating realistic face images. We demonstrate that the expression\n",
      "recognition performance can be enhanced by benefiting from our face synthesis\n",
      "model. Moreover, we also conduct experiments on a near-infrared dataset\n",
      "containing facial expression videos of drivers to assess the performance using\n",
      "in-the-wild data for driver emotion recognition. \n",
      "\n",
      "\n",
      "The recent success of Generative Adversarial Networks (GAN) is a result of\n",
      "their ability to generate high quality images from a latent vector space. An\n",
      "important application is the generation of images from a text description,\n",
      "where the text description is encoded and further used in the conditioning of\n",
      "the generated image. Thus the generative network has to additionally learn a\n",
      "mapping from the text latent vector space to a highly complex and multi-modal\n",
      "image data distribution, which makes the training of such models challenging.\n",
      "To handle the complexities of fashion image and meta data, we propose Ontology\n",
      "Generative Adversarial Networks (O-GANs) for fashion image synthesis that is\n",
      "conditioned on an hierarchical fashion ontology in order to improve the image\n",
      "generation fidelity. We show that the incorporation of the ontology leads to\n",
      "better image quality as measured by Fr\\'{e}chet Inception Distance and\n",
      "Inception Score. Additionally, we show that the O-GAN achieves better\n",
      "conditioning results evaluated by implicit similarity between the text and the\n",
      "generated image. \n",
      "\n",
      "\n",
      "Fog and haze are weathers with low visibility which are adversarial to the\n",
      "driving safety of intelligent vehicles equipped with optical sensors like\n",
      "cameras and LiDARs. Therefore image dehazing for perception enhancement and\n",
      "haze image synthesis for testing perception abilities are equivalently\n",
      "important in the development of such autonomous driving systems. From the view\n",
      "of image translation, these two problems are essentially dual with each other,\n",
      "which have the potentiality to be solved jointly. In this paper, we propose an\n",
      "unsupervised Image-to-Image Translation framework based on Variational\n",
      "Autoencoders (VAE) and Generative Adversarial Nets (GAN) to handle haze image\n",
      "synthesis and haze removal simultaneously. Since the KL divergence in the VAE\n",
      "objectives could not guarantee the optimal mapping under imbalanced and\n",
      "unpaired training samples with limited size, Maximum mean discrepancy (MMD)\n",
      "based VAE is utilized to ensure the translating consistency in both directions.\n",
      "The comprehensive analysis on both synthesis and dehazing performance of our\n",
      "method demonstrate the feasibility and practicability of the proposed method. \n",
      "\n",
      "\n",
      "Despite the rapid progress of generative adversarial networks (GANs) in image\n",
      "synthesis in recent years, the existing image synthesis approaches work in\n",
      "either geometry domain or appearance domain alone which often introduces\n",
      "various synthesis artifacts. This paper presents an innovative Hierarchical\n",
      "Composition GAN (HIC-GAN) that incorporates image synthesis in geometry and\n",
      "appearance domains into an end-to-end trainable network and achieves superior\n",
      "synthesis realism in both domains simultaneously. We design an innovative\n",
      "hierarchical composition mechanism that is capable of learning realistic\n",
      "composition geometry and handling occlusions while multiple foreground objects\n",
      "are involved in image composition. In addition, we introduce a novel attention\n",
      "mask mechanism that guides to adapt the appearance of foreground objects which\n",
      "also helps to provide better training reference for learning in geometry\n",
      "domain. Extensive experiments on scene text image synthesis, portrait editing\n",
      "and indoor rendering tasks show that the proposed HIC-GAN achieves superior\n",
      "synthesis performance qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) are increasingly attracting attention\n",
      "in the computer vision, natural language processing, speech synthesis and\n",
      "similar domains. Arguably the most striking results have been in the area of\n",
      "image synthesis. However, evaluating the performance of GANs is still an open\n",
      "and challenging problem. Existing evaluation metrics primarily measure the\n",
      "dissimilarity between real and generated images using automated statistical\n",
      "methods. They often require large sample sizes for evaluation and do not\n",
      "directly reflect human perception of image quality. In this work, we describe\n",
      "an evaluation metric we call Neuroscore, for evaluating the performance of\n",
      "GANs, that more directly reflects psychoperceptual image quality through the\n",
      "utilization of brain signals. Our results show that Neuroscore has superior\n",
      "performance to the current evaluation metrics in that: (1) It is more\n",
      "consistent with human judgment; (2) The evaluation process needs much smaller\n",
      "numbers of samples; and (3) It is able to rank the quality of images on a per\n",
      "GAN basis. A convolutional neural network (CNN) based neuro-AI interface is\n",
      "proposed to predict Neuroscore from GAN-generated images directly without the\n",
      "need for neural responses. Importantly, we show that including neural responses\n",
      "during the training phase of the network can significantly improve the\n",
      "prediction capability of the proposed model. Materials related to this work are\n",
      "provided at https://github.com/villawang/Neuro-AI-Interface. \n",
      "\n",
      "\n",
      "Thanks to the recent success of generative adversarial network (GAN) for\n",
      "image synthesis, there are many exciting GAN approaches that successfully\n",
      "synthesize MR image contrast from other images with different contrasts. These\n",
      "approaches are potentially important for image imputation problems, where\n",
      "complete set of data is often difficult to obtain and image synthesis is one of\n",
      "the key solutions for handling the missing data problem. Unfortunately, the\n",
      "lack of the scalability of the existing GAN-based image translation approaches\n",
      "poses a fundamental challenge to understand the nature of the MR contrast\n",
      "imputation problem: which contrast does matter? Here, we present a systematic\n",
      "approach using Collaborative Generative Adversarial Networks (CollaGAN), which\n",
      "enable the learning of the joint image manifold of multiple MR contrasts to\n",
      "investigate which contrasts are essential. Our experimental results showed that\n",
      "the exogenous contrast from contrast agents is not replaceable, but other\n",
      "endogenous contrast such as T1, T2, etc can be synthesized from other contrast.\n",
      "These findings may give important guidance to the acquisition protocol design\n",
      "for MR in real clinical environment. \n",
      "\n",
      "\n",
      "In this work we propose a new computational framework, based on generative\n",
      "deep models, for synthesis of photo-realistic food meal images from textual\n",
      "descriptions of its ingredients. Previous works on synthesis of images from\n",
      "text typically rely on pre-trained text models to extract text features,\n",
      "followed by a generative neural networks (GANs) aimed to generate realistic\n",
      "images conditioned on the text features. These works mainly focus on generating\n",
      "spatially compact and well-defined categories of objects, such as birds or\n",
      "flowers. In contrast, meal images are significantly more complex, consisting of\n",
      "multiple ingredients whose appearance and spatial qualities are further\n",
      "modified by cooking methods. We propose a method that first builds an\n",
      "attention-based ingredients-image association model, which is then used to\n",
      "condition a generative neural network tasked with synthesizing meal images.\n",
      "Furthermore, a cycle-consistent constraint is added to further improve image\n",
      "quality and control appearance. Extensive experiments show our model is able to\n",
      "generate meal image corresponding to the ingredients, which could be used to\n",
      "augment existing dataset for solving other computational food analysis\n",
      "problems. \n",
      "\n",
      "\n",
      "Facial landmark localization is a very crucial step in numerous face related\n",
      "applications, such as face recognition, facial pose estimation, face image\n",
      "synthesis, etc. However, previous competitions on facial landmark localization\n",
      "(i.e., the 300-W, 300-VW and Menpo challenges) aim to predict 68-point\n",
      "landmarks, which are incompetent to depict the structure of facial components.\n",
      "In order to overcome this problem, we construct a challenging dataset, named\n",
      "JD-landmark. Each image is manually annotated with 106-point landmarks. This\n",
      "dataset covers large variations on pose and expression, which brings a lot of\n",
      "difficulties to predict accurate landmarks. We hold a 106-point facial landmark\n",
      "localization competition1 on this dataset in conjunction with IEEE\n",
      "International Conference on Multimedia and Expo (ICME) 2019. The purpose of\n",
      "this competition is to discover effective and robust facial landmark\n",
      "localization approaches. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are a powerful class of generative\n",
      "models. Despite their successes, the most appropriate choice of a GAN network\n",
      "architecture is still not well understood. GAN models for image synthesis have\n",
      "adopted a deep convolutional network architecture, which eliminates or\n",
      "minimizes the use of fully connected and pooling layers in favor of convolution\n",
      "layers in the generator and discriminator of GANs. In this paper, we\n",
      "demonstrate that a convolution network architecture utilizing deep fully\n",
      "connected layers and pooling layers can be more effective than the traditional\n",
      "convolution-only architecture, and we propose FCC-GAN, a fully connected and\n",
      "convolutional GAN architecture. Models based on our FCC-GAN architecture learn\n",
      "both faster than the conventional architecture and also generate higher quality\n",
      "of samples. We demonstrate the effectiveness and stability of our approach\n",
      "across four popular image datasets. \n",
      "\n",
      "\n",
      "Attribute guided face image synthesis aims to manipulate attributes on a face\n",
      "image. Most existing methods for image-to-image translation can either perform\n",
      "a fixed translation between any two image domains using a single attribute or\n",
      "require training data with the attributes of interest for each subject.\n",
      "Therefore, these methods could only train one specific model for each pair of\n",
      "image domains, which limits their ability in dealing with more than two\n",
      "domains. Another disadvantage of these methods is that they often suffer from\n",
      "the common problem of mode collapse that degrades the quality of the generated\n",
      "images. To overcome these shortcomings, we propose attribute guided face image\n",
      "generation method using a single model, which is capable to synthesize multiple\n",
      "photo-realistic face images conditioned on the attributes of interest. In\n",
      "addition, we adopt the proposed model to increase the realism of the simulated\n",
      "face images while preserving the face characteristics. Compared to existing\n",
      "models, synthetic face images generated by our method present a good\n",
      "photorealistic quality on several face datasets. Finally, we demonstrate that\n",
      "generated facial images can be used for synthetic data augmentation, and\n",
      "improve the performance of the classifier used for facial expression\n",
      "recognition. \n",
      "\n",
      "\n",
      "We propose the fusion discriminator, a single unified framework for\n",
      "incorporating conditional information into a generative adversarial network\n",
      "(GAN) for a variety of distinct structured prediction tasks, including image\n",
      "synthesis, semantic segmentation, and depth estimation. Much like commonly used\n",
      "convolutional neural network -- conditional Markov random field (CNN-CRF)\n",
      "models, the proposed method is able to enforce higher-order consistency in the\n",
      "model, but without being limited to a very specific class of potentials. The\n",
      "method is conceptually simple and flexible, and our experimental results\n",
      "demonstrate improvement on several diverse structured prediction tasks. \n",
      "\n",
      "\n",
      "The modern computer graphics pipeline can synthesize images at remarkable\n",
      "visual quality; however, it requires well-defined, high-quality 3D content as\n",
      "input. In this work, we explore the use of imperfect 3D content, for instance,\n",
      "obtained from photo-metric reconstructions with noisy and incomplete surface\n",
      "geometry, while still aiming to produce photo-realistic (re-)renderings. To\n",
      "address this challenging problem, we introduce Deferred Neural Rendering, a new\n",
      "paradigm for image synthesis that combines the traditional graphics pipeline\n",
      "with learnable components. Specifically, we propose Neural Textures, which are\n",
      "learned feature maps that are trained as part of the scene capture process.\n",
      "Similar to traditional textures, neural textures are stored as maps on top of\n",
      "3D mesh proxies; however, the high-dimensional feature maps contain\n",
      "significantly more information, which can be interpreted by our new deferred\n",
      "neural rendering pipeline. Both neural textures and deferred neural renderer\n",
      "are trained end-to-end, enabling us to synthesize photo-realistic images even\n",
      "when the original 3D content was imperfect. In contrast to traditional,\n",
      "black-box 2D generative neural networks, our 3D representation gives us\n",
      "explicit control over the generated output, and allows for a wide range of\n",
      "application domains. For instance, we can synthesize temporally-consistent\n",
      "video re-renderings of recorded 3D scenes as our representation is inherently\n",
      "embedded in 3D space. This way, neural textures can be utilized to coherently\n",
      "re-render or manipulate existing video content in both static and dynamic\n",
      "environments at real-time rates. We show the effectiveness of our approach in\n",
      "several experiments on novel view synthesis, scene editing, and facial\n",
      "reenactment, and compare to state-of-the-art approaches that leverage the\n",
      "standard graphics pipeline as well as conventional generative neural networks. \n",
      "\n",
      "\n",
      "This paper proposes a novel framework for lung segmentation in chest X-rays.\n",
      "It consists of two key contributions, a criss-cross attention based\n",
      "segmentation network and radiorealistic chest X-ray image synthesis (i.e. a\n",
      "synthesized radiograph that appears anatomically realistic) for data\n",
      "augmentation. The criss-cross attention modules capture rich global contextual\n",
      "information in both horizontal and vertical directions for all the pixels thus\n",
      "facilitating accurate lung segmentation. To reduce the manual annotation burden\n",
      "and to train a robust lung segmentor that can be adapted to pathological lungs\n",
      "with hazy lung boundaries, an image-to-image translation module is employed to\n",
      "synthesize radiorealistic abnormal CXRs from the source of normal ones for data\n",
      "augmentation. The lung masks of synthetic abnormal CXRs are propagated from the\n",
      "segmentation results of their normal counterparts, and then serve as pseudo\n",
      "masks for robust segmentor training. In addition, we annotate 100 CXRs with\n",
      "lung masks on a more challenging NIH Chest X-ray dataset containing both\n",
      "posterioranterior and anteroposterior views for evaluation. Extensive\n",
      "experiments validate the robustness and effectiveness of the proposed\n",
      "framework. The code and data can be found from\n",
      "https://github.com/rsummers11/CADLab/tree/master/Lung_Segmentation_XLSor . \n",
      "\n",
      "\n",
      "With billions of personal images being generated from social media and\n",
      "cameras of all sorts on a daily basis, security and privacy are unprecedentedly\n",
      "challenged. Although extensive attempts have been made, existing face image\n",
      "de-identification techniques are either insufficient in photo-reality or\n",
      "incapable of balancing privacy and usability qualitatively and quantitatively,\n",
      "i.e., they fail to answer counterfactual questions such as \"is it private\n",
      "now?\", \"how private is it?\", and \"can it be more private?\" In this paper, we\n",
      "propose a novel framework called AnonymousNet, with an effort to address these\n",
      "issues systematically, balance usability, and enhance privacy in a natural and\n",
      "measurable manner. The framework encompasses four stages: facial attribute\n",
      "estimation, privacy-metric-oriented face obfuscation, directed natural image\n",
      "synthesis, and adversarial perturbation. Not only do we achieve the\n",
      "state-of-the-arts in terms of image quality and attribute prediction accuracy,\n",
      "we are also the first to show that facial privacy is measurable, can be\n",
      "factorized, and accordingly be manipulated in a photo-realistic fashion to\n",
      "fulfill different requirements and application scenarios. Experiments further\n",
      "demonstrate the effectiveness of the proposed framework. \n",
      "\n",
      "\n",
      "Polarimetric thermal to visible face verification entails matching two images\n",
      "that contain significant domain differences. Several recent approaches have\n",
      "attempted to synthesize visible faces from thermal images for cross-modal\n",
      "matching. In this paper, we take a different approach in which rather than\n",
      "focusing only on synthesizing visible faces from thermal faces, we also propose\n",
      "to synthesize thermal faces from visible faces. Our intuition is based on the\n",
      "fact that thermal images also contain some discriminative information about the\n",
      "person for verification. Deep features from a pre-trained Convolutional Neural\n",
      "Network (CNN) are extracted from the original as well as the synthesized\n",
      "images. These features are then fused to generate a template which is then used\n",
      "for verification. The proposed synthesis network is based on the self-attention\n",
      "generative adversarial network (SAGAN) which essentially allows efficient\n",
      "attention-guided image synthesis. Extensive experiments on the ARL polarimetric\n",
      "thermal face dataset demonstrate that the proposed method achieves\n",
      "state-of-the-art performance. \n",
      "\n",
      "\n",
      "Image synthesis is a core problem in modern deep learning, and many recent\n",
      "architectures such as autoencoders and Generative Adversarial networks produce\n",
      "spectacular results on highly complex data, such as images of faces or\n",
      "landscapes. While these results open up a wide range of new, advanced synthesis\n",
      "applications, there is also a severe lack of theoretical understanding of how\n",
      "these networks work. This results in a wide range of practical problems, such\n",
      "as difficulties in training, the tendency to sample images with little or no\n",
      "variability, and generalisation problems. In this paper, we propose to analyse\n",
      "the ability of the simplest generative network, the autoencoder, to encode and\n",
      "decode two simple geometric attributes : size and position. We believe that, in\n",
      "order to understand more complicated tasks, it is necessary to first understand\n",
      "how these networks process simple attributes. For the first property, we\n",
      "analyse the case of images of centred disks with variable radii. We explain how\n",
      "the autoencoder projects these images to and from a latent space of smallest\n",
      "possible dimension, a scalar. In particular, we describe a closed-form solution\n",
      "to the decoding training problem in a network without biases, and show that\n",
      "during training, the network indeed finds this solution. We then investigate\n",
      "the best regularisation approaches which yield networks that generalise well.\n",
      "For the second property, position, we look at the encoding and decoding of\n",
      "Dirac delta functions, also known as `one-hot' vectors. We describe a\n",
      "hand-crafted filter that achieves encoding perfectly, and show that the network\n",
      "naturally finds this filter during training. We also show experimentally that\n",
      "the decoding can be achieved if the dataset is sampled in an appropriate\n",
      "manner. \n",
      "\n",
      "\n",
      "As a sub-domain of text-to-image synthesis, text-to-face generation has huge\n",
      "potentials in public safety domain. With lack of dataset, there are almost no\n",
      "related research focusing on text-to-face synthesis. In this paper, we propose\n",
      "a fully-trained Generative Adversarial Network (FTGAN) that trains the text\n",
      "encoder and image decoder at the same time for fine-grained text-to-face\n",
      "generation. With a novel fully-trained generative network, FTGAN can synthesize\n",
      "higher-quality images and urge the outputs of the FTGAN are more relevant to\n",
      "the input sentences. In addition, we build a dataset called SCU-Text2face for\n",
      "text-to-face synthesis. Through extensive experiments, the FTGAN shows its\n",
      "superiority in boosting both generated images' quality and similarity to the\n",
      "input descriptions. The proposed FTGAN outperforms the previous state of the\n",
      "art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On\n",
      "SCU-text2face, the face images generated by our proposed FTGAN just based on\n",
      "the input descriptions is of average 59% similarity to the ground-truth, which\n",
      "set a baseline for text-to-face synthesis. \n",
      "\n",
      "\n",
      "In generative modeling, the Wasserstein distance (WD) has emerged as a useful\n",
      "metric to measure the discrepancy between generated and real data\n",
      "distributions. Unfortunately, it is challenging to approximate the WD of\n",
      "high-dimensional distributions. In contrast, the sliced Wasserstein distance\n",
      "(SWD) factorizes high-dimensional distributions into their multiple\n",
      "one-dimensional marginal distributions and is thus easier to approximate. In\n",
      "this paper, we introduce novel approximations of the primal and dual SWD.\n",
      "Instead of using a large number of random projections, as it is done by\n",
      "conventional SWD approximation methods, we propose to approximate SWDs with a\n",
      "small number of parameterized orthogonal projections in an end-to-end deep\n",
      "learning fashion. As concrete applications of our SWD approximations, we design\n",
      "two types of differentiable SWD blocks to equip modern generative\n",
      "frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\n",
      "the experiments, we not only show the superiority of the proposed generative\n",
      "models on standard image synthesis benchmarks, but also demonstrate the\n",
      "state-of-the-art performance on challenging high resolution image and video\n",
      "generation in an unsupervised manner. \n",
      "\n",
      "\n",
      "This paper presents a novel method to manipulate the visual appearance (pose\n",
      "and attribute) of a person image according to natural language descriptions.\n",
      "Our method can be boiled down to two stages: 1) text guided pose generation and\n",
      "2) visual appearance transferred image synthesis. In the first stage, our\n",
      "method infers a reasonable target human pose based on the text. In the second\n",
      "stage, our method synthesizes a realistic and appearance transferred person\n",
      "image according to the text in conjunction with the target pose. Our method\n",
      "extracts sufficient information from the text and establishes a mapping between\n",
      "the image space and the language space, making generating and editing images\n",
      "corresponding to the description possible. We conduct extensive experiments to\n",
      "reveal the effectiveness of our method, as well as using the VQA Perceptual\n",
      "Score as a metric for evaluating the method. It shows for the first time that\n",
      "we can automatically edit the person image from the natural language\n",
      "descriptions. \n",
      "\n",
      "\n",
      "Automatic Check-Out (ACO) receives increased interests in recent years. An\n",
      "important component of the ACO system is the visual item counting, which\n",
      "recognizes the categories and counts of the items chosen by the customers.\n",
      "However, the training of such a system is challenged by the domain adaptation\n",
      "problem, in which the training data are images from isolated items while the\n",
      "testing images are for collections of items. Existing methods solve this\n",
      "problem with data augmentation using synthesized images, but the image\n",
      "synthesis leads to unreal images that affect the training process. In this\n",
      "paper, we propose a new data priming method to solve the domain adaptation\n",
      "problem. Specifically, we first use pre-augmentation data priming, in which we\n",
      "remove distracting background from the training images using the coarse-to-fine\n",
      "strategy and select images with realistic view angles by the pose pruning\n",
      "method. In the post-augmentation step, we train a data priming network using\n",
      "detection and counting collaborative learning, and select more reliable images\n",
      "from testing data to fine-tune the final visual item tallying network.\n",
      "Experiments on the large scale Retail Product Checkout (RPC) dataset\n",
      "demonstrate the superiority of the proposed method, i.e., we achieve 80.51%\n",
      "checkout accuracy compared with 56.68% of the baseline methods. The source\n",
      "codes can be found in https://isrc.iscas.ac.cn/gitlab/research/acm-mm-2019-ACO. \n",
      "\n",
      "\n",
      "Parquetry is the art and craft of decorating a surface with a pattern of\n",
      "differently colored veneers of wood, stone or other materials. Traditionally,\n",
      "the process of designing and making parquetry has been driven by color, using\n",
      "the texture found in real wood only for stylization or as a decorative effect.\n",
      "Here, we introduce a computational pipeline that draws from the rich natural\n",
      "structure of strongly textured real-world veneers as a source of detail in\n",
      "order to approximate a target image as faithfully as possible using a\n",
      "manageable number of parts. This challenge is closely related to the\n",
      "established problems of patch-based image synthesis and stylization in some\n",
      "ways, but fundamentally different in others. Most importantly, the limited\n",
      "availability of resources (any piece of wood can only be used once) turns the\n",
      "relatively simple problem of finding the right piece for the target location\n",
      "into the combinatorial problem of finding optimal parts while avoiding resource\n",
      "collisions. We introduce an algorithm that allows to efficiently solve an\n",
      "approximation to the problem. It further addresses challenges like gamut\n",
      "mapping, feature characterization and the search for fabricable cuts. We\n",
      "demonstrate the effectiveness of the system by fabricating a selection of\n",
      "\"photo-realistic\" pieces of parquetry from different kinds of unstained wood\n",
      "veneer. \n",
      "\n",
      "\n",
      "Depth estimation from a single image represents a fascinating, yet\n",
      "challenging problem with countless applications. Recent works proved that this\n",
      "task could be learned without direct supervision from ground truth labels\n",
      "leveraging image synthesis on sequences or stereo pairs. Focusing on this\n",
      "second case, in this paper we leverage stereo matching in order to improve\n",
      "monocular depth estimation. To this aim we propose monoResMatch, a novel deep\n",
      "architecture designed to infer depth from a single input image by synthesizing\n",
      "features from a different point of view, horizontally aligned with the input\n",
      "image, performing stereo matching between the two cues. In contrast to previous\n",
      "works sharing this rationale, our network is the first trained end-to-end from\n",
      "scratch. Moreover, we show how obtaining proxy ground truth annotation through\n",
      "traditional stereo algorithms, such as Semi-Global Matching, enables more\n",
      "accurate monocular depth estimation still countering the need for expensive\n",
      "depth labels by keeping a self-supervised approach. Exhaustive experimental\n",
      "results prove how the synergy between i) the proposed monoResMatch architecture\n",
      "and ii) proxy-supervision attains state-of-the-art for self-supervised\n",
      "monocular depth estimation. The code is publicly available at\n",
      "https://github.com/fabiotosi92/monoResMatch-Tensorflow. \n",
      "\n",
      "\n",
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\n",
      "intelligence (AI) applications to general ophthalmology, comparatively less\n",
      "progress has been made toward solving problems in pediatric ophthalmology using\n",
      "similar techniques. This article discusses the unique needs of pediatric\n",
      "ophthalmology patients and how AI techniques can address these challenges,\n",
      "surveys recent applications of AI to pediatric ophthalmology, and discusses\n",
      "future directions in the field.\n",
      "  RECENT FINDINGS: The most significant advances involve the automated\n",
      "detection of retinopathy of prematurity (ROP), yielding results that rival\n",
      "experts. Machine learning (ML) has also been successfully applied to the\n",
      "classification of pediatric cataracts, prediction of post-operative\n",
      "complications following cataract surgery, detection of strabismus and\n",
      "refractive error, prediction of future high myopia, and diagnosis of reading\n",
      "disability via eye tracking. In addition, ML techniques have been used for the\n",
      "study of visual development, vessel segmentation in pediatric fundus images,\n",
      "and ophthalmic image synthesis.\n",
      "  SUMMARY: AI applications could significantly benefit clinical care for\n",
      "pediatric ophthalmology patients by optimizing disease detection and grading,\n",
      "broadening access to care, furthering scientific discovery, and improving\n",
      "clinical efficiency. These methods need to match or surpass physician\n",
      "performance in clinical trials before deployment with patients. Due to\n",
      "widespread use of closed-access data sets and software implementations, it is\n",
      "difficult to directly compare the performance of these approaches, and\n",
      "reproducibility is poor. Open-access data sets and software implementations\n",
      "could alleviate these issues, and encourage further AI applications to\n",
      "pediatric ophthalmology.\n",
      "  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\n",
      "deep learning \n",
      "\n",
      "\n",
      "Adversarial generative model have successfully manifest itself in image\n",
      "synthesis. However, the performance deteriorate and unstable, because\n",
      "discriminator is far stable than generator, and it is hard to control the game\n",
      "between the two modules. Various methods have been introduced to tackle the\n",
      "problem such as WGAN, Relativistic GAN and their successors by adding or\n",
      "restricting the loss function, which certainly help balance the min-max game,\n",
      "but they all focused on the loss function ignoring the intrinsic structure\n",
      "limitation. We present a UU-Net architecture inspired by U-net bridging the\n",
      "encoder and the decoder, UU-Net composed by two U-Net liked modules\n",
      "respectively served as generator and discriminator. Because the modules in\n",
      "U-net are symmetrical, therefore it shares weights easily between all four\n",
      "components. Thanks to UU-net's modules identical and symmetric property, we\n",
      "could not only carried the features from inner generator's encoder to its\n",
      "decoder, but also to the discriminator's encoder and decoder. By this design,\n",
      "it give us more control and condition flexibility to intervene the process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between the generator and the discriminator. \n",
      "\n",
      "\n",
      "Flow-based generative models show great potential in image synthesis due to\n",
      "its reversible pipeline and exact log-likelihood target, yet it suffers from\n",
      "weak ability for conditional image synthesis, especially for multi-label or\n",
      "unaware conditions. This is because the potential distribution of image\n",
      "conditions is hard to measure precisely from its latent variable $z$. In this\n",
      "paper, based on modeling a joint probabilistic density of an image and its\n",
      "conditions, we propose a novel flow-based generative model named conditional\n",
      "adversarial generative flow (CAGlow). Instead of disentangling attributes from\n",
      "latent space, we blaze a new trail for learning an encoder to estimate the\n",
      "mapping from condition space to latent space in an adversarial manner. Given a\n",
      "specific condition $c$, CAGlow can encode it to a sampled $z$, and then enable\n",
      "robust conditional image synthesis in complex situations like combining person\n",
      "identity with multiple attributes. The proposed CAGlow can be implemented in\n",
      "both supervised and unsupervised manners, thus can synthesize images with\n",
      "conditional information like categories, attributes, and even some unknown\n",
      "properties. Extensive experiments show that CAGlow ensures the independence of\n",
      "different conditions and outperforms regular Glow to a significant extent. \n",
      "\n",
      "\n",
      "In this paper, we focus on generating realistic images from text\n",
      "descriptions. Current methods first generate an initial image with rough shape\n",
      "and color, and then refine the initial image to a high-resolution one. Most\n",
      "existing text-to-image synthesis methods have two main problems. (1) These\n",
      "methods depend heavily on the quality of the initial images. If the initial\n",
      "image is not well initialized, the following processes can hardly refine the\n",
      "image to a satisfactory quality. (2) Each word contributes a different level of\n",
      "importance when depicting different image contents, however, unchanged text\n",
      "representation is used in existing image refinement processes. In this paper,\n",
      "we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to\n",
      "generate high-quality images. The proposed method introduces a dynamic memory\n",
      "module to refine fuzzy image contents, when the initial images are not well\n",
      "generated. A memory writing gate is designed to select the important text\n",
      "information based on the initial image content, which enables our method to\n",
      "accurately generate images from the text description. We also utilize a\n",
      "response gate to adaptively fuse the information read from the memories and the\n",
      "image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200\n",
      "dataset and the Microsoft Common Objects in Context dataset. Experimental\n",
      "results demonstrate that our DM-GAN model performs favorably against the\n",
      "state-of-the-art approaches. \n",
      "\n",
      "\n",
      "The collection of internet images has been growing in an astonishing speed.\n",
      "It is undoubted that these images contain rich visual information that can be\n",
      "useful in many applications, such as visual media creation and data-driven\n",
      "image synthesis. In this paper, we focus on the methodologies for building a\n",
      "visual object database from a collection of internet images. Such database is\n",
      "built to contain a large number of high-quality visual objects that can help\n",
      "with various data-driven image applications. Our method is based on dense\n",
      "proposal generation and objectness-based re-ranking. A novel deep convolutional\n",
      "neural network is designed for the inference of proposal objectness, the\n",
      "probability of a proposal containing optimally-located foreground object. In\n",
      "our work, the objectness is quantitatively measured in regard of completeness\n",
      "and fullness, reflecting two complementary features of an optimal proposal: a\n",
      "complete foreground and relatively small background. Our experiments indicate\n",
      "that object proposals re-ranked according to the output of our network\n",
      "generally achieve higher performance than those produced by other\n",
      "state-of-the-art methods. As a concrete example, a database of over 1.2 million\n",
      "visual objects has been built using the proposed method, and has been\n",
      "successfully used in various data-driven image applications. \n",
      "\n",
      "\n",
      "Generative adversary networks (GANs) have recently led to highly realistic\n",
      "image synthesis results. In this work, we describe a new method to expose\n",
      "GAN-synthesized images using the locations of the facial landmark points. Our\n",
      "method is based on the observations that the facial parts configuration\n",
      "generated by GAN models are different from those of the real faces, due to the\n",
      "lack of global constraints. We perform experiments demonstrating this\n",
      "phenomenon, and show that an SVM classifier trained using the locations of\n",
      "facial landmark points is sufficient to achieve good classification performance\n",
      "for GAN-synthesized faces. \n",
      "\n",
      "\n",
      "One of the grand challenges of deep learning is the requirement to obtain\n",
      "large labeled training data sets. While synthesized data sets can be used to\n",
      "overcome this challenge, it is important that these data sets close the reality\n",
      "gap, i.e., a model trained on synthetic image data is able to generalize to\n",
      "real images. Whereas, the reality gap can be considered bridged in several\n",
      "application scenarios, training on synthesized images containing reflecting\n",
      "materials requires further research. Since the appearance of objects with\n",
      "reflecting materials is dominated by the surrounding environment, this\n",
      "interaction needs to be considered during training data generation. Therefore,\n",
      "within this paper we examine the effect of reflecting materials in the context\n",
      "of synthetic image generation for training object detectors. We investigate the\n",
      "influence of rendering approach used for image synthesis, the effect of domain\n",
      "randomization, as well as the amount of used training data. To be able to\n",
      "compare our results to the state-of-the-art, we focus on indoor scenes as they\n",
      "have been investigated extensively. Within this scenario, bathroom furniture is\n",
      "a natural choice for objects with reflecting materials, for which we report our\n",
      "findings on real and synthetic testing data. \n",
      "\n",
      "\n",
      "Generating images via the generative adversarial network (GAN) has attracted\n",
      "much attention recently. However, most of the existing GAN-based methods can\n",
      "only produce low-resolution images of limited quality. Directly generating\n",
      "high-resolution images using GANs is nontrivial, and often produces problematic\n",
      "images with incomplete objects. To address this issue, we develop a novel GAN\n",
      "called Auto-Embedding Generative Adversarial Network (AEGAN), which\n",
      "simultaneously encodes the global structure features and captures the\n",
      "fine-grained details. In our network, we use an autoencoder to learn the\n",
      "intrinsic high-level structure of real images and design a novel denoiser\n",
      "network to provide photo-realistic details for the generated images. In the\n",
      "experiments, we are able to produce 512x512 images of promising quality\n",
      "directly from the input noise. The resultant images exhibit better perceptual\n",
      "photo-realism, i.e., with sharper structure and richer details, than other\n",
      "baselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds\n",
      "(CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale\n",
      "Scene Understanding (LSUN) and ImageNet. \n",
      "\n",
      "\n",
      "Typically, a medical image offers spatial information on the anatomy (and\n",
      "pathology) modulated by imaging specific characteristics. Many imaging\n",
      "modalities including Magnetic Resonance Imaging (MRI) and Computed Tomography\n",
      "(CT) can be interpreted in this way. We can venture further and consider that a\n",
      "medical image naturally factors into some spatial factors depicting anatomy and\n",
      "factors that denote the imaging characteristics. Here, we explicitly learn this\n",
      "decomposed (disentangled) representation of imaging data, focusing in\n",
      "particular on cardiac images. We propose Spatial Decomposition Network (SDNet),\n",
      "which factorises 2D medical images into spatial anatomical factors and\n",
      "non-spatial modality factors. We demonstrate that this high-level\n",
      "representation is ideally suited for several medical image analysis tasks, such\n",
      "as semi-supervised segmentation, multi-task segmentation and regression, and\n",
      "image-to-image synthesis. Specifically, we show that our model can match the\n",
      "performance of fully supervised segmentation models, using only a fraction of\n",
      "the labelled images. Critically, we show that our factorised representation\n",
      "also benefits from supervision obtained either when we use auxiliary tasks to\n",
      "train the model in a multi-task setting (e.g. regressing to known cardiac\n",
      "indices), or when aggregating multimodal data from different sources (e.g.\n",
      "pooling together MRI and CT data). To explore the properties of the learned\n",
      "factorisation, we perform latent-space arithmetic and show that we can\n",
      "synthesise CT from MR and vice versa, by swapping the modality factors. We also\n",
      "demonstrate that the factor holding image specific information can be used to\n",
      "predict the input modality with high accuracy. Code will be made available at\n",
      "https://github.com/agis85/anatomy_modality_decomposition. \n",
      "\n",
      "\n",
      "Neural network-based methods have recently demonstrated state-of-the-art\n",
      "results on image synthesis and super-resolution tasks, in particular by using\n",
      "variants of generative adversarial networks (GANs) with supervised feature\n",
      "losses. Nevertheless, previous feature loss formulations rely on the\n",
      "availability of large auxiliary classifier networks, and labeled datasets that\n",
      "enable such classifiers to be trained. Furthermore, there has been\n",
      "comparatively little work to explore the applicability of GAN-based methods to\n",
      "domains other than images and video. In this work we explore a GAN-based method\n",
      "for audio processing, and develop a convolutional neural network architecture\n",
      "to perform audio super-resolution. In addition to several new architectural\n",
      "building blocks for audio processing, a key component of our approach is the\n",
      "use of an autoencoder-based loss that enables training in the GAN framework,\n",
      "with feature losses derived from unlabeled data. We explore the impact of our\n",
      "architectural choices, and demonstrate significant improvements over previous\n",
      "works in terms of both objective and perceptual quality. \n",
      "\n",
      "\n",
      "We propose spatially-adaptive normalization, a simple but effective layer for\n",
      "synthesizing photorealistic images given an input semantic layout. Previous\n",
      "methods directly feed the semantic layout as input to the deep network, which\n",
      "is then processed through stacks of convolution, normalization, and\n",
      "nonlinearity layers. We show that this is suboptimal as the normalization\n",
      "layers tend to ``wash away'' semantic information. To address the issue, we\n",
      "propose using the input layout for modulating the activations in normalization\n",
      "layers through a spatially-adaptive, learned transformation. Experiments on\n",
      "several challenging datasets demonstrate the advantage of the proposed method\n",
      "over existing approaches, regarding both visual fidelity and alignment with\n",
      "input layouts. Finally, our model allows user control over both semantic and\n",
      "style. Code is available at https://github.com/NVlabs/SPADE . \n",
      "\n",
      "\n",
      "Large intra-class variation is the result of changes in multiple object\n",
      "characteristics. Images, however, only show the superposition of different\n",
      "variable factors such as appearance or shape. Therefore, learning to\n",
      "disentangle and represent these different characteristics poses a great\n",
      "challenge, especially in the unsupervised case. Moreover, large object\n",
      "articulation calls for a flexible part-based model. We present an unsupervised\n",
      "approach for disentangling appearance and shape by learning parts consistently\n",
      "over all instances of a category. Our model for learning an object\n",
      "representation is trained by simultaneously exploiting invariance and\n",
      "equivariance constraints between synthetically transformed images. Since no\n",
      "part annotation or prior information on an object class is required, the\n",
      "approach is applicable to arbitrary classes. We evaluate our approach on a wide\n",
      "range of object categories and diverse tasks including pose prediction,\n",
      "disentangled image synthesis, and video-to-video translation. The approach\n",
      "outperforms the state-of-the-art on unsupervised keypoint prediction and\n",
      "compares favorably even against supervised approaches on the task of shape and\n",
      "appearance transfer. \n",
      "\n",
      "\n",
      "While Generative Adversarial Networks (GANs) have seen huge successes in\n",
      "image synthesis tasks, they are notoriously difficult to adapt to different\n",
      "datasets, in part due to instability during training and sensitivity to\n",
      "hyperparameters. One commonly accepted reason for this instability is that\n",
      "gradients passing from the discriminator to the generator become uninformative\n",
      "when there isn't enough overlap in the supports of the real and fake\n",
      "distributions. In this work, we propose the Multi-Scale Gradient Generative\n",
      "Adversarial Network (MSG-GAN), a simple but effective technique for addressing\n",
      "this by allowing the flow of gradients from the discriminator to the generator\n",
      "at multiple scales. This technique provides a stable approach for high\n",
      "resolution image synthesis, and serves as an alternative to the commonly used\n",
      "progressive growing technique. We show that MSG-GAN converges stably on a\n",
      "variety of image datasets of different sizes, resolutions and domains, as well\n",
      "as different types of loss functions and architectures, all with the same set\n",
      "of fixed hyperparameters. When compared to state-of-the-art GANs, our approach\n",
      "matches or exceeds the performance in most of the cases we tried. \n",
      "\n",
      "\n",
      "Most conditional generation tasks expect diverse outputs given a single\n",
      "conditional context. However, conditional generative adversarial networks\n",
      "(cGANs) often focus on the prior conditional information and ignore the input\n",
      "noise vectors, which contribute to the output variations. Recent attempts to\n",
      "resolve the mode collapse issue for cGANs are usually task-specific and\n",
      "computationally expensive. In this work, we propose a simple yet effective\n",
      "regularization term to address the mode collapse issue for cGANs. The proposed\n",
      "method explicitly maximizes the ratio of the distance between generated images\n",
      "with respect to the corresponding latent codes, thus encouraging the generators\n",
      "to explore more minor modes during training. This mode seeking regularization\n",
      "term is readily applicable to various conditional generation tasks without\n",
      "imposing training overhead or modifying the original network structures. We\n",
      "validate the proposed algorithm on three conditional image synthesis tasks\n",
      "including categorical generation, image-to-image translation, and text-to-image\n",
      "synthesis with different baseline models. Both qualitative and quantitative\n",
      "results demonstrate the effectiveness of the proposed regularization method for\n",
      "improving diversity without loss of quality. \n",
      "\n",
      "\n",
      "Recent work has shown significant progress in the direction of synthetic data\n",
      "generation using Generative Adversarial Networks (GANs). GANs have been applied\n",
      "in many fields of computer vision including text-to-image conversion, domain\n",
      "transfer, super-resolution, and image-to-video applications. In computer\n",
      "vision, traditional GANs are based on deep convolutional neural networks.\n",
      "However, deep convolutional neural networks can require extensive computational\n",
      "resources because they are based on multiple operations performed by\n",
      "convolutional layers, which can consist of millions of trainable parameters.\n",
      "Training a GAN model can be difficult and it takes a significant amount of time\n",
      "to reach an equilibrium point. In this paper, we investigate the use of\n",
      "depthwise separable convolutions to reduce training time while maintaining data\n",
      "generation performance. Our results show that a DepthwiseGAN architecture can\n",
      "generate realistic images in shorter training periods when compared to a\n",
      "StarGan architecture, but that model capacity still plays a significant role in\n",
      "generative modelling. In addition, we show that depthwise separable\n",
      "convolutions perform best when only applied to the generator. For quality\n",
      "evaluation of generated images, we use the Fr\\'echet Inception Distance (FID),\n",
      "which compares the similarity between the generated image distribution and that\n",
      "of the training dataset. \n",
      "\n",
      "\n",
      "Generating plausible hair image given limited guidance, such as sparse\n",
      "sketches or low-resolution image, has been made possible with the rise of\n",
      "Generative Adversarial Networks (GANs). Traditional image-to-image translation\n",
      "networks can generate recognizable results, but finer textures are usually lost\n",
      "and blur artifacts commonly exist. In this paper, we propose a two-phase\n",
      "generative model for high-quality hair image synthesis. The two-phase pipeline\n",
      "first generates a coarse image by an existing image translation model, then\n",
      "applies a re-generating network with self-enhancing capability to the coarse\n",
      "image. The self-enhancing capability is achieved by a proposed structure\n",
      "extraction layer, which extracts the texture and orientation map from a hair\n",
      "image. Extensive experiments on two tasks, Sketch2Hair and Hair\n",
      "Super-Resolution, demonstrate that our approach is able to synthesize plausible\n",
      "hair image with finer details, and outperforms the state-of-the-art. \n",
      "\n",
      "\n",
      "In this paper, we propose Object-driven Attentive Generative Adversarial\n",
      "Newtorks (Obj-GANs) that allow object-centered text-to-image synthesis for\n",
      "complex scenes. Following the two-step (layout-image) generation process, a\n",
      "novel object-driven attentive image generator is proposed to synthesize salient\n",
      "objects by paying attention to the most relevant words in the text description\n",
      "and the pre-generated semantic layout. In addition, a new Fast R-CNN based\n",
      "object-wise discriminator is proposed to provide rich object-wise\n",
      "discrimination signals on whether the synthesized object matches the text\n",
      "description and the pre-generated layout. The proposed Obj-GAN significantly\n",
      "outperforms the previous state of the art in various metrics on the large-scale\n",
      "COCO benchmark, increasing the Inception score by 27% and decreasing the FID\n",
      "score by 11%. A thorough comparison between the traditional grid attention and\n",
      "the new object-driven attention is provided through analyzing their mechanisms\n",
      "and visualizing their attention layers, showing insights of how the proposed\n",
      "model generates complex scenes in high quality. \n",
      "\n",
      "\n",
      "We present an approach to synthesize highly photorealistic images of 3D\n",
      "object models, which we use to train a convolutional neural network for\n",
      "detecting the objects in real images. The proposed approach has three key\n",
      "ingredients: (1) 3D object models are rendered in 3D models of complete scenes\n",
      "with realistic materials and lighting, (2) plausible geometric configuration of\n",
      "objects and cameras in a scene is generated using physics simulations, and (3)\n",
      "high photorealism of the synthesized images achieved by physically based\n",
      "rendering. When trained on images synthesized by the proposed approach, the\n",
      "Faster R-CNN object detector achieves a 24% absolute improvement of mAP@.75IoU\n",
      "on Rutgers APC and 11% on LineMod-Occluded datasets, compared to a baseline\n",
      "where the training images are synthesized by rendering object models on top of\n",
      "random photographs. This work is a step towards being able to effectively train\n",
      "object detectors without capturing or annotating any real images. A dataset of\n",
      "600K synthetic images with ground truth annotations for various computer vision\n",
      "tasks will be released on the project website: thodan.github.io/objectsynth. \n",
      "\n",
      "\n",
      "The great success achieved by deep neural networks attracts increasing\n",
      "attention from the manufacturing and healthcare communities. However, the\n",
      "limited availability of data and high costs of data collection are the major\n",
      "challenges for the applications in those fields. We propose in this work AISEL,\n",
      "an active image synthesis method for efficient labeling to improve the\n",
      "performance of the small-data learning tasks. Specifically, a complementary\n",
      "AISEL dataset is generated, with labels actively acquired via a physics-based\n",
      "method to incorporate underlining physical knowledge at hand. An important\n",
      "component of our AISEL method is the bidirectional generative invertible\n",
      "network (GIN), which can extract interpretable features from the training\n",
      "images and generate physically meaningful virtual images. Our AISEL method then\n",
      "efficiently samples virtual images not only further exploits the uncertain\n",
      "regions, but also explores the entire image space. We then discuss the\n",
      "interpretability of GIN both theoretically and experimentally, demonstrating\n",
      "clear visual improvements over the benchmarks. Finally, we demonstrate the\n",
      "effectiveness of our AISEL framework on aortic stenosis application, in which\n",
      "our method lower the labeling cost by $90\\%$ while achieving a $15\\%$\n",
      "improvement in prediction accuracy. \n",
      "\n",
      "\n",
      "Visual compatibility is critical for fashion analysis, yet is missing in\n",
      "existing fashion image synthesis systems. In this paper, we propose to\n",
      "explicitly model visual compatibility through fashion image inpainting. To this\n",
      "end, we present Fashion Inpainting Networks (FiNet), a two-stage image-to-image\n",
      "generation framework that is able to perform compatible and diverse inpainting.\n",
      "Disentangling the generation of shape and appearance to ensure photorealistic\n",
      "results, our framework consists of a shape generation network and an appearance\n",
      "generation network. More importantly, for each generation network, we introduce\n",
      "two encoders interacting with one another to learn latent code in a shared\n",
      "compatibility space. The latent representations are jointly optimized with the\n",
      "corresponding generation network to condition the synthesis process,\n",
      "encouraging a diverse set of generated results that are visually compatible\n",
      "with existing fashion garments. In addition, our framework is readily extended\n",
      "to clothing reconstruction and fashion transfer, with impressive results.\n",
      "Extensive experiments with comparisons with state-of-the-art approaches on\n",
      "fashion synthesis task quantitatively and qualitatively demonstrate the\n",
      "effectiveness of our method. \n",
      "\n",
      "\n",
      "Generative flows are attractive because they admit exact likelihood\n",
      "optimization and efficient image synthesis. Recently, Kingma & Dhariwal (2018)\n",
      "demonstrated with Glow that generative flows are capable of generating high\n",
      "quality images. We generalize the 1 x 1 convolutions proposed in Glow to\n",
      "invertible d x d convolutions, which are more flexible since they operate on\n",
      "both channel and spatial axes. We propose two methods to produce invertible\n",
      "convolutions that have receptive fields identical to standard convolutions:\n",
      "Emerging convolutions are obtained by chaining specific autoregressive\n",
      "convolutions, and periodic convolutions are decoupled in the frequency domain.\n",
      "Our experiments show that the flexibility of d x d convolutions significantly\n",
      "improves the performance of generative flow models on galaxy images, CIFAR10\n",
      "and ImageNet. \n",
      "\n",
      "\n",
      "The two key players in Generative Adversarial Networks (GANs), the\n",
      "discriminator and generator, are usually parameterized as deep neural networks\n",
      "(DNNs). On many generative tasks, GANs achieve state-of-the-art performance but\n",
      "are often unstable to train and sometimes miss modes. A typical failure mode is\n",
      "the collapse of the generator to a single parameter configuration where its\n",
      "outputs are identical. When this collapse occurs, the gradient of the\n",
      "discriminator may point in similar directions for many similar points. We\n",
      "hypothesize that some of these shortcomings are in part due to primitive and\n",
      "redundant features extracted by discriminator and this can easily make the\n",
      "training stuck. We present a novel approach for regularizing adversarial models\n",
      "by enforcing diverse feature learning. In order to do this, both generator and\n",
      "discriminator are regularized by penalizing both negatively and positively\n",
      "correlated features according to their differentiation and based on their\n",
      "relative cosine distances. In addition to the gradient information from the\n",
      "adversarial loss made available by the discriminator, diversity regularization\n",
      "also ensures that a more stable gradient is provided to update both the\n",
      "generator and discriminator. Results indicate our regularizer enforces diverse\n",
      "features, stabilizes training, and improves image synthesis. \n",
      "\n",
      "\n",
      "Training of Generative Adversarial Networks (GANs) is notoriously fragile,\n",
      "requiring to maintain a careful balance between the generator and the\n",
      "discriminator in order to perform well. To mitigate this issue we introduce a\n",
      "new regularization technique - progressive augmentation of GANs (PA-GAN). The\n",
      "key idea is to gradually increase the task difficulty of the discriminator by\n",
      "progressively augmenting its input or feature space, thus enabling continuous\n",
      "learning of the generator. We show that the proposed progressive augmentation\n",
      "preserves the original GAN objective, does not compromise the discriminator's\n",
      "optimality and encourages a healthy competition between the generator and\n",
      "discriminator, leading to the better-performing generator. We experimentally\n",
      "demonstrate the effectiveness of PA-GAN across different architectures and on\n",
      "multiple benchmarks for the image synthesis task, on average achieving ~3 point\n",
      "improvement of the FID score. \n",
      "\n",
      "\n",
      "A large amount of annotated training images is critical for training accurate\n",
      "and robust deep network models but the collection of a large amount of\n",
      "annotated training images is often time-consuming and costly. Image synthesis\n",
      "alleviates this constraint by generating annotated training images\n",
      "automatically by machines which has attracted increasing interest in the recent\n",
      "deep learning research. We develop an innovative image synthesis technique that\n",
      "composes annotated training images by realistically embedding foreground\n",
      "objects of interest (OOI) into background images. The proposed technique\n",
      "consists of two key components that in principle boost the usefulness of the\n",
      "synthesized images in deep network training. The first is context-aware\n",
      "semantic coherence which ensures that the OOI are placed around semantically\n",
      "coherent regions within the background image. The second is harmonious\n",
      "appearance adaptation which ensures that the embedded OOI are agreeable to the\n",
      "surrounding background from both geometry alignment and appearance realism. The\n",
      "proposed technique has been evaluated over two related but very different\n",
      "computer vision challenges, namely, scene text detection and scene text\n",
      "recognition. Experiments over a number of public datasets demonstrate the\n",
      "effectiveness of our proposed image synthesis technique - the use of our\n",
      "synthesized images in deep network training is capable of achieving similar or\n",
      "even better scene text detection and scene text recognition performance as\n",
      "compared with using real images. \n",
      "\n",
      "\n",
      "Current approaches have made great progress on image-to-image translation\n",
      "tasks benefiting from the success of image synthesis methods especially\n",
      "generative adversarial networks (GANs). However, existing methods are limited\n",
      "to handling translation tasks between two species while keeping the content\n",
      "matching on the semantic level. A more challenging task would be the\n",
      "translation among more than two species. To explore this new area, we propose a\n",
      "simple yet effective structure of a multi-branch discriminator for enhancing an\n",
      "arbitrary generative adversarial architecture (GAN), named GAN-MBD. It takes\n",
      "advantage of the boosting strategy to break a common discriminator into several\n",
      "smaller ones with fewer parameters, which can enhance the generation and\n",
      "synthesis abilities of GANs efficiently and effectively. Comprehensive\n",
      "experiments show that the proposed multi-branch discriminator can dramatically\n",
      "improve the performance of popular GANs on cross-species image-to-image\n",
      "translation tasks while reducing the number of parameters for computation. The\n",
      "code and some datasets are attached as supplementary materials for reference. \n",
      "\n",
      "\n",
      "Image synthesis and image-to-image translation are two important generative\n",
      "learning tasks. Remarkable progress has been made by learning Generative\n",
      "Adversarial Networks (GANs)~\\cite{goodfellow2014generative} and\n",
      "cycle-consistent GANs (CycleGANs)~\\cite{zhu2017unpaired} respectively. This\n",
      "paper presents a method of learning Spatial Pyramid Attentive Pooling (SPAP)\n",
      "which is a novel architectural unit and can be easily integrated into both\n",
      "generators and discriminators in GANs and CycleGANs. The proposed SPAP\n",
      "integrates Atrous spatial pyramid~\\cite{chen2018deeplab}, a proposed cascade\n",
      "attention mechanism and residual connections~\\cite{he2016deep}. It leverages\n",
      "the advantages of the three components to facilitate effective end-to-end\n",
      "generative learning: (i) the capability of fusing multi-scale information by\n",
      "ASPP; (ii) the capability of capturing relative importance between both spatial\n",
      "locations (especially multi-scale context) or feature channels by attention;\n",
      "(iii) the capability of preserving information and enhancing optimization\n",
      "feasibility by residual connections. Coarse-to-fine and fine-to-coarse SPAP are\n",
      "studied and intriguing attention maps are observed in both tasks. In\n",
      "experiments, the proposed SPAP is tested in GANs on the Celeba-HQ-128\n",
      "dataset~\\cite{karras2017progressive}, and tested in CycleGANs on the\n",
      "Image-to-Image translation datasets including the Cityscape\n",
      "dataset~\\cite{cordts2016cityscapes}, Facade and Aerial Maps\n",
      "dataset~\\cite{zhu2017unpaired}, both obtaining better performance. \n",
      "\n",
      "\n",
      "In this paper, we describe how to apply image-to-image translation techniques\n",
      "to medical blood smear data to generate new data samples and meaningfully\n",
      "increase small datasets. Specifically, given the segmentation mask of the\n",
      "microscopy image, we are able to generate photorealistic images of blood cells\n",
      "which are further used alongside real data during the network training for\n",
      "segmentation and object detection tasks. This image data generation approach is\n",
      "based on conditional generative adversarial networks which have proven\n",
      "capabilities to high-quality image synthesis. In addition to synthesizing blood\n",
      "images, we synthesize segmentation mask as well which leads to a diverse\n",
      "variety of generated samples. The effectiveness of the technique is thoroughly\n",
      "analyzed and quantified through a number of experiments on a manually collected\n",
      "and annotated dataset of blood smear taken under a microscope. \n",
      "\n",
      "\n",
      "Affine transformation, layer blending, and artistic filters are popular\n",
      "processes that graphic designers employ to transform pixels of an image to\n",
      "create a desired effect. Here, we examine various approaches that synthesize\n",
      "new images: pixel-based compositing models and in particular, distributed\n",
      "representations of deep neural network models. This paper focuses on\n",
      "synthesizing new images from a learned representation model obtained from the\n",
      "VGG network. This approach offers an interesting creative process from its\n",
      "distributed representation of information in hidden layers of a deep VGG\n",
      "network i.e., information such as contour, shape, etc. are effectively captured\n",
      "in hidden layers of neural networks. Conceptually, if $\\Phi$ is the function\n",
      "that transforms input pixels into distributed representations of VGG layers\n",
      "${\\bf h}$, a new synthesized image $X$ can be generated from its inverse\n",
      "function, $X = \\Phi^{-1}({\\bf h})$. We describe the concept behind the\n",
      "approach, present some representative synthesized images and style-transferred\n",
      "image examples. \n",
      "\n",
      "\n",
      "Unconditional image generation has recently been dominated by generative\n",
      "adversarial networks (GANs). GAN methods train a generator which regresses\n",
      "images from random noise vectors, as well as a discriminator that attempts to\n",
      "differentiate between the generated images and a training set of real images.\n",
      "GANs have shown amazing results at generating realistic looking images. Despite\n",
      "their success, GANs suffer from critical drawbacks including: unstable training\n",
      "and mode-dropping. The weaknesses in GANs have motivated research into\n",
      "alternatives including: variational auto-encoders (VAEs), latent embedding\n",
      "learning methods (e.g. GLO) and nearest-neighbor based implicit maximum\n",
      "likelihood estimation (IMLE). Unfortunately at the moment, GANs still\n",
      "significantly outperform the alternative methods for image generation. In this\n",
      "work, we present a novel method - Generative Latent Nearest Neighbors (GLANN) -\n",
      "for training generative models without adversarial training. GLANN combines the\n",
      "strengths of IMLE and GLO in a way that overcomes the main drawbacks of each\n",
      "method. Consequently, GLANN generates images that are far better than GLO and\n",
      "IMLE. Our method does not suffer from mode collapse which plagues GAN training\n",
      "and is much more stable. Qualitative results show that GLANN outperforms a\n",
      "baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models\n",
      "are also shown to be effective for training truly non-adversarial unsupervised\n",
      "image translation. \n",
      "\n",
      "\n",
      "This paper presents a \"learning to learn\" approach to figure-ground image\n",
      "segmentation. By exploring webly-abundant images of specific visual effects,\n",
      "our method can effectively learn the visual-effect internal representations in\n",
      "an unsupervised manner and uses this knowledge to differentiate the figure from\n",
      "the ground in an image. Specifically, we formulate the meta-learning process as\n",
      "a compositional image editing task that learns to imitate a certain visual\n",
      "effect and derive the corresponding internal representation. Such a generative\n",
      "process can help instantiate the underlying figure-ground notion and enables\n",
      "the system to accomplish the intended image segmentation. Whereas existing\n",
      "generative methods are mostly tailored to image synthesis or style transfer,\n",
      "our approach offers a flexible learning mechanism to model a general concept of\n",
      "figure-ground segmentation from unorganized images that have no explicit\n",
      "pixel-level annotations. We validate our approach via extensive experiments on\n",
      "six datasets to demonstrate that the proposed model can be end-to-end trained\n",
      "without ground-truth pixel labeling yet outperforms the existing methods of\n",
      "unsupervised segmentation tasks. \n",
      "\n",
      "\n",
      "Most existing text-to-image synthesis tasks are static single-turn\n",
      "generation, based on pre-defined textual descriptions of images. To explore\n",
      "more practical and interactive real-life applications, we introduce a new task\n",
      "- Interactive Image Editing, where users can guide an agent to edit images via\n",
      "multi-turn textual commands on-the-fly. In each session, the agent takes a\n",
      "natural language description from the user as the input and modifies the image\n",
      "generated in the previous turn to a new design, following the user description.\n",
      "The main challenges in this sequential and interactive image generation task\n",
      "are two-fold: 1) contextual consistency between a generated image and the\n",
      "provided textual description; 2) step-by-step region-level modification to\n",
      "maintain visual consistency across the generated image sequence in each\n",
      "session. To address these challenges, we propose a novel Sequential Attention\n",
      "Generative Adversarial Net-work (SeqAttnGAN), which applies a neural state\n",
      "tracker to encode the previous image and the textual description in each turn\n",
      "of the sequence, and uses a GAN framework to generate a modified version of the\n",
      "image that is consistent with the preceding images and coherent with the\n",
      "description. To achieve better region-specific refinement, we also introduce a\n",
      "sequential attention mechanism into the model. To benchmark on the new task, we\n",
      "introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain\n",
      "multi-turn sessions with image-description sequences in the fashion domain.\n",
      "Experiments on both datasets show that the proposed SeqAttnGANmodel outperforms\n",
      "state-of-the-art approaches on the interactive image editing task across all\n",
      "evaluation metrics including visual quality, image sequence coherence, and\n",
      "text-image consistency. \n",
      "\n",
      "\n",
      "Recent advances in generative adversarial networks (GANs) have shown great\n",
      "potentials in realistic image synthesis whereas most existing works address\n",
      "synthesis realism in either appearance space or geometry space but few in both.\n",
      "This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a\n",
      "geometry synthesizer and an appearance synthesizer to achieve synthesis realism\n",
      "in both geometry and appearance spaces. The geometry synthesizer learns\n",
      "contextual geometries of background images and transforms and places foreground\n",
      "objects into the background images unanimously. The appearance synthesizer\n",
      "adjusts the color, brightness and styles of the foreground objects and embeds\n",
      "them into background images harmoniously, where a guided filter is introduced\n",
      "for detail preserving. The two synthesizers are inter-connected as mutual\n",
      "references which can be trained end-to-end without supervision. The SF-GAN has\n",
      "been evaluated in two tasks: (1) realistic scene text image synthesis for\n",
      "training better recognition models; (2) glass and hat wearing for realistic\n",
      "matching glasses and hats with real portraits. Qualitative and quantitative\n",
      "comparisons with the state-of-the-art demonstrate the superiority of the\n",
      "proposed SF-GAN. \n",
      "\n",
      "\n",
      "We describe a new approach that improves the training of generative\n",
      "adversarial nets (GANs) for synthesizing diverse images from a text input. Our\n",
      "approach is based on the conditional version of GANs and expands on previous\n",
      "work leveraging an auxiliary task in the discriminator. Our generated images\n",
      "are not limited to certain classes and do not suffer from mode collapse while\n",
      "semantically matching the text input. A key to our training methods is how to\n",
      "form positive and negative training examples with respect to the class label of\n",
      "a given image. Instead of selecting random training examples, we perform\n",
      "negative sampling based on the semantic distance from a positive example in the\n",
      "class. We evaluate our approach using the Oxford-102 flower dataset, adopting\n",
      "the inception score and multi-scale structural similarity index (MS-SSIM)\n",
      "metrics to assess discriminability and diversity of the generated images. The\n",
      "empirical results indicate greater diversity in the generated images,\n",
      "especially when we gradually select more negative training examples closer to a\n",
      "positive example in the semantic space. \n",
      "\n",
      "\n",
      "As biometric applications are fielded to serve large population groups,\n",
      "issues of performance differences between individual sub-groups are becoming\n",
      "increasingly important. In this paper we examine cases where we believe race is\n",
      "one such factor. We look in particular at two forms of problem; facial\n",
      "classification and image synthesis. We take the novel approach of considering\n",
      "race as a boundary for transfer learning in both the task (facial\n",
      "classification) and the domain (synthesis over distinct datasets). We\n",
      "demonstrate a series of techniques to improve transfer learning of facial\n",
      "classification; outperforming similar models trained in the target's own\n",
      "domain. We conduct a study to evaluate the performance drop of Generative\n",
      "Adversarial Networks trained to conduct image synthesis, in this process, we\n",
      "produce a new annotation for the Celeb-A dataset by race. These networks are\n",
      "trained solely on one race and tested on another - demonstrating the subsets of\n",
      "the CelebA to be distinct domains for this task. \n",
      "\n",
      "\n",
      "Image synthesis learns a transformation from the intensity features of an\n",
      "input image to yield a different tissue contrast of the output image. This\n",
      "process has been shown to have application in many medical image analysis tasks\n",
      "including imputation, registration, and segmentation. To carry out synthesis,\n",
      "the intensities of the input images are typically scaled--i.e.,\n",
      "normalized--both in training to learn the transformation and in testing when\n",
      "applying the transformation, but it is not presently known what type of input\n",
      "scaling is optimal. In this paper, we consider seven different intensity\n",
      "normalization algorithms and three different synthesis methods to evaluate the\n",
      "impact of normalization. Our experiments demonstrate that intensity\n",
      "normalization as a preprocessing step improves the synthesis results across all\n",
      "investigated synthesis algorithms. Furthermore, we show evidence that suggests\n",
      "intensity normalization is vital for successful deep learning-based MR image\n",
      "synthesis. \n",
      "\n",
      "\n",
      "Recent progress in deep generative models has led to tremendous breakthroughs\n",
      "in image generation. However, while existing models can synthesize\n",
      "photorealistic images, they lack an understanding of our underlying 3D world.\n",
      "We present a new generative model, Visual Object Networks (VON), synthesizing\n",
      "natural images of objects with a disentangled 3D representation. Inspired by\n",
      "classic graphics rendering pipelines, we unravel our image formation process\n",
      "into three conditionally independent factors---shape, viewpoint, and\n",
      "texture---and present an end-to-end adversarial learning framework that jointly\n",
      "models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes\n",
      "that are indistinguishable from real shapes. It then renders the object's 2.5D\n",
      "sketches (i.e., silhouette and depth map) from its shape under a sampled\n",
      "viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches\n",
      "to generate natural images. The VON not only generates images that are more\n",
      "realistic than state-of-the-art 2D image synthesis methods, but also enables\n",
      "many 3D operations such as changing the viewpoint of a generated image, editing\n",
      "of shape and texture, linear interpolation in texture and shape space, and\n",
      "transferring appearance across different objects and viewpoints. \n",
      "\n",
      "\n",
      "The performance of medical image analysis systems is constrained by the\n",
      "quantity of high-quality image annotations. Such systems require data to be\n",
      "annotated by experts with years of training, especially when diagnostic\n",
      "decisions are involved. Such datasets are thus hard to scale up. In this\n",
      "context, it is hard for supervised learning systems to generalize to the cases\n",
      "that are rare in the training set but would be present in real-world clinical\n",
      "practices. We believe that the synthetic image samples generated by a system\n",
      "trained on the real data can be useful for improving the supervised learning\n",
      "tasks in the medical image analysis applications. Allowing the image synthesis\n",
      "to be manipulable could help synthetic images provide complementary information\n",
      "to the training data rather than simply duplicating the real-data manifold. In\n",
      "this paper, we propose a framework for synthesizing 3D objects, such as\n",
      "pulmonary nodules, in 3D medical images with manipulable properties. The\n",
      "manipulation is enabled by decomposing of the object of interests into its\n",
      "segmentation mask and a 1D vector containing the residual information. The\n",
      "synthetic object is refined and blended into the image context with two\n",
      "adversarial discriminators. We evaluate the proposed framework on lung nodules\n",
      "in 3D chest CT images and show that the proposed framework could generate\n",
      "realistic nodules with manipulable shapes, textures and locations, etc. By\n",
      "sampling from both the synthetic nodules and the real nodules from 2800 3D CT\n",
      "volumes during the classifier training, we show the synthetic patches could\n",
      "improve the overall nodule detection performance by average 8.44% competition\n",
      "performance metric (CPM) score. \n",
      "\n",
      "\n",
      "The advance of Generative Adversarial Networks (GANs) enables realistic face\n",
      "image synthesis. However, synthesizing face images that preserve facial\n",
      "identity as well as have high diversity within each identity remains\n",
      "challenging. To address this problem, we present FaceFeat-GAN, a novel\n",
      "generative model that improves both image quality and diversity by using two\n",
      "stages. Unlike existing single-stage models that map random noise to image\n",
      "directly, our two-stage synthesis includes the first stage of diverse feature\n",
      "generation and the second stage of feature-to-image rendering. The competitions\n",
      "between generators and discriminators are carefully designed in both stages\n",
      "with different objective functions. Specially, in the first stage, they compete\n",
      "in the feature domain to synthesize various facial features rather than images.\n",
      "In the second stage, they compete in the image domain to render photo-realistic\n",
      "images that contain high diversity but preserve identity. Extensive experiments\n",
      "show that FaceFeat-GAN generates images that not only retain identity\n",
      "information but also have high diversity and quality, significantly\n",
      "outperforming previous methods. \n",
      "\n",
      "\n",
      "We propose a light-weight video frame interpolation algorithm. Our key\n",
      "innovation is an instance-level supervision that allows information to be\n",
      "learned from the high-resolution version of similar objects. Our experiment\n",
      "shows that the proposed method can generate state-of-the-art results across\n",
      "different datasets, with fractional computation resources (time and memory) of\n",
      "competing methods. Given two image frames, a cascade network creates an\n",
      "intermediate frame with 1) a flow-warping module that computes coarse\n",
      "bi-directional optical flow and creates an interpolated image via flow-based\n",
      "warping, followed by 2) an image synthesis module to make fine-scale\n",
      "corrections. In the learning stage, object detection proposals are generated on\n",
      "the interpolated image.Lower resolution objects are zoomed into, and the\n",
      "learning algorithms using an adversarial loss trained on high-resolution\n",
      "objects to guide the system towards the instance-level refinement corrects\n",
      "details of object shape and boundaries. \n",
      "\n",
      "\n",
      "Hand image synthesis and pose estimation from RGB images are both highly\n",
      "challenging tasks due to the large discrepancy between factors of variation\n",
      "ranging from image background content to camera viewpoint. To better analyze\n",
      "these factors of variation, we propose the use of disentangled representations\n",
      "and a disentangled variational autoencoder (dVAE) that allows for specific\n",
      "sampling and inference of these factors. The derived objective from the\n",
      "variational lower bound as well as the proposed training strategy are highly\n",
      "flexible, allowing us to handle cross-modal encoders and decoders as well as\n",
      "semi-supervised learning scenarios. Experiments show that our dVAE can\n",
      "synthesize highly realistic images of the hand specifiable by both pose and\n",
      "image background content and also estimate 3D hand poses from RGB images with\n",
      "accuracy competitive with state-of-the-art on two public benchmarks. \n",
      "\n",
      "\n",
      "Most existing methods for conditional image synthesis are only able to\n",
      "generate a single plausible image for any given input, or at best a fixed\n",
      "number of plausible images. In this paper, we focus on the problem of\n",
      "generating images from semantic segmentation maps and present a simple new\n",
      "method that can generate an arbitrary number of images with diverse appearance\n",
      "for the same semantic layout. Unlike most existing approaches which adopt the\n",
      "GAN framework, our method is based on the recently introduced Implicit Maximum\n",
      "Likelihood Estimation (IMLE) framework. Compared to the leading approach, our\n",
      "method is able to generate more diverse images while producing fewer artifacts\n",
      "despite using the same architecture. The learned latent space also has sensible\n",
      "structure despite the lack of supervision that encourages such behaviour.\n",
      "Videos and code are available at\n",
      "https://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/. \n",
      "\n",
      "\n",
      "We present a new deep learning approach to pose-guided resynthesis of human\n",
      "photographs. At the heart of the new approach is the estimation of the complete\n",
      "body surface texture based on a single photograph. Since the input photograph\n",
      "always observes only a part of the surface, we suggest a new inpainting method\n",
      "that completes the texture of the human body. Rather than working directly with\n",
      "colors of texture elements, the inpainting network estimates an appropriate\n",
      "source location in the input image for each element of the body surface. This\n",
      "correspondence field between the input image and the texture is then further\n",
      "warped into the target image coordinate frame based on the desired pose,\n",
      "effectively establishing the correspondence between the source and the target\n",
      "view even when the pose change is drastic. The final convolutional network then\n",
      "uses the established correspondence and all other available information to\n",
      "synthesize the output image. A fully-convolutional architecture with deformable\n",
      "skip connections guided by the estimated correspondence field is used. We show\n",
      "state-of-the-art result for pose-guided image synthesis. Additionally, we\n",
      "demonstrate the performance of our system for garment transfer and pose-guided\n",
      "face resynthesis. \n",
      "\n",
      "\n",
      "Conditional GANs are at the forefront of natural image synthesis. The main\n",
      "drawback of such models is the necessity for labeled data. In this work we\n",
      "exploit two popular unsupervised learning techniques, adversarial training and\n",
      "self-supervision, and take a step towards bridging the gap between conditional\n",
      "and unconditional GANs. In particular, we allow the networks to collaborate on\n",
      "the task of representation learning, while being adversarial with respect to\n",
      "the classic GAN game. The role of self-supervision is to encourage the\n",
      "discriminator to learn meaningful feature representations which are not\n",
      "forgotten during training. We test empirically both the quality of the learned\n",
      "image representations, and the quality of the synthesized images. Under the\n",
      "same conditions, the self-supervised GAN attains a similar performance to\n",
      "state-of-the-art conditional counterparts. Finally, we show that this approach\n",
      "to fully unsupervised learning can be scaled to attain an FID of 23.4 on\n",
      "unconditional ImageNet generation. \n",
      "\n",
      "\n",
      "We propose a learned image-guided rendering technique that combines the\n",
      "benefits of image-based rendering and GAN-based image synthesis. The goal of\n",
      "our method is to generate photo-realistic re-renderings of reconstructed\n",
      "objects for virtual and augmented reality applications (e.g., virtual\n",
      "showrooms, virtual tours \\& sightseeing, the digital inspection of historical\n",
      "artifacts). A core component of our work is the handling of view-dependent\n",
      "effects. Specifically, we directly train an object-specific deep neural network\n",
      "to synthesize the view-dependent appearance of an object. As input data we are\n",
      "using an RGB video of the object. This video is used to reconstruct a proxy\n",
      "geometry of the object via multi-view stereo. Based on this 3D proxy, the\n",
      "appearance of a captured view can be warped into a new target view as in\n",
      "classical image-based rendering. This warping assumes diffuse surfaces, in case\n",
      "of view-dependent effects, such as specular highlights, it leads to artifacts.\n",
      "To this end, we propose EffectsNet, a deep neural network that predicts\n",
      "view-dependent effects. Based on these estimations, we are able to convert\n",
      "observed images to diffuse images. These diffuse images can be projected into\n",
      "other views. In the target view, our pipeline reinserts the new view-dependent\n",
      "effects. To composite multiple reprojected images to a final output, we learn a\n",
      "composition network that outputs photo-realistic results. Using this\n",
      "image-guided approach, the network does not have to allocate capacity on\n",
      "``remembering'' object appearance, instead it learns how to combine the\n",
      "appearance of captured images. We demonstrate the effectiveness of our approach\n",
      "both qualitatively and quantitatively on synthetic as well as on real data. \n",
      "\n",
      "\n",
      "We present a novel CNN-based image editing strategy that allows the user to\n",
      "change the semantic information of an image over an arbitrary region by\n",
      "manipulating the feature-space representation of the image in a trained GAN\n",
      "model. We will present two variants of our strategy: (1) spatial conditional\n",
      "batch normalization (sCBN), a type of conditional batch normalization with\n",
      "user-specifiable spatial weight maps, and (2) feature-blending, a method of\n",
      "directly modifying the intermediate features. Our methods can be used to edit\n",
      "both artificial image and real image, and they both can be used together with\n",
      "any GAN with conditional normalization layers. We will demonstrate the power of\n",
      "our method through experiments on various types of GANs trained on different\n",
      "datasets. Code will be available at\n",
      "https://github.com/pfnet-research/neural-collage. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have shown considerable promise for\n",
      "mitigating the challenge of data scarcity when building machine learning-driven\n",
      "analysis algorithms. Specifically, a number of studies have shown that\n",
      "GAN-based image synthesis for data augmentation can aid in improving\n",
      "classification accuracy in a number of medical image analysis tasks, such as\n",
      "brain and liver image analysis. However, the efficacy of leveraging GANs for\n",
      "tackling prostate cancer analysis has not been previously explored. Motivated\n",
      "by this, in this study we introduce ProstateGAN, a GAN-based model for\n",
      "synthesizing realistic prostate diffusion imaging data. More specifically, in\n",
      "order to generate new diffusion imaging data corresponding to a particular\n",
      "cancer grade (Gleason score), we propose a conditional deep convolutional GAN\n",
      "architecture that takes Gleason scores into consideration during the training\n",
      "process. Experimental results show that high-quality synthetic prostate\n",
      "diffusion imaging data can be generated using the proposed ProstateGAN for\n",
      "specified Gleason scores. \n",
      "\n",
      "\n",
      "An interpretable generative model for handwritten digits synthesis is\n",
      "proposed in this work. Modern image generative models, such as Generative\n",
      "Adversarial Networks (GANs) and Variational Autoencoders (VAEs), are trained by\n",
      "backpropagation (BP). The training process is complex and the underlying\n",
      "mechanism is difficult to explain. We propose an interpretable multi-stage PCA\n",
      "method to achieve the same goal and use handwritten digit images synthesis as\n",
      "an illustrative example. First, we derive principal-component-analysis-based\n",
      "(PCA-based) transform kernels at each stage based on the covariance of its\n",
      "inputs. This results in a sequence of transforms that convert input images of\n",
      "correlated pixels to spectral vectors of uncorrelated components. In other\n",
      "words, it is a whitening process. Then, we can synthesize an image based on\n",
      "random vectors and multi-stage transform kernels through a coloring process.\n",
      "The generative model is a feedforward (FF) design since no BP is used in model\n",
      "parameter determination. Its design complexity is significantly lower, and the\n",
      "whole design process is explainable. Finally, we design an FF generative model\n",
      "using the MNIST dataset, compare synthesis results with those obtained by\n",
      "state-of-the-art GAN and VAE methods, and show that the proposed generative\n",
      "model achieves comparable performance. \n",
      "\n",
      "\n",
      "There is a growing interest in using generative adversarial networks (GANs)\n",
      "to produce image content that is indistinguishable from real images as judged\n",
      "by a typical person. A number of GAN variants for this purpose have been\n",
      "proposed, however, evaluating GANs performance is inherently difficult because\n",
      "current methods for measuring the quality of their output are not always\n",
      "consistent with what a human perceives. We propose a novel approach that\n",
      "combines a brain-computer interface (BCI) with GANs to generate a measure we\n",
      "call Neuroscore, which closely mirrors the behavioral ground truth measured\n",
      "from participants tasked with discerning real from synthetic images. This\n",
      "technique we call a neuro-AI interface, as it provides an interface between a\n",
      "human's neural systems and an AI process. In this paper, we first compare the\n",
      "three most widely used metrics in the literature for evaluating GANs in terms\n",
      "of visual quality and compare their outputs with human judgments. Secondly we\n",
      "propose and demonstrate a novel approach using neural signals and rapid serial\n",
      "visual presentation (RSVP) that directly measures a human perceptual response\n",
      "to facial production quality, independent of a behavioral response measurement.\n",
      "The correlation between our proposed Neuroscore and human perceptual judgments\n",
      "has Pearson correlation statistics: $\\mathrm{r}(48) = -0.767, \\mathrm{p} =\n",
      "2.089e-10$. We also present the bootstrap result for the correlation i.e.,\n",
      "$\\mathrm{p}\\leq 0.0001$. Results show that our Neuroscore is more consistent\n",
      "with human judgment compared to the conventional metrics we evaluated. We\n",
      "conclude that neural signals have potential applications for high quality,\n",
      "rapid evaluation of GANs in the context of visual image synthesis. \n",
      "\n",
      "\n",
      "We propose an algorithm to generate realistic face images of both real and\n",
      "synthetic identities (people who do not exist) with different facial yaw, shape\n",
      "and resolution.The synthesized images can be used to augment datasets to train\n",
      "CNNs or as massive distractor sets for biometric verification experiments\n",
      "without any privacy concerns. Additionally, law enforcement can make use of\n",
      "this technique to train forensic experts to recognize faces. Our method samples\n",
      "face components from a pool of multiple face images of real identities to\n",
      "generate the synthetic texture. Then, a real 3D head model compatible to the\n",
      "generated texture is used to render it under different facial yaw\n",
      "transformations. We perform multiple quantitative experiments to assess the\n",
      "effectiveness of our synthesis procedure in CNN training and its potential use\n",
      "to generate distractor face images. Additionally, we compare our method with\n",
      "popular GAN models in terms of visual quality and execution time. \n",
      "\n",
      "\n",
      "Despite remarkable advances in image synthesis research, existing works often\n",
      "fail in manipulating images under the context of large geometric\n",
      "transformations. Synthesizing person images conditioned on arbitrary poses is\n",
      "one of the most representative examples where the generation quality largely\n",
      "relies on the capability of identifying and modeling arbitrary transformations\n",
      "on different body parts. Current generative models are often built on local\n",
      "convolutions and overlook the key challenges (e.g. heavy occlusions, different\n",
      "views or dramatic appearance changes) when distinct geometric changes happen\n",
      "for each part, caused by arbitrary pose manipulations. This paper aims to\n",
      "resolve these challenges induced by geometric variability and spatial\n",
      "displacements via a new Soft-Gated Warping Generative Adversarial Network\n",
      "(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\n",
      "target part segmentation map given a target pose, which depicts the\n",
      "region-level spatial layouts for guiding image synthesis with higher-level\n",
      "structure constraints; 2) the Warping-GAN equipped with a soft-gated\n",
      "warping-block learns feature-level mapping to render textures from the original\n",
      "image into the generated segmentation map. Warping-GAN is capable of\n",
      "controlling different transformation degrees given distinct target poses.\n",
      "Moreover, the proposed warping-block is light-weight and flexible enough to be\n",
      "injected into any networks. Human perceptual studies and quantitative\n",
      "evaluations demonstrate the superiority of our Warping-GAN that significantly\n",
      "outperforms all existing methods on two large datasets. \n",
      "\n",
      "\n",
      "The identification of lesion within medical image data is necessary for\n",
      "diagnosis, treatment and prognosis. Segmentation and classification approaches\n",
      "are mainly based on supervised learning with well-paired image-level or\n",
      "voxel-level labels. However, labeling the lesion in medical images is laborious\n",
      "requiring highly specialized knowledge. We propose a medical image synthesis\n",
      "model named abnormal-to-normal translation generative adversarial network\n",
      "(ANT-GAN) to generate a normal-looking medical image based on its\n",
      "abnormal-looking counterpart without the need for paired training data. Unlike\n",
      "typical GANs, whose aim is to generate realistic samples with variations, our\n",
      "more restrictive model aims at producing a normal-looking image corresponding\n",
      "to one containing lesions, and thus requires a special design. Being able to\n",
      "provide a \"normal\" counterpart to a medical image can provide useful side\n",
      "information for medical imaging tasks like lesion segmentation or\n",
      "classification validated by our experiments. In the other aspect, the ANT-GAN\n",
      "model is also capable of producing highly realistic lesion-containing image\n",
      "corresponding to the healthy one, which shows the potential in data\n",
      "augmentation verified in our experiments. \n",
      "\n",
      "\n",
      "Despite recent progress in generative image modeling, successfully generating\n",
      "high-resolution, diverse samples from complex datasets such as ImageNet remains\n",
      "an elusive goal. To this end, we train Generative Adversarial Networks at the\n",
      "largest scale yet attempted, and study the instabilities specific to such\n",
      "scale. We find that applying orthogonal regularization to the generator renders\n",
      "it amenable to a simple \"truncation trick,\" allowing fine control over the\n",
      "trade-off between sample fidelity and variety by reducing the variance of the\n",
      "Generator's input. Our modifications lead to models which set the new state of\n",
      "the art in class-conditional image synthesis. When trained on ImageNet at\n",
      "128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of\n",
      "166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous\n",
      "best IS of 52.52 and FID of 18.6. \n",
      "\n",
      "\n",
      "Generative models are successfully used for image synthesis in the recent\n",
      "years. But when it comes to other modalities like audio, text etc little\n",
      "progress has been made. Recent works focus on generating audio from a\n",
      "generative model in an unsupervised setting. We explore the possibility of\n",
      "using generative models conditioned on class labels. Concatenation based\n",
      "conditioning and conditional scaling were explored in this work with various\n",
      "hyper-parameter tuning methods. In this paper we introduce Conditional WaveGANs\n",
      "(cWaveGAN). Find our implementation at https://github.com/acheketa/cwavegan \n",
      "\n",
      "\n",
      "We present a novel approach for synthesizing photo-realistic images of people\n",
      "in arbitrary poses using generative adversarial learning. Given an input image\n",
      "of a person and a desired pose represented by a 2D skeleton, our model renders\n",
      "the image of the same person under the new pose, synthesizing novel views of\n",
      "the parts visible in the input image and hallucinating those that are not seen.\n",
      "This problem has recently been addressed in a supervised manner, i.e., during\n",
      "training the ground truth images under the new poses are given to the network.\n",
      "We go beyond these approaches by proposing a fully unsupervised strategy. We\n",
      "tackle this challenging scenario by splitting the problem into two principal\n",
      "subtasks. First, we consider a pose conditioned bidirectional generator that\n",
      "maps back the initially rendered image to the original pose, hence being\n",
      "directly comparable to the input image without the need to resort to any\n",
      "training image. Second, we devise a novel loss function that incorporates\n",
      "content and style terms, and aims at producing images of high perceptual\n",
      "quality. Extensive experiments conducted on the DeepFashion dataset demonstrate\n",
      "that the images rendered by our model are very close in appearance to those\n",
      "obtained by fully supervised approaches. \n",
      "\n",
      "\n",
      "Generating an image from its description is a challenging task worth solving\n",
      "because of its numerous practical applications ranging from image editing to\n",
      "virtual reality. All existing methods use one single caption to generate a\n",
      "plausible image. A single caption by itself, can be limited, and may not be\n",
      "able to capture the variety of concepts and behavior that may be present in the\n",
      "image. We propose two deep generative models that generate an image by making\n",
      "use of multiple captions describing it. This is achieved by ensuring\n",
      "'Cross-Caption Cycle Consistency' between the multiple captions and the\n",
      "generated image(s). We report quantitative and qualitative results on the\n",
      "standard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate\n",
      "the efficacy of the proposed approach. \n",
      "\n",
      "\n",
      "On-board estimation of the pose of an uncooperative target spacecraft is an\n",
      "essential task for future on-orbit servicing and close-proximity formation\n",
      "flying missions. However, two issues hinder reliable on-board monocular vision\n",
      "based pose estimation: robustness to illumination conditions due to a lack of\n",
      "reliable visual features and scarcity of image datasets required for training\n",
      "and benchmarking. To address these two issues, this work details the design and\n",
      "validation of a monocular vision based pose determination architecture for\n",
      "spaceborne applications. The primary contribution to the state-of-the-art of\n",
      "this work is the introduction of a novel pose determination method based on\n",
      "Convolutional Neural Networks (CNN) to provide an initial guess of the pose in\n",
      "real-time on-board. The method involves discretizing the pose space and\n",
      "training the CNN with images corresponding to the resulting pose labels. Since\n",
      "reliable training of the CNN requires massive image datasets and computational\n",
      "resources, the parameters of the CNN must be determined prior to the mission\n",
      "with synthetic imagery. Moreover, reliable training of the CNN requires\n",
      "datasets that appropriately account for noise, color, and illumination\n",
      "characteristics expected in orbit. Therefore, the secondary contribution of\n",
      "this work is the introduction of an image synthesis pipeline, which is tailored\n",
      "to generate high fidelity images of any spacecraft 3D model. The proposed\n",
      "technique is scalable to spacecraft of different structural and physical\n",
      "properties as well as robust to the dynamic illumination conditions of space.\n",
      "Through metrics measuring classification and pose accuracy, it is shown that\n",
      "the presented architecture has desirable robustness and scalable properties. \n",
      "\n",
      "\n",
      "The task of generating natural images from 3D scenes has been a long standing\n",
      "goal in computer graphics. On the other hand, recent developments in deep\n",
      "neural networks allow for trainable models that can produce natural-looking\n",
      "images with little or no knowledge about the scene structure. While the\n",
      "generated images often consist of realistic looking local patterns, the overall\n",
      "structure of the generated images is often inconsistent. In this work we\n",
      "propose a trainable, geometry-aware image generation method that leverages\n",
      "various types of scene information, including geometry and segmentation, to\n",
      "create realistic looking natural images that match the desired scene structure.\n",
      "Our geometrically-consistent image synthesis method is a deep neural network,\n",
      "called Geometry to Image Synthesis (GIS) framework, which retains the\n",
      "advantages of a trainable method, e.g., differentiability and adaptiveness,\n",
      "but, at the same time, makes a step towards the generalizability, control and\n",
      "quality output of modern graphics rendering engines. We utilize the GIS\n",
      "framework to insert vehicles in outdoor driving scenes, as well as to generate\n",
      "novel views of objects from the Linemod dataset. We qualitatively show that our\n",
      "network is able to generalize beyond the training set to novel scene\n",
      "geometries, object shapes and segmentations. Furthermore, we quantitatively\n",
      "show that the GIS framework can be used to synthesize large amounts of training\n",
      "data which proves beneficial for training instance segmentation models. \n",
      "\n",
      "\n",
      "The cycleGAN is becoming an influential method in medical image synthesis.\n",
      "However, due to a lack of direct constraints between input and synthetic\n",
      "images, the cycleGAN cannot guarantee structural consistency between these two\n",
      "images, and such consistency is of extreme importance in medical imaging. To\n",
      "overcome this, we propose a structure-constrained cycleGAN for brain MR-to-CT\n",
      "synthesis using unpaired data that defines an extra structure-consistency loss\n",
      "based on the modality independent neighborhood descriptor to constrain\n",
      "structural consistency. Additionally, we use a position-based selection\n",
      "strategy for selecting training images instead of a completely random selection\n",
      "scheme. Experimental results on synthesizing CT images from brain MR images\n",
      "demonstrate that our method is better than the conventional cycleGAN and\n",
      "approximates the cycleGAN trained with paired data. \n",
      "\n",
      "\n",
      "We propose a method for generating video-realistic animations of real humans\n",
      "under user control. In contrast to conventional human character rendering, we\n",
      "do not require the availability of a production-quality photo-realistic 3D\n",
      "model of the human, but instead rely on a video sequence in conjunction with a\n",
      "(medium-quality) controllable 3D template model of the person. With that, our\n",
      "approach significantly reduces production cost compared to conventional\n",
      "rendering approaches based on production-quality 3D models, and can also be\n",
      "used to realistically edit existing videos. Technically, this is achieved by\n",
      "training a neural network that translates simple synthetic images of a human\n",
      "character into realistic imagery. For training our networks, we first track the\n",
      "3D motion of the person in the video using the template model, and subsequently\n",
      "generate a synthetically rendered version of the video. These images are then\n",
      "used to train a conditional generative adversarial network that translates\n",
      "synthetic images of the 3D model into realistic imagery of the human. We\n",
      "evaluate our method for the reenactment of another person that is tracked in\n",
      "order to obtain the motion data, and show video results generated from\n",
      "artist-designed skeleton motion. Our results outperform the state-of-the-art in\n",
      "learning-based human image synthesis. Project page:\n",
      "http://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/ \n",
      "\n",
      "\n",
      "As a classic statistical model of 3D facial shape and albedo, 3D Morphable\n",
      "Model (3DMM) is widely used in facial analysis, e.g., model fitting, image\n",
      "synthesis. Conventional 3DMM is learned from a set of 3D face scans with\n",
      "associated well-controlled 2D face images, and represented by two sets of PCA\n",
      "basis functions. Due to the type and amount of training data, as well as, the\n",
      "linear bases, the representation power of 3DMM can be limited. To address these\n",
      "problems, this paper proposes an innovative framework to learn a nonlinear 3DMM\n",
      "model from a large set of in-the-wild face images, without collecting 3D face\n",
      "scans. Specifically, given a face image as input, a network encoder estimates\n",
      "the projection, lighting, shape and albedo parameters. Two decoders serve as\n",
      "the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape\n",
      "and albedo, respectively. With the projection parameter, lighting, 3D shape,\n",
      "and albedo, a novel analytically-differentiable rendering layer is designed to\n",
      "reconstruct the original input face. The entire network is end-to-end trainable\n",
      "with only weak supervision. We demonstrate the superior representation power of\n",
      "our nonlinear 3DMM over its linear counterpart, and its contribution to face\n",
      "alignment, 3D reconstruction, and face editing. \n",
      "\n",
      "\n",
      "Text-to-image synthesis aims to automatically generate images according to\n",
      "text descriptions given by users, which is a highly challenging task. The main\n",
      "issues of text-to-image synthesis lie in two gaps: the heterogeneous and\n",
      "homogeneous gaps. The heterogeneous gap is between the high-level concepts of\n",
      "text descriptions and the pixel-level contents of images, while the homogeneous\n",
      "gap exists between synthetic image distributions and real image distributions.\n",
      "For addressing these problems, we exploit the excellent capability of generic\n",
      "discriminative models (e.g. VGG19), which can guide the training process of a\n",
      "new generative model on multiple levels to bridge the two gaps. The high-level\n",
      "representations can teach the generative model to extract necessary visual\n",
      "information from text descriptions, which can bridge the heterogeneous gap. The\n",
      "mid-level and low-level representations can lead it to learn structures and\n",
      "details of images respectively, which relieves the homogeneous gap. Therefore,\n",
      "we propose Symmetrical Distillation Networks (SDN) composed of a source\n",
      "discriminative model as \"teacher\" and a target generative model as \"student\".\n",
      "The target generative model has a symmetrical structure with the source\n",
      "discriminative model, in order to transfer hierarchical knowledge accessibly.\n",
      "Moreover, we decompose the training process into two stages with different\n",
      "distillation paradigms for promoting the performance of the target generative\n",
      "model. Experiments on two widely-used datasets are conducted to verify the\n",
      "effectiveness of our proposed SDN. \n",
      "\n",
      "\n",
      "We study the problem of video-to-video synthesis, whose goal is to learn a\n",
      "mapping function from an input source video (e.g., a sequence of semantic\n",
      "segmentation masks) to an output photorealistic video that precisely depicts\n",
      "the content of the source video. While its image counterpart, the\n",
      "image-to-image synthesis problem, is a popular topic, the video-to-video\n",
      "synthesis problem is less explored in the literature. Without understanding\n",
      "temporal dynamics, directly applying existing image synthesis approaches to an\n",
      "input video often results in temporally incoherent videos of low visual\n",
      "quality. In this paper, we propose a novel video-to-video synthesis approach\n",
      "under the generative adversarial learning framework. Through carefully-designed\n",
      "generator and discriminator architectures, coupled with a spatio-temporal\n",
      "adversarial objective, we achieve high-resolution, photorealistic, temporally\n",
      "coherent video results on a diverse set of input formats including segmentation\n",
      "masks, sketches, and poses. Experiments on multiple benchmarks show the\n",
      "advantage of our method compared to strong baselines. In particular, our model\n",
      "is capable of synthesizing 2K resolution videos of street scenes up to 30\n",
      "seconds long, which significantly advances the state-of-the-art of video\n",
      "synthesis. Finally, we apply our approach to future video prediction,\n",
      "outperforming several state-of-the-art competing systems. \n",
      "\n",
      "\n",
      "We address the problem of generating images across two drastically different\n",
      "views, namely ground (street) and aerial (overhead) views. Image synthesis by\n",
      "itself is a very challenging computer vision task and is even more so when\n",
      "generation is conditioned on an image in another view. Due the difference in\n",
      "viewpoints, there is small overlapping field of view and little common content\n",
      "between these two views. Here, we try to preserve the pixel information between\n",
      "the views so that the generated image is a realistic representation of cross\n",
      "view input image. For this, we propose to use homography as a guide to map the\n",
      "images between the views based on the common field of view to preserve the\n",
      "details in the input image. We then use generative adversarial networks to\n",
      "inpaint the missing regions in the transformed image and add realism to it. Our\n",
      "exhaustive evaluation and model comparison demonstrate that utilizing geometry\n",
      "constraints adds fine details to the generated images and can be a better\n",
      "approach for cross view image synthesis than purely pixel based synthesis\n",
      "methods. \n",
      "\n",
      "\n",
      "Humans can imagine a scene from a sound. We want machines to do so by using\n",
      "conditional generative adversarial networks (GANs). By applying the techniques\n",
      "including spectral norm, projection discriminator and auxiliary classifier,\n",
      "compared with naive conditional GAN, the model can generate images with better\n",
      "quality in terms of both subjective and objective evaluations. Almost\n",
      "three-fourth of people agree that our model have the ability to generate images\n",
      "related to sounds. By inputting different volumes of the same sound, our model\n",
      "output different scales of changes based on the volumes, showing that our model\n",
      "truly knows the relationship between sounds and images to some extent. \n",
      "\n",
      "\n",
      "Recently, the cycle-consistent generative adversarial networks (CycleGAN) has\n",
      "been widely used for synthesis of multi-domain medical images. The\n",
      "domain-specific nonlinear deformations captured by CycleGAN make the\n",
      "synthesized images difficult to be used for some applications, for example,\n",
      "generating pseudo-CT for PET-MR attenuation correction. This paper presents a\n",
      "deformation-invariant CycleGAN (DicycleGAN) method using deformable\n",
      "convolutional layers and new cycle-consistency losses. Its robustness dealing\n",
      "with data that suffer from domain-specific nonlinear deformations has been\n",
      "evaluated through comparison experiments performed on a multi-sequence brain MR\n",
      "dataset and a multi-modality abdominal dataset. Our method has displayed its\n",
      "ability to generate synthesized data that is aligned with the source while\n",
      "maintaining a proper quality of signal compared to CycleGAN-generated data. The\n",
      "proposed model also obtained comparable performance with CycleGAN when data\n",
      "from the source and target domains are alignable through simple affine\n",
      "transformations. \n",
      "\n",
      "\n",
      "Data diversity is critical to success when training deep learning models.\n",
      "Medical imaging data sets are often imbalanced as pathologic findings are\n",
      "generally rare, which introduces significant challenges when training deep\n",
      "learning models. In this work, we propose a method to generate synthetic\n",
      "abnormal MRI images with brain tumors by training a generative adversarial\n",
      "network using two publicly available data sets of brain MRI. We demonstrate two\n",
      "unique benefits that the synthetic images provide. First, we illustrate\n",
      "improved performance on tumor segmentation by leveraging the synthetic images\n",
      "as a form of data augmentation. Second, we demonstrate the value of generative\n",
      "models as an anonymization tool, achieving comparable tumor segmentation\n",
      "results when trained on the synthetic data versus when trained on real subject\n",
      "data. Together, these results offer a potential solution to two of the largest\n",
      "challenges facing machine learning in medical imaging, namely the small\n",
      "incidence of pathological findings, and the restrictions around sharing of\n",
      "patient data. \n",
      "\n",
      "\n",
      "Cameras are the most widely exploited sensor in both robotics and computer\n",
      "vision communities. Despite their popularity, two dominant attributes (i.e.,\n",
      "gain and exposure time) have been determined empirically and images are\n",
      "captured in very passive manner. In this paper, we present an active and\n",
      "generic camera attribute control scheme using Bayesian optimization. We extend\n",
      "from our previous work [1] in two aspects. First, we propose a method that\n",
      "jointly controls camera gain and exposure time. Secondly, to speed up the\n",
      "Bayesian optimization process, we introduce image synthesis using the camera\n",
      "response function (CRF). These synthesized images allowed us to diminish the\n",
      "image acquisition time during the Bayesian optimization phase, substantially\n",
      "improving overall control performance. The proposed method is validated both in\n",
      "an indoor and an outdoor environment where light condition rapidly changes.\n",
      "Supplementary material is available at https://youtu.be/XTYR_Mih3OU . \n",
      "\n",
      "\n",
      "Deformable Image Registration (DIR) of MR and CT images is one of the most\n",
      "challenging registration task, due to the inherent structural differences of\n",
      "the modalities and the missing dense ground truth. Recently cycle Generative\n",
      "Adversarial Networks (cycle-GANs) have been used to learn the intensity\n",
      "relationship between these 2 modalities for unpaired brain data. Yet its\n",
      "usefulness for DIR was not assessed.\n",
      "  In this study we evaluate the DIR performance for thoracic and abdominal\n",
      "organs after synthesis by cycle-GAN. We show that geometric changes, which\n",
      "differentiate the two populations (e.g. inhale vs. exhale), are readily\n",
      "synthesized as well. This causes substantial problems for any application which\n",
      "relies on spatial correspondences being preserved between the real and the\n",
      "synthesized image (e.g. plan, segmentation, landmark propagation). To alleviate\n",
      "this problem, we investigated reducing the spatial information provided to the\n",
      "discriminator by decreasing the size of its receptive fields.\n",
      "  Image synthesis was learned from 17 unpaired subjects per modality.\n",
      "Registration performance was evaluated with respect to manual segmentations of\n",
      "11 structures for 3 subjects from the VISERAL challenge. State-of-the-art DIR\n",
      "methods based on Normalized Mutual Information (NMI), Modality Independent\n",
      "Neighborhood Descriptor (MIND) and their novel combination achieved a mean\n",
      "segmentation overlap ratio of 76.7, 67.7, 76.9%, respectively. This dropped to\n",
      "69.1% or less when registering images synthesized by cycle-GAN based on local\n",
      "correlation, due to the poor performance on the thoracic region, where large\n",
      "lung volume changes were synthesized. Performance for the abdominal region was\n",
      "similar to that of CT-MRI NMI registration (77.4 vs. 78.8%) when using 3D\n",
      "synthesizing MRIs (12 slices) and medium sized receptive fields for the\n",
      "discriminator. \n",
      "\n",
      "\n",
      "We present a novel introspective variational autoencoder (IntroVAE) model for\n",
      "synthesizing high-resolution photographic images. IntroVAE is capable of\n",
      "self-evaluating the quality of its generated samples and improving itself\n",
      "accordingly. Its inference and generator models are jointly trained in an\n",
      "introspective way. On one hand, the generator is required to reconstruct the\n",
      "input images from the noisy outputs of the inference model as normal VAEs. On\n",
      "the other hand, the inference model is encouraged to classify between the\n",
      "generated and real samples while the generator tries to fool it as GANs. These\n",
      "two famous generative frameworks are integrated in a simple yet efficient\n",
      "single-stream architecture that can be trained in a single stage. IntroVAE\n",
      "preserves the advantages of VAEs, such as stable training and nice latent\n",
      "manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\n",
      "no extra discriminators, because the inference model itself serves as a\n",
      "discriminator to distinguish between the generated and real samples.\n",
      "Experiments demonstrate that our method produces high-resolution\n",
      "photo-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\n",
      "comparable to or better than the state-of-the-art GANs. \n",
      "\n",
      "\n",
      "A capsule is a group of neurons whose activity vector models different\n",
      "properties of the same entity. This paper extends the capsule to a generative\n",
      "version, named variational capsules (VCs). Each VC produces a latent variable\n",
      "for a specific entity, making it possible to integrate image analysis and image\n",
      "synthesis into a unified framework. Variational capsules model an image as a\n",
      "composition of entities in a probabilistic model. Different capsules'\n",
      "divergence with a specific prior distribution represents the presence of\n",
      "different entities, which can be applied in image analysis tasks such as\n",
      "classification. In addition, variational capsules encode multiple entities in a\n",
      "semantically-disentangling way. Diverse instantiations of capsules are related\n",
      "to various properties of the same entity, making it easy to generate diverse\n",
      "samples with fine-grained semantic attributes. Extensive experiments\n",
      "demonstrate that deep networks designed with variational capsules can not only\n",
      "achieve promising performance on image analysis tasks (including image\n",
      "classification and attribute prediction) but can also improve the diversity and\n",
      "controllability of image synthesis. \n",
      "\n",
      "\n",
      "The ability to generate synthetic medical images is useful for data\n",
      "augmentation, domain transfer, and out-of-distribution detection. However,\n",
      "generating realistic, high-resolution medical images is challenging,\n",
      "particularly for Full Field Digital Mammograms (FFDM), due to the textural\n",
      "heterogeneity, fine structural details and specific tissue properties. In this\n",
      "paper, we explore the use of progressively trained generative adversarial\n",
      "networks (GANs) to synthesize mammograms, overcoming the underlying\n",
      "instabilities when training such adversarial models. This work is the first to\n",
      "show that generation of realistic synthetic medical images is feasible at up to\n",
      "1280x1024 pixels, the highest resolution achieved for medical image synthesis,\n",
      "enabling visualizations within standard mammographic hanging protocols. We hope\n",
      "this work can serve as a useful guide and facilitate further research on GANs\n",
      "in the medical imaging domain. \n",
      "\n",
      "\n",
      "We introduce a novel generative autoencoder network model that learns to\n",
      "encode and reconstruct images with high quality and resolution, and supports\n",
      "smooth random sampling from the latent space of the encoder. Generative\n",
      "adversarial networks (GANs) are known for their ability to simulate random\n",
      "high-quality images, but they cannot reconstruct existing images. Previous\n",
      "works have attempted to extend GANs to support such inference but, so far, have\n",
      "not delivered satisfactory high-quality results. Instead, we propose the\n",
      "Progressively Growing Generative Autoencoder (PIONEER) network which achieves\n",
      "high-quality reconstruction with $128{\\times}128$ images without requiring a\n",
      "GAN discriminator. We merge recent techniques for progressively building up the\n",
      "parts of the network with the recently introduced adversarial encoder-generator\n",
      "network. The ability to reconstruct input images is crucial in many real-world\n",
      "applications, and allows for precise intelligent manipulation of existing\n",
      "images. We show promising results in image synthesis and inference, with\n",
      "state-of-the-art results in CelebA inference tasks. \n",
      "\n",
      "\n",
      "The requirement of large amounts of annotated images has become one grand\n",
      "challenge while training deep neural network models for various visual\n",
      "detection and recognition tasks. This paper presents a novel image synthesis\n",
      "technique that aims to generate a large amount of annotated scene text images\n",
      "for training accurate and robust scene text detection and recognition models.\n",
      "The proposed technique consists of three innovative designs. First, it realizes\n",
      "\"semantic coherent\" synthesis by embedding texts at semantically sensible\n",
      "regions within the background image, where the semantic coherence is achieved\n",
      "by leveraging the semantic annotations of objects and image regions that have\n",
      "been created in the prior semantic segmentation research. Second, it exploits\n",
      "visual saliency to determine the embedding locations within each semantic\n",
      "sensible region, which coincides with the fact that texts are often placed\n",
      "around homogeneous regions for better visibility in scenes. Third, it designs\n",
      "an adaptive text appearance model that determines the color and brightness of\n",
      "embedded texts by learning from the feature of real scene text images\n",
      "adaptively. The proposed technique has been evaluated over five public datasets\n",
      "and the experiments show its superior performance in training accurate and\n",
      "robust scene text detection and recognition models. \n",
      "\n",
      "\n",
      "Cross modal image syntheses is gaining significant interests for its ability\n",
      "to estimate target images of a different modality from a given set of source\n",
      "images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for\n",
      "an actual acquisition.Though they show potential for applications in radiation\n",
      "therapy planning,image super resolution, atlas construction, image segmentation\n",
      "etc.The synthesis results are not as accurate as the actual acquisition.In this\n",
      "paper,we address the problem of multi modal image synthesis by proposing a\n",
      "fully convolutional deep learning architecture called the SynNet.We extend the\n",
      "proposed architecture for various input output configurations. And finally, we\n",
      "propose a structure preserving custom loss function for cross-modal image\n",
      "synthesis.We validate the proposed SynNet and its extended framework on BRATS\n",
      "dataset with comparisons against three state-of-the art methods.And the results\n",
      "of the proposed custom loss function is validated against the traditional loss\n",
      "function used by the state-of-the-art methods for cross modal image synthesis. \n",
      "\n",
      "\n",
      "Sample patterns have many uses in Computer Graphics, ranging from procedural\n",
      "object placement over Monte Carlo image synthesis to non-photorealistic\n",
      "depiction. Their properties such as discrepancy, spectra, anisotropy, or\n",
      "progressiveness have been analyzed extensively. However, designing methods to\n",
      "produce sampling patterns with certain properties can require substantial\n",
      "hand-crafting effort, both in coding, mathematical derivation and compute time.\n",
      "In particular, there is no systematic way to derive the best sampling algorithm\n",
      "for a specific end-task.\n",
      "  Tackling this issue, we suggest another level of abstraction: a toolkit to\n",
      "end-to-end optimize over all sampling methods to find the one producing\n",
      "user-prescribed properties such as discrepancy or a spectrum that best fit the\n",
      "end-task. A user simply implements the forward losses and the sampling method\n",
      "is found automatically -- without coding or mathematical derivation -- by\n",
      "making use of back-propagation abilities of modern deep learning frameworks.\n",
      "While this optimization takes long, at deployment time the sampling method is\n",
      "quick to execute as iterated unstructured non-linear filtering using radial\n",
      "basis functions (RBFs) to represent high-dimensional kernels. Several important\n",
      "previous methods are special cases of this approach, which we compare to\n",
      "previous work and demonstrate its usefulness in several typical Computer\n",
      "Graphics applications. Finally, we propose sampling patterns with properties\n",
      "not shown before, such as high-dimensional blue noise with projective\n",
      "properties. \n",
      "\n",
      "\n",
      "We suggest a rasterization pipeline tailored towards the need of head-mounted\n",
      "displays (HMD), where latency and field-of-view requirements pose new\n",
      "challenges beyond those of traditional desktop displays. Instead of rendering\n",
      "and warping for low latency, or using multiple passes for foveation, we show\n",
      "how both can be produced directly in a single perceptual rasterization pass. We\n",
      "do this with per-fragment ray-casting. This is enabled by derivations of tight\n",
      "space-time-fovea pixel bounds, introducing just enough flexibility for\n",
      "requisite geometric tests, but retaining most of the the simplicity and\n",
      "efficiency of the traditional rasterizaton pipeline. To produce foveated\n",
      "images, we rasterize to an image with spatially varying pixel density. To\n",
      "reduce latency, we extend the image formation model to directly produce\n",
      "\"rolling\" images where the time at each pixel depends on its display location.\n",
      "Our approach overcomes limitations of warping with respect to disocclusions,\n",
      "object motion and view-dependent shading, as well as geometric aliasing\n",
      "artifacts in other foveated rendering techniques. A set of perceptual user\n",
      "studies demonstrates the efficacy of our approach. \n",
      "\n",
      "\n",
      "In this paper, we propose Generative Adversarial Network (GAN) architectures\n",
      "that use Capsule Networks for image-synthesis. Based on the principal of\n",
      "positional-equivariance of features, Capsule Network's ability to encode\n",
      "spatial relationships between the features of the image helps it become a more\n",
      "powerful critic in comparison to Convolutional Neural Networks (CNNs) used in\n",
      "current architectures for image synthesis. Our proposed GAN architectures learn\n",
      "the data manifold much faster and therefore, synthesize visually accurate\n",
      "images in significantly lesser number of training samples and training epochs\n",
      "in comparison to GANs and its variants that use CNNs. Apart from analyzing the\n",
      "quantitative results corresponding the images generated by different\n",
      "architectures, we also explore the reasons for the lower coverage and diversity\n",
      "explored by the GAN architectures that use CNN critics. \n",
      "\n",
      "\n",
      "This paper proposes the decision tree latent controller generative\n",
      "adversarial network (DTLC-GAN), an extension of a GAN that can learn\n",
      "hierarchically interpretable representations without relying on detailed\n",
      "supervision. To impose a hierarchical inclusion structure on latent variables,\n",
      "we incorporate a new architecture called the DTLC into the generator input. The\n",
      "DTLC has a multiple-layer tree structure in which the ON or OFF of the child\n",
      "node codes is controlled by the parent node codes. By using this architecture\n",
      "hierarchically, we can obtain the latent space in which the lower layer codes\n",
      "are selectively used depending on the higher layer ones. To make the latent\n",
      "codes capture salient semantic features of images in a hierarchically\n",
      "disentangled manner in the DTLC, we also propose a hierarchical conditional\n",
      "mutual information regularization and optimize it with a newly defined\n",
      "curriculum learning method that we propose as well. This makes it possible to\n",
      "discover hierarchically interpretable representations in a layer-by-layer\n",
      "manner on the basis of information gain by only using a single DTLC-GAN model.\n",
      "We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny\n",
      "ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn\n",
      "hierarchically interpretable representations with either unsupervised or weakly\n",
      "supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval\n",
      "tasks and showed its effectiveness in representation learning. \n",
      "\n",
      "\n",
      "This paper discusses how distribution matching losses, such as those used in\n",
      "CycleGAN, when used to synthesize medical images can lead to mis-diagnosis of\n",
      "medical conditions. It seems appealing to use these new image synthesis methods\n",
      "for translating images from a source to a target domain because they can\n",
      "produce high quality images and some even do not require paired data. However,\n",
      "the basis of how these image translation models work is through matching the\n",
      "translation output to the distribution of the target domain. This can cause an\n",
      "issue when the data provided in the target domain has an over or under\n",
      "representation of some classes (e.g. healthy or sick). When the output of an\n",
      "algorithm is a transformed image there are uncertainties whether all known and\n",
      "unknown class labels have been preserved or changed. Therefore, we recommend\n",
      "that these translated images should not be used for direct interpretation (e.g.\n",
      "by doctors) because they may lead to misdiagnosis of patients based on\n",
      "hallucinated image features by an algorithm that matches a distribution.\n",
      "However there are many recent papers that seem as though this is the goal. \n",
      "\n",
      "\n",
      "Zero-shot artistic style transfer is an important image synthesis problem\n",
      "aiming at transferring arbitrary style into content images. However, the\n",
      "trade-off between the generalization and efficiency in existing methods impedes\n",
      "a high quality zero-shot style transfer in real-time. In this paper, we resolve\n",
      "this dilemma and propose an efficient yet effective Avatar-Net that enables\n",
      "visually plausible multi-scale transfer for arbitrary style. The key ingredient\n",
      "of our method is a style decorator that makes up the content features by\n",
      "semantically aligned style features from an arbitrary style image, which does\n",
      "not only holistically match their feature distributions but also preserve\n",
      "detailed style patterns in the decorated features. By embedding this module\n",
      "into an image reconstruction network that fuses multi-scale style abstractions,\n",
      "the Avatar-Net renders multi-scale stylization for any style image in one\n",
      "feed-forward pass. We demonstrate the state-of-the-art effectiveness and\n",
      "efficiency of the proposed method in generating high-quality stylized images,\n",
      "with a series of applications include multiple style integration, video\n",
      "stylization and etc. \n",
      "\n",
      "\n",
      "In this paper, we focus on image inpainting task, aiming at recovering the\n",
      "missing area of an incomplete image given the context information. Recent\n",
      "development in deep generative models enables an efficient end-to-end framework\n",
      "for image synthesis and inpainting tasks, but existing methods based on\n",
      "generative models don't exploit the segmentation information to constrain the\n",
      "object shapes, which usually lead to blurry results on the boundary. To tackle\n",
      "this problem, we propose to introduce the semantic segmentation information,\n",
      "which disentangles the inter-class difference and intra-class variation for\n",
      "image inpainting. This leads to much clearer recovered boundary between\n",
      "semantically different regions and better texture within semantically\n",
      "consistent segments. Our model factorizes the image inpainting process into\n",
      "segmentation prediction (SP-Net) and segmentation guidance (SG-Net) as two\n",
      "steps, which predict the segmentation labels in the missing area first, and\n",
      "then generate segmentation guided inpainting results. Experiments on multiple\n",
      "public datasets show that our approach outperforms existing methods in\n",
      "optimizing the image inpainting quality, and the interactive segmentation\n",
      "guidance provides possibilities for multi-modal predictions of image\n",
      "inpainting. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) are a class of unsupervised machine\n",
      "learning algorithms that can produce realistic images from randomly-sampled\n",
      "vectors in a multi-dimensional space. Until recently, it was not possible to\n",
      "generate realistic high-resolution images using GANs, which has limited their\n",
      "applicability to medical images that contain biomarkers only detectable at\n",
      "native resolution. Progressive growing of GANs is an approach wherein an image\n",
      "generator is trained to initially synthesize low resolution synthetic images\n",
      "(8x8 pixels), which are then fed to a discriminator that distinguishes these\n",
      "synthetic images from real downsampled images. Additional convolutional layers\n",
      "are then iteratively introduced to produce images at twice the previous\n",
      "resolution until the desired resolution is reached. In this work, we\n",
      "demonstrate that this approach can produce realistic medical images in two\n",
      "different domains; fundus photographs exhibiting vascular pathology associated\n",
      "with retinopathy of prematurity (ROP), and multi-modal magnetic resonance\n",
      "images of glioma. We also show that fine-grained details associated with\n",
      "pathology, such as retinal vessels or tumor heterogeneity, can be preserved and\n",
      "enhanced by including segmentation maps as additional channels. We envisage\n",
      "several applications of the approach, including image augmentation and\n",
      "unsupervised classification of pathology. \n",
      "\n",
      "\n",
      "Building on top of the success of generative adversarial networks (GANs),\n",
      "conditional GANs attempt to better direct the data generation process by\n",
      "conditioning with certain additional information. Inspired by the most recent\n",
      "AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In\n",
      "addition to the real/fake classifier used in vanilla GANs, our discriminator\n",
      "has an advanced auxiliary classifier which distinguishes each real class from\n",
      "an extra `fake' class. The `fake' class avoids mixing generated data with real\n",
      "data, which can potentially confuse the classification of real data as AC-GAN\n",
      "does, and makes the advanced auxiliary classifier behave as another real/fake\n",
      "classifier. As a result, FC-GAN can accelerate the process of differentiation\n",
      "of all classes, thus boost the convergence speed. Experimental results on image\n",
      "synthesis demonstrate our model is competitive in the quality of images\n",
      "generated while achieving a faster convergence rate. \n",
      "\n",
      "\n",
      "In this paper, we introduce a new method for generating an object image from\n",
      "text attributes on a desired location, when the base image is given. One step\n",
      "further to the existing studies on text-to-image generation mainly focusing on\n",
      "the object's appearance, the proposed method aims to generate an object image\n",
      "preserving the given background information, which is the first attempt in this\n",
      "field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which\n",
      "controls both the object and background information jointly. As a core\n",
      "component of MC-GAN, we propose a synthesis block which disentangles the object\n",
      "and background information in the training stage. This block enables MC-GAN to\n",
      "generate a realistic object image with the desired background by controlling\n",
      "the amount of the background information from the given base image using the\n",
      "foreground information from the text attributes. From the experiments with\n",
      "Caltech-200 bird and Oxford-102 flower datasets, we show that our model is able\n",
      "to generate photo-realistic images with a resolution of 128 x 128. The source\n",
      "code of MC-GAN is released. \n",
      "\n",
      "\n",
      "Generating images from natural language is one of the primary applications of\n",
      "recent conditional generative models. Besides testing our ability to model\n",
      "conditional, highly dimensional distributions, text to image synthesis has many\n",
      "exciting and practical applications such as photo editing or computer-aided\n",
      "content creation. Recent progress has been made using Generative Adversarial\n",
      "Networks (GANs). This material starts with a gentle introduction to these\n",
      "topics and discusses the existent state of the art models. Moreover, I propose\n",
      "Wasserstein GAN-CLS, a new model for conditional image generation based on the\n",
      "Wasserstein distance which offers guarantees of stability. Then, I show how the\n",
      "novel loss function of Wasserstein GAN-CLS can be used in a Conditional\n",
      "Progressive Growing GAN. In combination with the proposed loss, the model\n",
      "boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the\n",
      "models which use only the sentence-level visual semantics. The only model which\n",
      "performs better than the Conditional Wasserstein Progressive Growing GAN is the\n",
      "recently proposed AttnGAN which uses word-level visual semantics as well. \n",
      "\n",
      "\n",
      "We present a semi-parametric approach to photographic image synthesis from\n",
      "semantic layouts. The approach combines the complementary strengths of\n",
      "parametric and nonparametric techniques. The nonparametric component is a\n",
      "memory bank of image segments constructed from a training set of images. Given\n",
      "a novel semantic layout at test time, the memory bank is used to retrieve\n",
      "photographic references that are provided as source material to a deep network.\n",
      "The synthesis is performed by a deep network that draws on the provided\n",
      "photographic material. Experiments on multiple semantic segmentation datasets\n",
      "show that the presented approach yields considerably more realistic images than\n",
      "recent purely parametric techniques. The results are shown in the supplementary\n",
      "video at https://youtu.be/U4Q98lenGLQ \n",
      "\n",
      "\n",
      "We address the computational problem of novel human pose synthesis. Given an\n",
      "image of a person and a desired pose, we produce a depiction of that person in\n",
      "that pose, retaining the appearance of both the person and background. We\n",
      "present a modular generative neural network that synthesizes unseen poses using\n",
      "training pairs of images and poses taken from human action videos. Our network\n",
      "separates a scene into different body part and background layers, moves body\n",
      "parts to new locations and refines their appearances, and composites the new\n",
      "foreground with a hole-filled background. These subtasks, implemented with\n",
      "separate modules, are trained jointly using only a single target image as a\n",
      "supervised label. We use an adversarial discriminator to force our network to\n",
      "synthesize realistic details conditioned on pose. We demonstrate image\n",
      "synthesis results on three action classes: golf, yoga/workouts and tennis, and\n",
      "show that our method produces accurate results within action classes as well as\n",
      "across action classes. Given a sequence of desired poses, we also produce\n",
      "coherent videos of actions. \n",
      "\n",
      "\n",
      "As more and more personal photos are shared and tagged in social media,\n",
      "avoiding privacy risks such as unintended recognition becomes increasingly\n",
      "challenging. We propose a new hybrid approach to obfuscate identities in photos\n",
      "by head replacement. Our approach combines state of the art parametric face\n",
      "synthesis with latest advances in Generative Adversarial Networks (GAN) for\n",
      "data-driven image synthesis. On the one hand, the parametric part of our method\n",
      "gives us control over the facial parameters and allows for explicit\n",
      "manipulation of the identity. On the other hand, the data-driven aspects allow\n",
      "for adding fine details and overall realism as well as seamless blending into\n",
      "the scene context. In our experiments, we show highly realistic output of our\n",
      "system that improves over the previous state of the art in obfuscation rate\n",
      "while preserving a higher similarity to the original image content. \n",
      "\n",
      "\n",
      "Deep generative models have demonstrated great performance in image\n",
      "synthesis. However, results deteriorate in case of spatial deformations, since\n",
      "they generate images of objects directly, rather than modeling the intricate\n",
      "interplay of their inherent shape and appearance. We present a conditional\n",
      "U-Net for shape-guided image generation, conditioned on the output of a\n",
      "variational autoencoder for appearance. The approach is trained end-to-end on\n",
      "images, without requiring samples of the same object with varying pose or\n",
      "appearance. Experiments show that the model enables conditional image\n",
      "generation and transfer. Therefore, either shape or appearance can be retained\n",
      "from a query image, while freely altering the other. Moreover, appearance can\n",
      "be sampled due to its stochastic latent representation, while preserving shape.\n",
      "In quantitative and qualitative experiments on COCO, DeepFashion, shoes,\n",
      "Market-1501 and handbags, the approach demonstrates significant improvements\n",
      "over the state-of-the-art. \n",
      "\n",
      "\n",
      "Magnetic Resonance Angiography (MRA) has become an essential MR contrast for\n",
      "imaging and evaluation of vascular anatomy and related diseases. MRA\n",
      "acquisitions are typically ordered for vascular interventions, whereas in\n",
      "typical scenarios, MRA sequences can be absent in the patient scans. This\n",
      "motivates the need for a technique that generates inexistent MRA from existing\n",
      "MR multi-contrast, which could be a valuable tool in retrospective subject\n",
      "evaluations and imaging studies. In this paper, we present a generative\n",
      "adversarial network (GAN) based technique to generate MRA from T1-weighted and\n",
      "T2-weighted MRI images, for the first time to our knowledge. To better model\n",
      "the representation of vessels which the MRA inherently highlights, we design a\n",
      "loss term dedicated to a faithful reproduction of vascularities. To that end,\n",
      "we incorporate steerable filter responses of the generated and reference images\n",
      "inside a Huber function loss term. Extending the well- established\n",
      "generator-discriminator architecture based on the recent PatchGAN model with\n",
      "the addition of steerable filter loss, the proposed steerable GAN (sGAN) method\n",
      "is evaluated on the large public database IXI. Experimental results show that\n",
      "the sGAN outperforms the baseline GAN method in terms of an overlap score with\n",
      "similar PSNR values, while it leads to improved visual perceptual quality. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have been successfully used to\n",
      "synthesize realistically looking images of faces, scenery and even medical\n",
      "images. Unfortunately, they usually require large training datasets, which are\n",
      "often scarce in the medical field, and to the best of our knowledge GANs have\n",
      "been only applied for medical image synthesis at fairly low resolution.\n",
      "However, many state-of-the-art machine learning models operate on high\n",
      "resolution data as such data carries indispensable, valuable information. In\n",
      "this work, we try to generate realistically looking high resolution images of\n",
      "skin lesions with GANs, using only a small training dataset of 2000 samples.\n",
      "The nature of the data allows us to do a direct comparison between the image\n",
      "statistics of the generated samples and the real dataset. We both\n",
      "quantitatively and qualitatively compare state-of-the-art GAN architectures\n",
      "such as DCGAN and LAPGAN against a modification of the latter for the task of\n",
      "image generation at a resolution of 256x256px. Our investigation shows that we\n",
      "can approximate the real data distribution with all of the models, but we\n",
      "notice major differences when visually rating sample realism, diversity and\n",
      "artifacts. In a set of use-case experiments on skin lesion classification, we\n",
      "further show that we can successfully tackle the problem of heavy class\n",
      "imbalance with the help of synthesized high resolution melanoma samples. \n",
      "\n",
      "\n",
      "As a classic statistical model of 3D facial shape and texture, 3D Morphable\n",
      "Model (3DMM) is widely used in facial analysis, e.g., model fitting, image\n",
      "synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face\n",
      "images with associated 3D face scans, and represented by two sets of PCA basis\n",
      "functions. Due to the type and amount of training data, as well as the linear\n",
      "bases, the representation power of 3DMM can be limited. To address these\n",
      "problems, this paper proposes an innovative framework to learn a nonlinear 3DMM\n",
      "model from a large set of unconstrained face images, without collecting 3D face\n",
      "scans. Specifically, given a face image as input, a network encoder estimates\n",
      "the projection, shape and texture parameters. Two decoders serve as the\n",
      "nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and\n",
      "texture, respectively. With the projection parameter, 3D shape, and texture, a\n",
      "novel analytically-differentiable rendering layer is designed to reconstruct\n",
      "the original input face. The entire network is end-to-end trainable with only\n",
      "weak supervision. We demonstrate the superior representation power of our\n",
      "nonlinear 3DMM over its linear counterpart, and its contribution to face\n",
      "alignment and 3D reconstruction. \n",
      "\n",
      "\n",
      "We present a novel automated method to segment the myocardium of both left\n",
      "and right ventricles in MRI volumes. The segmentation is consistent in 3D\n",
      "across the slices such that it can be directly used for mesh generation. Two\n",
      "specific neural networks with multi-scale coarse-to-fine prediction structure\n",
      "are proposed to cope with the small training dataset and trained using an\n",
      "original loss function. The former segments a slice in the middle of the\n",
      "volume. Then the latter iteratively propagates the slice segmentations towards\n",
      "the base and the apex, in a spatially consistent way. We perform 5-fold\n",
      "cross-validation on the 15 cases from STACOM to validate the method. For\n",
      "training, we use real cases and their synthetic variants generated by combining\n",
      "motion simulation and image synthesis. Accurate and consistent testing results\n",
      "are obtained. \n",
      "\n",
      "\n",
      "We introduce BSD-GAN, a novel multi-branch and scale-disentangled training\n",
      "method which enables unconditional Generative Adversarial Networks (GANs) to\n",
      "learn image representations at multiple scales, benefiting a wide range of\n",
      "generation and editing tasks. The key feature of BSD-GAN is that it is trained\n",
      "in multiple branches, progressively covering both the breadth and depth of the\n",
      "network, as resolutions of the training images increase to reveal finer-scale\n",
      "features. Specifically, each noise vector, as input to the generator network of\n",
      "BSD-GAN, is deliberately split into several sub-vectors, each corresponding to,\n",
      "and is trained to learn, image representations at a particular scale. During\n",
      "training, we progressively \"de-freeze\" the sub-vectors, one at a time, as a new\n",
      "set of higher-resolution images is employed for training and more network\n",
      "layers are added. A consequence of such an explicit sub-vector designation is\n",
      "that we can directly manipulate and even combine latent (sub-vector) codes\n",
      "which model different feature scales.Extensive experiments demonstrate the\n",
      "effectiveness of our training method in scale-disentangled learning of image\n",
      "representations and synthesis of novel image contents, without any extra labels\n",
      "and without compromising quality of the synthesized high-resolution images. We\n",
      "further demonstrate several image generation and manipulation applications\n",
      "enabled or improved by BSD-GAN. Source codes are available at\n",
      "https://github.com/duxingren14/BSD-GAN. \n",
      "\n",
      "\n",
      "Reliable facial expression recognition plays a critical role in human-machine\n",
      "interactions. However, most of the facial expression analysis methodologies\n",
      "proposed to date pay little or no attention to the protection of a user's\n",
      "privacy. In this paper, we propose a Privacy-Preserving Representation-Learning\n",
      "Variational Generative Adversarial Network (PPRL-VGAN) to learn an image\n",
      "representation that is explicitly disentangled from the identity information.\n",
      "At the same time, this representation is discriminative from the standpoint of\n",
      "facial expression recognition and generative as it allows expression-equivalent\n",
      "face image synthesis. We evaluate the proposed model on two public datasets\n",
      "under various threat scenarios. Quantitative and qualitative results\n",
      "demonstrate that our approach strikes a balance between the preservation of\n",
      "privacy and data utility. We further demonstrate that our model can be\n",
      "effectively applied to other tasks such as expression morphing and image\n",
      "completion. \n",
      "\n",
      "\n",
      "Drawing a beautiful painting is a dream of many people since childhood. In\n",
      "this paper, we propose a novel scheme, Line Artist, to synthesize artistic\n",
      "style paintings with freehand sketch images, leveraging the power of deep\n",
      "learning and advanced algorithms. Our scheme includes three models. The Sketch\n",
      "Image Extraction (SIE) model is applied to generate the training data. It\n",
      "includes smoothing reality images and pencil sketch extraction. The Detailed\n",
      "Image Synthesis (DIS) model trains a conditional generative adversarial network\n",
      "to generate detailed real-world information. The Adaptively Weighted Artistic\n",
      "Style Transfer (AWAST) model is capable to combine multiple style images with a\n",
      "content with the VGG19 network and PageRank algorithm. The appealing artistic\n",
      "images are then generated by optimization iterations. Experiments are operated\n",
      "on the Kaggle Cats dataset and The Oxford Buildings Dataset. Our synthesis\n",
      "results are proved to be artistic, beautiful and robust. \n",
      "\n",
      "\n",
      "CT is commonly used in orthopedic procedures. MRI is used along with CT to\n",
      "identify muscle structures and diagnose osteonecrosis due to its superior soft\n",
      "tissue contrast. However, MRI has poor contrast for bone structures. Clearly,\n",
      "it would be helpful if a corresponding CT were available, as bone boundaries\n",
      "are more clearly seen and CT has standardized (i.e., Hounsfield) units.\n",
      "Therefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied\n",
      "to unpaired CT and MR images of the head, these images do not have as much\n",
      "variation of intensity pairs as do images in the pelvic region due to the\n",
      "presence of joints and muscles. In this paper, we extended the CycleGAN\n",
      "approach by adding the gradient consistency loss to improve the accuracy at the\n",
      "boundaries. We conducted two experiments. To evaluate image synthesis, we\n",
      "investigated dependency of image synthesis accuracy on 1) the number of\n",
      "training data and 2) the gradient consistency loss. To demonstrate the\n",
      "applicability of our method, we also investigated a segmentation accuracy on\n",
      "synthesized images. \n",
      "\n",
      "\n",
      "There has been a drastic growth of research in Generative Adversarial Nets\n",
      "(GANs) in the past few years. Proposed in 2014, GAN has been applied to various\n",
      "applications such as computer vision and natural language processing, and\n",
      "achieves impressive performance. Among the many applications of GAN, image\n",
      "synthesis is the most well-studied one, and research in this area has already\n",
      "demonstrated the great potential of using GAN in image synthesis. In this\n",
      "paper, we provide a taxonomy of methods used in image synthesis, review\n",
      "different models for text-to-image synthesis and image-to-image translation,\n",
      "and discuss some evaluation metrics as well as possible future research\n",
      "directions in image synthesis with GAN. \n",
      "\n",
      "\n",
      "Learning to generate natural scenes has always been a challenging task in\n",
      "computer vision. It is even more painstaking when the generation is conditioned\n",
      "on images with drastically different views. This is mainly because\n",
      "understanding, corresponding, and transforming appearance and semantic\n",
      "information across the views is not trivial. In this paper, we attempt to solve\n",
      "the novel problem of cross-view image synthesis, aerial to street-view and vice\n",
      "versa, using conditional generative adversarial networks (cGAN). Two new\n",
      "architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq)\n",
      "are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels.\n",
      "X-Fork architecture has a single discriminator and a single generator. The\n",
      "generator hallucinates both the image and its semantic segmentation in the\n",
      "target view. X-Seq architecture utilizes two cGANs. The first one generates the\n",
      "target image which is subsequently fed to the second cGAN for generating its\n",
      "corresponding semantic segmentation map. The feedback from the second cGAN\n",
      "helps the first cGAN generate sharper images. Both of our proposed\n",
      "architectures learn to generate natural images as well as their semantic\n",
      "segmentation maps. The proposed methods show that they are able to capture and\n",
      "maintain the true semantics of objects in source and target views better than\n",
      "the traditional image-to-image translation method which considers only the\n",
      "visual appearance of the scene. Extensive qualitative and quantitative\n",
      "evaluations support the effectiveness of our frameworks, compared to two state\n",
      "of the art methods, for natural scene generation across drastically different\n",
      "views. \n",
      "\n",
      "\n",
      "Recent developments in deep domain adaptation have allowed knowledge transfer\n",
      "from a labeled source domain to an unlabeled target domain at the level of\n",
      "intermediate features or input pixels. We propose that advantages may be\n",
      "derived by combining them, in the form of different insights that lead to a\n",
      "novel design and complementary properties that result in better performance. At\n",
      "the feature level, inspired by insights from semi-supervised learning, we\n",
      "propose a classification-aware domain adversarial neural network that brings\n",
      "target examples into more classifiable regions of source domain. Next, we posit\n",
      "that computer vision insights are more amenable to injection at the pixel\n",
      "level. In particular, we use 3D geometry and image synthesis based on a\n",
      "generalized appearance flow to preserve identity across pose transformations,\n",
      "while using an attribute-conditioned CycleGAN to translate a single source into\n",
      "multiple target images that differ in lower-level properties such as lighting.\n",
      "Besides standard UDA benchmark, we validate on a novel and apt problem of car\n",
      "recognition in unlabeled surveillance images using labeled images from the web,\n",
      "handling explicitly specified, nameable factors of variation through\n",
      "pixel-level and implicit, unspecified factors through feature-level adaptation. \n",
      "\n",
      "\n",
      "Synthesized medical images have several important applications, e.g., as an\n",
      "intermedium in cross-modality image registration and as supplementary training\n",
      "samples to boost the generalization capability of a classifier. Especially,\n",
      "synthesized computed tomography (CT) data can provide X-ray attenuation map for\n",
      "radiation therapy planning. In this work, we propose a generic cross-modality\n",
      "synthesis approach with the following targets: 1) synthesizing realistic\n",
      "looking 3D images using unpaired training data, 2) ensuring consistent\n",
      "anatomical structures, which could be changed by geometric distortion in\n",
      "cross-modality synthesis and 3) improving volume segmentation by using\n",
      "synthetic data for modalities with limited training samples. We show that these\n",
      "goals can be achieved with an end-to-end 3D convolutional neural network (CNN)\n",
      "composed of mutually-beneficial generators and segmentors for image synthesis\n",
      "and segmentation tasks. The generators are trained with an adversarial loss, a\n",
      "cycle-consistency loss, and also a shape-consistency loss, which is supervised\n",
      "by segmentors, to reduce the geometric distortion. From the segmentation view,\n",
      "the segmentors are boosted by synthetic data from generators in an online\n",
      "manner. Generators and segmentors prompt each other alternatively in an\n",
      "end-to-end training fashion. With extensive experiments on a dataset including\n",
      "a total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular\n",
      "volumes, we show both tasks are beneficial to each other and coupling these two\n",
      "tasks results in better performance than solving them exclusively. \n",
      "\n",
      "\n",
      "This paper presents a novel method to deal with the challenging task of\n",
      "generating photographic images conditioned on semantic image descriptions. Our\n",
      "method introduces accompanying hierarchical-nested adversarial objectives\n",
      "inside the network hierarchies, which regularize mid-level representations and\n",
      "assist generator training to capture the complex image statistics. We present\n",
      "an extensile single-stream generator architecture to better adapt the jointed\n",
      "discriminators and push generated images up to high resolutions. We adopt a\n",
      "multi-purpose adversarial loss to encourage more effective image and text\n",
      "information usage in order to improve the semantic consistency and image\n",
      "fidelity simultaneously. Furthermore, we introduce a new visual-semantic\n",
      "similarity measure to evaluate the semantic consistency of generated images.\n",
      "With extensive experimental validation on three public datasets, our method\n",
      "significantly improves previous state of the arts on all datasets over\n",
      "different evaluation metrics. \n",
      "\n",
      "\n",
      "Enhancing low resolution images via super-resolution or image synthesis for\n",
      "cross-resolution face recognition has been well studied. Several image\n",
      "processing and machine learning paradigms have been explored for addressing the\n",
      "same. In this research, we propose Synthesis via Deep Sparse Representation\n",
      "algorithm for synthesizing a high resolution face image from a low resolution\n",
      "input image. The proposed algorithm learns multi-level sparse representation\n",
      "for both high and low resolution gallery images, along with an identity aware\n",
      "dictionary and a transformation function between the two representations for\n",
      "face identification scenarios. With low resolution test data as input, the high\n",
      "resolution test image is synthesized using the identity aware dictionary and\n",
      "transformation which is then used for face recognition. The performance of the\n",
      "proposed SDSR algorithm is evaluated on four databases, including one real\n",
      "world dataset. Experimental results and comparison with existing seven\n",
      "algorithms demonstrate the efficacy of the proposed algorithm in terms of both\n",
      "face identification and image quality measures. \n",
      "\n",
      "\n",
      "Multi-view face synthesis from a single image is an ill-posed problem and\n",
      "often suffers from serious appearance distortion. Producing photo-realistic and\n",
      "identity preserving multi-view results is still a not well defined synthesis\n",
      "problem. This paper proposes Load Balanced Generative Adversarial Networks\n",
      "(LB-GAN) to precisely rotate the yaw angle of an input face image to any\n",
      "specified angle. LB-GAN decomposes the challenging synthesis problem into two\n",
      "well constrained subtasks that correspond to a face normalizer and a face\n",
      "editor respectively. The normalizer first frontalizes an input image, and then\n",
      "the editor rotates the frontalized image to a desired pose guided by a remote\n",
      "code. In order to generate photo-realistic local details, the normalizer and\n",
      "the editor are trained in a two-stage manner and regulated by a conditional\n",
      "self-cycle loss and an attention based L2 loss. Exhaustive experiments on\n",
      "controlled and uncontrolled environments demonstrate that the proposed method\n",
      "not only improves the visual realism of multi-view synthetic images, but also\n",
      "preserves identity information well. \n",
      "\n",
      "\n",
      "Employing deep learning-based approaches for fine-grained facial expression\n",
      "analysis, such as those involving the estimation of Action Unit (AU)\n",
      "intensities, is difficult due to the lack of a large-scale dataset of real\n",
      "faces with sufficiently diverse AU labels for training. In this paper, we\n",
      "consider how AU-level facial image synthesis can be used to substantially\n",
      "augment such a dataset. We propose an AU synthesis framework that combines the\n",
      "well-known 3D Morphable Model (3DMM), which intrinsically disentangles\n",
      "expression parameters from other face attributes, with models that\n",
      "adversarially generate 3DMM expression parameters conditioned on given target\n",
      "AU labels, in contrast to the more conventional approach of generating facial\n",
      "images directly. In this way, we are able to synthesize new combinations of\n",
      "expression parameters and facial images from desired AU labels. Extensive\n",
      "quantitative and qualitative results on the benchmark DISFA dataset demonstrate\n",
      "the effectiveness of our method on 3DMM facial expression parameter synthesis\n",
      "and data augmentation for deep learning-based AU intensity estimation. \n",
      "\n",
      "\n",
      "Acquiring images of the same anatomy with multiple different contrasts\n",
      "increases the diversity of diagnostic information available in an MR exam. Yet,\n",
      "scan time limitations may prohibit acquisition of certain contrasts, and images\n",
      "for some contrast may be corrupted by noise and artifacts. In such cases, the\n",
      "ability to synthesize unacquired or corrupted contrasts from remaining\n",
      "contrasts can improve diagnostic utility. For multi-contrast synthesis, current\n",
      "methods learn a nonlinear intensity transformation between the source and\n",
      "target images, either via nonlinear regression or deterministic neural\n",
      "networks. These methods can in turn suffer from loss of high-spatial-frequency\n",
      "information in synthesized images. Here we propose a new approach for\n",
      "multi-contrast MRI synthesis based on conditional generative adversarial\n",
      "networks. The proposed approach preserves high-frequency details via an\n",
      "adversarial loss; and it offers enhanced synthesis performance via a pixel-wise\n",
      "loss for registered multi-contrast images and a cycle-consistency loss for\n",
      "unregistered images. Information from neighboring cross-sections are utilized\n",
      "to further improved synthesis quality. Demonstrations on T1- and T2-weighted\n",
      "images from healthy subjects and patients clearly indicate the superior\n",
      "performance of the proposed approach compared to previous state-of-the-art\n",
      "methods. Our synthesis approach can help improve quality and versatility of\n",
      "multi-contrast MRI exams without the need for prolonged examinations. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) while being very versatile in\n",
      "realistic image synthesis, still are sensitive to the input distribution. Given\n",
      "a set of data that has an imbalance in the distribution, the networks are\n",
      "susceptible to missing modes and not capturing the data distribution. While\n",
      "various methods have been tried to improve training of GANs, these have not\n",
      "addressed the challenges of covering the full data distribution. Specifically,\n",
      "a generator is not penalized for missing a mode. We show that these are\n",
      "therefore still susceptible to not capturing the full data distribution.\n",
      "  In this paper, we propose a simple approach that combines an encoder based\n",
      "objective with novel loss functions for generator and discriminator that\n",
      "improves the solution in terms of capturing missing modes. We validate that the\n",
      "proposed method results in substantial improvements through its detailed\n",
      "analysis on toy and real datasets. The quantitative and qualitative results\n",
      "demonstrate that the proposed method improves the solution for the problem of\n",
      "missing modes and improves training of GANs. \n",
      "\n",
      "\n",
      "Advances in fluorescence microscopy enable acquisition of 3D image volumes\n",
      "with better image quality and deeper penetration into tissue. Segmentation is a\n",
      "required step to characterize and analyze biological structures in the images\n",
      "and recent 3D segmentation using deep learning has achieved promising results.\n",
      "One issue is that deep learning techniques require a large set of groundtruth\n",
      "data which is impractical to annotate manually for large 3D microscopy volumes.\n",
      "This paper describes a 3D deep learning nuclei segmentation method using\n",
      "synthetic 3D volumes for training. A set of synthetic volumes and the\n",
      "corresponding groundtruth are generated using spatially constrained\n",
      "cycle-consistent adversarial networks. Segmentation results demonstrate that\n",
      "our proposed method is capable of segmenting nuclei successfully for various\n",
      "data sets. \n",
      "\n",
      "\n",
      "We present FusedGAN, a deep network for conditional image synthesis with\n",
      "controllable sampling of diverse images. Fidelity, diversity and controllable\n",
      "sampling are the main quality measures of a good image generation model. Most\n",
      "existing models are insufficient in all three aspects. The FusedGAN can perform\n",
      "controllable sampling of diverse images with very high fidelity. We argue that\n",
      "controllability can be achieved by disentangling the generation process into\n",
      "various stages. In contrast to stacked GANs, where multiple stages of GANs are\n",
      "trained separately with full supervision of labeled intermediate images, the\n",
      "FusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike\n",
      "existing methods, which requires full supervision with paired conditions and\n",
      "images, the FusedGAN can effectively leverage more abundant images without\n",
      "corresponding conditions in training, to produce more diverse samples with high\n",
      "fidelity. We achieve this by fusing two generators: one for unconditional image\n",
      "generation, and the other for conditional image generation, where the two\n",
      "partly share a common latent space thereby disentangling the generation. We\n",
      "demonstrate the efficacy of the FusedGAN in fine grained image generation tasks\n",
      "such as text-to-image, and attribute-to-face generation. \n",
      "\n",
      "\n",
      "We propose a novel hierarchical approach for text-to-image synthesis by\n",
      "inferring semantic layout. Instead of learning a direct mapping from text to\n",
      "image, our algorithm decomposes the generation process into multiple steps, in\n",
      "which it first constructs a semantic layout from the text by the layout\n",
      "generator and converts the layout to an image by the image generator. The\n",
      "proposed layout generator progressively constructs a semantic layout in a\n",
      "coarse-to-fine manner by generating object bounding boxes and refining each box\n",
      "by estimating object shapes inside the box. The image generator synthesizes an\n",
      "image conditioned on the inferred semantic layout, which provides a useful\n",
      "semantic structure of an image matching with the text description. Our model\n",
      "not only generates semantically more meaningful images, but also allows\n",
      "automatic annotation of generated images and user-controlled generation process\n",
      "by modifying the generated scene layout. We demonstrate the capability of the\n",
      "proposed model on challenging MS-COCO dataset and show that the model can\n",
      "substantially improve the image quality, interpretability of output and\n",
      "semantic alignment to input text over existing approaches. \n",
      "\n",
      "\n",
      "While it is nearly effortless for humans to quickly assess the perceptual\n",
      "similarity between two images, the underlying processes are thought to be quite\n",
      "complex. Despite this, the most widely used perceptual metrics today, such as\n",
      "PSNR and SSIM, are simple, shallow functions, and fail to account for many\n",
      "nuances of human perception. Recently, the deep learning community has found\n",
      "that features of the VGG network trained on ImageNet classification has been\n",
      "remarkably useful as a training loss for image synthesis. But how perceptual\n",
      "are these so-called \"perceptual losses\"? What elements are critical for their\n",
      "success? To answer these questions, we introduce a new dataset of human\n",
      "perceptual similarity judgments. We systematically evaluate deep features\n",
      "across different architectures and tasks and compare them with classic metrics.\n",
      "We find that deep features outperform all previous metrics by large margins on\n",
      "our dataset. More surprisingly, this result is not restricted to\n",
      "ImageNet-trained VGG features, but holds across different deep architectures\n",
      "and levels of supervision (supervised, self-supervised, or even unsupervised).\n",
      "Our results suggest that perceptual similarity is an emergent property shared\n",
      "across deep visual representations. \n",
      "\n",
      "\n",
      "Semantic layouts based Image synthesizing, which has benefited from the\n",
      "success of Generative Adversarial Network (GAN), has drawn much attention in\n",
      "these days. How to enhance the synthesis image equality while keeping the\n",
      "stochasticity of the GAN is still a challenge. We propose a novel denoising\n",
      "framework to handle this problem. The overlapped objects generation is another\n",
      "challenging task when synthesizing images from a semantic layout to a realistic\n",
      "RGB photo. To overcome this deficiency, we include a one-hot semantic label map\n",
      "to force the generator paying more attention on the overlapped objects\n",
      "generation. Furthermore, we improve the loss function of the discriminator by\n",
      "considering perturb loss and cascade layer loss to guide the generation\n",
      "process. We applied our methods on the Cityscapes, Facades and NYU datasets and\n",
      "demonstrate the image generation ability of our model. \n",
      "\n",
      "\n",
      "Synthesizing realistic images from human drawn sketches is a challenging\n",
      "problem in computer graphics and vision. Existing approaches either need exact\n",
      "edge maps, or rely on retrieval of existing photographs. In this work, we\n",
      "propose a novel Generative Adversarial Network (GAN) approach that synthesizes\n",
      "plausible images from 50 categories including motorcycles, horses and couches.\n",
      "We demonstrate a data augmentation technique for sketches which is fully\n",
      "automatic, and we show that the augmented data is helpful to our task. We\n",
      "introduce a new network building block suitable for both the generator and\n",
      "discriminator which improves the information flow by injecting the input image\n",
      "at multiple scales. Compared to state-of-the-art image translation methods, our\n",
      "approach generates more realistic images and achieves significantly higher\n",
      "Inception Scores. \n",
      "\n",
      "\n",
      "An important task in image processing and neuroimaging is to extract\n",
      "quantitative information from the acquired images in order to make observations\n",
      "about the presence of disease or markers of development in populations. Having\n",
      "a lowdimensional manifold of an image allows for easier statistical comparisons\n",
      "between groups and the synthesis of group representatives. Previous studies\n",
      "have sought to identify the best mapping of brain MRI to a low-dimensional\n",
      "manifold, but have been limited by assumptions of explicit similarity measures.\n",
      "In this work, we use deep learning techniques to investigate implicit manifolds\n",
      "of normal brains and generate new, high-quality images. We explore implicit\n",
      "manifolds by addressing the problems of image synthesis and image denoising as\n",
      "important tools in manifold learning. First, we propose the unsupervised\n",
      "synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN)\n",
      "by learning from 528 examples of 2D axial slices of brain MRI. Synthesized\n",
      "images were first shown to be unique by performing a crosscorrelation with the\n",
      "training set. Real and synthesized images were then assessed in a blinded\n",
      "manner by two imaging experts providing an image quality score of 1-5. The\n",
      "quality score of the synthetic image showed substantial overlap with that of\n",
      "the real images. Moreover, we use an autoencoder with skip connections for\n",
      "image denoising, showing that the proposed method results in higher PSNR than\n",
      "FSL SUSAN after denoising. This work shows the power of artificial networks to\n",
      "synthesize realistic imaging data, which can be used to improve image\n",
      "processing techniques and provide a quantitative framework to structural\n",
      "changes in the brain. \n",
      "\n",
      "\n",
      "A lack of generalizability is one key limitation of deep learning based\n",
      "segmentation. Typically, one manually labels new training images when\n",
      "segmenting organs in different imaging modalities or segmenting abnormal organs\n",
      "from distinct disease cohorts. The manual efforts can be alleviated if one is\n",
      "able to reuse manual labels from one modality (e.g., MRI) to train a\n",
      "segmentation network for a new modality (e.g., CT). Previously, two stage\n",
      "methods have been proposed to use cycle generative adversarial networks\n",
      "(CycleGAN) to synthesize training images for a target modality. Then, these\n",
      "efforts trained a segmentation network independently using synthetic images.\n",
      "However, these two independent stages did not use the complementary information\n",
      "between synthesis and segmentation. Herein, we proposed a novel end-to-end\n",
      "synthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT\n",
      "image synthesis and CT splenomegaly segmentation simultaneously without using\n",
      "manual labels on CT. The end-to-end EssNet achieved significantly higher median\n",
      "Dice similarity coefficient (0.9188) than the two stages strategy (0.8801), and\n",
      "even higher than canonical multi-atlas segmentation (0.9125) and ResNet method\n",
      "(0.9107), which used the CT manual labels. \n",
      "\n",
      "\n",
      "Many image processing tasks can be formulated as translating images between\n",
      "two image domains, such as colorization, super resolution and conditional image\n",
      "synthesis. In most of these tasks, an input image may correspond to multiple\n",
      "outputs. However, current existing approaches only show very minor diversity of\n",
      "the outputs. In this paper, we present a novel approach to synthesize diverse\n",
      "realistic images corresponding to a semantic layout. We introduce a diversity\n",
      "loss objective, which maximizes the distance between synthesized image pairs\n",
      "and links the input noise to the semantic segments in the synthesized images.\n",
      "Thus, our approach can not only produce diverse images, but also allow users to\n",
      "manipulate the output images by adjusting the noise manually. Experimental\n",
      "results show that images synthesized by our approach are significantly more\n",
      "diverse than that of the current existing works and equipping our diversity\n",
      "loss does not degrade the reality of the base networks. \n",
      "\n",
      "\n",
      "As both light transport simulation and reinforcement learning are ruled by\n",
      "the same Fredholm integral equation of the second kind, reinforcement learning\n",
      "techniques may be used for photorealistic image synthesis: Efficiency may be\n",
      "dramatically improved by guiding light transport paths by an approximate\n",
      "solution of the integral equation that is learned during rendering. In the\n",
      "light of the recent advances in reinforcement learning for playing games, we\n",
      "investigate the representation of an approximate solution of an integral\n",
      "equation by artificial neural networks and derive a loss function for that\n",
      "purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural\n",
      "networks with standard information instead of linear information and naturally\n",
      "are able to generate an arbitrary number of training samples. The methods are\n",
      "demonstrated for applications in light transport simulation. \n",
      "\n",
      "\n",
      "Hematoxylin and Eosin stained histopathology image analysis is essential for\n",
      "the diagnosis and study of complicated diseases such as cancer. Existing\n",
      "state-of-the-art approaches demand extensive amount of supervised training data\n",
      "from trained pathologists. In this work we synthesize in an unsupervised\n",
      "manner, large histopathology image datasets, suitable for supervised training\n",
      "tasks. We propose a unified pipeline that: a) generates a set of initial\n",
      "synthetic histopathology images with paired information about the nuclei such\n",
      "as segmentation masks; b) refines the initial synthetic images through a\n",
      "Generative Adversarial Network (GAN) to reference styles; c) trains a\n",
      "task-specific CNN and boosts the performance of the task-specific CNN with\n",
      "on-the-fly generated adversarial examples. Our main contribution is that the\n",
      "synthetic images are not only realistic, but also representative (in reference\n",
      "styles) and relatively challenging for training task-specific CNNs. We test our\n",
      "method for nucleus segmentation using images from four cancer types. When no\n",
      "supervised data exists for a cancer type, our method without supervision cost\n",
      "significantly outperforms supervised methods which perform across-cancer\n",
      "generalization. Even when supervised data exists for all cancer types, our\n",
      "approach without supervision cost performs better than supervised methods. \n",
      "\n",
      "\n",
      "In many domains of computer vision, generative adversarial networks (GANs)\n",
      "have achieved great success, among which the family of Wasserstein GANs (WGANs)\n",
      "is considered to be state-of-the-art due to the theoretical contributions and\n",
      "competitive qualitative performance. However, it is very challenging to\n",
      "approximate the $k$-Lipschitz constraint required by the Wasserstein-1\n",
      "metric~(W-met). In this paper, we propose a novel Wasserstein\n",
      "divergence~(W-div), which is a relaxed version of W-met and does not require\n",
      "the $k$-Lipschitz constraint. As a concrete application, we introduce a\n",
      "Wasserstein divergence objective for GANs~(WGAN-div), which can faithfully\n",
      "approximate W-div through optimization. Under various settings, including\n",
      "progressive growing training, we demonstrate the stability of the proposed\n",
      "WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we\n",
      "study the quantitative and visual performance of WGAN-div on standard image\n",
      "synthesis benchmarks of computer vision, showing the superior performance of\n",
      "WGAN-div compared to the state-of-the-art methods. \n",
      "\n",
      "\n",
      "We present a new method for synthesizing high-resolution photo-realistic\n",
      "images from semantic label maps using conditional generative adversarial\n",
      "networks (conditional GANs). Conditional GANs have enabled a variety of\n",
      "applications, but the results are often limited to low-resolution and still far\n",
      "from realistic. In this work, we generate 2048x1024 visually appealing results\n",
      "with a novel adversarial loss, as well as new multi-scale generator and\n",
      "discriminator architectures. Furthermore, we extend our framework to\n",
      "interactive visual manipulation with two additional features. First, we\n",
      "incorporate object instance segmentation information, which enables object\n",
      "manipulations such as removing/adding objects and changing the object category.\n",
      "Second, we propose a method to generate diverse results given the same input,\n",
      "allowing users to edit the object appearance interactively. Human opinion\n",
      "studies demonstrate that our method significantly outperforms existing methods,\n",
      "advancing both the quality and the resolution of deep image synthesis and\n",
      "editing. \n",
      "\n",
      "\n",
      "We present compositional nearest neighbors (CompNN), a simple approach to\n",
      "visually interpreting distributed representations learned by a convolutional\n",
      "neural network (CNN) for pixel-level tasks (e.g., image synthesis and\n",
      "segmentation). It does so by reconstructing both a CNN's input and output image\n",
      "by copy-pasting corresponding patches from the training set with similar\n",
      "feature embeddings. To do so efficiently, it makes of a patch-match-based\n",
      "algorithm that exploits the fact that the patch representations learned by a\n",
      "CNN for pixel level tasks vary smoothly. Finally, we show that CompNN can be\n",
      "used to establish semantic correspondences between two images and control\n",
      "properties of the output image by modifying the images contained in the\n",
      "training set. We present qualitative and quantitative experiments for semantic\n",
      "segmentation and image-to-image translation that demonstrate that CompNN is a\n",
      "good tool for interpreting the embeddings learned by pixel-level CNNs. \n",
      "\n",
      "\n",
      "Compositionality of semantic concepts in image synthesis and analysis is\n",
      "appealing as it can help in decomposing known and generatively recomposing\n",
      "unknown data. For instance, we may learn concepts of changing illumination,\n",
      "geometry or albedo of a scene, and try to recombine them to generate physically\n",
      "meaningful, but unseen data for training and testing. In practice however we\n",
      "often do not have samples from the joint concept space available: We may have\n",
      "data on illumination change in one data set and on geometric change in another\n",
      "one without complete overlap. We pose the following question: How can we learn\n",
      "two or more concepts jointly from different data sets with mutual consistency\n",
      "where we do not have samples from the full joint space? We present a novel\n",
      "answer in this paper based on cyclic consistency over multiple concepts,\n",
      "represented individually by generative adversarial networks (GANs). Our method,\n",
      "ConceptGAN, can be understood as a drop in for data augmentation to improve\n",
      "resilience for real world applications. Qualitative and quantitative\n",
      "evaluations demonstrate its efficacy in generating semantically meaningful\n",
      "images, as well as one shot face verification as an example application. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GAN) have received wide attention in the\n",
      "machine learning field for their potential to learn high-dimensional, complex\n",
      "real data distribution. Specifically, they do not rely on any assumptions about\n",
      "the distribution and can generate real-like samples from latent space in a\n",
      "simple manner. This powerful property leads GAN to be applied to various\n",
      "applications such as image synthesis, image attribute editing, image\n",
      "translation, domain adaptation and other academic fields. In this paper, we aim\n",
      "to discuss the details of GAN for those readers who are familiar with, but do\n",
      "not comprehend GAN deeply or who wish to view GAN from various perspectives. In\n",
      "addition, we explain how GAN operates and the fundamental meaning of various\n",
      "objective functions that have been suggested recently. We then focus on how the\n",
      "GAN can be combined with an autoencoder framework. Finally, we enumerate the\n",
      "GAN variants that are applied to various tasks and other fields for those who\n",
      "are interested in exploiting GAN for their research. \n",
      "\n",
      "\n",
      "We study the problem of building models that disentangle independent factors\n",
      "of variation. Such models could be used to encode features that can efficiently\n",
      "be used for classification and to transfer attributes between different images\n",
      "in image synthesis. As data we use a weakly labeled training set. Our weak\n",
      "labels indicate what single factor has changed between two data samples,\n",
      "although the relative value of the change is unknown. This labeling is of\n",
      "particular interest as it may be readily available without annotation costs. To\n",
      "make use of weak labels we introduce an autoencoder model and train it through\n",
      "constraints on image pairs and triplets. We formally prove that without\n",
      "additional knowledge there is no guarantee that two images with the same factor\n",
      "of variation will be mapped to the same feature. We call this issue the\n",
      "reference ambiguity. Moreover, we show the role of the feature dimensionality\n",
      "and adversarial training. We demonstrate experimentally that the proposed model\n",
      "can successfully transfer attributes on several datasets, but show also cases\n",
      "when the reference ambiguity occurs. \n",
      "\n",
      "\n",
      "Hybrid imaging promises large potential in medical imaging applications. To\n",
      "fully utilize the possibilities of corresponding information from different\n",
      "modalities, the information must be transferable between the domains. In\n",
      "radiation therapy planning, existing methods make use of reconstructed 3D\n",
      "magnetic resonance imaging data to synthesize corresponding X-ray attenuation\n",
      "maps. In contrast, for fluoroscopic procedures only line integral data, i.e.,\n",
      "2D projection images, are present. The question arises which approaches could\n",
      "potentially be used for this MR to X-ray projection image-to-image translation.\n",
      "We examine three network architectures and two loss-functions regarding their\n",
      "suitability as generator networks for this task. All generators proved to yield\n",
      "suitable results for this task. A cascaded refinement network paired with a\n",
      "perceptual-loss function achieved the best qualitative results in our\n",
      "evaluation. The perceptual-loss showed to be able to preserve most of the\n",
      "high-frequency details in the projection images and, thus, is recommended for\n",
      "the underlying task and similar problems. \n",
      "\n",
      "\n",
      "Although Generative Adversarial Networks (GANs) have shown remarkable success\n",
      "in various tasks, they still face challenges in generating high quality images.\n",
      "In this paper, we propose Stacked Generative Adversarial Networks (StackGAN)\n",
      "aiming at generating high-resolution photo-realistic images. First, we propose\n",
      "a two-stage generative adversarial network architecture, StackGAN-v1, for\n",
      "text-to-image synthesis. The Stage-I GAN sketches the primitive shape and\n",
      "colors of the object based on given text description, yielding low-resolution\n",
      "images. The Stage-II GAN takes Stage-I results and text descriptions as inputs,\n",
      "and generates high-resolution images with photo-realistic details. Second, an\n",
      "advanced multi-stage generative adversarial network architecture, StackGAN-v2,\n",
      "is proposed for both conditional and unconditional generative tasks. Our\n",
      "StackGAN-v2 consists of multiple generators and discriminators in a tree-like\n",
      "structure; images at multiple scales corresponding to the same scene are\n",
      "generated from different branches of the tree. StackGAN-v2 shows more stable\n",
      "training behavior than StackGAN-v1 by jointly approximating multiple\n",
      "distributions. Extensive experiments demonstrate that the proposed stacked\n",
      "generative adversarial networks significantly outperform other state-of-the-art\n",
      "methods in generating photo-realistic images. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) provide a way to learn deep\n",
      "representations without extensively annotated training data. They achieve this\n",
      "through deriving backpropagation signals through a competitive process\n",
      "involving a pair of networks. The representations that can be learned by GANs\n",
      "may be used in a variety of applications, including image synthesis, semantic\n",
      "image editing, style transfer, image super-resolution and classification. The\n",
      "aim of this review paper is to provide an overview of GANs for the signal\n",
      "processing community, drawing on familiar analogies and concepts where\n",
      "possible. In addition to identifying different methods for training and\n",
      "constructing GANs, we also point to remaining challenges in their theory and\n",
      "application. \n",
      "\n",
      "\n",
      "We present an overview and evaluation of a new, systematic approach for\n",
      "generation of highly realistic, annotated synthetic data for training of deep\n",
      "neural networks in computer vision tasks. The main contribution is a procedural\n",
      "world modeling approach enabling high variability coupled with physically\n",
      "accurate image synthesis, and is a departure from the hand-modeled virtual\n",
      "worlds and approximate image synthesis methods used in real-time applications.\n",
      "The benefits of our approach include flexible, physically accurate and scalable\n",
      "image synthesis, implicit wide coverage of classes and features, and complete\n",
      "data introspection for annotations, which all contribute to quality and cost\n",
      "efficiency. To evaluate our approach and the efficacy of the resulting data, we\n",
      "use semantic segmentation for autonomous vehicles and robotic navigation as the\n",
      "main application, and we train multiple deep learning architectures using\n",
      "synthetic data with and without fine tuning on organic (i.e. real-world) data.\n",
      "The evaluation shows that our approach improves the neural network's\n",
      "performance and that even modest implementation efforts produce\n",
      "state-of-the-art results. \n",
      "\n",
      "\n",
      "Recently, more and more attention is drawn to the field of medical image\n",
      "synthesis across modalities. Among them, the synthesis of computed tomography\n",
      "(CT) image from T1-weighted magnetic resonance (MR) image is of great\n",
      "importance, although the mapping between them is highly complex due to large\n",
      "gaps of appearances of the two modalities. In this work, we aim to tackle this\n",
      "MR-to-CT synthesis by a novel deep embedding convolutional neural network\n",
      "(DECNN). Specifically, we generate the feature maps from MR images, and then\n",
      "transform these feature maps forward through convolutional layers in the\n",
      "network. We can further compute a tentative CT synthesis from the midway of the\n",
      "flow of feature maps, and then embed this tentative CT synthesis back to the\n",
      "feature maps. This embedding operation results in better feature maps, which\n",
      "are further transformed forward in DECNN. After repeat-ing this embedding\n",
      "procedure for several times in the network, we can eventually synthesize a\n",
      "final CT image in the end of the DECNN. We have validated our proposed method\n",
      "on both brain and prostate datasets, by also compar-ing with the\n",
      "state-of-the-art methods. Experimental results suggest that our DECNN (with\n",
      "repeated embedding op-erations) demonstrates its superior performances, in\n",
      "terms of both the perceptive quality of the synthesized CT image and the\n",
      "run-time cost for synthesizing a CT image. \n",
      "\n",
      "\n",
      "This paper presents an automatic image synthesis method to transfer the style\n",
      "of an example image to a content image. When standard neural style transfer\n",
      "approaches are used, the textures and colours in different semantic regions of\n",
      "the style image are often applied inappropriately to the content image,\n",
      "ignoring its semantic layout, and ruining the transfer result. In order to\n",
      "reduce or avoid such effects, we propose a novel method based on automatically\n",
      "segmenting the objects and extracting their soft semantic masks from the style\n",
      "and content images, in order to preserve the structure of the content image\n",
      "while having the style transferred. Each soft mask of the style image\n",
      "represents a specific part of the style image, corresponding to the soft mask\n",
      "of the content image with the same semantics. Both the soft masks and source\n",
      "images are provided as multichannel input to an augmented deep CNN framework\n",
      "for style transfer which incorporates a generative Markov random field (MRF)\n",
      "model. Results on various images show that our method outperforms the most\n",
      "recent techniques. \n",
      "\n",
      "\n",
      "This paper proposes a series of new approaches to improve Generative\n",
      "Adversarial Network (GAN) for conditional image synthesis and we name the\n",
      "proposed model as ArtGAN. One of the key innovation of ArtGAN is that, the\n",
      "gradient of the loss function w.r.t. the label (randomly assigned to each\n",
      "generated image) is back-propagated from the categorical discriminator to the\n",
      "generator. With the feedback from the label information, the generator is able\n",
      "to learn more efficiently and generate image with better quality. Inspired by\n",
      "recent works, an autoencoder is incorporated into the categorical discriminator\n",
      "for additional complementary information. Last but not least, we introduce a\n",
      "novel strategy to improve the image quality. In the experiments, we evaluate\n",
      "ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results\n",
      "showed that our proposed model outperforms the state-of-the-art results on\n",
      "CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN\n",
      "is able to generate plausible-looking images on Oxford-102 and CUB-200, as well\n",
      "as able to draw realistic artworks based on style, artist, and genre. The\n",
      "source code and models are available at: https://github.com/cs-chan/ArtGAN \n",
      "\n",
      "\n",
      "Recent approaches in generative adversarial networks (GANs) can automatically\n",
      "synthesize realistic images from descriptive text. Despite the overall fair\n",
      "quality, the generated images often expose visible flaws that lack structural\n",
      "definition for an object of interest. In this paper, we aim to extend state of\n",
      "the art for GAN-based text-to-image synthesis by improving perceptual quality\n",
      "of generated images. Differentiated from previous work, our synthetic image\n",
      "generator optimizes on perceptual loss functions that measure pixel, feature\n",
      "activation, and texture differences against a natural image. We present\n",
      "visually more compelling synthetic images of birds and flowers generated from\n",
      "text descriptions in comparison to some of the most prominent existing work. \n",
      "\n",
      "\n",
      "We present a simple nearest-neighbor (NN) approach that synthesizes\n",
      "high-frequency photorealistic images from an \"incomplete\" signal such as a\n",
      "low-resolution image, a surface normal map, or edges. Current state-of-the-art\n",
      "deep generative models designed for such conditional image synthesis lack two\n",
      "important things: (1) they are unable to generate a large set of diverse\n",
      "outputs, due to the mode collapse problem. (2) they are not interpretable,\n",
      "making it difficult to control the synthesized output. We demonstrate that NN\n",
      "approaches potentially address such limitations, but suffer in accuracy on\n",
      "small datasets. We design a simple pipeline that combines the best of both\n",
      "worlds: the first stage uses a convolutional neural network (CNN) to maps the\n",
      "input to a (overly-smoothed) image, and the second stage uses a pixel-wise\n",
      "nearest neighbor method to map the smoothed output to multiple high-quality,\n",
      "high-frequency outputs in a controllable manner. We demonstrate our approach\n",
      "for various input modalities, and for various domains ranging from human faces\n",
      "to cats-and-dogs to shoes and handbags. \n",
      "\n",
      "\n",
      "In this paper, we propose a novel application of Generative Adversarial\n",
      "Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\n",
      "Compared to natural images, cells tend to have a simpler and more geometric\n",
      "global structure that facilitates image generation. However, the correlation\n",
      "between the spatial pattern of different fluorescent proteins reflects\n",
      "important biological functions, and synthesized images have to capture these\n",
      "relationships to be relevant for biological applications. We adapt GANs to the\n",
      "task at hand and propose new models with casual dependencies between image\n",
      "channels that can generate multi-channel images, which would be impossible to\n",
      "obtain experimentally. We evaluate our approach using two independent\n",
      "techniques and compare it against sensible baselines. Finally, we demonstrate\n",
      "that by interpolating across the latent space we can mimic the known changes in\n",
      "protein localization that occur through time during the cell cycle, allowing us\n",
      "to predict temporal evolution from static images. \n",
      "\n",
      "\n",
      "The ability to edit materials of objects in images is desirable by many\n",
      "content creators. However, this is an extremely challenging task as it requires\n",
      "to disentangle intrinsic physical properties of an image. We propose an\n",
      "end-to-end network architecture that replicates the forward image formation\n",
      "process to accomplish this task. Specifically, given a single image, the\n",
      "network first predicts intrinsic properties, i.e. shape, illumination, and\n",
      "material, which are then provided to a rendering layer. This layer performs\n",
      "in-network image synthesis, thereby enabling the network to understand the\n",
      "physics behind the image formation process. The proposed rendering layer is\n",
      "fully differentiable, supports both diffuse and specular materials, and thus\n",
      "can be applicable in a variety of problem settings. We demonstrate a rich set\n",
      "of visually plausible material editing examples and provide an extensive\n",
      "comparative study. \n",
      "\n",
      "\n",
      "Positron emission tomography (PET) image synthesis plays an important role,\n",
      "which can be used to boost the training data for computer aided diagnosis\n",
      "systems. However, existing image synthesis methods have problems in\n",
      "synthesizing the low resolution PET images. To address these limitations, we\n",
      "propose multi-channel generative adversarial networks (M-GAN) based PET image\n",
      "synthesis method. Different to the existing methods which rely on using\n",
      "low-level features, the proposed M-GAN is capable to represent the features in\n",
      "a high-level of semantic based on the adversarial learning concept. In\n",
      "addition, M-GAN enables to take the input from the annotation (label) to\n",
      "synthesize the high uptake regions e.g., tumors and from the computed\n",
      "tomography (CT) images to constrain the appearance consistency and output the\n",
      "synthetic PET images directly. Our results on 50 lung cancer PET-CT studies\n",
      "indicate that our method was much closer to the real PET images when compared\n",
      "with the existing methods. \n",
      "\n",
      "\n",
      "We present an approach to synthesizing photographic images conditioned on\n",
      "semantic layouts. Given a semantic label map, our approach produces an image\n",
      "with photographic appearance that conforms to the input layout. The approach\n",
      "thus functions as a rendering engine that takes a two-dimensional semantic\n",
      "specification of the scene and produces a corresponding photographic image.\n",
      "Unlike recent and contemporaneous work, our approach does not rely on\n",
      "adversarial training. We show that photographic images can be synthesized from\n",
      "semantic layouts by a single feedforward network with appropriate structure,\n",
      "trained end-to-end with a direct regression objective. The presented approach\n",
      "scales seamlessly to high resolutions; we demonstrate this by synthesizing\n",
      "photographic images at 2-megapixel resolution, the full resolution of our\n",
      "training data. Extensive perceptual experiments on datasets of outdoor and\n",
      "indoor scenes demonstrate that images synthesized by the presented approach are\n",
      "considerably more realistic than alternative approaches. The results are shown\n",
      "in the supplementary video at https://youtu.be/0fhUJT21-bs \n",
      "\n",
      "\n",
      "We present a conceptually new and flexible method for multi-class open set\n",
      "classification. Unlike previous methods where unknown classes are inferred with\n",
      "respect to the feature or decision distance to the known classes, our approach\n",
      "is able to provide explicit modelling and decision score for unknown classes.\n",
      "The proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax\n",
      "by employing generative adversarial networks (GANs) for novel category image\n",
      "synthesis. We validate the proposed method on two datasets of handwritten\n",
      "digits and characters, resulting in superior results over previous deep\n",
      "learning based method OpenMax Moreover, G-OpenMax provides a way to visualize\n",
      "samples representing the unknown classes from open space. Our simple and\n",
      "effective approach could serve as a new direction to tackle the challenging\n",
      "multi-class open set classification problem. \n",
      "\n",
      "\n",
      "In this paper, we propose a way of synthesizing realistic images directly\n",
      "with natural language description, which has many useful applications, e.g.\n",
      "intelligent image manipulation. We attempt to accomplish such synthesis: given\n",
      "a source image and a target text description, our model synthesizes images to\n",
      "meet two requirements: 1) being realistic while matching the target text\n",
      "description; 2) maintaining other image features that are irrelevant to the\n",
      "text description. The model should be able to disentangle the semantic\n",
      "information from the two modalities (image and text), and generate new images\n",
      "from the combined semantics. To achieve this, we proposed an end-to-end neural\n",
      "architecture that leverages adversarial learning to automatically learn\n",
      "implicit loss functions, which are optimized to fulfill the aforementioned two\n",
      "requirements. We have evaluated our model by conducting experiments on\n",
      "Caltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated\n",
      "that our model is capable of synthesizing realistic images that match the given\n",
      "descriptions, while still maintain other features of original images. \n",
      "\n",
      "\n",
      "In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN)\n",
      "for image synthesis that leverages information from few labels (as little as\n",
      "0.22%, max. 10% of the dataset) to learn semantically meaningful and\n",
      "controllable data representations where latent variables correspond to label\n",
      "categories. The architecture builds on Information Maximizing Generative\n",
      "Adversarial Networks (InfoGAN) and is shown to learn both continuous and\n",
      "categorical codes and achieves higher quality of synthetic samples compared to\n",
      "fully unsupervised settings. Furthermore, we show that using small amounts of\n",
      "labeled data speeds-up training convergence. The architecture maintains the\n",
      "ability to disentangle latent variables for which no labels are available.\n",
      "Finally, we contribute an information-theoretic reasoning on how introducing\n",
      "semi-supervision increases mutual information between synthetic and real data. \n",
      "\n",
      "\n",
      "Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to\n",
      "synthesize a new image that retains the high-level structure of a content\n",
      "image, rendered in the low-level texture of a style image. This is achieved by\n",
      "constraining the new image to have high-level CNN features similar to the\n",
      "content image, and lower-level CNN features similar to the style image. However\n",
      "in the traditional optimization objective, low-level features of the content\n",
      "image are absent, and the low-level features of the style image dominate the\n",
      "low-level detail structures of the new image. Hence in the synthesized image,\n",
      "many details of the content image are lost, and a lot of inconsistent and\n",
      "unpleasing artifacts appear. As a remedy, we propose to steer image synthesis\n",
      "with a novel loss function: the Laplacian loss. The Laplacian matrix\n",
      "(\"Laplacian\" in short), produced by a Laplacian operator, is widely used in\n",
      "computer vision to detect edges and contours. The Laplacian loss measures the\n",
      "difference of the Laplacians, and correspondingly the difference of the detail\n",
      "structures, between the content image and a new image. It is flexible and\n",
      "compatible with the traditional style transfer constraints. By incorporating\n",
      "the Laplacian loss, we obtain a new optimization objective for neural style\n",
      "transfer named Lapstyle. Minimizing this objective will produce a stylized\n",
      "image that better preserves the detail structures of the content image and\n",
      "eliminates the artifacts. Experiments show that Lapstyle produces more\n",
      "appealing stylized images with less artifacts, without compromising their\n",
      "\"stylishness\". \n",
      "\n",
      "\n",
      "Cross-modal image synthesis is a topical problem in medical image computing.\n",
      "Existing methods for image synthesis are either tailored to a specific\n",
      "application, require large scale training sets, or are based on partitioning\n",
      "images into overlapping patches. In this paper, we propose a novel Dual\n",
      "cOnvolutional filTer lEarning (DOTE) approach to overcome the drawbacks of\n",
      "these approaches. We construct a closed loop joint filter learning strategy\n",
      "that generates informative feedback for model self-optimization. Our method can\n",
      "leverage data more efficiently thus reducing the size of the required training\n",
      "set. We extensively evaluate DOTE in two challenging tasks: image\n",
      "super-resolution and cross-modality synthesis. The experimental results\n",
      "demonstrate superior performance of our method over other state-of-the-art\n",
      "methods. \n",
      "\n",
      "\n",
      "Face synthesis has been a fascinating yet challenging problem in computer\n",
      "vision and machine learning. Its main research effort is to design algorithms\n",
      "to generate photo-realistic face images via given semantic domain. It has been\n",
      "a crucial prepossessing step of main-stream face recognition approaches and an\n",
      "excellent test of AI ability to use complicated probability distributions. In\n",
      "this paper, we provide a comprehensive review of typical face synthesis works\n",
      "that involve traditional methods as well as advanced deep learning approaches.\n",
      "Particularly, Generative Adversarial Net (GAN) is highlighted to generate\n",
      "photo-realistic and identity preserving results. Furthermore, the public\n",
      "available databases and evaluation metrics are introduced in details. We end\n",
      "the review with discussing unsolved difficulties and promising directions for\n",
      "future research. \n",
      "\n",
      "\n",
      "In this paper, we investigate deep image synthesis guided by sketch, color,\n",
      "and texture. Previous image synthesis methods can be controlled by sketch and\n",
      "color strokes but we are the first to examine texture control. We allow a user\n",
      "to place a texture patch on a sketch at arbitrary locations and scales to\n",
      "control the desired output texture. Our generative network learns to synthesize\n",
      "objects consistent with these texture suggestions. To achieve this, we develop\n",
      "a local texture loss in addition to adversarial and content loss to train the\n",
      "generative network. We conduct experiments using sketches generated from real\n",
      "images and textures sampled from a separate texture database and results show\n",
      "that our proposed algorithm is able to generate plausible images that are\n",
      "faithful to user controls. Ablation studies show that our proposed pipeline can\n",
      "generate more realistic images than adapting existing methods directly. \n",
      "\n",
      "\n",
      "In generative modeling, the Wasserstein distance (WD) has emerged as a useful\n",
      "metric to measure the discrepancy between generated and real data\n",
      "distributions. Unfortunately, it is challenging to approximate the WD of\n",
      "high-dimensional distributions. In contrast, the sliced Wasserstein distance\n",
      "(SWD) factorizes high-dimensional distributions into their multiple\n",
      "one-dimensional marginal distributions and is thus easier to approximate. In\n",
      "this paper, we introduce novel approximations of the primal and dual SWD.\n",
      "Instead of using a large number of random projections, as it is done by\n",
      "conventional SWD approximation methods, we propose to approximate SWDs with a\n",
      "small number of parameterized orthogonal projections in an end-to-end deep\n",
      "learning fashion. As concrete applications of our SWD approximations, we design\n",
      "two types of differentiable SWD blocks to equip modern generative\n",
      "frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\n",
      "the experiments, we not only show the superiority of the proposed generative\n",
      "models on standard image synthesis benchmarks, but also demonstrate the\n",
      "state-of-the-art performance on challenging high resolution image and video\n",
      "generation in an unsupervised manner. \n",
      "\n",
      "\n",
      "The large pose discrepancy between two face images is one of the fundamental\n",
      "challenges in automatic face recognition. Conventional approaches to\n",
      "pose-invariant face recognition either perform face frontalization on, or learn\n",
      "a pose-invariant representation from, a non-frontal face image. We argue that\n",
      "it is more desirable to perform both tasks jointly to allow them to leverage\n",
      "each other. To this end, this paper proposes a Disentangled Representation\n",
      "learning-Generative Adversarial Network (DR-GAN) with three distinct novelties.\n",
      "First, the encoder-decoder structure of the generator enables DR-GAN to learn a\n",
      "representation that is both generative and discriminative, which can be used\n",
      "for face image synthesis and pose-invariant face recognition. Second, this\n",
      "representation is explicitly disentangled from other face variations such as\n",
      "pose, through the pose code provided to the decoder and pose estimation in the\n",
      "discriminator. Third, DR-GAN can take one or multiple images as the input, and\n",
      "generate one unified identity representation along with an arbitrary number of\n",
      "synthetic face images. Extensive quantitative and qualitative evaluation on a\n",
      "number of controlled and in-the-wild databases demonstrate the superiority of\n",
      "DR-GAN over the state of the art in both learning representations and rotating\n",
      "large-pose face images. \n",
      "\n",
      "\n",
      "Implicit models, which allow for the generation of samples but not for\n",
      "point-wise evaluation of probabilities, are omnipresent in real-world problems\n",
      "tackled by machine learning and a hot topic of current research. Some examples\n",
      "include data simulators that are widely used in engineering and scientific\n",
      "research, generative adversarial networks (GANs) for image synthesis, and\n",
      "hot-off-the-press approximate inference techniques relying on implicit\n",
      "distributions. The majority of existing approaches to learning implicit models\n",
      "rely on approximating the intractable distribution or optimisation objective\n",
      "for gradient-based optimisation, which is liable to produce inaccurate updates\n",
      "and thus poor models. This paper alleviates the need for such approximations by\n",
      "proposing the Stein gradient estimator, which directly estimates the score\n",
      "function of the implicitly defined distribution. The efficacy of the proposed\n",
      "estimator is empirically demonstrated by examples that include meta-learning\n",
      "for approximate inference, and entropy regularised GANs that provide improved\n",
      "sample diversity. \n",
      "\n",
      "\n",
      "Magnetic Resonance Imaging (MRI) offers high-resolution \\emph{in vivo}\n",
      "imaging and rich functional and anatomical multimodality tissue contrast. In\n",
      "practice, however, there are challenges associated with considerations of\n",
      "scanning costs, patient comfort, and scanning time that constrain how much data\n",
      "can be acquired in clinical or research studies. In this paper, we explore the\n",
      "possibility of generating high-resolution and multimodal images from\n",
      "low-resolution single-modality imagery. We propose the weakly-supervised joint\n",
      "convolutional sparse coding to simultaneously solve the problems of\n",
      "super-resolution (SR) and cross-modality image synthesis. The learning process\n",
      "requires only a few registered multimodal image pairs as the training set.\n",
      "Additionally, the quality of the joint dictionary learning can be improved\n",
      "using a larger set of unpaired images. To combine unpaired data from different\n",
      "image resolutions/modalities, a hetero-domain image alignment term is proposed.\n",
      "Local image neighborhoods are naturally preserved by operating on the whole\n",
      "image domain (as opposed to image patches) and using joint convolutional sparse\n",
      "coding. The paired images are enhanced in the joint learning process with\n",
      "unpaired data and an additional maximum mean discrepancy term, which minimizes\n",
      "the dissimilarity between their feature distributions. Experiments show that\n",
      "the proposed method outperforms state-of-the-art techniques on both SR\n",
      "reconstruction and simultaneous SR and cross-modality synthesis. \n",
      "\n",
      "\n",
      "Recently, realistic image generation using deep neural networks has become a\n",
      "hot topic in machine learning and computer vision. Images can be generated at\n",
      "the pixel level by learning from a large collection of images. Learning to\n",
      "generate colorful cartoon images from black-and-white sketches is not only an\n",
      "interesting research problem, but also a potential application in digital\n",
      "entertainment. In this paper, we investigate the sketch-to-image synthesis\n",
      "problem by using conditional generative adversarial networks (cGAN). We propose\n",
      "the auto-painter model which can automatically generate compatible colors for a\n",
      "sketch. The new model is not only capable of painting hand-draw sketch with\n",
      "proper colors, but also allowing users to indicate preferred colors.\n",
      "Experimental results on two sketch datasets show that the auto-painter performs\n",
      "better that existing image-to-image methods. \n",
      "\n",
      "\n",
      "In this work, we investigate the value of uncertainty modeling in 3D\n",
      "super-resolution with convolutional neural networks (CNNs). Deep learning has\n",
      "shown success in a plethora of medical image transformation problems, such as\n",
      "super-resolution (SR) and image synthesis. However, the highly ill-posed nature\n",
      "of such problems results in inevitable ambiguity in the learning of networks.\n",
      "We propose to account for intrinsic uncertainty through a per-patch\n",
      "heteroscedastic noise model and for parameter uncertainty through approximate\n",
      "Bayesian inference in the form of variational dropout. We show that the\n",
      "combined benefits of both lead to the state-of-the-art performance SR of\n",
      "diffusion MR brain images in terms of errors compared to ground truth. We\n",
      "further show that the reduced error scores produce tangible benefits in\n",
      "downstream tractography. In addition, the probabilistic nature of the methods\n",
      "naturally confers a mechanism to quantify uncertainty over the super-resolved\n",
      "output. We demonstrate through experiments on both healthy and pathological\n",
      "brains the potential utility of such an uncertainty measure in the risk\n",
      "assessment of the super-resolved images for subsequent clinical use. \n",
      "\n",
      "\n",
      "Recent works have shown that it is possible to automatically predict\n",
      "intrinsic image properties like memorability. In this paper, we take a step\n",
      "forward addressing the question: \"Can we make an image more memorable?\".\n",
      "Methods for automatically increasing image memorability would have an impact in\n",
      "many application fields like education, gaming or advertising. Our work is\n",
      "inspired by the popular editing-by-applying-filters paradigm adopted in photo\n",
      "editing applications, like Instagram and Prisma. In this context, the problem\n",
      "of increasing image memorability maps to that of retrieving \"memorabilizing\"\n",
      "filters or style \"seeds\". Still, users generally have to go through most of the\n",
      "available filters before finding the desired solution, thus turning the editing\n",
      "process into a resource and time consuming task. In this work, we show that it\n",
      "is possible to automatically retrieve the best style seeds for a given image,\n",
      "thus remarkably reducing the number of human attempts needed to find a good\n",
      "match. Our approach leverages from recent advances in the field of image\n",
      "synthesis and adopts a deep architecture for generating a memorable picture\n",
      "from a given input image and a style seed. Importantly, to automatically select\n",
      "the best style a novel learning-based solution, also relying on deep models, is\n",
      "proposed. Our experimental evaluation, conducted on publicly available\n",
      "benchmarks, demonstrates the effectiveness of the proposed approach for\n",
      "generating memorable images through automatic style seed selection \n",
      "\n",
      "\n",
      "Translating information between text and image is a fundamental problem in\n",
      "artificial intelligence that connects natural language processing and computer\n",
      "vision. In the past few years, performance in image caption generation has seen\n",
      "significant improvement through the adoption of recurrent neural networks\n",
      "(RNN). Meanwhile, text-to-image generation begun to generate plausible images\n",
      "using datasets of specific categories like birds and flowers. We've even seen\n",
      "image generation from multi-category datasets such as the Microsoft Common\n",
      "Objects in Context (MSCOCO) through the use of generative adversarial networks\n",
      "(GANs). Synthesizing objects with a complex shape, however, is still\n",
      "challenging. For example, animals and humans have many degrees of freedom,\n",
      "which means that they can take on many complex shapes. We propose a new\n",
      "training method called Image-Text-Image (I2T2I) which integrates text-to-image\n",
      "and image-to-text (image captioning) synthesis to improve the performance of\n",
      "text-to-image synthesis. We demonstrate that %the capability of our method to\n",
      "understand the sentence descriptions, so as to I2T2I can generate better\n",
      "multi-categories images using MSCOCO than the state-of-the-art. We also\n",
      "demonstrate that I2T2I can achieve transfer learning by using a pre-trained\n",
      "image captioning module to generate human images on the MPII Human Pose \n",
      "\n",
      "\n",
      "In comparison to the radio and sub-millimetric domains, imaging with optical\n",
      "interferometry is still in its infancy. Due to the limited number of telescopes\n",
      "in existing arrays, image generation is a demanding process that relies on\n",
      "time-consuming reconfiguration of the interferometer array and super-synthesis.\n",
      "Using single mode optical fibres for the coherent transport of light from the\n",
      "collecting telescopes to the focal plane, a new generation of interferometers\n",
      "optimized for imaging can be designed. To support this claim, we report on the\n",
      "successful completion of the `OHANA Iki project: an end-to-end, on-sky\n",
      "demonstration of a two-telescope interferometer, built around near-infrared\n",
      "single mode fibres, carried out as part of the `OHANA project. Having\n",
      "demonstrated that coherent transport by single-mode fibres is feasible, we\n",
      "explore the concepts, performances, and limitations of a new imaging facility\n",
      "with single mode fibres at its heart: Agile Guided Interferometer for\n",
      "Longbaseline Imaging Synthesis (AGILIS). AGILIS has the potential of becoming a\n",
      "next generation facility or a precursor to a much larger project like the\n",
      "Planet Formation Imager (PFI). \n",
      "\n",
      "\n",
      "PixelCNN achieves state-of-the-art results in density estimation for natural\n",
      "images. Although training is fast, inference is costly, requiring one network\n",
      "evaluation per pixel; O(N) for N pixels. This can be sped up by caching\n",
      "activations, but still involves generating each pixel sequentially. In this\n",
      "work, we propose a parallelized PixelCNN that allows more efficient inference\n",
      "by modeling certain pixel groups as conditionally independent. Our new PixelCNN\n",
      "model achieves competitive density estimation and orders of magnitude speedup -\n",
      "O(log N) sampling instead of O(N) - enabling the practical generation of\n",
      "512x512 images. We evaluate the model on class-conditional image generation,\n",
      "text-to-image synthesis, and action-conditional video generation, showing that\n",
      "our model achieves the best results among non-pixel-autoregressive density\n",
      "models that allow efficient sampling. \n",
      "\n",
      "\n",
      "The maximum entropy method (MEM) is a well known deconvolution technique in\n",
      "radio-interferometry. This method solves a non-linear optimization problem with\n",
      "an entropy regularization term. Other heuristics such as CLEAN are faster but\n",
      "highly user dependent. Nevertheless, MEM has the following advantages: it is\n",
      "unsupervised, it has a statistical basis, it has a better resolution and better\n",
      "image quality under certain conditions. This work presents a high performance\n",
      "GPU version of non-gridding MEM, which is tested using real and simulated data.\n",
      "We propose a single-GPU and a multi-GPU implementation for single and\n",
      "multi-spectral data, respectively. We also make use of the Peer-to-Peer and\n",
      "Unified Virtual Addressing features of newer GPUs which allows to exploit\n",
      "transparently and efficiently multiple GPUs. Several ALMA data sets are used to\n",
      "demonstrate the effectiveness in imaging and to evaluate GPU performance. The\n",
      "results show that a speedup from 1000 to 5000 times faster than a sequential\n",
      "version can be achieved, depending on data and image size. This allows to\n",
      "reconstruct the HD142527 CO(6-5) short baseline data set in 2.1 minutes,\n",
      "instead of 2.5 days that takes a sequential version on CPU. \n",
      "\n",
      "\n",
      "Adversarial training was recently shown to be competitive against supervised\n",
      "learning methods on computer vision tasks, however, studies have mainly been\n",
      "confined to generative tasks such as image synthesis. In this paper, we apply\n",
      "adversarial training techniques to the discriminative task of learning a\n",
      "steganographic algorithm. Steganography is a collection of techniques for\n",
      "concealing information by embedding it within a non-secret medium, such as\n",
      "cover texts or images. We show that adversarial training can produce robust\n",
      "steganographic techniques: our unsupervised training scheme produces a\n",
      "steganographic algorithm that competes with state-of-the-art steganographic\n",
      "techniques, and produces a robust steganalyzer, which performs the\n",
      "discriminative task of deciding if an image contains secret information. We\n",
      "define a game between three parties, Alice, Bob and Eve, in order to\n",
      "simultaneously train both a steganographic algorithm and a steganalyzer. Alice\n",
      "and Bob attempt to communicate a secret message contained within an image,\n",
      "while Eve eavesdrops on their conversation and attempts to determine if secret\n",
      "information is embedded within the image. We represent Alice, Bob and Eve by\n",
      "neural networks, and validate our scheme on two independent image datasets,\n",
      "showing our novel method of studying steganographic problems is surprisingly\n",
      "competitive against established steganographic techniques. \n",
      "\n",
      "\n",
      "Real-world face recognition using a single sample per person (SSPP) is a\n",
      "challenging task. The problem is exacerbated if the conditions under which the\n",
      "gallery image and the probe set are captured are completely different. To\n",
      "address these issues from the perspective of domain adaptation, we introduce an\n",
      "SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain\n",
      "adaptation, feature extraction, and classification are performed jointly using\n",
      "a deep architecture with domain-adversarial training. However, the SSPP\n",
      "characteristic of one training sample per class is insufficient to train the\n",
      "deep architecture. To overcome this shortage, we generate synthetic images with\n",
      "varying poses using a 3D face model. Experimental evaluations using a realistic\n",
      "SSPP dataset show that deep domain adaptation and image synthesis complement\n",
      "each other and dramatically improve accuracy. Experiments on a benchmark\n",
      "dataset using the proposed approach show state-of-the-art performance. All the\n",
      "dataset and the source code can be found in our online repository\n",
      "(https://github.com/csehong/SSPP-DAN). \n",
      "\n",
      "\n",
      "A general concept of 3D volumetric visualization systems is described based\n",
      "on 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete\n",
      "voxel scene (world) basic elements and main steps of the image synthesis\n",
      "algorithm are formulated. An algorithm for solving the problem of the voxelized\n",
      "world 3D image synthesis, intended for the systems of volumetric spatial\n",
      "visualization, is proposed. A computer-based architecture for 3D volumetric\n",
      "visualization of 3D discrete voxel world is presented. On the basis of the\n",
      "proposed overall concept of discrete voxel representation, the proposed\n",
      "architecture successfully adapts the ray tracing technique for the synthesis of\n",
      "3D volumetric images. Since it is algorithmically simple and effectively\n",
      "supports parallelism, it can efficiently be implemented.\n",
      "  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,\n",
      "discrete voxel world, ray tracing. \n",
      "\n",
      "\n",
      "This paper presents a realization of the approach to spatial 3D stereo of\n",
      "visualization of 3D images with use parallel Graphics processing unit (GPU).\n",
      "The experiments of realization of synthesis of images of a 3D stage by a method\n",
      "of trace of beams on GPU with Compute Unified Device Architecture (CUDA) have\n",
      "shown that 60 % of the time is spent for the decision of a computing problem\n",
      "approximately, the major part of time (40 %) is spent for transfer of data\n",
      "between the central processing unit and GPU for computations and the\n",
      "organization process of visualization. The study of the influence of increase\n",
      "in the size of the GPU network at the speed of computations showed importance\n",
      "of the correct task of structure of formation of the parallel computer network\n",
      "and general mechanism of parallelization. Keywords: Volumetric 3D\n",
      "visualization, stereo 3D visualization, ray tracing, parallel computing on GPU,\n",
      "CUDA \n",
      "\n",
      "\n",
      "Synthesizing images of the eye fundus is a challenging task that has been\n",
      "previously approached by formulating complex models of the anatomy of the eye.\n",
      "New images can then be generated by sampling a suitable parameter space. In\n",
      "this work, we propose a method that learns to synthesize eye fundus images\n",
      "directly from data. For that, we pair true eye fundus images with their\n",
      "respective vessel trees, by means of a vessel segmentation technique. These\n",
      "pairs are then used to learn a mapping from a binary vessel tree to a new\n",
      "retinal image. For this purpose, we use a recent image-to-image translation\n",
      "technique, based on the idea of adversarial learning. Experimental results show\n",
      "that the original and the generated images are visually different in terms of\n",
      "their global appearance, in spite of sharing the same vessel tree.\n",
      "Additionally, a quantitative quality analysis of the synthetic retinal images\n",
      "confirms that the produced images retain a high proportion of the true image\n",
      "set quality. \n",
      "\n",
      "\n",
      "We present a generalization of the Cauchy/Lorentzian, Geman-McClure,\n",
      "Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2\n",
      "loss functions. By introducing robustness as a continuous parameter, our loss\n",
      "function allows algorithms built around robust loss minimization to be\n",
      "generalized, which improves performance on basic vision tasks such as\n",
      "registration and clustering. Interpreting our loss as the negative log of a\n",
      "univariate density yields a general probability distribution that includes\n",
      "normal and Cauchy distributions as special cases. This probabilistic\n",
      "interpretation enables the training of neural networks in which the robustness\n",
      "of the loss automatically adapts itself during training, which improves\n",
      "performance on learning-based tasks such as generative image synthesis and\n",
      "unsupervised monocular depth estimation, without requiring any manual parameter\n",
      "tuning. \n",
      "\n",
      "\n",
      "Geophysical inversion should ideally produce geologically realistic\n",
      "subsurface models that explain the available data. Multiple-point statistics is\n",
      "a geostatistical approach to construct subsurface models that are consistent\n",
      "with site-specific data, but also display the same type of patterns as those\n",
      "found in a training image. The training image can be seen as a conceptual model\n",
      "of the subsurface and is used as a non-parametric model of spatial variability.\n",
      "Inversion based on multiple-point statistics is challenging due to high\n",
      "nonlinearity and time-consuming geostatistical resimulation steps that are\n",
      "needed to create new model proposals. We propose an entirely new model proposal\n",
      "mechanism for geophysical inversion that is inspired by texture synthesis in\n",
      "computer vision. Instead of resimulating pixels based on higher-order patterns\n",
      "in the training image, we identify a suitable patch of the training image that\n",
      "replace a corresponding patch in the current model without breaking the\n",
      "patterns found in the training image, that is, remaining consistent with the\n",
      "given prior. We consider three cross-hole ground-penetrating radar examples in\n",
      "which the new model proposal mechanism is employed within an extended\n",
      "Metropolis Markov chain Monte Carlo (MCMC) inversion. The model proposal step\n",
      "is about 40 times faster than state-of-the-art multiple-point statistics\n",
      "resimulation techniques, the number of necessary MCMC steps is lower and the\n",
      "quality of the final model realizations is of similar quality. The model\n",
      "proposal mechanism is presently limited to 2-D fields, but the method is\n",
      "general and can be applied to a wide range of subsurface settings and\n",
      "geophysical data types. \n",
      "\n",
      "\n",
      "Computed tomography (CT) is critical for various clinical applications, e.g.,\n",
      "radiotherapy treatment planning and also PET attenuation correction. However,\n",
      "CT exposes radiation during acquisition, which may cause side effects to\n",
      "patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and\n",
      "does not involve any radiations. Therefore, recently, researchers are greatly\n",
      "motivated to estimate CT image from its corresponding MR image of the same\n",
      "subject for the case of radiotherapy planning. In this paper, we propose a\n",
      "data-driven approach to address this challenging problem. Specifically, we\n",
      "train a fully convolutional network to generate CT given an MR image. To better\n",
      "model the nonlinear relationship from MRI to CT and to produce more realistic\n",
      "images, we propose to use the adversarial training strategy and an image\n",
      "gradient difference loss function. We further apply AutoContext Model to\n",
      "implement a context-aware generative adversarial network. Experimental results\n",
      "show that our method is accurate and robust for predicting CT images from MRI\n",
      "images, and also outperforms three state-of-the-art methods under comparison. \n",
      "\n",
      "\n",
      "Artistic style transfer is an image synthesis problem where the content of an\n",
      "image is reproduced with the style of another. Recent works show that a\n",
      "visually appealing style transfer can be achieved by using the hidden\n",
      "activations of a pretrained convolutional neural network. However, existing\n",
      "methods either apply (i) an optimization procedure that works for any style\n",
      "image but is very expensive, or (ii) an efficient feedforward network that only\n",
      "allows a limited number of trained styles. In this work we propose a simpler\n",
      "optimization objective based on local matching that combines the content\n",
      "structure and style textures in a single layer of the pretrained network. We\n",
      "show that our objective has desirable properties such as a simpler optimization\n",
      "landscape, intuitive parameter tuning, and consistent frame-by-frame\n",
      "performance on video. Furthermore, we use 80,000 natural images and 80,000\n",
      "paintings to train an inverse network that approximates the result of the\n",
      "optimization. This results in a procedure for artistic style transfer that is\n",
      "efficient but also allows arbitrary content and style images. \n",
      "\n",
      "\n",
      "Synthesizing high-quality images from text descriptions is a challenging\n",
      "problem in computer vision and has many practical applications. Samples\n",
      "generated by existing text-to-image approaches can roughly reflect the meaning\n",
      "of the given descriptions, but they fail to contain necessary details and vivid\n",
      "object parts. In this paper, we propose Stacked Generative Adversarial Networks\n",
      "(StackGAN) to generate 256x256 photo-realistic images conditioned on text\n",
      "descriptions. We decompose the hard problem into more manageable sub-problems\n",
      "through a sketch-refinement process. The Stage-I GAN sketches the primitive\n",
      "shape and colors of the object based on the given text description, yielding\n",
      "Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\n",
      "descriptions as inputs, and generates high-resolution images with\n",
      "photo-realistic details. It is able to rectify defects in Stage-I results and\n",
      "add compelling details with the refinement process. To improve the diversity of\n",
      "the synthesized images and stabilize the training of the conditional-GAN, we\n",
      "introduce a novel Conditioning Augmentation technique that encourages\n",
      "smoothness in the latent conditioning manifold. Extensive experiments and\n",
      "comparisons with state-of-the-arts on benchmark datasets demonstrate that the\n",
      "proposed method achieves significant improvements on generating photo-realistic\n",
      "images conditioned on text descriptions. \n",
      "\n",
      "\n",
      "High dynamic range (HDR) image synthesis from multiple low dynamic range\n",
      "(LDR) exposures continues to be actively researched. The extension to HDR video\n",
      "synthesis is a topic of significant current interest due to potential cost\n",
      "benefits. For HDR video, a stiff practical challenge presents itself in the\n",
      "form of accurate correspondence estimation of objects between video frames. In\n",
      "particular, loss of data resulting from poor exposures and varying intensity\n",
      "make conventional optical flow methods highly inaccurate. We avoid exact\n",
      "correspondence estimation by proposing a statistical approach via maximum a\n",
      "posterior (MAP) estimation, and under appropriate statistical assumptions and\n",
      "choice of priors and models, we reduce it to an optimization problem of solving\n",
      "for the foreground and background of the target frame. We obtain the background\n",
      "through rank minimization and estimate the foreground via a novel multiscale\n",
      "adaptive kernel regression technique, which implicitly captures local structure\n",
      "and temporal motion by solving an unconstrained optimization problem. Extensive\n",
      "experimental results on both real and synthetic datasets demonstrate that our\n",
      "algorithm is more capable of delivering high-quality HDR videos than current\n",
      "state-of-the-art methods, under both subjective and objective assessments.\n",
      "Furthermore, a thorough complexity analysis reveals that our algorithm achieves\n",
      "better complexity-performance trade-off than conventional methods. \n",
      "\n",
      "\n",
      "Recently, there have been several promising methods to generate realistic\n",
      "imagery from deep convolutional networks. These methods sidestep the\n",
      "traditional computer graphics rendering pipeline and instead generate imagery\n",
      "at the pixel level by learning from large collections of photos (e.g. faces or\n",
      "bedrooms). However, these methods are of limited utility because it is\n",
      "difficult for a user to control what the network produces. In this paper, we\n",
      "propose a deep adversarial image synthesis architecture that is conditioned on\n",
      "sketched boundaries and sparse color strokes to generate realistic cars,\n",
      "bedrooms, or faces. We demonstrate a sketch based image synthesis system which\n",
      "allows users to 'scribble' over the sketch to indicate preferred color for\n",
      "objects. Our network can then generate convincing images that satisfy both the\n",
      "color and the sketch constraints of user. The network is feed-forward which\n",
      "allows users to see the effect of their edits in real time. We compare to\n",
      "recent work on sketch to image synthesis and show that our approach can\n",
      "generate more realistic, more diverse, and more controllable outputs. The\n",
      "architecture is also effective at user-guided colorization of grayscale images. \n",
      "\n",
      "\n",
      "Automatic image synthesis research has been rapidly growing with deep\n",
      "networks getting more and more expressive. In the last couple of years, we have\n",
      "observed images of digits, indoor scenes, birds, chairs, etc. being\n",
      "automatically generated. The expressive power of image generators have also\n",
      "been enhanced by introducing several forms of conditioning variables such as\n",
      "object names, sentences, bounding box and key-point locations. In this work, we\n",
      "propose a novel deep conditional generative adversarial network architecture\n",
      "that takes its strength from the semantic layout and scene attributes\n",
      "integrated as conditioning variables. We show that our architecture is able to\n",
      "generate realistic outdoor scene images under different conditions, e.g.\n",
      "day-night, sunny-foggy, with clear object boundaries. \n",
      "\n",
      "\n",
      "Synthesizing high resolution photorealistic images has been a long-standing\n",
      "challenge in machine learning. In this paper we introduce new methods for the\n",
      "improved training of generative adversarial networks (GANs) for image\n",
      "synthesis. We construct a variant of GANs employing label conditioning that\n",
      "results in 128x128 resolution image samples exhibiting global coherence. We\n",
      "expand on previous work for image quality assessment to provide two new\n",
      "analyses for assessing the discriminability and diversity of samples from\n",
      "class-conditional image synthesis models. These analyses demonstrate that high\n",
      "resolution samples provide class information not present in low resolution\n",
      "samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as\n",
      "discriminable as artificially resized 32x32 samples. In addition, 84.7% of the\n",
      "classes have samples exhibiting diversity comparable to real ImageNet data. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have recently demonstrated the\n",
      "capability to synthesize compelling real-world images, such as room interiors,\n",
      "album covers, manga, faces, birds, and flowers. While existing models can\n",
      "synthesize images based on global constraints such as a class label or caption,\n",
      "they do not provide control over pose or object location. We propose a new\n",
      "model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes\n",
      "images given instructions describing what content to draw in which location. We\n",
      "show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset,\n",
      "conditioned on both informal text descriptions and also object location. Our\n",
      "system exposes control over both the bounding box around the bird and its\n",
      "constituent parts. By modeling the conditional distributions over part\n",
      "locations, our system also enables conditioning on arbitrary subsets of parts\n",
      "(e.g. only the beak and tail), yielding an efficient interface for picking part\n",
      "locations. We also show preliminary results on the more challenging domain of\n",
      "text- and location-controllable synthesis of images of human actions on the\n",
      "MPII Human Pose dataset. \n",
      "\n",
      "\n",
      "Image and texture synthesis is a challenging task that has long been drawing\n",
      "attention in the fields of image processing, graphics, and machine learning.\n",
      "This problem consists of modelling the desired type of images, either through\n",
      "training examples or via a parametric modeling, and then generating images that\n",
      "belong to the same statistical origin.\n",
      "  This work addresses the image synthesis task, focusing on two specific\n",
      "families of images -- handwritten digits and face images. This paper offers two\n",
      "main contributions. First, we suggest a simple and intuitive algorithm capable\n",
      "of generating such images in a unified way. The proposed approach taken is\n",
      "pyramidal, consisting of upscaling and refining the estimated image several\n",
      "times. For each upscaling stage, the algorithm randomly draws small patches\n",
      "from a patch database, and merges these to form a coherent and novel image with\n",
      "high visual quality. The second contribution is a general framework for the\n",
      "evaluation of the generation performance, which combines three aspects: the\n",
      "likelihood, the originality and the spread of the synthesized images. We assess\n",
      "the proposed synthesis scheme and show that the results are similar in nature,\n",
      "and yet different from the ones found in the training set, suggesting that true\n",
      "synthesis effect has been obtained. \n",
      "\n",
      "\n",
      "Automatic synthesis of realistic images from text would be interesting and\n",
      "useful, but current AI systems are still far from this goal. However, in recent\n",
      "years generic and powerful recurrent neural network architectures have been\n",
      "developed to learn discriminative text feature representations. Meanwhile, deep\n",
      "convolutional generative adversarial networks (GANs) have begun to generate\n",
      "highly compelling images of specific categories, such as faces, album covers,\n",
      "and room interiors. In this work, we develop a novel deep architecture and GAN\n",
      "formulation to effectively bridge these advances in text and image model- ing,\n",
      "translating visual concepts from characters to pixels. We demonstrate the\n",
      "capability of our model to generate plausible images of birds and flowers from\n",
      "detailed text descriptions. \n",
      "\n",
      "\n",
      "Shadows often create unwanted artifacts in photographs, and removing them can\n",
      "be very challenging. Previous shadow removal methods often produce de-shadowed\n",
      "regions that are visually inconsistent with the rest of the image. In this work\n",
      "we propose a fully automatic shadow region harmonization approach that improves\n",
      "the appearance compatibility of the de-shadowed region as typically produced by\n",
      "previous methods. It is based on a shadow-guided patch-based image synthesis\n",
      "approach that reconstructs the shadow region using patches sampled from\n",
      "non-shadowed regions. The result is then refined based on the reconstruction\n",
      "confidence to handle unique image patterns. Many shadow removal results and\n",
      "comparisons are show the effectiveness of our improvement. Quantitative\n",
      "evaluation on a benchmark dataset suggests that our automatic shadow\n",
      "harmonization approach effectively improves upon the state-of-the-art. \n",
      "\n",
      "\n",
      "In computer vision, convolutional neural networks (CNNs) have recently\n",
      "achieved new levels of performance for several inverse problems where RGB pixel\n",
      "appearance is mapped to attributes such as positions, normals or reflectance.\n",
      "In computer graphics, screen-space shading has recently increased the visual\n",
      "quality in interactive image synthesis, where per-pixel attributes such as\n",
      "positions, normals or reflectance of a virtual 3D scene are converted into RGB\n",
      "pixel appearance, enabling effects like ambient occlusion, indirect light,\n",
      "scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we\n",
      "consider the diagonal problem: synthesizing appearance from given per-pixel\n",
      "attributes using a CNN. The resulting Deep Shading simulates various\n",
      "screen-space effects at competitive quality and speed while not being\n",
      "programmed by human experts but learned from example images. \n",
      "\n",
      "\n",
      "Convolutional neural networks (CNNs) have proven highly effective at image\n",
      "synthesis and style transfer. For most users, however, using them as tools can\n",
      "be a challenging task due to their unpredictable behavior that goes against\n",
      "common intuitions. This paper introduces a novel concept to augment such\n",
      "generative architectures with semantic annotations, either by manually\n",
      "authoring pixel labels or using existing solutions for semantic segmentation.\n",
      "The result is a content-aware generative algorithm that offers meaningful\n",
      "control over the outcome. Thus, we increase the quality of images generated by\n",
      "avoiding common glitches, make the results look significantly more plausible,\n",
      "and extend the functional range of these algorithms---whether for portraits or\n",
      "landscapes, etc. Applications include semantic style transfer and turning\n",
      "doodles with few colors into masterful paintings! \n",
      "\n",
      "\n",
      "Most signal processing problems involve the challenging task of\n",
      "multidimensional probability density function (PDF) estimation. In this work,\n",
      "we propose a solution to this problem by using a family of Rotation-based\n",
      "Iterative Gaussianization (RBIG) transforms. The general framework consists of\n",
      "the sequential application of a univariate marginal Gaussianization transform\n",
      "followed by an orthonormal transform. The proposed procedure looks for\n",
      "differentiable transforms to a known PDF so that the unknown PDF can be\n",
      "estimated at any point of the original domain. In particular, we aim at a zero\n",
      "mean unit covariance Gaussian for convenience. RBIG is formally similar to\n",
      "classical iterative Projection Pursuit (PP) algorithms. However, we show that,\n",
      "unlike in PP methods, the particular class of rotations used has no special\n",
      "qualitative relevance in this context, since looking for interestingness is not\n",
      "a critical issue for PDF estimation. The key difference is that our approach\n",
      "focuses on the univariate part (marginal Gaussianization) of the problem rather\n",
      "than on the multivariate part (rotation). This difference implies that one may\n",
      "select the most convenient rotation suited to each practical application. The\n",
      "differentiability, invertibility and convergence of RBIG are theoretically and\n",
      "experimentally analyzed. Relation to other methods, such as Radial\n",
      "Gaussianization (RG), one-class support vector domain description (SVDD), and\n",
      "deep neural networks (DNN) is also pointed out. The practical performance of\n",
      "RBIG is successfully illustrated in a number of multidimensional problems such\n",
      "as image synthesis, classification, denoising, and multi-information\n",
      "estimation. \n",
      "\n",
      "\n",
      "This paper studies a combination of generative Markov random field (MRF)\n",
      "models and discriminatively trained deep convolutional neural networks (dCNNs)\n",
      "for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN\n",
      "feature pyramid, controling the image layout at an abstract level. We apply the\n",
      "method to both photographic and non-photo-realistic (artwork) synthesis tasks.\n",
      "The MRF regularizer prevents over-excitation artifacts and reduces implausible\n",
      "feature mixtures common to previous dCNN inversion approaches, permitting\n",
      "synthezing photographic content with increased visual plausibility. Unlike\n",
      "standard MRF-based texture synthesis, the combined system can both match and\n",
      "adapt local features with considerable variability, yielding results far out of\n",
      "reach of classic generative MRF methods. \n",
      "\n",
      "\n",
      "We investigate the upper chromosphere and the transition region of the\n",
      "sunspot umbra using the radio brightness temperature at 34 GHz (corresponding\n",
      "to 8.8-mm observations) as observed by the Nobeyama Radioheliograph (NoRH).\n",
      "Radio free-free emission in the longer millimeter range is generated around the\n",
      "transition region, and its brightness temperature yields the region's\n",
      "temperature and density distribution. We use the NoRH data at 34 GHz by\n",
      "applying the Steer-CLEAN image synthesis. These data and the analysis method\n",
      "enable us to investigate the chromospheric structures in the longer millimeter\n",
      "range with high spatial resolution and sufficient visibilities. We also perform\n",
      "simultaneous observations of one sunspot using the NoRH and the Nobeyama 45-m\n",
      "telescope operating at 115 GHz. We determine that 115-GHz emission mainly\n",
      "originates from the lower chromosphere while 34-GHz emission mainly originates\n",
      "from the upper chromosphere and transition region. These observational results\n",
      "are consistent with the radio emission characteristics estimated from the\n",
      "current atmospheric models of the chromosphere. On the other hand, the observed\n",
      "brightness temperature of the umbral region is almost the same as that of the\n",
      "quiet region. This result is inconsistent with the current sunspot models,\n",
      "which predict a considerably higher brightness temperature of the sunspot umbra\n",
      "at 34 GHz. This inconsistency suggests that the temperature of the region at\n",
      "which the 34 GHz radio emission becomes optically thick should be lower than\n",
      "that predicted by the models. \n",
      "\n",
      "\n",
      "Deep networks are increasingly being applied to problems involving image\n",
      "synthesis, e.g., generating images from textual descriptions and reconstructing\n",
      "an input image from a compact representation. Supervised training of\n",
      "image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the\n",
      "mismatch between a generated image and its corresponding target image. We\n",
      "propose instead to use a loss function that is better calibrated to human\n",
      "perceptual judgments of image quality: the multiscale structural-similarity\n",
      "score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated\n",
      "into gradient-descent learning. We compare the consequences of using MS-SSIM\n",
      "versus PL loss on training deterministic and stochastic autoencoders. For three\n",
      "different architectures, we collected human judgments of the quality of image\n",
      "reconstructions. Observers reliably prefer images synthesized by\n",
      "MS-SSIM-optimized models over those synthesized by PL-optimized models, for two\n",
      "distinct PL measures ($\\ell_1$ and $\\ell_2$ distances). We also explore the\n",
      "effect of training objective on image encoding and analyze conditions under\n",
      "which perceptually-optimized representations yield better performance on image\n",
      "classification. Finally, we demonstrate the superiority of\n",
      "perceptually-optimized networks for super-resolution imaging. Just as computer\n",
      "vision has advanced through the use of convolutional architectures that mimic\n",
      "the structure of the mammalian visual system, we argue that significant\n",
      "additional advances can be made in modeling images through the use of training\n",
      "objectives that are well aligned to characteristics of human perception. \n",
      "\n",
      "\n",
      "In preparing the way for the Square Kilometre Array and its pathfinders,\n",
      "there is a pressing need to begin probing the transient sky in a fully robotic\n",
      "fashion using the current generation of radio telescopes. Effective\n",
      "exploitation of such surveys requires a largely automated data-reduction\n",
      "process. This paper introduces an end-to-end automated reduction pipeline,\n",
      "AMIsurvey, used for calibrating and imaging data from the Arcminute Microkelvin\n",
      "Imager Large Array. AMIsurvey makes use of several component libraries which\n",
      "have been packaged separately for open-source release. The most scientifically\n",
      "significant of these is chimenea, which implements a telescope-agnostic\n",
      "algorithm for automated imaging of pre-calibrated multi-epoch radio-synthesis\n",
      "data, of the sort typically acquired for transient surveys or follow-up. The\n",
      "algorithm aims to improve upon standard imaging pipelines by utilizing\n",
      "iterative RMS-estimation and automated source-detection to avoid so called\n",
      "`Clean-bias', and makes use of CASA subroutines for the underlying\n",
      "image-synthesis operations. At a lower level, AMIsurvey relies upon two\n",
      "libraries, drive-ami and drive-casa, built to allow use of mature\n",
      "radio-astronomy software packages from within Python scripts. While targeted at\n",
      "automated imaging, the drive-casa interface can also be used to automate\n",
      "interaction with any of the CASA subroutines from a generic Python process.\n",
      "Additionally, these packages may be of wider technical interest beyond\n",
      "radio-astronomy, since they demonstrate use of the Python library pexpect to\n",
      "emulate terminal interaction with an external process. This approach allows for\n",
      "rapid development of a Python interface to any legacy or externally-maintained\n",
      "pipeline which accepts command-line input, without requiring alterations to the\n",
      "original code. \n",
      "\n",
      "\n",
      "Object viewpoint estimation from 2D images is an essential task in computer\n",
      "vision. However, two issues hinder its progress: scarcity of training data with\n",
      "viewpoint annotations, and a lack of powerful features. Inspired by the growing\n",
      "availability of 3D models, we propose a framework to address both issues by\n",
      "combining render-based image synthesis and CNNs. We believe that 3D models have\n",
      "the potential in generating a large number of images of high variation, which\n",
      "can be well exploited by deep CNN with a high learning capacity. Towards this\n",
      "goal, we propose a scalable and overfit-resistant image synthesis pipeline,\n",
      "together with a novel CNN specifically tailored for the viewpoint estimation\n",
      "task. Experimentally, we show that the viewpoint estimation from our pipeline\n",
      "can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark. \n",
      "\n",
      "\n",
      "Compressive displays are an emerging technology exploring the co-design of\n",
      "new optical device configurations and compressive computation. Previously,\n",
      "research has shown how to improve the dynamic range of displays and facilitate\n",
      "high-quality light field or glasses-free 3D image synthesis. In this paper, we\n",
      "introduce a new multi-mode compressive display architecture that supports\n",
      "switching between 3D and high dynamic range (HDR) modes as well as a new\n",
      "super-resolution mode. The proposed hardware consists of readily-available\n",
      "components and is driven by a novel splitting algorithm that computes the pixel\n",
      "states from a target high-resolution image. In effect, the display pixels\n",
      "present a compressed representation of the target image that is perceived as a\n",
      "single, high resolution image. \n",
      "\n",
      "\n",
      "A radio interferometer indirectly measures the intensity distribution of the\n",
      "sky over the celestial sphere. Since measurements are made over an irregularly\n",
      "sampled Fourier plane, synthesising an intensity image from interferometric\n",
      "measurements requires substantial processing. Furthermore there are distortions\n",
      "that have to be corrected. In this thesis, a new high-performance image\n",
      "synthesis tool (imaging tool) for radio interferometry is developed.\n",
      "Implemented in C++ and CUDA, the imaging tool achieves unprecedented\n",
      "performance by means of Graphics Processing Units (GPUs). The imaging tool is\n",
      "divided into several components, and the back-end handling numerical\n",
      "calculations is generalised in a new framework. A new feature termed\n",
      "compression arbitrarily increases the performance of an already highly\n",
      "efficient GPU-based implementation of the w-projection algorithm. Compression\n",
      "takes advantage of the behaviour of oversampled convolution functions and the\n",
      "baseline trajectories. A CPU-based component prepares data for the GPU which is\n",
      "multi-threaded to ensure maximum use of modern multi-core CPUs. Best\n",
      "performance can only be achieved if all hardware components in a system do work\n",
      "in parallel. The imaging tool is designed such that disk I/O and work on CPU\n",
      "and GPUs is done concurrently. Test cases show that the imaging tool performs\n",
      "nearly 100$\\times$ faster than another general CPU-based imaging tool.\n",
      "Unfortunately, the tool is limited in use since deconvolution and A-projection\n",
      "are not yet supported. It is also limited by GPU memory. Future work will\n",
      "implement deconvolution and A-projection, whilst finding ways of overcoming the\n",
      "memory limitation. \n",
      "\n",
      "\n",
      "CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful\n",
      "technique for tensor completion through explicitly capturing the multilinear\n",
      "latent factors. The existing CP algorithms require the tensor rank to be\n",
      "manually specified, however, the determination of tensor rank remains a\n",
      "challenging problem especially for CP rank. In addition, existing approaches do\n",
      "not take into account uncertainty information of latent factors, as well as\n",
      "missing entries. To address these issues, we formulate CP factorization using a\n",
      "hierarchical probabilistic model and employ a fully Bayesian treatment by\n",
      "incorporating a sparsity-inducing prior over multiple latent factors and the\n",
      "appropriate hyperpriors over all hyperparameters, resulting in automatic rank\n",
      "determination. To learn the model, we develop an efficient deterministic\n",
      "Bayesian inference algorithm, which scales linearly with data size. Our method\n",
      "is characterized as a tuning parameter-free approach, which can effectively\n",
      "infer underlying multilinear factors with a low-rank constraint, while also\n",
      "providing predictive distributions over missing entries. Extensive simulations\n",
      "on synthetic data illustrate the intrinsic capability of our method to recover\n",
      "the ground-truth of CP rank and prevent the overfitting problem, even when a\n",
      "large amount of entries are missing. Moreover, the results from real-world\n",
      "applications, including image inpainting and facial image synthesis,\n",
      "demonstrate that our method outperforms state-of-the-art approaches for both\n",
      "tensor factorization and tensor completion in terms of predictive performance. \n",
      "\n",
      "\n",
      "The digital revolution is transforming astronomy from a data-starved to a\n",
      "data-submerged science. Instruments such as the Atacama Large Millimeter Array\n",
      "(ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer\n",
      "Array (SKA) will measure their accumulated data in petabytes. The capacity to\n",
      "produce enormous volumes of data must be matched with the computing power to\n",
      "process that data and produce meaningful results. In addition to handling huge\n",
      "data rates, we need adaptive calibration and beamforming to handle atmospheric\n",
      "fluctuations and radio frequency interference, and to provide a user\n",
      "environment which makes the full power of large telescope arrays accessible to\n",
      "both expert and non-expert users. Delayed calibration and analysis limit the\n",
      "science which can be done. To make the best use of both telescope and human\n",
      "resources we must reduce the burden of data reduction.\n",
      "  Our instrumentation comprises of a flexible correlator, beam former and\n",
      "imager with digital signal processing closely coupled with a computing cluster.\n",
      "This instrumentation will be highly accessible to scientists, engineers, and\n",
      "students for research and development of real-time processing algorithms, and\n",
      "will tap into the pool of talented and innovative students and visiting\n",
      "scientists from engineering, computing, and astronomy backgrounds.\n",
      "  Adaptive real-time imaging will transform radio astronomy by providing\n",
      "real-time feedback to observers. Calibration of the data is made in close to\n",
      "real time using a model of the sky brightness distribution. The derived\n",
      "calibration parameters are fed back into the imagers and beam formers. The\n",
      "regions imaged are used to update and improve the a-priori model, which becomes\n",
      "the final calibrated image by the time the observations are complete. \n",
      "\n",
      "\n",
      "We consider the problem of multifrequency VLBA image synthesis and\n",
      "spectral-index mapping for active galactic nuclei related to the necessity of\n",
      "taking into account the frequency-dependent image shift. We describe our\n",
      "generalized multifrequency synthesis algorithm with a spectral correction based\n",
      "on the maximum entropy method. The results of our processing of multifrequency\n",
      "VLBI data for the radio sources J2202+4216, J0336+3218, and J1419+5423 are\n",
      "presented. \n",
      "\n",
      "\n",
      "Context. The star HD 87643, exhibiting the \"B[e] phenomenon\", has one of the\n",
      "most extreme infrared excesses for this object class. It harbours a large\n",
      "amount of both hot and cold dust, and is surrounded by an extended reflection\n",
      "nebula. Aims. One of our major goals was to investigate the presence of a\n",
      "companion in HD87643. In addition, the presence of close dusty material was\n",
      "tested through a combination of multi-wavelength high spatial 5Aresolution\n",
      "observations. Methods. We observed HD 87643 with high spatial resolution\n",
      "techniques, using the near-IR AMBER/VLTI interferometer with baselines ranging\n",
      "from 60 m to 130 m and the mid-IR MIDI/VLTI interferometer with baselines\n",
      "ranging from 25 m to 65 m. These observations are complemented by NACO/VLT\n",
      "adaptive-optics-corrected images in the K and L-bands, ESO-2.2m optical\n",
      "Wide-Field Imager large-scale images in the B, V and R-bands, Results. We\n",
      "report the direct detection of a companion to HD 87643 by means of image\n",
      "synthesis using the AMBER/VLTI instrument. The presence of the companion is\n",
      "confirmed by the MIDI and NACO data, although with a lower confidence. The\n",
      "companion is separated by ~ 34 mas with a roughly north-south orientation. The\n",
      "period must be large (several tens of years) and hence the orbital parameters\n",
      "are not determined yet. Binarity with high eccentricity might be the key to\n",
      "interpreting the extreme characteristics of this system, namely a dusty\n",
      "circumstellar envelope around the primary, a compact dust nebulosity around the\n",
      "binary system and a complex extended nebula witnessing past violent ejections. \n",
      "\n",
      "\n",
      "The implementation of the simultaneous combination of several telescopes\n",
      "(from four to eight) available at Very Large Telescope Interferometer (VLTI)\n",
      "will allow the new generation interferometric instrumentation to achieve\n",
      "interferometric image synthesis with unprecedented resolution and efficiency.\n",
      "The VLTI Spectro Imager (VSI) is the proposed second-generation near-infrared\n",
      "multi-beam instrument for the Very Large Telescope Interferometer, featuring\n",
      "three band operations (J, H and K), high angular resolutions (down to 1.1\n",
      "milliarcsecond) and high spectral resolutions. VSI will be equipped with its\n",
      "own internal Fringe Tracker (FT), which will measure and compensate the\n",
      "atmospheric perturbations to the relative beam phase, and in turn will provide\n",
      "stable and prolonged observing conditions down to the magnitude K=13 for the\n",
      "scientific combiner. In its baseline configuration, VSI FT is designed to\n",
      "implement, from the very start, the minimum redundancy combination in a nearest\n",
      "neighbor scheme of six telescopes over six baselines, thus offering better\n",
      "options for rejection of large intensity or phase fluctuations over each beam,\n",
      "due to the symmetric set-up. The planar geometry solution of the FT beam\n",
      "combiner is devised to be easily scalable either to four or eight telescopes,\n",
      "in accordance to the three phase development considered for VSI. The proposed\n",
      "design, based on minimum redundancy combination and bulk optics solution, is\n",
      "described in terms of opto-mechanical concept, performance and key operational\n",
      "aspects. \n",
      "\n",
      "\n",
      "VLTi Spectro-Imager (VSI) is a proposition for a second generation VLTI\n",
      "instrument which is aimed at providing the ESO community with the capability of\n",
      "performing image synthesis at milli-arcsecond angular resolution. VSI provides\n",
      "the VLTI with an instrument able to combine 4 telescopes in a baseline version\n",
      "and optionally up to 6 telescopes in the near-infrared spectral domain with\n",
      "moderate to high spectral resolution. The instrument contains its own fringe\n",
      "tracker in order to relax the constraints onto the VLTI infrastructure. VSI\n",
      "will do imaging at the milli-arcsecond scale with spectral resolution of: a)\n",
      "the close environments of young stars probing the initial conditions for planet\n",
      "formation; b) the surfaces of stars; c) the environment of evolved stars,\n",
      "stellar remnants and stellar winds, and d) the central region of active\n",
      "galactic nuclei and supermassive black holes. The science cases allowed us to\n",
      "specify the astrophysical requirements of the instrument and to define the\n",
      "necessary studies of the science group for phase A. \n",
      "\n",
      "\n",
      "We present a method to accelerate global illumination computation in dynamic\n",
      "environments by taking advantage of limitations of the human visual system. A\n",
      "model of visual attention is used to locate regions of interest in a scene and\n",
      "to modulate spatiotemporal sensitivity. The method is applied in the form of a\n",
      "spatiotemporal error tolerance map. Perceptual acceleration combined with good\n",
      "sampling protocols provide a global illumination solution feasible for use in\n",
      "animation. Results indicate an order of magnitude improvement in computational\n",
      "speed. The method is adaptable and can also be used in image-based rendering,\n",
      "geometry level of detail selection, realistic image synthesis, video telephony\n",
      "and video compression. \n",
      "\n",
      "\n",
      "An approach to description of interactive psychoinformation systems, based on\n",
      "the concept of \"virtualization\" and essentially using the technique of the\n",
      "\"secondary image synthesis\" (adap-org/9409002), is sketched. The article has a\n",
      "rather discussional character and maybe considered as a comment to some\n",
      "statements of the introduction to the author's paper \"Complex projective\n",
      "geometry and quantum projective field theory\" (Theor. Math. Phys., 1994). \n",
      "\n",
      "\n",
      "The principal scheme of the secondary image synthesis in electronic computer\n",
      "photography is described. The crucial role of the Clebsch-Gordan coefficient\n",
      "calculus is pointed out. \n",
      "\n",
      "\n",
      "Text-to-image generation models have recently attracted unprecedented\n",
      "attention as they unlatch imaginative applications in all areas of life.\n",
      "However, developing such models requires huge amounts of data that might\n",
      "contain privacy-sensitive information, e.g., face identity. While privacy risks\n",
      "have been extensively demonstrated in the image classification and GAN\n",
      "generation domains, privacy risks in the text-to-image generation domain are\n",
      "largely unexplored. In this paper, we perform the first privacy analysis of\n",
      "text-to-image generation models through the lens of membership inference.\n",
      "Specifically, we propose three key intuitions about membership information and\n",
      "design four attack methodologies accordingly. We conduct comprehensive\n",
      "evaluations on two mainstream text-to-image generation models including\n",
      "sequence-to-sequence modeling and diffusion-based modeling. The empirical\n",
      "results show that all of the proposed attacks can achieve significant\n",
      "performance, in some cases even close to an accuracy of 1, and thus the\n",
      "corresponding risk is much more severe than that shown by existing membership\n",
      "inference attacks. We further conduct an extensive ablation study to analyze\n",
      "the factors that may affect the attack performance, which can guide developers\n",
      "and researchers to be alert to vulnerabilities in text-to-image generation\n",
      "models. All these findings indicate that our proposed attacks pose a realistic\n",
      "privacy threat to the text-to-image generation models. \n",
      "\n",
      "\n",
      "Designing proper training pairs is critical for super-resolving the\n",
      "real-world low-quality (LQ) images, yet suffers from the difficulties in either\n",
      "acquiring paired ground-truth HQ images or synthesizing photo-realistic\n",
      "degraded observations. Recent works mainly circumvent this by simulating the\n",
      "degradation with handcrafted or estimated degradation parameters. However,\n",
      "existing synthetic degradation models are incapable to model complicated real\n",
      "degradation types, resulting in limited improvement on these scenarios, \\eg,\n",
      "old photos. Notably, face images, which have the same degradation process with\n",
      "the natural images, can be robustly restored with photo-realistic textures by\n",
      "exploiting their specific structure priors. In this work, we use these\n",
      "real-world LQ face images and their restored HQ counterparts to model the\n",
      "complex real degradation (namely ReDegNet), and then transfer it to HQ natural\n",
      "images to synthesize their realistic LQ ones. Specifically, we take these\n",
      "paired HQ and LQ face images as inputs to explicitly predict the\n",
      "degradation-aware and content-independent representations, which control the\n",
      "degraded image generation. Subsequently, we transfer these real degradation\n",
      "representations from face to natural images to synthesize the degraded LQ\n",
      "natural images. Experiments show that our ReDegNet can well learn the real\n",
      "degradation process from face images, and the restoration network trained with\n",
      "our synthetic pairs performs favorably against SOTAs. More importantly, our\n",
      "method provides a new manner to handle the unsynthesizable real-world scenarios\n",
      "by learning their degradation representations through face images within them,\n",
      "which can be used for specifically fine-tuning. The source code is available at\n",
      "https://github.com/csxmli2016/ReDegNet. \n",
      "\n",
      "\n",
      "High-quality face images are required to guarantee the stability and\n",
      "reliability of automatic face recognition (FR) systems in surveillance and\n",
      "security scenarios. However, a massive amount of face data is usually\n",
      "compressed before being analyzed due to limitations on transmission or storage.\n",
      "The compressed images may lose the powerful identity information, resulting in\n",
      "the performance degradation of the FR system. Herein, we make the first attempt\n",
      "to study just noticeable difference (JND) for the FR system, which can be\n",
      "defined as the maximum distortion that the FR system cannot notice. More\n",
      "specifically, we establish a JND dataset including 3530 original images and\n",
      "137,670 compressed images generated by advanced reference encoding/decoding\n",
      "software based on the Versatile Video Coding (VVC) standard (VTM-15.0).\n",
      "Subsequently, we develop a novel JND prediction model to directly infer JND\n",
      "images for the FR system. In particular, in order to maximum redundancy removal\n",
      "without impairment of robust identity information, we apply the encoder with\n",
      "multiple feature extraction and attention-based feature decomposition modules\n",
      "to progressively decompose face features into two uncorrelated components,\n",
      "i.e., identity and residual features, via self-supervised learning. Then, the\n",
      "residual feature is fed into the decoder to generate the residual map. Finally,\n",
      "the predicted JND map is obtained by subtracting the residual map from the\n",
      "original image. Experimental results have demonstrated that the proposed model\n",
      "achieves higher accuracy of JND map prediction compared with the\n",
      "state-of-the-art JND models, and is capable of saving more bits while\n",
      "maintaining the performance of the FR system compared with VTM-15.0. \n",
      "\n",
      "\n",
      "3D-aware GANs based on generative neural radiance fields (GNeRF) have\n",
      "achieved impressive high-quality image generation, while preserving strong 3D\n",
      "consistency. The most notable achievements are made in the face generation\n",
      "domain. However, most of these models focus on improving view consistency but\n",
      "neglect a disentanglement aspect, thus these models cannot provide high-quality\n",
      "semantic/attribute control over generation. To this end, we introduce a\n",
      "conditional GNeRF model that uses specific attribute labels as input in order\n",
      "to improve the controllabilities and disentangling abilities of 3D-aware\n",
      "generative models. We utilize the pre-trained 3D-aware model as the basis and\n",
      "integrate a dual-branches attribute-editing module (DAEM), that utilize\n",
      "attribute labels to provide control over generation. Moreover, we propose a\n",
      "TRIOT (TRaining as Init, and Optimizing for Tuning) method to optimize the\n",
      "latent vector to improve the precision of the attribute-editing further.\n",
      "Extensive experiments on the widely used FFHQ show that our model yields\n",
      "high-quality editing with better view consistency while preserving the\n",
      "non-target regions. The code is available at\n",
      "https://github.com/zhangqianhui/TT-GNeRF. \n",
      "\n",
      "\n",
      "3D-controllable portrait synthesis has significantly advanced, thanks to\n",
      "breakthroughs in generative adversarial networks (GANs). However, it is still\n",
      "challenging to manipulate existing face images with precise 3D control. While\n",
      "concatenating GAN inversion and a 3D-aware, noise-to-image GAN is a\n",
      "straight-forward solution, it is inefficient and may lead to noticeable drop in\n",
      "editing quality. To fill this gap, we propose 3D-FM GAN, a novel conditional\n",
      "GAN framework designed specifically for 3D-controllable face manipulation, and\n",
      "does not require any tuning after the end-to-end learning phase. By carefully\n",
      "encoding both the input face image and a physically-based rendering of 3D edits\n",
      "into a StyleGAN's latent spaces, our image generator provides high-quality,\n",
      "identity-preserved, 3D-controllable face manipulation. To effectively learn\n",
      "such novel framework, we develop two essential training strategies and a novel\n",
      "multiplicative co-modulation architecture that improves significantly upon\n",
      "naive schemes. With extensive evaluations, we show that our method outperforms\n",
      "the prior arts on various tasks, with better editability, stronger identity\n",
      "preservation, and higher photo-realism. In addition, we demonstrate a better\n",
      "generalizability of our design on large pose editing and out-of-domain images. \n",
      "\n",
      "\n",
      "Realistic generative face video synthesis has long been a pursuit in both\n",
      "computer vision and graphics community. However, existing face video generation\n",
      "methods tend to produce low-quality frames with drifted facial identities and\n",
      "unnatural movements. To tackle these challenges, we propose a principled\n",
      "framework named StyleFaceV, which produces high-fidelity identity-preserving\n",
      "face videos with vivid movements. Our core insight is to decompose appearance\n",
      "and pose information and recompose them in the latent space of StyleGAN3 to\n",
      "produce stable and dynamic results. Specifically, StyleGAN3 provides strong\n",
      "priors for high-fidelity facial image generation, but the latent space is\n",
      "intrinsically entangled. By carefully examining its latent properties, we\n",
      "propose our decomposition and recomposition designs which allow for the\n",
      "disentangled combination of facial appearance and movements. Moreover, a\n",
      "temporal-dependent model is built upon the decomposed latent features, and\n",
      "samples reasonable sequences of motions that are capable of generating\n",
      "realistic and temporally coherent face videos. Particularly, our pipeline is\n",
      "trained with a joint training strategy on both static images and high-quality\n",
      "video data, which is of higher data efficiency. Extensive experiments\n",
      "demonstrate that our framework achieves state-of-the-art face video generation\n",
      "results both qualitatively and quantitatively. Notably, StyleFaceV is capable\n",
      "of generating realistic $1024\\times1024$ face videos even without\n",
      "high-resolution training videos. \n",
      "\n",
      "\n",
      "In recent years, image generation has made great strides in improving the\n",
      "quality of images, producing high-fidelity ones. Also, quite recently, there\n",
      "are architecture designs, which enable GAN to unsupervisedly learn the semantic\n",
      "attributes represented in different layers. However, there is still a lack of\n",
      "research on generating face images more consistent with human aesthetics. Based\n",
      "on EigenGAN [He et al., ICCV 2021], we build the techniques of reinforcement\n",
      "learning into the generator of EigenGAN. The agent tries to figure out how to\n",
      "alter the semantic attributes of the generated human faces towards more\n",
      "preferable ones. To accomplish this, we trained an aesthetics scoring model\n",
      "that can conduct facial beauty prediction. We also can utilize this scoring\n",
      "model to analyze the correlation between face attributes and aesthetics scores.\n",
      "Empirically, using off-the-shelf techniques from reinforcement learning would\n",
      "not work well. So instead, we present a new variant incorporating the\n",
      "ingredients emerging in the reinforcement learning communities in recent years.\n",
      "Compared to the original generated images, the adjusted ones show clear\n",
      "distinctions concerning various attributes. Experimental results using the\n",
      "MindSpore, show the effectiveness of the proposed method. Altered facial images\n",
      "are commonly more attractive, with significantly improved aesthetic levels. \n",
      "\n",
      "\n",
      "As a general type of machine learning approach, artificial neural networks\n",
      "have established state-of-art benchmarks in many pattern recognition and data\n",
      "analysis tasks. Among various kinds of neural networks architectures,\n",
      "polynomial neural networks (PNNs) have been recently shown to be analyzable by\n",
      "spectrum analysis via neural tangent kernel, and particularly effective at\n",
      "image generation and face recognition. However, acquiring theoretical insight\n",
      "into the computation and sample complexity of PNNs remains an open problem. In\n",
      "this paper, we extend the analysis in previous literature to PNNs and obtain\n",
      "novel results on sample complexity of PNNs, which provides some insights in\n",
      "explaining the generalization ability of PNNs. \n",
      "\n",
      "\n",
      "Sketching is an intuitive and effective way for content creation. While\n",
      "significant progress has been made for photorealistic image generation by using\n",
      "generative adversarial networks, it remains challenging to take a fine-grained\n",
      "control on synthetic content. The instance normalization layer, which is widely\n",
      "adopted in existing image translation networks, washes away details in the\n",
      "input sketch and leads to loss of precise control on the desired shape of the\n",
      "generated face images. In this paper, we comprehensively investigate the effect\n",
      "of instance normalization on generating photorealistic face images from\n",
      "hand-drawn sketches. We first introduce a visualization approach to analyze the\n",
      "feature embedding for sketches with a group of specific changes. Based on the\n",
      "visual analysis, we modify the instance normalization layers in the baseline\n",
      "image translation model. We elaborate a new set of hand-drawn sketches with 11\n",
      "categories of specially designed changes and conduct extensive experimental\n",
      "analysis. The results and user studies demonstrate that our method markedly\n",
      "improve the quality of synthesized images and the conformance with user\n",
      "intention. \n",
      "\n",
      "\n",
      "Existing few-shot image generation approaches typically employ fusion-based\n",
      "strategies, either on the image or the feature level, to produce new images.\n",
      "However, previous approaches struggle to synthesize high-frequency signals with\n",
      "fine details, deteriorating the synthesis quality. To address this, we propose\n",
      "WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we\n",
      "disentangle encoded features into multiple frequency components and perform\n",
      "low-frequency skip connections to preserve outline and structural information.\n",
      "Then we alleviate the generator's struggles of synthesizing fine details by\n",
      "employing high-frequency skip connections, thus providing informative frequency\n",
      "information to the generator. Moreover, we utilize a frequency L1-loss on the\n",
      "generated and real images to further impede frequency information loss.\n",
      "Extensive experiments demonstrate the effectiveness and advancement of our\n",
      "method on three datasets. Noticeably, we achieve new state-of-the-art with FID\n",
      "42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822\n",
      "respectively on Flower, Animal Faces, and VGGFace. GitHub:\n",
      "https://github.com/kobeshegu/ECCV2022_WaveGAN \n",
      "\n",
      "\n",
      "One key challenge of exemplar-guided image generation lies in establishing\n",
      "fine-grained correspondences between input and guided images. Prior approaches,\n",
      "despite the promising results, have relied on either estimating dense attention\n",
      "to compute per-point matching, which is limited to only coarse scales due to\n",
      "the quadratic memory cost, or fixing the number of correspondences to achieve\n",
      "linear complexity, which lacks flexibility. In this paper, we propose a dynamic\n",
      "sparse attention based Transformer model, termed Dynamic Sparse Transformer\n",
      "(DynaST), to achieve fine-level matching with favorable efficiency. The heart\n",
      "of our approach is a novel dynamic-attention unit, dedicated to covering the\n",
      "variation on the optimal number of tokens one position should focus on.\n",
      "Specifically, DynaST leverages the multi-layer nature of Transformer structure,\n",
      "and performs the dynamic attention scheme in a cascaded manner to refine\n",
      "matching results and synthesize visually-pleasing outputs. In addition, we\n",
      "introduce a unified training objective for DynaST, making it a versatile\n",
      "reference-based image translation framework for both supervised and\n",
      "unsupervised scenarios. Extensive experiments on three applications,\n",
      "pose-guided person image generation, edge-based face synthesis, and undistorted\n",
      "image style transfer, demonstrate that DynaST achieves superior performance in\n",
      "local details, outperforming the state of the art while reducing the\n",
      "computational cost significantly. Our code is available at\n",
      "https://github.com/Huage001/DynaST \n",
      "\n",
      "\n",
      "In recent years, Generative Adversarial Networks (GANs) have become a hot\n",
      "topic among researchers and engineers that work with deep learning. It has been\n",
      "a ground-breaking technique which can generate new pieces of content of data in\n",
      "a consistent way. The topic of GANs has exploded in popularity due to its\n",
      "applicability in fields like image generation and synthesis, and music\n",
      "production and composition. GANs have two competing neural networks: a\n",
      "generator and a discriminator. The generator is used to produce new samples or\n",
      "pieces of content, while the discriminator is used to recognize whether the\n",
      "piece of content is real or generated. What makes it different from other\n",
      "generative models is its ability to learn unlabeled samples. In this review\n",
      "paper, we will discuss the evolution of GANs, several improvements proposed by\n",
      "the authors and a brief comparison between the different models. Index Terms\n",
      "generative adversarial networks, unsupervised learning, deep learning. \n",
      "\n",
      "\n",
      "Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,\n",
      "are evaluated for evidence of a bias previously observed in social and\n",
      "experimental psychology: equating American identity with being White. Embedding\n",
      "association tests (EATs) using standardized images of self-identified Asian,\n",
      "Black, Latina/o, and White individuals from the Chicago Face Database (CFD)\n",
      "reveal that White individuals are more associated with collective in-group\n",
      "words than are Asian, Black, or Latina/o individuals. In assessments of three\n",
      "core aspects of American identity reported by social psychologists,\n",
      "single-category EATs reveal that images of White individuals are more\n",
      "associated with patriotism and with being born in America, but that, consistent\n",
      "with prior findings in psychology, White individuals are associated with being\n",
      "less likely to treat people of all races and backgrounds equally. Three\n",
      "downstream machine learning tasks demonstrate biases associating American with\n",
      "White. In a visual question answering task using BLIP, 97% of White individuals\n",
      "are identified as American, compared to only 3% of Asian individuals. When\n",
      "asked in what state the individual depicted lives in, the model responds China\n",
      "53% of the time for Asian individuals, but always with an American state for\n",
      "White individuals. In an image captioning task, BLIP remarks upon the race of\n",
      "Asian individuals as much as 36% of the time, but never remarks upon race for\n",
      "White individuals. Finally, provided with an initialization image from the CFD\n",
      "and the text \"an American person,\" a synthetic image generator (VQGAN) using\n",
      "the text-based guidance of CLIP lightens the skin tone of individuals of all\n",
      "races (by 35% for Black individuals, based on pixel brightness). The results\n",
      "indicate that biases equating American identity with being White are learned by\n",
      "language-and-image AI, and propagate to downstream applications of such models. \n",
      "\n",
      "\n",
      "Deep learning models are found to be vulnerable to adversarial examples, as\n",
      "wrong predictions can be caused by small perturbation in input for deep\n",
      "learning models. Most of the existing works of adversarial image generation try\n",
      "to achieve attacks for most models, while few of them make efforts on\n",
      "guaranteeing the perceptual quality of the adversarial examples. High quality\n",
      "adversarial examples matter for many applications, especially for the privacy\n",
      "preserving. In this work, we develop a framework based on the Minimum\n",
      "Noticeable Difference (MND) concept to generate adversarial privacy preserving\n",
      "images that have minimum perceptual difference from the clean ones but are able\n",
      "to attack deep learning models. To achieve this, an adversarial loss is firstly\n",
      "proposed to make the deep learning models attacked by the adversarial images\n",
      "successfully. Then, a perceptual quality-preserving loss is developed by taking\n",
      "the magnitude of perturbation and perturbation-caused structural and gradient\n",
      "changes into account, which aims to preserve high perceptual quality for\n",
      "adversarial image generation. To the best of our knowledge, this is the first\n",
      "work on exploring quality-preserving adversarial image generation based on the\n",
      "MND concept for privacy preserving. To evaluate its performance in terms of\n",
      "perceptual quality, the deep models on image classification and face\n",
      "recognition are tested with the proposed method and several anchor methods in\n",
      "this work. Extensive experimental results demonstrate that the proposed MND\n",
      "framework is capable of generating adversarial images with remarkably improved\n",
      "performance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the\n",
      "anchor methods. \n",
      "\n",
      "\n",
      "Modern image generative models show remarkable sample quality when trained on\n",
      "a single domain or class of objects. In this work, we introduce a generative\n",
      "adversarial network that can simultaneously generate aligned image samples from\n",
      "multiple related domains. We leverage the fact that a variety of object classes\n",
      "share common attributes, with certain geometric differences. We propose\n",
      "Polymorphic-GAN which learns shared features across all domains and a\n",
      "per-domain morph layer to morph shared features according to each domain. In\n",
      "contrast to previous works, our framework allows simultaneous modelling of\n",
      "images with highly varying geometries, such as images of human faces, painted\n",
      "and artistic faces, as well as multiple different animal faces. We demonstrate\n",
      "that our model produces aligned samples for all domains and show how it can be\n",
      "used for applications such as segmentation transfer and cross-domain image\n",
      "editing, as well as training in low-data regimes. Additionally, we apply our\n",
      "Polymorphic-GAN on image-to-image translation tasks and show that we can\n",
      "greatly surpass previous approaches in cases where the geometric differences\n",
      "between domains are large. \n",
      "\n",
      "\n",
      "Synthesizing images from text descriptions has become an active research area\n",
      "with the advent of Generative Adversarial Networks. The main goal here is to\n",
      "generate photo-realistic images that are aligned with the input descriptions.\n",
      "Text-to-Face generation (T2F) is a sub-domain of Text-to-Image generation (T2I)\n",
      "that is more challenging due to the complexity and variation of facial\n",
      "attributes. It has a number of applications mainly in the domain of public\n",
      "safety. Even though several models are available for T2F, there is still the\n",
      "need to improve the image quality and the semantic alignment. In this research,\n",
      "we propose a novel framework, to generate facial images that are well-aligned\n",
      "with the input descriptions. Our framework utilizes the high-resolution face\n",
      "generator, StyleGAN2, and explores the possibility of using it in T2F. Here, we\n",
      "embed text in the input latent space of StyleGAN2 using BERT embeddings and\n",
      "oversee the generation of facial images using text descriptions. We trained our\n",
      "framework on attribute-based descriptions to generate images of 1024x1024 in\n",
      "resolution. The images generated exhibit a 57% similarity to the ground truth\n",
      "images, with a face semantic distance of 0.92, outperforming\n",
      "state-of-the-artwork. The generated images have a FID score of 118.097 and the\n",
      "experimental results show that our model generates promising images. \n",
      "\n",
      "\n",
      "The fashion industry has diverse applications in multi-modal image generation\n",
      "and editing. It aims to create a desired high-fidelity image with the\n",
      "multi-modal conditional signal as guidance. Most existing methods learn\n",
      "different condition guidance controls by introducing extra models or ignoring\n",
      "the style prior knowledge, which is difficult to handle multiple signal\n",
      "combinations and faces a low-fidelity problem. In this paper, we adapt both\n",
      "style prior knowledge and flexibility of multi-modal control into one unified\n",
      "two-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion\n",
      "design. It decouples style codes in both spatial and semantic dimensions to\n",
      "guarantee high-fidelity image generation in the first stage. M6-Fashion\n",
      "utilizes self-correction for the non-autoregressive generation to improve\n",
      "inference speed, enhance holistic consistency, and support various signal\n",
      "controls. Extensive experiments on a large-scale clothing dataset M2C-Fashion\n",
      "demonstrate superior performances on various image generation and editing\n",
      "tasks. M6-Fashion model serves as a highly potential AI designer for the\n",
      "fashion industry. \n",
      "\n",
      "\n",
      "Unconditional human image generation is an important task in vision and\n",
      "graphics, which enables various applications in the creative industry. Existing\n",
      "studies in this field mainly focus on \"network engineering\" such as designing\n",
      "new components and objective functions. This work takes a data-centric\n",
      "perspective and investigates multiple critical aspects in \"data engineering\",\n",
      "which we believe would complement the current practice. To facilitate a\n",
      "comprehensive study, we collect and annotate a large-scale human image dataset\n",
      "with over 230K samples capturing diverse poses and textures. Equipped with this\n",
      "large dataset, we rigorously investigate three essential factors in data\n",
      "engineering for StyleGAN-based human generation, namely data size, data\n",
      "distribution, and data alignment. Extensive experiments reveal several valuable\n",
      "observations w.r.t. these aspects: 1) Large-scale data, more than 40K images,\n",
      "are needed to train a high-fidelity unconditional human generation model with\n",
      "vanilla StyleGAN. 2) A balanced training set helps improve the generation\n",
      "quality with rare face poses compared to the long-tailed counterpart, whereas\n",
      "simply balancing the clothing texture distribution does not effectively bring\n",
      "an improvement. 3) Human GAN models with body centers for alignment outperform\n",
      "models trained using face centers or pelvis points as alignment anchors. In\n",
      "addition, a model zoo and human editing applications are demonstrated to\n",
      "facilitate future research in the community. \n",
      "\n",
      "\n",
      "AI-driven image generation has improved significantly in recent years.\n",
      "Generative adversarial networks (GANs), like StyleGAN, are able to generate\n",
      "high-quality realistic data and have artistic control over the output, as well.\n",
      "In this work, we present StyleT2F, a method of controlling the output of\n",
      "StyleGAN2 using text, in order to be able to generate a detailed human face\n",
      "from textual description. We utilize StyleGAN's latent space to manipulate\n",
      "different facial features and conditionally sample the required latent code,\n",
      "which embeds the facial features mentioned in the input text. Our method proves\n",
      "to capture the required features correctly and shows consistency between the\n",
      "input text and the output images. Moreover, our method guarantees\n",
      "disentanglement on manipulating a wide range of facial features that\n",
      "sufficiently describes a human face. \n",
      "\n",
      "\n",
      "With rapid advancements in image generation technology, face swapping for\n",
      "privacy protection has emerged as an active area of research. The ultimate\n",
      "benefit is improved access to video datasets, e.g. in healthcare settings.\n",
      "Recent literature has proposed deep network-based architectures to perform\n",
      "facial swaps and reported the associated reduction in facial recognition\n",
      "accuracy. However, there is not much reporting on how well these methods\n",
      "preserve the types of semantic information needed for the privatized videos to\n",
      "remain useful for their intended application. Our main contribution is a novel\n",
      "end-to-end face swapping pipeline for recorded videos of standardized\n",
      "assessments of autism symptoms in children. Through this design, we are the\n",
      "first to provide a methodology for assessing the privacy-utility trade-offs for\n",
      "the face swapping approach to patient privacy protection. Our methodology can\n",
      "show, for example, that current deep network based face swapping is\n",
      "bottle-necked by face detection in real world videos, and the extent to which\n",
      "gaze and expression information is preserved by face swaps relative to baseline\n",
      "privatization methods such as blurring. \n",
      "\n",
      "\n",
      "Layout design is ubiquitous in many applications, e.g. architecture/urban\n",
      "planning, etc, which involves a lengthy iterative design process. Recently,\n",
      "deep learning has been leveraged to automatically generate layouts via image\n",
      "generation, showing a huge potential to free designers from laborious routines.\n",
      "While automatic generation can greatly boost productivity, designer input is\n",
      "undoubtedly crucial. An ideal AI-aided design tool should automate repetitive\n",
      "routines, and meanwhile accept human guidance and provide smart/proactive\n",
      "suggestions. However, the capability of involving humans into the loop has been\n",
      "largely ignored in existing methods which are mostly end-to-end approaches. To\n",
      "this end, we propose a new human-in-the-loop generative model, iPLAN, which is\n",
      "capable of automatically generating layouts, but also interacting with\n",
      "designers throughout the whole procedure, enabling humans and AI to co-evolve a\n",
      "sketchy idea gradually into the final design. iPLAN is evaluated on diverse\n",
      "datasets and compared with existing methods. The results show that iPLAN has\n",
      "high fidelity in producing similar layouts to those from human designers, great\n",
      "flexibility in accepting designer inputs and providing design suggestions\n",
      "accordingly, and strong generalizability when facing unseen design tasks and\n",
      "limited training data. \n",
      "\n",
      "\n",
      "Recent text-to-image generation methods provide a simple yet exciting\n",
      "conversion capability between text and image domains. While these methods have\n",
      "incrementally improved the generated image fidelity and text relevancy, several\n",
      "pivotal gaps remain unanswered, limiting applicability and quality. We propose\n",
      "a novel text-to-image method that addresses these gaps by (i) enabling a simple\n",
      "control mechanism complementary to text in the form of a scene, (ii)\n",
      "introducing elements that substantially improve the tokenization process by\n",
      "employing domain-specific knowledge over key image regions (faces and salient\n",
      "objects), and (iii) adapting classifier-free guidance for the transformer use\n",
      "case. Our model achieves state-of-the-art FID and human evaluation results,\n",
      "unlocking the ability to generate high fidelity images in a resolution of\n",
      "512x512 pixels, significantly improving visual quality. Through scene\n",
      "controllability, we introduce several new capabilities: (i) Scene editing, (ii)\n",
      "text editing with anchor scenes, (iii) overcoming out-of-distribution text\n",
      "prompts, and (iv) story illustration generation, as demonstrated in the story\n",
      "we wrote. \n",
      "\n",
      "\n",
      "We present a novel algorithm to reduce tensor compute required by a\n",
      "conditional image generation autoencoder and make it as-lite-as-possible,\n",
      "without sacrificing quality of photo-realistic image generation. Our method is\n",
      "device agnostic, and can optimize an autoencoder for a given CPU-only, GPU\n",
      "compute device(s) in about normal time it takes to train an autoencoder on a\n",
      "generic workstation. We achieve this via a two-stage novel strategy where,\n",
      "first, we condense the channel weights, such that, as few as possible channels\n",
      "are used. Then, we prune the nearly zeroed out weight activations, and\n",
      "fine-tune this lite autoencoder. To maintain image quality, fine-tuning is done\n",
      "via student-teacher training, where we reuse the condensed autoencoder as the\n",
      "teacher. We show performance gains for various conditional image generation\n",
      "tasks: segmentation mask to face images, face images to cartoonization, and\n",
      "finally CycleGAN-based model on horse to zebra dataset over multiple compute\n",
      "devices. We perform various ablation studies to justify the claims and design\n",
      "choices, and achieve real-time versions of various autoencoders on CPU-only\n",
      "devices while maintaining image quality, thus enabling at-scale deployment of\n",
      "such autoencoders. \n",
      "\n",
      "\n",
      "While GANs can produce photo-realistic images in ideal conditions for certain\n",
      "domains, the generation of full-body human images remains difficult due to the\n",
      "diversity of identities, hairstyles, clothing, and the variance in pose.\n",
      "Instead of modeling this complex domain with a single GAN, we propose a novel\n",
      "method to combine multiple pretrained GANs, where one GAN generates a global\n",
      "canvas (e.g., human body) and a set of specialized GANs, or insets, focus on\n",
      "different parts (e.g., faces, shoes) that can be seamlessly inserted onto the\n",
      "global canvas. We model the problem as jointly exploring the respective latent\n",
      "spaces such that the generated images can be combined, by inserting the parts\n",
      "from the specialized generators onto the global canvas, without introducing\n",
      "seams. We demonstrate the setup by combining a full body GAN with a dedicated\n",
      "high-quality face GAN to produce plausible-looking humans. We evaluate our\n",
      "results with quantitative metrics and user studies. \n",
      "\n",
      "\n",
      "The research topic of sketch-to-portrait generation has witnessed a boost of\n",
      "progress with deep learning techniques. The recently proposed StyleGAN\n",
      "architectures achieve state-of-the-art generation ability but the original\n",
      "StyleGAN is not friendly for sketch-based creation due to its unconditional\n",
      "generation nature. To address this issue, we propose a direct conditioning\n",
      "strategy to better preserve the spatial information under the StyleGAN\n",
      "framework. Specifically, we introduce Spatially Conditioned StyleGAN\n",
      "(SC-StyleGAN for short), which explicitly injects spatial constraints to the\n",
      "original StyleGAN generation process. We explore two input modalities, sketches\n",
      "and semantic maps, which together allow users to express desired generation\n",
      "results more precisely and easily. Based on SC-StyleGAN, we present\n",
      "DrawingInStyles, a novel drawing interface for non-professional users to easily\n",
      "produce high-quality, photo-realistic face images with precise control, either\n",
      "from scratch or editing existing ones. Qualitative and quantitative evaluations\n",
      "show the superior generation ability of our method to existing and alternative\n",
      "solutions. The usability and expressiveness of our system are confirmed by a\n",
      "user study. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are very popular frameworks for\n",
      "generating high-quality data, and are immensely used in both the academia and\n",
      "industry in many domains. Arguably, their most substantial impact has been in\n",
      "the area of computer vision, where they achieve state-of-the-art image\n",
      "generation. This chapter gives an introduction to GANs, by discussing their\n",
      "principle mechanism and presenting some of their inherent problems during\n",
      "training and evaluation. We focus on these three issues: (1) mode collapse, (2)\n",
      "vanishing gradients, and (3) generation of low-quality images. We then list\n",
      "some architecture-variant and loss-variant GANs that remedy the above\n",
      "challenges. Lastly, we present two utilization examples of GANs for real-world\n",
      "applications: Data augmentation and face images generation. \n",
      "\n",
      "\n",
      "Polynomial neural networks (PNNs) have been recently shown to be particularly\n",
      "effective at image generation and face recognition, where high-frequency\n",
      "information is critical. Previous studies have revealed that neural networks\n",
      "demonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which\n",
      "yields faster learning of low-frequency components during training. Inspired by\n",
      "such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK)\n",
      "of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed\n",
      "parametrization of PNNs, speeds up the learning of the higher frequencies. We\n",
      "verify the theoretical bias through extensive experiments. We expect our\n",
      "analysis to provide novel insights into designing architectures and learning\n",
      "frameworks by incorporating multiplicative interactions via polynomials. \n",
      "\n",
      "\n",
      "Generating new images with desired properties (e.g. new view/poses) from\n",
      "source images has been enthusiastically pursued recently, due to its wide range\n",
      "of potential applications. One way to ensure high-quality generation is to use\n",
      "multiple sources with complementary information such as different views of the\n",
      "same object. However, as source images are often misaligned due to the large\n",
      "disparities among the camera settings, strong assumptions have been made in the\n",
      "past with respect to the camera(s) or/and the object in interest, limiting the\n",
      "application of such techniques. Therefore, we propose a new general approach\n",
      "which models multiple types of variations among sources, such as view angles,\n",
      "poses, facial expressions, in a unified framework, so that it can be employed\n",
      "on datasets of vastly different nature. We verify our approach on a variety of\n",
      "data including humans bodies, faces, city scenes and 3D objects. Both the\n",
      "qualitative and quantitative results demonstrate the better performance of our\n",
      "method than the state of the art. \n",
      "\n",
      "\n",
      "In 2017 Apple introduced the TrueDepth sensor with the iPhone X release.\n",
      "Although its primary use case is biometric face recognition, the exploitation\n",
      "of accurate depth data for other computer vision tasks like segmentation,\n",
      "portrait image generation and metric 3D reconstruction seems natural and lead\n",
      "to the development of various applications. In this report, we investigate the\n",
      "reliability of TrueDepth data - accessed through two different APIs - on\n",
      "various devices including different iPhone and iPad generations and reveal two\n",
      "different and significant issues on all tested iPads. \n",
      "\n",
      "\n",
      "The semantically disentangled latent subspace in GAN provides rich\n",
      "interpretable controls in image generation. This paper includes two\n",
      "contributions on semantic latent subspace analysis in the scenario of face\n",
      "generation using StyleGAN2. First, we propose a novel approach to disentangle\n",
      "latent subspace semantics by exploiting existing face analysis models, e.g.,\n",
      "face parsers and face landmark detectors. These models provide the flexibility\n",
      "to construct various criterions with very concrete and interpretable semantic\n",
      "meanings (e.g., change face shape or change skin color) to restrict latent\n",
      "subspace disentanglement. Rich latent space controls unknown previously can be\n",
      "discovered using the constructed criterions. Second, we propose a new\n",
      "perspective to explain the behavior of a CNN classifier by generating\n",
      "counterfactuals in the interpretable latent subspaces we discovered. This\n",
      "explanation helps reveal whether the classifier learns semantics as intended.\n",
      "Experiments on various disentanglement criterions demonstrate the effectiveness\n",
      "of our approach. We believe this approach contributes to both areas of image\n",
      "manipulation and counterfactual explainability of CNNs. The code is available\n",
      "at \\url{https://github.com/prclibo/ice}. \n",
      "\n",
      "\n",
      "While recent advances in deep neural networks have made it possible to render\n",
      "high-quality images, generating photo-realistic and personalized talking head\n",
      "remains challenging. With given audio, the key to tackling this task is\n",
      "synchronizing lip movement and simultaneously generating personalized\n",
      "attributes like head movement and eye blink. In this work, we observe that the\n",
      "input audio is highly correlated to lip motion while less correlated to other\n",
      "personalized attributes (e.g., head movements). Inspired by this, we propose a\n",
      "novel framework based on neural radiance field to pursue high-fidelity and\n",
      "personalized talking head generation. Specifically, neural radiance field takes\n",
      "lip movements features and personalized attributes as two disentangled\n",
      "conditions, where lip movements are directly predicted from the audio inputs to\n",
      "achieve lip-synchronized generation. In the meanwhile, personalized attributes\n",
      "are sampled from a probabilistic model, where we design a Transformer-based\n",
      "variational autoencoder sampled from Gaussian Process to learn plausible and\n",
      "natural-looking head pose and eye blink. Experiments on several benchmarks\n",
      "demonstrate that our method achieves significantly better results than\n",
      "state-of-the-art methods. \n",
      "\n",
      "\n",
      "Occlusions are very common in face images in the wild, leading to the\n",
      "degraded performance of face-related tasks. Although much effort has been\n",
      "devoted to removing occlusions from face images, the varying shapes and\n",
      "textures of occlusions still challenge the robustness of current methods. As a\n",
      "result, current methods either rely on manual occlusion masks or only apply to\n",
      "specific occlusions. This paper proposes a novel face de-occlusion model based\n",
      "on face segmentation and 3D face reconstruction, which automatically removes\n",
      "all kinds of face occlusions with even blurred boundaries,e.g., hairs. The\n",
      "proposed model consists of a 3D face reconstruction module, a face segmentation\n",
      "module, and an image generation module. With the face prior and the occlusion\n",
      "mask predicted by the first two, respectively, the image generation module can\n",
      "faithfully recover the missing facial textures. To supervise the training, we\n",
      "further build a large occlusion dataset, with both manually labeled and\n",
      "synthetic occlusions. Qualitative and quantitative results demonstrate the\n",
      "effectiveness and robustness of the proposed method. \n",
      "\n",
      "\n",
      "Can deep learning models achieve greater generalization if their training is\n",
      "guided by reference to human perceptual abilities? And how can we implement\n",
      "this in a practical manner? This paper proposes a training strategy to ConveY\n",
      "Brain Oversight to Raise Generalization (CYBORG). This new approach\n",
      "incorporates human-annotated saliency maps into a loss function that guides the\n",
      "model's learning to focus on image regions that humans deem salient for the\n",
      "task. The Class Activation Mapping (CAM) mechanism is used to probe the model's\n",
      "current saliency in each training batch, juxtapose this model saliency with\n",
      "human saliency, and penalize large differences. Results on the task of\n",
      "synthetic face detection, selected to illustrate the effectiveness of the\n",
      "approach, show that CYBORG leads to significant improvement in accuracy on\n",
      "unseen samples consisting of face images generated from six Generative\n",
      "Adversarial Networks across multiple classification network architectures. We\n",
      "also show that scaling to even seven times the training data, or using\n",
      "non-human-saliency auxiliary information, such as segmentation masks, and\n",
      "standard loss cannot beat the performance of CYBORG-trained models. As a side\n",
      "effect of this work, we observe that the addition of explicit region annotation\n",
      "to the task of synthetic face detection increased human classification\n",
      "accuracy. This work opens a new area of research on how to incorporate human\n",
      "visual saliency into loss functions in practice. All data, code and pre-trained\n",
      "models used in this work are offered with this paper. \n",
      "\n",
      "\n",
      "Previous portrait image generation methods roughly fall into two categories:\n",
      "2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but\n",
      "with low view consistency. 3D-aware GAN methods can maintain view consistency\n",
      "but their generated images are not locally editable. To overcome these\n",
      "limitations, we propose FENeRF, a 3D-aware generator that can produce\n",
      "view-consistent and locally-editable portrait images. Our method uses two\n",
      "decoupled latent codes to generate corresponding facial semantics and texture\n",
      "in a spatial aligned 3D volume with shared geometry. Benefiting from such\n",
      "underlying 3D representation, FENeRF can jointly render the boundary-aligned\n",
      "image and semantic mask and use the semantic mask to edit the 3D volume via GAN\n",
      "inversion. We further show such 3D representation can be learned from widely\n",
      "available monocular image and semantic mask pairs. Moreover, we reveal that\n",
      "joint learning semantics and texture helps to generate finer geometry. Our\n",
      "experiments demonstrate that FENeRF outperforms state-of-the-art methods in\n",
      "various face editing tasks. \n",
      "\n",
      "\n",
      "Person image generation aims to perform non-rigid deformation on source\n",
      "images, which generally requires unaligned data pairs for training. Recently,\n",
      "self-supervised methods express great prospects in this task by merging the\n",
      "disentangled representations for self-reconstruction. However, such methods\n",
      "fail to exploit the spatial correlation between the disentangled features. In\n",
      "this paper, we propose a Self-supervised Correlation Mining Network (SCM-Net)\n",
      "to rearrange the source images in the feature space, in which two collaborative\n",
      "modules are integrated, Decomposed Style Encoder (DSE) and Correlation Mining\n",
      "Module (CMM). Specifically, the DSE first creates unaligned pairs at the\n",
      "feature level. Then, the CMM establishes the spatial correlation field for\n",
      "feature rearrangement. Eventually, a translation module transforms the\n",
      "rearranged features to realistic results. Meanwhile, for improving the fidelity\n",
      "of cross-scale pose transformation, we propose a graph based Body Structure\n",
      "Retaining Loss (BSR Loss) to preserve reasonable body structures on half body\n",
      "to full body generation. Extensive experiments conducted on DeepFashion dataset\n",
      "demonstrate the superiority of our method compared with other supervised and\n",
      "unsupervised approaches. Furthermore, satisfactory results on face generation\n",
      "show the versatility of our method in other deformation tasks. \n",
      "\n",
      "\n",
      "Quality scores provide a measure to evaluate the utility of biometric samples\n",
      "for biometric recognition. Biometric recognition systems require high-quality\n",
      "samples to achieve optimal performance. This paper focuses on face images and\n",
      "the measurement of face image utility with general and face-specific image\n",
      "quality metrics. While face-specific metrics rely on features of aligned face\n",
      "images, general image quality metrics can be used on the global image and\n",
      "relate to human perceptions. In this paper, we analyze the gap between the\n",
      "general image quality metrics and the face image quality metrics. Our\n",
      "contribution lies in a thorough examination of how different the image quality\n",
      "assessment algorithms relate to the utility for the face recognition task. The\n",
      "results of image quality assessment algorithms are further compared with those\n",
      "of dedicated face image quality assessment algorithms. In total, 25 different\n",
      "quality metrics are evaluated on three face image databases, BioSecure, LFW,\n",
      "and VGGFace2 using three open-source face recognition solutions, SphereFace,\n",
      "ArcFace, and FaceNet. Our results reveal a clear correlation between learned\n",
      "image metrics to face image utility even without being specifically trained as\n",
      "a face utility measure. Individual handcrafted features lack general stability\n",
      "and perform significantly worse than general face-specific quality metrics. We\n",
      "additionally provide a visual insight into the image areas contributing to the\n",
      "quality score of a selected set of quality assessment methods. \n",
      "\n",
      "\n",
      "Recently, there has been an increasing interest in image editing methods that\n",
      "employ pre-trained unconditional image generators (e.g., StyleGAN). However,\n",
      "applying these methods to translate images to multiple visual domains remains\n",
      "challenging. Existing works do not often preserve the domain-invariant part of\n",
      "the image (e.g., the identity in human face translations), they do not usually\n",
      "handle multiple domains, or do not allow for multi-modal translations. This\n",
      "work proposes an implicit style function (ISF) to straightforwardly achieve\n",
      "multi-modal and multi-domain image-to-image translation from pre-trained\n",
      "unconditional generators. The ISF manipulates the semantics of an input latent\n",
      "code to make the image generated from it lying in the desired visual domain.\n",
      "Our results in human face and animal manipulations show significantly improved\n",
      "results over the baselines. Our model enables cost-effective multi-modal\n",
      "unsupervised image-to-image translations at high resolution using pre-trained\n",
      "unconditional GANs. The code and data are available at:\n",
      "\\url{https://github.com/yhlleo/stylegan-mmuit}. \n",
      "\n",
      "\n",
      "Generating portrait images by controlling the motions of existing faces is an\n",
      "important task of great consequence to social media industries. For easy use\n",
      "and intuitive control, semantically meaningful and fully disentangled\n",
      "parameters should be used as modifications. However, many existing techniques\n",
      "do not provide such fine-grained controls or use indirect editing methods i.e.\n",
      "mimic motions of other individuals. In this paper, a Portrait Image Neural\n",
      "Renderer (PIRenderer) is proposed to control the face motions with the\n",
      "parameters of three-dimensional morphable face models (3DMMs). The proposed\n",
      "model can generate photo-realistic portrait images with accurate movements\n",
      "according to intuitive modifications. Experiments on both direct and indirect\n",
      "editing tasks demonstrate the superiority of this model. Meanwhile, we further\n",
      "extend this model to tackle the audio-driven facial reenactment task by\n",
      "extracting sequential motions from audio inputs. We show that our model can\n",
      "generate coherent videos with convincing movements from only a single reference\n",
      "image and a driving audio stream. Our source code is available at\n",
      "https://github.com/RenYurui/PIRender. \n",
      "\n",
      "\n",
      "Machine learning tools are becoming increasingly powerful and widely used.\n",
      "Unfortunately membership attacks, which seek to uncover information from data\n",
      "sets used in machine learning, have the potential to limit data sharing. In\n",
      "this paper we consider an approach to increase the privacy protection of data\n",
      "sets, as applied to face recognition. Using an auxiliary face recognition\n",
      "model, we build on the StyleGAN generative adversarial network and feed it with\n",
      "latent codes combining two distinct sub-codes, one encoding visual identity\n",
      "factors, and, the other, non-identity factors. By independently varying these\n",
      "vectors during image generation, we create a synthetic data set of fictitious\n",
      "face identities. We use this data set to train a face recognition model. The\n",
      "model performance degrades in comparison to the state-of-the-art of face\n",
      "verification. When tested with a simple membership attack our model provides\n",
      "good privacy protection, however the model performance degrades in comparison\n",
      "to the state-of-the-art of face verification. We find that the addition of a\n",
      "small amount of private data greatly improves the performance of our model,\n",
      "which highlights the limitations of using synthetic data to train machine\n",
      "learning models. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) nowadays are capable of producing\n",
      "images of incredible realism. One concern raised is whether the\n",
      "state-of-the-art GAN's learned distribution still suffers from mode collapse,\n",
      "and what to do if so. Existing diversity tests of samples from GANs are usually\n",
      "conducted qualitatively on a small scale, and/or depends on the access to\n",
      "original training data as well as the trained model parameters. This paper\n",
      "explores to diagnose GAN intra-mode collapse and calibrate that, in a novel\n",
      "black-box setting: no access to training data, nor the trained model\n",
      "parameters, is assumed. The new setting is practically demanded, yet rarely\n",
      "explored and significantly more challenging. As a first stab, we devise a set\n",
      "of statistical tools based on sampling, that can visualize, quantify, and\n",
      "rectify intra-mode collapse. We demonstrate the effectiveness of our proposed\n",
      "diagnosis and calibration techniques, via extensive simulations and\n",
      "experiments, on unconditional GAN image generation (e.g., face and vehicle).\n",
      "Our study reveals that the intra-mode collapse is still a prevailing problem in\n",
      "state-of-the-art GANs and the mode collapse is diagnosable and calibratable in\n",
      "black-box settings. Our codes are available at:\n",
      "https://github.com/VITA-Group/BlackBoxGANCollapse. \n",
      "\n",
      "\n",
      "We propose an audio-driven talking-head method to generate photo-realistic\n",
      "talking-head videos from a single reference image. In this work, we tackle two\n",
      "key challenges: (i) producing natural head motions that match speech prosody,\n",
      "and (ii) maintaining the appearance of a speaker in a large head motion while\n",
      "stabilizing the non-face regions. We first design a head pose predictor by\n",
      "modeling rigid 6D head movements with a motion-aware recurrent neural network\n",
      "(RNN). In this way, the predicted head poses act as the low-frequency holistic\n",
      "movements of a talking head, thus allowing our latter network to focus on\n",
      "detailed facial movement generation. To depict the entire image motions arising\n",
      "from audio, we exploit a keypoint based dense motion field representation.\n",
      "Then, we develop a motion field generator to produce the dense motion fields\n",
      "from input audio, head poses, and a reference image. As this keypoint based\n",
      "representation models the motions of facial regions, head, and backgrounds\n",
      "integrally, our method can better constrain the spatial and temporal\n",
      "consistency of the generated videos. Finally, an image generation network is\n",
      "employed to render photo-realistic talking-head videos from the estimated\n",
      "keypoint based motion fields and the input reference image. Extensive\n",
      "experiments demonstrate that our method produces videos with plausible head\n",
      "motions, synchronized facial expressions, and stable backgrounds and\n",
      "outperforms the state-of-the-art. \n",
      "\n",
      "\n",
      "In this paper, we propose a novel encoder, called ShapeEditor, for\n",
      "high-resolution, realistic and high-fidelity face exchange. First of all, in\n",
      "order to ensure sufficient clarity and authenticity, our key idea is to use an\n",
      "advanced pretrained high-quality random face image generator, i.e. StyleGAN, as\n",
      "backbone. Secondly, we design ShapeEditor, a two-step encoder, to make the\n",
      "swapped face integrate the identity and attribute of the input faces. In the\n",
      "first step, we extract the identity vector of the source image and the\n",
      "attribute vector of the target image respectively; in the second step, we map\n",
      "the concatenation of identity vector and attribute vector into the\n",
      "$\\mathcal{W+}$ potential space. In addition, for learning to map into the\n",
      "latent space of StyleGAN, we propose a set of self-supervised loss functions\n",
      "with which the training data do not need to be labeled manually. Extensive\n",
      "experiments on the test dataset show that the results of our method not only\n",
      "have a great advantage in clarity and authenticity than other state-of-the-art\n",
      "methods, but also reflect the sufficient integration of identity and attribute. \n",
      "\n",
      "\n",
      "We propose a novel and unified Cycle in Cycle Generative Adversarial Network\n",
      "(C2GAN) for generating human faces, hands, bodies, and natural scenes. Our\n",
      "proposed C2GAN is a cross-modal model exploring the joint exploitation of the\n",
      "input image data and guidance data in an interactive manner. C2GAN contains two\n",
      "different generators, i.e., an image-generation generator and a\n",
      "guidance-generation generator. Both generators are mutually connected and\n",
      "trained in an end-to-end fashion and explicitly form three cycled subnets,\n",
      "i.e., one image generation cycle and two guidance generation cycles. Each cycle\n",
      "aims at reconstructing the input domain and simultaneously produces a useful\n",
      "output involved in the generation of another cycle. In this way, the cycles\n",
      "constrain each other implicitly providing complementary information from both\n",
      "image and guidance modalities and bringing an extra supervision gradient across\n",
      "the cycles, facilitating a more robust optimization of the whole model.\n",
      "Extensive results on four guided image-to-image translation subtasks\n",
      "demonstrate that the proposed C2GAN is effective in generating more realistic\n",
      "images compared with state-of-the-art models. The code is available at\n",
      "https://github.com/Ha0Tang/C2GAN. \n",
      "\n",
      "\n",
      "Although much progress has been made recently in 3D face reconstruction, most\n",
      "previous work has been devoted to predicting accurate and fine-grained 3D\n",
      "shapes. In contrast, relatively little work has focused on generating\n",
      "high-fidelity face textures. Compared with the prosperity of photo-realistic 2D\n",
      "face image generation, high-fidelity 3D face texture generation has yet to be\n",
      "studied. In this paper, we proposed a novel UV map generation model that\n",
      "predicts the UV map from a single face image. The model consists of a UV\n",
      "sampler and a UV generator. By selectively sampling the input face image's\n",
      "pixels and adjusting their relative locations, the UV sampler generates an\n",
      "incomplete UV map that could faithfully reconstruct the original face. Missing\n",
      "textures in the incomplete UV map are further full-filled by the UV generator.\n",
      "The training is based on pseudo ground truth blended by the 3DMM texture and\n",
      "the input face texture, thus weakly supervised. To deal with the artifacts in\n",
      "the imperfect pseudo UV map, multiple partial UV map discriminators are\n",
      "leveraged. \n",
      "\n",
      "\n",
      "Contact pressure between the human body and its surroundings has important\n",
      "implications. For example, it plays a role in comfort, safety, posture, and\n",
      "health. We present a method that infers contact pressure between a human body\n",
      "and a mattress from a depth image. Specifically, we focus on using a depth\n",
      "image from a downward facing camera to infer pressure on a body at rest in bed\n",
      "occluded by bedding, which is directly applicable to the prevention of pressure\n",
      "injuries in healthcare. Our approach involves augmenting a real dataset with\n",
      "synthetic data generated via a soft-body physics simulation of a human body, a\n",
      "mattress, a pressure sensing mat, and a blanket. We introduce a novel deep\n",
      "network that we trained on an augmented dataset and evaluated with real data.\n",
      "The network contains an embedded human body mesh model and uses a white-box\n",
      "model of depth and pressure image generation. Our network successfully infers\n",
      "body pose, outperforming prior work. It also infers contact pressure across a\n",
      "3D mesh model of the human body, which is a novel capability, and does so in\n",
      "the presence of occlusion from blankets. \n",
      "\n",
      "\n",
      "Image-to-image (I2I) translation has matured in recent years and is able to\n",
      "generate high-quality realistic images. However, despite current success, it\n",
      "still faces important challenges when applied to small domains. Existing\n",
      "methods use transfer learning for I2I translation, but they still require the\n",
      "learning of millions of parameters from scratch. This drawback severely limits\n",
      "its application on small domains. In this paper, we propose a new transfer\n",
      "learning for I2I translation (TransferI2I). We decouple our learning process\n",
      "into the image generation step and the I2I translation step. In the first step\n",
      "we propose two novel techniques: source-target initialization and\n",
      "self-initialization of the adaptor layer. The former finetunes the pretrained\n",
      "generative model (e.g., StyleGAN) on source and target data. The latter allows\n",
      "to initialize all non-pretrained network parameters without the need of any\n",
      "data. These techniques provide a better initialization for the I2I translation\n",
      "step. In addition, we introduce an auxiliary GAN that further facilitates the\n",
      "training of deep I2I systems even from small datasets. In extensive experiments\n",
      "on three datasets, (Animal faces, Birds, and Foods), we show that we outperform\n",
      "existing methods and that mFID improves on several datasets with over 25\n",
      "points. \n",
      "\n",
      "\n",
      "Blind face restoration (BFR) from severely degraded face images in the wild\n",
      "is a very challenging problem. Due to the high illness of the problem and the\n",
      "complex unknown degradation, directly training a deep neural network (DNN)\n",
      "usually cannot lead to acceptable results. Existing generative adversarial\n",
      "network (GAN) based methods can produce better results but tend to generate\n",
      "over-smoothed restorations. In this work, we propose a new method by first\n",
      "learning a GAN for high-quality face image generation and embedding it into a\n",
      "U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN\n",
      "with a set of synthesized low-quality face images. The GAN blocks are designed\n",
      "to ensure that the latent code and noise input to the GAN can be respectively\n",
      "generated from the deep and shallow features of the DNN, controlling the global\n",
      "face structure, local face details and background of the reconstructed image.\n",
      "The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can\n",
      "generate visually photo-realistic results. Our experiments demonstrated that\n",
      "the proposed GPEN achieves significantly superior results to state-of-the-art\n",
      "BFR methods both quantitatively and qualitatively, especially for the\n",
      "restoration of severely degraded face images in the wild. The source code and\n",
      "models can be found at https://github.com/yangxy/GPEN. \n",
      "\n",
      "\n",
      "Face recognition is a popular and well-studied area with wide applications in\n",
      "our society. However, racial bias had been proven to be inherent in most State\n",
      "Of The Art (SOTA) face recognition systems. Many investigative studies on face\n",
      "recognition algorithms have reported higher false positive rates of African\n",
      "subjects cohorts than the other cohorts. Lack of large-scale African face image\n",
      "databases in public domain is one of the main restrictions in studying the\n",
      "racial bias problem of face recognition. To this end, we collect a face image\n",
      "database namely CASIA-Face-Africa which contains 38,546 images of 1,183 African\n",
      "subjects. Multi-spectral cameras are utilized to capture the face images under\n",
      "various illumination settings. Demographic attributes and facial expressions of\n",
      "the subjects are also carefully recorded. For landmark detection, each face\n",
      "image in the database is manually labeled with 68 facial keypoints. A group of\n",
      "evaluation protocols are constructed according to different applications,\n",
      "tasks, partitions and scenarios. The performances of SOTA face recognition\n",
      "algorithms without re-training are reported as baselines. The proposed database\n",
      "along with its face landmark annotations, evaluation protocols and preliminary\n",
      "results form a good benchmark to study the essential aspects of face biometrics\n",
      "for African subjects, especially face image preprocessing, face feature\n",
      "analysis and matching, facial expression recognition, sex/age estimation,\n",
      "ethnic classification, face image generation, etc. The database can be\n",
      "downloaded from our http://www.cripacsir.cn/dataset/ \n",
      "\n",
      "\n",
      "We present SR3, an approach to image Super-Resolution via Repeated\n",
      "Refinement. SR3 adapts denoising diffusion probabilistic models to conditional\n",
      "image generation and performs super-resolution through a stochastic denoising\n",
      "process. Inference starts with pure Gaussian noise and iteratively refines the\n",
      "noisy output using a U-Net model trained on denoising at various noise levels.\n",
      "SR3 exhibits strong performance on super-resolution tasks at different\n",
      "magnification factors, on faces and natural images. We conduct human evaluation\n",
      "on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\n",
      "GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\n",
      "outputs, while GANs do not exceed a fool rate of 34%. We further show the\n",
      "effectiveness of SR3 in cascaded image generation, where generative models are\n",
      "chained with super-resolution models, yielding a competitive FID score of 11.3\n",
      "on ImageNet. \n",
      "\n",
      "\n",
      "Face verification has come into increasing focus in various applications\n",
      "including the European Entry/Exit System, which integrates face recognition\n",
      "mechanisms. At the same time, the rapid advancement of biometric authentication\n",
      "requires extensive performance tests in order to inhibit the discriminatory\n",
      "treatment of travellers due to their demographic background. However, the use\n",
      "of face images collected as part of border controls is restricted by the\n",
      "European General Data Protection Law to be processed for no other reason than\n",
      "its original purpose. Therefore, this paper investigates the suitability of\n",
      "synthetic face images generated with StyleGAN and StyleGAN2 to compensate for\n",
      "the urgent lack of publicly available large-scale test data. Specifically, two\n",
      "deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR\n",
      "29794-5) face image quality assessment algorithm is utilized to compare the\n",
      "applicability of synthetic face images compared to real face images extracted\n",
      "from the FRGC dataset. Finally, based on the analysis of impostor score\n",
      "distributions and utility score distributions, our experiments reveal\n",
      "negligible differences between StyleGAN vs. StyleGAN2, and further also minor\n",
      "discrepancies compared to real face images. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in\n",
      "various image generation and synthesis tasks, yet their notoriously high\n",
      "computational cost hinders their efficient deployment on edge devices. Directly\n",
      "applying generic compression approaches yields poor results on GANs, which\n",
      "motivates a number of recent GAN compression works. While prior works mainly\n",
      "accelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing\n",
      "state-of-the-art unconditional GANs has rarely been explored and is more\n",
      "challenging. In this paper, we propose novel approaches for unconditional GAN\n",
      "compression. We first introduce effective channel pruning and knowledge\n",
      "distillation schemes specialized for unconditional GANs. We then propose a\n",
      "novel content-aware method to guide the processes of both pruning and\n",
      "distillation. With content-awareness, we can effectively prune channels that\n",
      "are unimportant to the contents of interest, e.g., human faces, and focus our\n",
      "distillation on these regions, which significantly enhances the distillation\n",
      "quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the\n",
      "state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2\n",
      "by 11x with visually negligible image quality loss compared to the full-size\n",
      "model. More interestingly, when applied to various image manipulation tasks,\n",
      "our compressed model forms a smoother and better disentangled latent manifold,\n",
      "making it more effective for image editing. \n",
      "\n",
      "\n",
      "Modern neural networks have been successful in many regression-based tasks\n",
      "such as face recognition, facial landmark detection, and image generation. In\n",
      "this work, we investigate an intuitive but understudied characteristic of\n",
      "modern neural networks, namely, the nonsmoothness. The experiments using\n",
      "synthetic data confirm that such operations as ReLU and max pooling in modern\n",
      "neural networks lead to nonsmoothness. We quantify the nonsmoothness using a\n",
      "feature named the sum of the magnitude of peaks (SMP) and model the\n",
      "input-output relationships for building blocks of modern neural networks.\n",
      "Experimental results confirm that our model can accurately predict the\n",
      "statistical behaviors of the nonsmoothness as it propagates through such\n",
      "building blocks as the convolutional layer, the ReLU activation, and the max\n",
      "pooling layer. We envision that the nonsmoothness feature can potentially be\n",
      "used as a forensic tool for regression-based applications of neural networks. \n",
      "\n",
      "\n",
      "Low-quality face image restoration is a popular research direction in today's\n",
      "computer vision field. It can be used as a pre-work for tasks such as face\n",
      "detection and face recognition. At present, there is a lot of work to solve the\n",
      "problem of low-quality faces under various environmental conditions. This paper\n",
      "mainly focuses on the restoration of motion-blurred faces. In increasingly\n",
      "abundant mobile scenes, the fast recovery of motion-blurred faces can bring\n",
      "highly effective speed improvements in tasks such as face matching. In order to\n",
      "achieve this goal, a deblurring method for motion-blurred facial image signals\n",
      "based on generative adversarial networks(GANs) is proposed. It uses an\n",
      "end-to-end method to train a sharp image generator, i.e., a processor for\n",
      "motion-blurred facial images. This paper introduce the processing progress of\n",
      "motion-blurred images, the development and changes of GANs and some basic\n",
      "concepts. After that, it give the details of network structure and training\n",
      "optimization design of the image processor. Then we conducted a motion blur\n",
      "image generation experiment on some general facial data set, and used the pairs\n",
      "of blurred and sharp face image data to perform the training and testing\n",
      "experiments of the processor GAN, and gave some visual displays. Finally, MTCNN\n",
      "is used to detect the faces of the image generated by the deblurring processor,\n",
      "and compare it with the result of the blurred image. From the results, the\n",
      "processing effect of the deblurring processor on the motion-blurred picture has\n",
      "a significant improvement both in terms of intuition and evaluation indicators\n",
      "of face detection. \n",
      "\n",
      "\n",
      "The field of face recognition (FR) has witnessed great progress with the\n",
      "surge of deep learning. Existing methods mainly focus on extracting\n",
      "discriminative features, and directly compute the cosine or L2 distance by the\n",
      "point-to-point way without considering the context information. In this study,\n",
      "we make a key observation that the local con-text represented by the\n",
      "similarities between the instance and its inter-class neighbors1plays an\n",
      "important role forFR. Specifically, we attempt to incorporate the local\n",
      "in-formation in the feature space into the metric, and pro-pose a unified\n",
      "framework calledInter-class DiscrepancyAlignment(IDA), with two dedicated\n",
      "modules, Discrepancy Alignment Operator(IDA-DAO) andSupport Set\n",
      "Estimation(IDA-SSE). IDA-DAO is used to align the similarity scores considering\n",
      "the discrepancy between the images and its neighbors, which is defined by\n",
      "adaptive support sets on the hypersphere. For practical inference, it is\n",
      "difficult to acquire support set during online inference. IDA-SSE can provide\n",
      "convincing inter-class neighbors by introducing virtual candidate images\n",
      "generated with GAN. Further-more, we propose the learnable IDA-SSE, which can\n",
      "implicitly give estimation without the need of any other images in the\n",
      "evaluation process. The proposed IDA can be incorporated into existing FR\n",
      "systems seamlessly and efficiently. Extensive experiments demonstrate that this\n",
      "frame-work can 1) significantly improve the accuracy, and 2) make the model\n",
      "robust to the face images of various distributions.Without bells and whistles,\n",
      "our method achieves state-of-the-art performance on multiple standard FR\n",
      "benchmarks. \n",
      "\n",
      "\n",
      "StyleGAN is one of the state-of-the-art image generators which is well-known\n",
      "for synthesizing high-resolution and hyper-realistic face images. Though images\n",
      "generated by vanilla StyleGAN model are visually appealing, they sometimes\n",
      "contain prominent circular artifacts which severely degrade the quality of\n",
      "generated images. In this work, we provide a systematic investigation on how\n",
      "those circular artifacts are formed by studying the functionalities of\n",
      "different stages of vanilla StyleGAN architecture, with both mechanism analysis\n",
      "and extensive experiments. The key modules of vanilla StyleGAN that promote\n",
      "such undesired artifacts are highlighted. Our investigation also explains why\n",
      "the artifacts are usually circular, relatively small and rarely split into 2 or\n",
      "more parts. Besides, we propose a simple yet effective solution to remove the\n",
      "prominent circular artifacts for vanilla StyleGAN, by applying a novel\n",
      "pixel-instance normalization (PIN) layer. \n",
      "\n",
      "\n",
      "Although training data is essential for machine learning, railway companies\n",
      "are facing difficulties in gathering adequate images of defective equipment due\n",
      "to their proactive replacement of would be defective equipment. Nevertheless,\n",
      "proactive replacement is indispensable for safe and undisturbed operation of\n",
      "public transport. In this research, we have developed a model using CycleGAN to\n",
      "generate artificial images of defective equipment instead of real images. By\n",
      "adopting these generated images as training data, we verified that these images\n",
      "are indistinguishable from real images and they play a vital role in enhancing\n",
      "the accuracy of the defect detection models. \n",
      "\n",
      "\n",
      "Deep Neural Networks have achieved unprecedented success in the field of face\n",
      "recognition such that any individual can crawl the data of others from the\n",
      "Internet without their explicit permission for the purpose of training\n",
      "high-precision face recognition models, creating a serious violation of\n",
      "privacy. Recently, a well-known system named Fawkes (published in USENIX\n",
      "Security 2020) claimed this privacy threat can be neutralized by uploading\n",
      "cloaked user images instead of their original images. In this paper, we present\n",
      "Oriole, a system that combines the advantages of data poisoning attacks and\n",
      "evasion attacks, to thwart the protection offered by Fawkes, by training the\n",
      "attacker face recognition model with multi-cloaked images generated by Oriole.\n",
      "Consequently, the face recognition accuracy of the attack model is maintained\n",
      "and the weaknesses of Fawkes are revealed. Experimental results show that our\n",
      "proposed Oriole system is able to effectively interfere with the performance of\n",
      "the Fawkes system to achieve promising attacking results. Our ablation study\n",
      "highlights multiple principal factors that affect the performance of the Oriole\n",
      "system, including the DSSIM perturbation budget, the ratio of leaked clean user\n",
      "images, and the numbers of multi-cloaks for each uncloaked image. We also\n",
      "identify and discuss at length the vulnerabilities of Fawkes. We hope that the\n",
      "new methodology presented in this paper will inform the security community of a\n",
      "need to design more robust privacy-preserving deep learning models. \n",
      "\n",
      "\n",
      "Image generation from a single image using generative adversarial networks is\n",
      "quite interesting due to the realism of generated images. However, recent\n",
      "approaches need improvement for such realistic and diverse image generation,\n",
      "when the global context of the image is important such as in face, animal, and\n",
      "architectural image generation. This is mainly due to the use of fewer\n",
      "convolutional layers for mainly capturing the patch statistics and, thereby,\n",
      "not being able to capture global statistics very well. We solve this problem by\n",
      "using attention blocks at selected scales and feeding a random Gaussian blurred\n",
      "image to the discriminator for training. Our results are visually better than\n",
      "the state-of-the-art particularly in generating images that require global\n",
      "context. The diversity of our image generation, measured using the average\n",
      "standard deviation of pixels, is also better. \n",
      "\n",
      "\n",
      "The biggest challenge faced by a Machine Learning Engineer is the lack of\n",
      "data they have, especially for 2-dimensional images. The image is processed to\n",
      "be trained into a Machine Learning model so that it can recognize patterns in\n",
      "the data and provide predictions. This research is intended to create a\n",
      "solution using the Cycle Generative Adversarial Networks (GANs) algorithm in\n",
      "overcoming the problem of lack of data. Then use Style Transfer to be able to\n",
      "generate a new image based on the given style. Based on the results of testing\n",
      "the resulting model has been carried out several improvements, previously the\n",
      "loss value of the photo generator: 3.1267, monet style generator: 3.2026, photo\n",
      "discriminator: 0.6325, and monet style discriminator: 0.6931 to photo\n",
      "generator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956,\n",
      "and monet style discriminator: 0.4940. It is hoped that the research will make\n",
      "the application of this solution useful in the fields of Education, Arts,\n",
      "Information Technology, Medicine, Astronomy, Automotive and other important\n",
      "fields. \n",
      "\n",
      "\n",
      "We present a framework for training GANs with explicit control over generated\n",
      "images. We are able to control the generated image by settings exact attributes\n",
      "such as age, pose, expression, etc. Most approaches for editing GAN-generated\n",
      "images achieve partial control by leveraging the latent space disentanglement\n",
      "properties, obtained implicitly after standard GAN training. Such methods are\n",
      "able to change the relative intensity of certain attributes, but not explicitly\n",
      "set their values. Recently proposed methods, designed for explicit control over\n",
      "human faces, harness morphable 3D face models to allow fine-grained control\n",
      "capabilities in GANs. Unlike these methods, our control is not constrained to\n",
      "morphable 3D face model parameters and is extendable beyond the domain of human\n",
      "faces. Using contrastive learning, we obtain GANs with an explicitly\n",
      "disentangled latent space. This disentanglement is utilized to train\n",
      "control-encoders mapping human-interpretable inputs to suitable latent vectors,\n",
      "thus allowing explicit control. In the domain of human faces we demonstrate\n",
      "control over identity, age, pose, expression, hair color and illumination. We\n",
      "also demonstrate control capabilities of our framework in the domains of\n",
      "painted portraits and dog image generation. We demonstrate that our approach\n",
      "achieves state-of-the-art performance both qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "The last few years have witnessed the great success of non-linear generative\n",
      "models in synthesizing high-quality photorealistic face images. Many recent 3D\n",
      "facial texture reconstruction and pose manipulation from a single image\n",
      "approaches still rely on large and clean face datasets to train image-to-image\n",
      "Generative Adversarial Networks (GANs). Yet the collection of such a large\n",
      "scale high-resolution 3D texture dataset is still very costly and difficult to\n",
      "maintain age/ethnicity balance. Moreover, regression-based approaches suffer\n",
      "from generalization to the in-the-wild conditions and are unable to fine-tune\n",
      "to a target-image. In this work, we propose an unsupervised approach for\n",
      "one-shot 3D facial texture completion that does not require large-scale texture\n",
      "datasets, but rather harnesses the knowledge stored in 2D face generators. The\n",
      "proposed approach rotates an input image in 3D and fill-in the unseen regions\n",
      "by reconstructing the rotated image in a 2D face generator, based on the\n",
      "visible parts. Finally, we stitch the most visible textures at different angles\n",
      "in the UV image-plane. Further, we frontalize the target image by projecting\n",
      "the completed texture into the generator. The qualitative and quantitative\n",
      "experiments demonstrate that the completed UV textures and frontalized images\n",
      "are of high quality, resembles the original identity, can be used to train a\n",
      "texture GAN model for 3DMM fitting and improve pose-invariant face recognition. \n",
      "\n",
      "\n",
      "Although significant progress has been made in synthesizing high-quality and\n",
      "visually realistic face images by unconditional Generative Adversarial Networks\n",
      "(GANs), there still lacks of control over the generation process in order to\n",
      "achieve semantic face editing. In addition, it remains very challenging to\n",
      "maintain other face information untouched while editing the target attributes.\n",
      "In this paper, we propose a novel learning framework, called GuidedStyle, to\n",
      "achieve semantic face editing on StyleGAN by guiding the image generation\n",
      "process with a knowledge network. Furthermore, we allow an attention mechanism\n",
      "in StyleGAN generator to adaptively select a single layer for style\n",
      "manipulation. As a result, our method is able to perform disentangled and\n",
      "controllable edits along various attributes, including smiling, eyeglasses,\n",
      "gender, mustache and hair color. Both qualitative and quantitative results\n",
      "demonstrate the superiority of our method over other competing methods for\n",
      "semantic face editing. Moreover, we show that our model can be also applied to\n",
      "different types of real and artistic face editing, demonstrating strong\n",
      "generalization ability. \n",
      "\n",
      "\n",
      "Human pose transfer, as a misaligned image generation task, is very\n",
      "challenging. Existing methods cannot effectively utilize the input information,\n",
      "which often fail to preserve the style and shape of hair and clothes. In this\n",
      "paper, we propose an adaptive human pose transfer network with two hierarchical\n",
      "deformation levels. The first level generates human semantic parsing aligned\n",
      "with the target pose, and the second level generates the final textured person\n",
      "image in the target pose with the semantic guidance. To avoid the drawback of\n",
      "vanilla convolution that treats all the pixels as valid information, we use\n",
      "gated convolution in both two levels to dynamically select the important\n",
      "features and adaptively deform the image layer by layer. Our model has very few\n",
      "parameters and is fast to converge. Experimental results demonstrate that our\n",
      "model achieves better performance with more consistent hair, face and clothes\n",
      "with fewer parameters than state-of-the-art methods. Furthermore, our method\n",
      "can be applied to clothing texture transfer. \n",
      "\n",
      "\n",
      "To detect bias in face recognition networks, it can be useful to probe a\n",
      "network under test using samples in which only specific attributes vary in some\n",
      "controlled way. However, capturing a sufficiently large dataset with specific\n",
      "control over the attributes of interest is difficult. In this work, we describe\n",
      "a simulator that applies specific head pose and facial expression adjustments\n",
      "to images of previously unseen people. The simulator first fits a 3D morphable\n",
      "model to a provided image, applies the desired head pose and facial expression\n",
      "controls, then renders the model into an image. Next, a conditional Generative\n",
      "Adversarial Network (GAN) conditioned on the original image and the rendered\n",
      "morphable model is used to produce the image of the original person with the\n",
      "new facial expression and head pose. We call this conditional GAN -- MorphGAN.\n",
      "Images generated using MorphGAN conserve the identity of the person in the\n",
      "original image, and the provided control over head pose and facial expression\n",
      "allows test sets to be created to identify robustness issues of a facial\n",
      "recognition deep network with respect to pose and expression. Images generated\n",
      "by MorphGAN can also serve as data augmentation when training data are scarce.\n",
      "We show that by augmenting small datasets of faces with new poses and\n",
      "expressions improves the recognition performance by up to 9% depending on the\n",
      "augmentation and data scarcity. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) advance face synthesis through\n",
      "learning the underlying distribution of observed data. Despite the high-quality\n",
      "generated faces, some minority groups can be rarely generated from the trained\n",
      "models due to a biased image generation process. To study the issue, we first\n",
      "conduct an empirical study on a pre-trained face synthesis model. We observe\n",
      "that after training the GAN model not only carries the biases in the training\n",
      "data but also amplifies them to some degree in the image generation process. To\n",
      "further improve the fairness of image generation, we propose an interpretable\n",
      "baseline method to balance the output facial attributes without retraining. The\n",
      "proposed method shifts the interpretable semantic distribution in the latent\n",
      "space for a more balanced image generation while preserving the sample\n",
      "diversity. Besides producing more balanced data regarding a particular\n",
      "attribute (e.g., race, gender, etc.), our method is generalizable to handle\n",
      "more than one attribute at a time and synthesize samples of fine-grained\n",
      "subgroups. We further show the positive applicability of the balanced data\n",
      "sampled from GANs to quantify the biases in other face recognition systems,\n",
      "like commercial face attribute classifiers and face super-resolution\n",
      "algorithms. \n",
      "\n",
      "\n",
      "We present dynamic neural radiance fields for modeling the appearance and\n",
      "dynamics of a human face. Digitally modeling and reconstructing a talking human\n",
      "is a key building-block for a variety of applications. Especially, for\n",
      "telepresence applications in AR or VR, a faithful reproduction of the\n",
      "appearance including novel viewpoints or head-poses is required. In contrast to\n",
      "state-of-the-art approaches that model the geometry and material properties\n",
      "explicitly, or are purely image-based, we introduce an implicit representation\n",
      "of the head based on scene representation networks. To handle the dynamics of\n",
      "the face, we combine our scene representation network with a low-dimensional\n",
      "morphable model which provides explicit control over pose and expressions. We\n",
      "use volumetric rendering to generate images from this hybrid representation and\n",
      "demonstrate that such a dynamic neural scene representation can be learned from\n",
      "monocular input data only, without the need of a specialized capture setup. In\n",
      "our experiments, we show that this learned volumetric representation allows for\n",
      "photo-realistic image generation that surpasses the quality of state-of-the-art\n",
      "video-based reenactment methods. \n",
      "\n",
      "\n",
      "Few-shot image generation seeks to generate more data of a given domain, with\n",
      "only few available training examples. As it is unreasonable to expect to fully\n",
      "infer the distribution from just a few observations (e.g., emojis), we seek to\n",
      "leverage a large, related source domain as pretraining (e.g., human faces).\n",
      "Thus, we wish to preserve the diversity of the source domain, while adapting to\n",
      "the appearance of the target. We adapt a pretrained model, without introducing\n",
      "any additional parameters, to the few examples of the target domain. Crucially,\n",
      "we regularize the changes of the weights during this adaptation, in order to\n",
      "best preserve the information of the source dataset, while fitting the target.\n",
      "We demonstrate the effectiveness of our algorithm by generating high-quality\n",
      "results of different target domains, including those with extremely few\n",
      "examples (e.g., <10). We also analyze the performance of our method with\n",
      "respect to some important factors, such as the number of examples and the\n",
      "dissimilarity between the source and target domain. \n",
      "\n",
      "\n",
      "Recent studies have shown remarkable success in face image generations.\n",
      "However, most of the existing methods only generate face images from random\n",
      "noise, and cannot generate face images according to the specific attributes. In\n",
      "this paper, we focus on the problem of face synthesis from attributes, which\n",
      "aims at generating faces with specific characteristics corresponding to the\n",
      "given attributes. To this end, we propose a novel attributes aware face image\n",
      "generator method with generative adversarial networks called AFGAN.\n",
      "Specifically, we firstly propose a two-path embedding layer and self-attention\n",
      "mechanism to convert binary attribute vector to rich attribute features. Then\n",
      "three stacked generators generate $64 \\times 64$, $128 \\times 128$ and $256\n",
      "\\times 256$ resolution face images respectively by taking the attribute\n",
      "features as input. In addition, an image-attribute matching loss is proposed to\n",
      "enhance the correlation between the generated images and input attributes.\n",
      "Extensive experiments on CelebA demonstrate the superiority of our AFGAN in\n",
      "terms of both qualitative and quantitative evaluations. \n",
      "\n",
      "\n",
      "Interactive facial image manipulation attempts to edit single and multiple\n",
      "face attributes using a photo-realistic face and/or semantic mask as input. In\n",
      "the absence of the photo-realistic image (only sketch/mask available), previous\n",
      "methods only retrieve the original face but ignore the potential of aiding\n",
      "model controllability and diversity in the translation process. This paper\n",
      "proposes a sketch-to-image generation framework called S2FGAN, aiming to\n",
      "improve users' ability to interpret and flexibility of face attribute editing\n",
      "from a simple sketch. The proposed framework modifies the constrained latent\n",
      "space semantics trained on Generative Adversarial Networks (GANs). We employ\n",
      "two latent spaces to control the face appearance and adjust the desired\n",
      "attributes of the generated face. Instead of constraining the translation\n",
      "process by using a reference image, the users can command the model to retouch\n",
      "the generated images by involving the semantic information in the generation\n",
      "process. In this way, our method can manipulate single or multiple face\n",
      "attributes by only specifying attributes to be changed. Extensive experimental\n",
      "results on CelebAMask-HQ dataset empirically shows our superior performance and\n",
      "effectiveness on this task. Our method successfully outperforms\n",
      "state-of-the-art methods on attribute manipulation by exploiting greater\n",
      "control of attribute intensity. \n",
      "\n",
      "\n",
      "The restricted Boltzmann machine (RBM) is a representative generative model\n",
      "based on the concept of statistical mechanics. In spite of the strong merit of\n",
      "interpretability, unavailability of backpropagation makes it less competitive\n",
      "than other generative models. Here we derive differentiable loss functions for\n",
      "both binary and multinary RBMs. Then we demonstrate their learnability and\n",
      "performance by generating colored face images. \n",
      "\n",
      "\n",
      "In contrast to great success of memory-consuming face editing methods at a\n",
      "low resolution, to manipulate high-resolution (HR) facial images, i.e.,\n",
      "typically larger than 7682 pixels, with very limited memory is still\n",
      "challenging. This is due to the reasons of 1) intractable huge demand of\n",
      "memory; 2) inefficient multi-scale features fusion. To address these issues, we\n",
      "propose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for\n",
      "HR facial image editing. This framework features a local path for fine-grained\n",
      "local facial patch generation (i.e., patch-level HR, LOW memory) and a global\n",
      "path for global lowresolution (LR) facial structure monitoring (i.e.,\n",
      "image-level LR, LOW memory), which largely reduce memory requirements. Both\n",
      "paths work in a cooperative manner under a local-to-global consistency\n",
      "objective (i.e., for smooth stitching). In addition, we propose a lighter\n",
      "selective transfer unit for more efficient multi-scale features fusion,\n",
      "yielding higher fidelity facial attributes manipulation. Extensive experiments\n",
      "on CelebAHQ well demonstrate the memory efficiency as well as the high image\n",
      "generation quality of the proposed framework. \n",
      "\n",
      "\n",
      "Despite the recent success of face image generation with GANs, conditional\n",
      "hair editing remains challenging due to the under-explored complexity of its\n",
      "geometry and appearance. In this paper, we present MichiGAN\n",
      "(Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation\n",
      "method for interactive portrait hair manipulation. To provide user control over\n",
      "every major hair visual factor, we explicitly disentangle hair into four\n",
      "orthogonal attributes, including shape, structure, appearance, and background.\n",
      "For each of them, we design a corresponding condition module to represent,\n",
      "process, and convert user inputs, and modulate the image generation pipeline in\n",
      "ways that respect the natures of different visual attributes. All these\n",
      "condition modules are integrated with the backbone generator to form the final\n",
      "end-to-end network, which allows fully-conditioned hair generation from\n",
      "multiple user inputs. Upon it, we also build an interactive portrait hair\n",
      "editing system that enables straightforward manipulation of hair by projecting\n",
      "intuitive and high-level user inputs such as painted masks, guiding strokes, or\n",
      "reference photos to well-defined condition representations. Through extensive\n",
      "experiments and evaluations, we demonstrate the superiority of our method\n",
      "regarding both result quality and user controllability. The code is available\n",
      "at https://github.com/tzt101/MichiGAN. \n",
      "\n",
      "\n",
      "Pose-guided person image generation and animation aim to transform a source\n",
      "person image to target poses. These tasks require spatial manipulation of\n",
      "source data. However, Convolutional Neural Networks are limited by the lack of\n",
      "ability to spatially transform the inputs. In this paper, we propose a\n",
      "differentiable global-flow local-attention framework to reassemble the inputs\n",
      "at the feature level. This framework first estimates global flow fields between\n",
      "sources and targets. Then, corresponding local source feature patches are\n",
      "sampled with content-aware local attention coefficients. We show that our\n",
      "framework can spatially transform the inputs in an efficient manner. Meanwhile,\n",
      "we further model the temporal consistency for the person image animation task\n",
      "to generate coherent videos. The experiment results of both image generation\n",
      "and animation tasks demonstrate the superiority of our model. Besides,\n",
      "additional results of novel view synthesis and face image animation show that\n",
      "our model is applicable to other tasks requiring spatial transformation. The\n",
      "source code of our project is available at\n",
      "https://github.com/RenYurui/Global-Flow-Local-Attention. \n",
      "\n",
      "\n",
      "We propose a generative Causal Adversarial Network (CAN) for learning and\n",
      "sampling from conditional and interventional distributions. In contrast to the\n",
      "existing CausalGAN which requires the causal graph to be given, our proposed\n",
      "framework learns the causal relations from the data and generates samples\n",
      "accordingly. The proposed CAN comprises a two-fold process namely Label\n",
      "Generation Network (LGN) and Conditional Image Generation Network (CIGN). The\n",
      "LGN is a GAN-based architecture which learns and samples from the causal model\n",
      "over labels. The sampled labels are then fed to CIGN, a conditional GAN\n",
      "architecture, which learns the relationships amongst labels and pixels and\n",
      "pixels themselves and generates samples based on them. This framework is\n",
      "equipped with an intervention mechanism which enables. the model to generate\n",
      "samples from interventional distributions. We quantitatively and qualitatively\n",
      "assess the performance of CAN and empirically show that our model is able to\n",
      "generate both interventional and conditional samples without having access to\n",
      "the causal graph for the application of face generation on CelebA data. \n",
      "\n",
      "\n",
      "Recently, deep generative adversarial networks for image generation have\n",
      "advanced rapidly; yet, only a small amount of research has focused on\n",
      "generative models for irregular structures, particularly meshes. Nonetheless,\n",
      "mesh generation and synthesis remains a fundamental topic in computer graphics.\n",
      "In this work, we propose a novel framework for synthesizing geometric textures.\n",
      "It learns geometric texture statistics from local neighborhoods (i.e., local\n",
      "triangular patches) of a single reference 3D model. It learns deep features on\n",
      "the faces of the input triangulation, which is used to subdivide and generate\n",
      "offsets across multiple scales, without parameterization of the reference or\n",
      "target mesh. Our network displaces mesh vertices in any direction (i.e., in the\n",
      "normal and tangential direction), enabling synthesis of geometric textures,\n",
      "which cannot be expressed by a simple 2D displacement map. Learning and\n",
      "synthesizing on local geometric patches enables a genus-oblivious framework,\n",
      "facilitating texture transfer between shapes of different genus. \n",
      "\n",
      "\n",
      "Deep Convolutional Neural Networks (DCNNs) are currently the method of choice\n",
      "both for generative, as well as for discriminative learning in computer vision\n",
      "and machine learning. The success of DCNNs can be attributed to the careful\n",
      "selection of their building blocks (e.g., residual blocks, rectifiers,\n",
      "sophisticated normalization schemes, to mention but a few). In this paper, we\n",
      "propose $\\Pi$-Nets, a new class of function approximators based on polynomial\n",
      "expansions. $\\Pi$-Nets are polynomial neural networks, i.e., the output is a\n",
      "high-order polynomial of the input. The unknown parameters, which are naturally\n",
      "represented by high-order tensors, are estimated through a collective tensor\n",
      "factorization with factors sharing. We introduce three tensor decompositions\n",
      "that significantly reduce the number of parameters and show how they can be\n",
      "efficiently implemented by hierarchical neural networks. We empirically\n",
      "demonstrate that $\\Pi$-Nets are very expressive and they even produce good\n",
      "results without the use of non-linear activation functions in a large battery\n",
      "of tasks and signals, i.e., images, graphs, and audio. When used in conjunction\n",
      "with activation functions, $\\Pi$-Nets produce state-of-the-art results in three\n",
      "challenging tasks, i.e. image generation, face verification and 3D mesh\n",
      "representation learning. The source code is available at\n",
      "\\url{https://github.com/grigorisg9gr/polynomial_nets}. \n",
      "\n",
      "\n",
      "Text-to-Face (TTF) synthesis is a challenging task with great potential for\n",
      "diverse computer vision applications. Compared to Text-to-Image (TTI) synthesis\n",
      "tasks, the textual description of faces can be much more complicated and\n",
      "detailed due to the variety of facial attributes and the parsing of high\n",
      "dimensional abstract natural language. In this paper, we propose a Text-to-Face\n",
      "model that not only produces images in high resolution (1024x1024) with\n",
      "text-to-image consistency, but also outputs multiple diverse faces to cover a\n",
      "wide range of unspecified facial features in a natural way. By fine-tuning the\n",
      "multi-label classifier and image encoder, our model obtains the vectors and\n",
      "image embeddings which are used to transform the input noise vector sampled\n",
      "from the normal distribution. Afterwards, the transformed noise vector is fed\n",
      "into a pre-trained high-resolution image generator to produce a set of faces\n",
      "with the desired facial attributes. We refer to our model as TTF-HD.\n",
      "Experimental results show that TTF-HD generates high-quality faces with\n",
      "state-of-the-art performance. \n",
      "\n",
      "\n",
      "While deep learning technologies are now capable of generating realistic\n",
      "images confusing humans, the research efforts are turning to the synthesis of\n",
      "images for more concrete and application-specific purposes. Facial image\n",
      "generation based on vocal characteristics from speech is one of such important\n",
      "yet challenging tasks. It is the key enabler to influential use cases of image\n",
      "generation, especially for business in public security and entertainment.\n",
      "Existing solutions to the problem of speech2face renders limited image quality\n",
      "and fails to preserve facial similarity due to the lack of quality dataset for\n",
      "training and appropriate integration of vocal features. In this paper, we\n",
      "investigate these key technical challenges and propose Speech Fusion to Face,\n",
      "or SF2F in short, attempting to address the issue of facial image quality and\n",
      "the poor connection between vocal feature domain and modern image generation\n",
      "models. By adopting new strategies on data model and training, we demonstrate\n",
      "dramatic performance boost over state-of-the-art solution, by doubling the\n",
      "recall of individual identity, and lifting the quality score from 15 to 19\n",
      "based on the mutual information score with VGGFace classifier. \n",
      "\n",
      "\n",
      "Learning disentangled representations of data is a fundamental problem in\n",
      "artificial intelligence. Specifically, disentangled latent representations\n",
      "allow generative models to control and compose the disentangled factors in the\n",
      "synthesis process. Current methods, however, require extensive supervision and\n",
      "training, or instead, noticeably compromise quality. In this paper, we present\n",
      "a method that learns how to represent data in a disentangled way, with minimal\n",
      "supervision, manifested solely using available pre-trained networks. Our key\n",
      "insight is to decouple the processes of disentanglement and synthesis, by\n",
      "employing a leading pre-trained unconditional image generator, such as\n",
      "StyleGAN. By learning to map into its latent space, we leverage both its\n",
      "state-of-the-art quality, and its rich and expressive latent space, without the\n",
      "burden of training it. We demonstrate our approach on the complex and high\n",
      "dimensional domain of human heads. We evaluate our method qualitatively and\n",
      "quantitatively, and exhibit its success with de-identification operations and\n",
      "with temporal identity coherency in image sequences. Through extensive\n",
      "experimentation, we show that our method successfully disentangles identity\n",
      "from other facial attributes, surpassing existing methods, even though they\n",
      "require more training and supervision. \n",
      "\n",
      "\n",
      "In recent years, Generative Adversarial Networks (GANs) have improved\n",
      "steadily towards generating increasingly impressive real-world images. It is\n",
      "useful to steer the image generation process for purposes such as content\n",
      "creation. This can be done by conditioning the model on additional information.\n",
      "However, when conditioning on additional information, there still exists a\n",
      "large set of images that agree with a particular conditioning. This makes it\n",
      "unlikely that the generated image is exactly as envisioned by a user, which is\n",
      "problematic for practical content creation scenarios such as generating facial\n",
      "composites or stock photos. To solve this problem, we propose a single pipeline\n",
      "for text-to-image generation and manipulation. In the first part of our\n",
      "pipeline we introduce textStyleGAN, a model that is conditioned on text. In the\n",
      "second part of our pipeline we make use of the pre-trained weights of\n",
      "textStyleGAN to perform semantic facial image manipulation. The approach works\n",
      "by finding semantic directions in latent space. We show that this method can be\n",
      "used to manipulate facial images for a wide range of attributes. Finally, we\n",
      "introduce the CelebTD-HQ dataset, an extension to CelebA-HQ, consisting of\n",
      "faces and corresponding textual descriptions. \n",
      "\n",
      "\n",
      "Our ability to sample realistic natural images, particularly faces, has\n",
      "advanced by leaps and bounds in recent years, yet our ability to exert\n",
      "fine-tuned control over the generative process has lagged behind. If this new\n",
      "technology is to find practical uses, we need to achieve a level of control\n",
      "over generative networks which, without sacrificing realism, is on par with\n",
      "that seen in computer graphics and character animation. To this end we propose\n",
      "ConfigNet, a neural face model that allows for controlling individual aspects\n",
      "of output images in semantically meaningful ways and that is a significant step\n",
      "on the path towards finely-controllable neural rendering. ConfigNet is trained\n",
      "on real face images as well as synthetic face renders. Our novel method uses\n",
      "synthetic data to factorize the latent space into elements that correspond to\n",
      "the inputs of a traditional rendering pipeline, separating aspects such as head\n",
      "pose, facial expression, hair style, illumination, and many others which are\n",
      "very hard to annotate in real data. The real images, which are presented to the\n",
      "network without labels, extend the variety of the generated images and\n",
      "encourage realism. Finally, we propose an evaluation criterion using an\n",
      "attribute detection network combined with a user study and demonstrate\n",
      "state-of-the-art individual control over attributes in the output images. \n",
      "\n",
      "\n",
      "We explore different design choices for injecting noise into generative\n",
      "adversarial networks (GANs) with the goal of disentangling the latent space.\n",
      "Instead of traditional approaches, we propose feeding multiple noise codes\n",
      "through separate fully-connected layers respectively. The aim is restricting\n",
      "the influence of each noise code to specific parts of the generated image. We\n",
      "show that disentanglement in the first layer of the generator network leads to\n",
      "disentanglement in the generated image. Through a grid-based structure, we\n",
      "achieve several aspects of disentanglement without complicating the network\n",
      "architecture and without requiring labels. We achieve spatial disentanglement,\n",
      "scale-space disentanglement, and disentanglement of the foreground object from\n",
      "the background style allowing fine-grained control over the generated images.\n",
      "Examples include changing facial expressions in face images, changing beak\n",
      "length in bird images, and changing car dimensions in car images. This\n",
      "empirically leads to better disentanglement scores than state-of-the-art\n",
      "methods on the FFHQ dataset. \n",
      "\n",
      "\n",
      "Face verification aims at determining whether a pair of face images belongs\n",
      "to the same identity. Recent studies have revealed the negative impact of\n",
      "facial makeup on the verification performance. With the rapid development of\n",
      "deep generative models, this paper proposes a semanticaware makeup cleanser\n",
      "(SAMC) to remove facial makeup under different poses and expressions and\n",
      "achieve verification via generation. The intuition lies in the fact that makeup\n",
      "is a combined effect of multiple cosmetics and tailored treatments should be\n",
      "imposed on different cosmetic regions. To this end, we present both\n",
      "unsupervised and supervised semantic-aware learning strategies in SAMC. At\n",
      "image level, an unsupervised attention module is jointly learned with the\n",
      "generator to locate cosmetic regions and estimate the degree. At feature level,\n",
      "we resort to the effort of face parsing merely in training phase and design a\n",
      "localized texture loss to serve complements and pursue superior synthetic\n",
      "quality. The experimental results on four makeuprelated datasets verify that\n",
      "SAMC not only produces appealing de-makeup outputs at a resolution of 256*256,\n",
      "but also facilitates makeup-invariant face verification through image\n",
      "generation. \n",
      "\n",
      "\n",
      "This paper addresses a major flaw of the cycle consistency loss when used to\n",
      "preserve the input appearance in the face-to-face synthesis domain. In\n",
      "particular, we show that the images generated by a network trained using this\n",
      "loss conceal a noise that hinders their use for further tasks. To overcome this\n",
      "limitation, we propose a ''recurrent cycle consistency loss\" which for\n",
      "different sequences of target attributes minimises the distance between the\n",
      "output images, independent of any intermediate step. We empirically validate\n",
      "not only that our loss enables the re-use of generated images, but that it also\n",
      "improves their quality. In addition, we propose the very first network that\n",
      "covers the task of unconstrained landmark-guided face-to-face synthesis.\n",
      "Contrary to previous works, our proposed approach enables the transfer of a\n",
      "particular set of input features to a large span of poses and expressions,\n",
      "whereby the target landmarks become the ground-truth points. We then evaluate\n",
      "the consistency of our proposed approach to synthesise faces at the target\n",
      "landmarks. To the best of our knowledge, we are the first to propose a loss to\n",
      "overcome the limitation of the cycle consistency loss, and the first to propose\n",
      "an ''in-the-wild'' landmark guided synthesis approach. Code and models for this\n",
      "paper can be found in https://github.com/ESanchezLozano/GANnotation \n",
      "\n",
      "\n",
      "Gaze redirection aims at manipulating the gaze of a given face image with\n",
      "respect to a desired direction (i.e., a reference angle) and it can be applied\n",
      "to many real life scenarios, such as video-conferencing or taking group photos.\n",
      "However, previous work on this topic mainly suffers of two limitations: (1)\n",
      "Low-quality image generation and (2) Low redirection precision. In this paper,\n",
      "we propose to alleviate these problems by means of a novel gaze redirection\n",
      "framework which exploits both a numerical and a pictorial direction guidance,\n",
      "jointly with a coarse-to-fine learning strategy. Specifically, the coarse\n",
      "branch learns the spatial transformation which warps input image according to\n",
      "desired gaze. On the other hand, the fine-grained branch consists of a\n",
      "generator network with conditional residual image learning and a multi-task\n",
      "discriminator. This second branch reduces the gap between the previously warped\n",
      "image and the ground-truth image and recovers finer texture details. Moreover,\n",
      "we propose a numerical and pictorial guidance module~(NPG) which uses a\n",
      "pictorial gazemap description and numerical angles as an extra guide to further\n",
      "improve the precision of gaze redirection. Extensive experiments on a benchmark\n",
      "dataset show that the proposed method outperforms the state-of-the-art\n",
      "approaches in terms of both image quality and redirection precision. The code\n",
      "is available at https://github.com/jingjingchen777/CFGR \n",
      "\n",
      "\n",
      "The instability in GAN training has been a long-standing problem despite\n",
      "remarkable research efforts. We identify that instability issues stem from\n",
      "difficulties of performing feature matching with mini-batch statistics, due to\n",
      "a fragile balance between the fixed target distribution and the progressively\n",
      "generated distribution. In this work, we propose Feature Quantization (FQ) for\n",
      "the discriminator, to embed both true and fake data samples into a shared\n",
      "discrete space. The quantized values of FQ are constructed as an evolving\n",
      "dictionary, which is consistent with feature statistics of the recent\n",
      "distribution history. Hence, FQ implicitly enables robust feature matching in a\n",
      "compact space. Our method can be easily plugged into existing GAN models, with\n",
      "little computational overhead in training. We apply FQ to 3 representative GAN\n",
      "models on 9 benchmarks: BigGAN for image generation, StyleGAN for face\n",
      "synthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive\n",
      "experimental results show that the proposed FQ-GAN can improve the FID scores\n",
      "of baseline methods by a large margin on a variety of tasks, achieving new\n",
      "state-of-the-art performance. \n",
      "\n",
      "\n",
      "Histopathology images; microscopy images of stained tissue biopsies contain\n",
      "fundamental prognostic information that forms the foundation of pathological\n",
      "analysis and diagnostic medicine. However, diagnostics from histopathology\n",
      "images generally rely on a visual cognitive assessment of tissue slides which\n",
      "implies an inherent element of interpretation and hence subjectivity. Access to\n",
      "digitized histopathology images enabled the development of computational\n",
      "systems aiming at reducing manual intervention and automating parts of\n",
      "pathologists' workflow. Specifically, applications of deep learning to\n",
      "histopathology image analysis now offer opportunities for better quantitative\n",
      "modeling of disease appearance and hence possibly improved prediction of\n",
      "disease aggressiveness and patient outcome. However digitized histopathology\n",
      "tissue slides are unique in a variety of ways and come with their own set of\n",
      "computational challenges. In this survey, we summarize the different challenges\n",
      "facing computational systems for digital pathology and provide a review of\n",
      "state-of-the-art works that developed deep learning-based solutions for the\n",
      "predictive modeling of histopathology images from a detection, stain\n",
      "normalization, segmentation, and tissue classification perspective. We then\n",
      "discuss the challenges facing the validation and integration of such deep\n",
      "learning-based computational systems in clinical workflow and reflect on future\n",
      "opportunities for histopathology derived image measurements and better\n",
      "predictive modeling. \n",
      "\n",
      "\n",
      "In this work we present an adversarial training algorithm that exploits\n",
      "correlations in video to learn --without supervision-- an image generator model\n",
      "with a disentangled latent space. The proposed methodology requires only a few\n",
      "modifications to the standard algorithm of Generative Adversarial Networks\n",
      "(GAN) and involves training with sets of frames taken from short videos. We\n",
      "train our model over two datasets of face-centered videos which present\n",
      "different people speaking or moving the head: VidTIMIT and YouTube Faces\n",
      "datasets. We found that our proposal allows us to split the generator latent\n",
      "space into two subspaces. One of them controls content attributes, those that\n",
      "do not change along short video sequences. For the considered datasets, this is\n",
      "the identity of the generated face. The other subspace controls motion\n",
      "attributes, those attributes that are observed to change along short videos. We\n",
      "observed that these motion attributes are face expressions, head orientation,\n",
      "lips and eyes movement. The presented experiments provide quantitative and\n",
      "qualitative evidence supporting that the proposed methodology induces a\n",
      "disentangling of this two kinds of attributes in the latent space. \n",
      "\n",
      "\n",
      "Sparse representation-based classification (SRC) has been shown to achieve a\n",
      "high level of accuracy in face recognition (FR). However, matching faces\n",
      "captured in unconstrained video against a gallery with a single reference\n",
      "facial still per individual typically yields low accuracy. For improved\n",
      "robustness to intra-class variations, SRC techniques for FR have recently been\n",
      "extended to incorporate variational information from an external generic set\n",
      "into an auxiliary dictionary. Despite their success in handling linear\n",
      "variations, non-linear variations (e.g., pose and expressions) between probe\n",
      "and reference facial images cannot be accurately reconstructed with a linear\n",
      "combination of images in the gallery and auxiliary dictionaries because they do\n",
      "not share the same type of variations. In order to account for non-linear\n",
      "variations due to pose, a paired sparse representation model is introduced\n",
      "allowing for joint use of variational information and synthetic face images.\n",
      "The proposed model, called synthetic plus variational model, reconstructs a\n",
      "probe image by jointly using (1) a variational dictionary and (2) a gallery\n",
      "dictionary augmented with a set of synthetic images generated over a wide\n",
      "diversity of pose angles. The augmented gallery dictionary is then encouraged\n",
      "to pair the same sparsity pattern with the variational dictionary for similar\n",
      "pose angles by solving a newly formulated simultaneous sparsity-based\n",
      "optimization problem. Experimental results obtained on Chokepoint and COX-S2V\n",
      "datasets, using different face representations, indicate that the proposed\n",
      "approach can outperform state-of-the-art SRC-based methods for still-to-video\n",
      "FR with a single sample per person. \n",
      "\n",
      "\n",
      "Current developments in computer vision and deep learning allow to\n",
      "automatically generate hyper-realistic images, hardly distinguishable from real\n",
      "ones. In particular, human face generation achieved a stunning level of\n",
      "realism, opening new opportunities for the creative industry but, at the same\n",
      "time, new scary scenarios where such content can be maliciously misused.\n",
      "Therefore, it is essential to develop innovative methodologies to automatically\n",
      "tell apart real from computer generated multimedia, possibly able to follow the\n",
      "evolution and continuous improvement of data in terms of quality and realism.\n",
      "In the last few years, several deep learning-based solutions have been proposed\n",
      "for this problem, mostly based on Convolutional Neural Networks (CNNs).\n",
      "Although results are good in controlled conditions, it is not clear how such\n",
      "proposals can adapt to real-world scenarios, where learning needs to\n",
      "continuously evolve as new types of generated data appear. In this work, we\n",
      "tackle this problem by proposing an approach based on incremental learning for\n",
      "the detection and classification of GAN-generated images. Experiments on a\n",
      "dataset comprising images generated by several GAN-based architectures show\n",
      "that the proposed method is able to correctly perform discrimination when new\n",
      "GANs are presented to the network \n",
      "\n",
      "\n",
      "In this paper we present, to the best of our knowledge, the first method to\n",
      "learn a generative model of 3D shapes from natural images in a fully\n",
      "unsupervised way. For example, we do not use any ground truth 3D or 2D\n",
      "annotations, stereo video, and ego-motion during the training. Our approach\n",
      "follows the general strategy of Generative Adversarial Networks, where an image\n",
      "generator network learns to create image samples that are realistic enough to\n",
      "fool a discriminator network into believing that they are natural images. In\n",
      "contrast, in our approach the image generation is split into 2 stages. In the\n",
      "first stage a generator network outputs 3D objects. In the second, a\n",
      "differentiable renderer produces an image of the 3D objects from random\n",
      "viewpoints. The key observation is that a realistic 3D object should yield a\n",
      "realistic rendering from any plausible viewpoint. Thus, by randomizing the\n",
      "choice of the viewpoint our proposed training forces the generator network to\n",
      "learn an interpretable 3D representation disentangled from the viewpoint. In\n",
      "this work, a 3D representation consists of a triangle mesh and a texture map\n",
      "that is used to color the triangle surface by using the UV-mapping technique.\n",
      "We provide analysis of our learning approach, expose its ambiguities and show\n",
      "how to overcome them. Experimentally, we demonstrate that our method can learn\n",
      "realistic 3D shapes of faces by using only the natural images of the FFHQ\n",
      "dataset. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) have demonstrated great success in\n",
      "generating various visual content. However, images generated by existing GANs\n",
      "are often of attributes (e.g., smiling expression) learned from one image\n",
      "domain. As a result, generating images of multiple attributes requires many\n",
      "real samples possessing multiple attributes which are very resource expensive\n",
      "to be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to\n",
      "learn multiple attributes from different image domains through an intersecting\n",
      "architecture. For example, given two image domains $X_1$ and $X_2$ with certain\n",
      "attributes, the intersection $X_1 \\cap X_2$ denotes a new domain where images\n",
      "possess the attributes from both $X_1$ and $X_2$ domains. The proposed\n",
      "IntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish\n",
      "between generated and real samples of different domains, and three generators\n",
      "where the intersection generator is trained against both discriminators. And an\n",
      "overall adversarial loss function is defined over three generators. As a\n",
      "result, our proposed IntersectGAN can be trained on multiple domains of which\n",
      "each presents one specific attribute, and eventually eliminates the need of\n",
      "real sample images simultaneously possessing multiple attributes. By using the\n",
      "CelebFaces Attributes dataset, our proposed IntersectGAN is able to produce\n",
      "high quality face images possessing multiple attributes (e.g., a face with\n",
      "black hair and a smiling expression). Both qualitative and quantitative\n",
      "evaluations are conducted to compare our proposed IntersectGAN with other\n",
      "baseline methods. Besides, several different applications of IntersectGAN have\n",
      "been explored with promising results. \n",
      "\n",
      "\n",
      "Image generating neural networks are mostly viewed as black boxes, where any\n",
      "change in the input can have a number of globally effective changes on the\n",
      "output. In this work, we propose a method for learning disentangled\n",
      "representations to allow for localized image manipulations. We use face images\n",
      "as our example of choice. Depending on the image region, identity and other\n",
      "facial attributes can be modified. The proposed network can transfer parts of a\n",
      "face such as shape and color of eyes, hair, mouth, etc.~directly between\n",
      "persons while all other parts of the face remain unchanged. The network allows\n",
      "to generate modified images which appear like realistic images. Our model\n",
      "learns disentangled representations by weak supervision. We propose a localized\n",
      "resnet autoencoder optimized using several loss functions including a loss\n",
      "based on the semantic segmentation, which we interpret as masks, and a loss\n",
      "which enforces disentanglement by decomposition of the latent space into\n",
      "statistically independent subspaces. We evaluate the proposed solution w.r.t.\n",
      "disentanglement and generated image quality. Convincing results are\n",
      "demonstrated using the CelebA dataset. \n",
      "\n",
      "\n",
      "Neural architecture search (NAS) has witnessed prevailing success in image\n",
      "classification and (very recently) segmentation tasks. In this paper, we\n",
      "present the first preliminary study on introducing the NAS algorithm to\n",
      "generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and\n",
      "GANs faces its unique challenges. We define the search space for the generator\n",
      "architectural variations and use an RNN controller to guide the search, with\n",
      "parameter sharing and dynamic-resetting to accelerate the process. Inception\n",
      "score is adopted as the reward, and a multi-level search strategy is introduced\n",
      "to perform NAS in a progressive way. Experiments validate the effectiveness of\n",
      "AutoGAN on the task of unconditional image generation. Specifically, our\n",
      "discovered architectures achieve highly competitive performance compared to\n",
      "current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art\n",
      "FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also\n",
      "conclude with a discussion of the current limitations and future potential of\n",
      "AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN \n",
      "\n",
      "\n",
      "In this work, we propose a novel Cycle In Cycle Generative Adversarial\n",
      "Network (C$^2$GAN) for the task of keypoint-guided image generation. The\n",
      "proposed C$^2$GAN is a cross-modal framework exploring a joint exploitation of\n",
      "the keypoint and the image data in an interactive manner. C$^2$GAN contains two\n",
      "different types of generators, i.e., keypoint-oriented generator and\n",
      "image-oriented generator. Both of them are mutually connected in an end-to-end\n",
      "learnable fashion and explicitly form three cycled sub-networks, i.e., one\n",
      "image generation cycle and two keypoint generation cycles. Each cycle not only\n",
      "aims at reconstructing the input domain, and also produces useful output\n",
      "involving in the generation of another cycle. By so doing, the cycles constrain\n",
      "each other implicitly, which provides complementary information from the two\n",
      "different modalities and brings extra supervision across cycles, thus\n",
      "facilitating more robust optimization of the whole network. Extensive\n",
      "experimental results on two publicly available datasets, i.e., Radboud Faces\n",
      "and Market-1501, demonstrate that our approach is effective to generate more\n",
      "photo-realistic images compared with state-of-the-art models. \n",
      "\n",
      "\n",
      "The task of image generation started to receive some attention from artists\n",
      "and designers to inspire them in new creations. However, exploiting the results\n",
      "of deep generative models such as Generative Adversarial Networks can be long\n",
      "and tedious given the lack of existing tools. In this work, we propose a simple\n",
      "strategy to inspire creators with new generations learned from a dataset of\n",
      "their choice, while providing some control on them. We design a simple\n",
      "optimization method to find the optimal latent parameters corresponding to the\n",
      "closest generation to any input inspirational image. Specifically, we allow the\n",
      "generation given an inspirational image of the user choice by performing\n",
      "several optimization steps to recover optimal parameters from the model's\n",
      "latent space. We tested several exploration methods starting with classic\n",
      "gradient descents to gradient-free optimizers. Many gradient-free optimizers\n",
      "just need comparisons (better/worse than another image), so that they can even\n",
      "be used without numerical criterion, without inspirational image, but with only\n",
      "with human preference. Thus, by iterating on one's preferences we could make\n",
      "robust Facial Composite or Fashion Generation algorithms. High resolution of\n",
      "the produced design generations are obtained using progressive growing of GANs.\n",
      "Our results on four datasets of faces, fashion images, and textures show that\n",
      "satisfactory images are effectively retrieved in most cases. \n",
      "\n",
      "\n",
      "Style transfer is a problem of rendering image with some content in the style\n",
      "of another image, for example a family photo in the style of a painting of some\n",
      "famous artist. The drawback of classical style transfer algorithm is that it\n",
      "imposes style uniformly on all parts of the content image, which perturbs\n",
      "central objects on the content image, such as faces or text, and makes them\n",
      "unrecognizable. This work proposes a novel style transfer algorithm which\n",
      "automatically detects central objects on the content image, generates spatial\n",
      "importance mask and imposes style non-uniformly: central objects are stylized\n",
      "less to preserve their recognizability and other parts of the image are\n",
      "stylized as usual to preserve the style. Three methods of automatic central\n",
      "object detection are proposed and evaluated qualitatively and via a user\n",
      "evaluation study. Both comparisons demonstrate higher quality of stylization\n",
      "compared to the classical style transfer method. \n",
      "\n",
      "\n",
      "Caricature is an abstraction of a real person which distorts or exaggerates\n",
      "certain features, but still retains a likeness. While most existing works focus\n",
      "on 3D caricature reconstruction from 2D caricatures or translating 2D photos to\n",
      "2D caricatures, this paper presents a real-time and automatic algorithm for\n",
      "creating expressive 3D caricatures with caricature style texture map from 2D\n",
      "photos or videos. To solve this challenging ill-posed reconstruction problem\n",
      "and cross-domain translation problem, we first reconstruct the 3D face shape\n",
      "for each frame, and then translate 3D face shape from normal style to\n",
      "caricature style by a novel identity and expression preserving VAE-CycleGAN.\n",
      "Based on a labeling formulation, the caricature texture map is constructed from\n",
      "a set of multi-view caricature images generated by CariGANs. The effectiveness\n",
      "and efficiency of our method are demonstrated by comparison with baseline\n",
      "implementations. The perceptual study shows that the 3D caricatures generated\n",
      "by our method meet people's expectations of 3D caricature style. \n",
      "\n",
      "\n",
      "Image translation across different domains has attracted much attention in\n",
      "both machine learning and computer vision communities. Taking the translation\n",
      "from source domain $\\mathcal{D}_s$ to target domain $\\mathcal{D}_t$ as an\n",
      "example, existing algorithms mainly rely on two kinds of loss for training: One\n",
      "is the discrimination loss, which is used to differentiate images generated by\n",
      "the models and natural images; the other is the reconstruction loss, which\n",
      "measures the difference between an original image and the reconstructed version\n",
      "through $\\mathcal{D}_s\\to\\mathcal{D}_t\\to\\mathcal{D}_s$ translation. In this\n",
      "work, we introduce a new kind of loss, multi-path consistency loss, which\n",
      "evaluates the differences between direct translation\n",
      "$\\mathcal{D}_s\\to\\mathcal{D}_t$ and indirect translation\n",
      "$\\mathcal{D}_s\\to\\mathcal{D}_a\\to\\mathcal{D}_t$ with $\\mathcal{D}_a$ as an\n",
      "auxiliary domain, to regularize training. For multi-domain translation (at\n",
      "least, three) which focuses on building translation models between any two\n",
      "domains, at each training iteration, we randomly select three domains, set them\n",
      "respectively as the source, auxiliary and target domains, build the multi-path\n",
      "consistency loss and optimize the network. For two-domain translation, we need\n",
      "to introduce an additional auxiliary domain and construct the multi-path\n",
      "consistency loss. We conduct various experiments to demonstrate the\n",
      "effectiveness of our proposed methods, including face-to-face translation,\n",
      "paint-to-photo translation, and de-raining/de-noising translation. \n",
      "\n",
      "\n",
      "Image generation has raised tremendous attention in both academic and\n",
      "industrial areas, especially for the conditional and target-oriented image\n",
      "generation, such as criminal portrait and fashion design. Although the current\n",
      "studies have achieved preliminary results along this direction, they always\n",
      "focus on class labels as the condition where spatial contents are randomly\n",
      "generated from latent vectors. Edge details are usually blurred since spatial\n",
      "information is difficult to preserve. In light of this, we propose a novel\n",
      "Spatially Constrained Generative Adversarial Network (SCGAN), which decouples\n",
      "the spatial constraints from the latent vector and makes these constraints\n",
      "feasible as additional controllable signals. To enhance the spatial\n",
      "controllability, a generator network is specially designed to take a semantic\n",
      "segmentation, a latent vector and an attribute-level label as inputs step by\n",
      "step. Besides, a segmentor network is constructed to impose spatial constraints\n",
      "on the generator. Experimentally, we provide both visual and quantitative\n",
      "results on CelebA and DeepFashion datasets, and demonstrate that the proposed\n",
      "SCGAN is very effective in controlling the spatial contents as well as\n",
      "generating high-quality images. \n",
      "\n",
      "\n",
      "Over the past few years, Generative Adversarial Networks (GANs) have garnered\n",
      "increased interest among researchers in Computer Vision, with applications\n",
      "including, but not limited to, image generation, translation, imputation, and\n",
      "super-resolution. Nevertheless, no GAN-based method has been proposed in the\n",
      "literature that can successfully represent, generate or translate 3D facial\n",
      "shapes (meshes). This can be primarily attributed to two facts, namely that (a)\n",
      "publicly available 3D face databases are scarce as well as limited in terms of\n",
      "sample size and variability (e.g., few subjects, little diversity in race and\n",
      "gender), and (b) mesh convolutions for deep networks present several challenges\n",
      "that are not entirely tackled in the literature, leading to operator\n",
      "approximations and model instability, often failing to preserve high-frequency\n",
      "components of the distribution. As a result, linear methods such as Principal\n",
      "Component Analysis (PCA) have been mainly utilized towards 3D shape analysis,\n",
      "despite being unable to capture non-linearities and high frequency details of\n",
      "the 3D face - such as eyelid and lip variations. In this work, we present\n",
      "3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D\n",
      "facial surfaces, while retaining the high frequency details of 3D face shapes.\n",
      "We conduct an extensive series of both qualitative and quantitative\n",
      "experiments, where the merits of 3DFaceGAN are clearly demonstrated against\n",
      "other, state-of-the-art methods in tasks such as 3D shape representation,\n",
      "generation, and translation. \n",
      "\n",
      "\n",
      "Digitally retouching images has become a popular trend, with people posting\n",
      "altered images on social media and even magazines posting flawless facial\n",
      "images of celebrities. Further, with advancements in Generative Adversarial\n",
      "Networks (GANs), now changing attributes and retouching have become very easy.\n",
      "Such synthetic alterations have adverse effect on face recognition algorithms.\n",
      "While researchers have proposed to detect image tampering, detecting GANs\n",
      "generated images has still not been explored. This paper proposes a supervised\n",
      "deep learning algorithm using Convolutional Neural Networks (CNNs) to detect\n",
      "synthetically altered images. The algorithm yields an accuracy of 99.65% on\n",
      "detecting retouching on the ND-IIITD dataset. It outperforms the previous state\n",
      "of the art which reported an accuracy of 87% on the database. For\n",
      "distinguishing between real images and images generated using GANs, the\n",
      "proposed algorithm yields an accuracy of 99.83%. \n",
      "\n",
      "\n",
      "The primary motivation of Image-to-Image Transformation is to convert an\n",
      "image of one domain to another domain. Most of the research has been focused on\n",
      "the task of image transformation for a set of pre-defined domains. Very few\n",
      "works are reported that actually developed a common framework for\n",
      "image-to-image transformation for different domains. With the introduction of\n",
      "Generative Adversarial Networks (GANs) as a general framework for the image\n",
      "generation problem, there is a tremendous growth in the area of image-to-image\n",
      "transformation. Most of the research focuses over the suitable objective\n",
      "function for image-to-image transformation. In this paper, we propose a new\n",
      "Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image\n",
      "transformation. The proposed CSGAN uses a new objective function (loss) called\n",
      "Cyclic-Synthesized Loss (CS) between the synthesized image of one domain and\n",
      "cycled image of another domain. The performance of the proposed CSGAN is\n",
      "evaluated on two benchmark image-to-image transformation datasets, including\n",
      "CUHK Face dataset and CMP Facades dataset. The results are computed using the\n",
      "widely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The\n",
      "experimental results of the proposed CSGAN approach are compared with the\n",
      "latest state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and\n",
      "PS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK\n",
      "dataset and exhibits the promising and comparable performance over Facades\n",
      "dataset in terms of both qualitative and quantitative measures. The code is\n",
      "available at https://github.com/KishanKancharagunta/CSGAN. \n",
      "\n",
      "\n",
      "This paper studies the task of full generative modelling of realistic images\n",
      "of humans, guided only by coarse sketch of the pose, while providing control\n",
      "over the specific instance or type of outfit worn by the user. This is a\n",
      "difficult problem because input and output domain are very different and direct\n",
      "image-to-image translation becomes infeasible. We propose an end-to-end\n",
      "trainable network under the generative adversarial framework, that provides\n",
      "detailed control over the final appearance while not requiring paired training\n",
      "data and hence allows us to forgo the challenging problem of fitting 3D poses\n",
      "to 2D images. The model allows to generate novel samples conditioned on either\n",
      "an image taken from the target domain or a class label indicating the style of\n",
      "clothing (e.g., t-shirt). We thoroughly evaluate the architecture and the\n",
      "contributions of the individual components experimentally. Finally, we show in\n",
      "a large scale perceptual study that our approach can generate realistic looking\n",
      "images and that participants struggle in detecting fake images versus real\n",
      "samples, especially if faces are blurred. \n",
      "\n",
      "\n",
      "Deep Convolutional Neural Networks (CNNs) have been one of the most\n",
      "influential recent developments in computer vision, particularly for\n",
      "categorization. There is an increasing demand for explainable AI as these\n",
      "systems are deployed in the real world. However, understanding the information\n",
      "represented and processed in CNNs remains in most cases challenging. Within\n",
      "this paper, we explore the use of new information theoretic techniques\n",
      "developed in the field of neuroscience to enable novel understanding of how a\n",
      "CNN represents information. We trained a 10-layer ResNet architecture to\n",
      "identify 2,000 face identities from 26M images generated using a rigorously\n",
      "controlled 3D face rendering model that produced variations of intrinsic (i.e.\n",
      "face morphology, gender, age, expression and ethnicity) and extrinsic factors\n",
      "(i.e. 3D pose, illumination, scale and 2D translation). With our methodology,\n",
      "we demonstrate that unlike human's network overgeneralizes face identities even\n",
      "with extreme changes of face shape, but it is more sensitive to changes of\n",
      "texture. To understand the processing of information underlying these\n",
      "counterintuitive properties, we visualize the features of shape and texture\n",
      "that the network processes to identify faces. Then, we shed a light into the\n",
      "inner workings of the black box and reveal how hidden layers represent these\n",
      "features and whether the representations are invariant to pose. We hope that\n",
      "our methodology will provide an additional valuable tool for interpretability\n",
      "of CNNs. \n",
      "\n",
      "\n",
      "In this paper we investigate the feasibility of using synthetic data to\n",
      "augment face datasets. In particular, we propose a novel generative adversarial\n",
      "network (GAN) that can disentangle identity-related attributes from\n",
      "non-identity-related attributes. This is done by training an embedding network\n",
      "that maps discrete identity labels to an identity latent space that follows a\n",
      "simple prior distribution, and training a GAN conditioned on samples from that\n",
      "distribution. Our proposed GAN allows us to augment face datasets by generating\n",
      "both synthetic images of subjects in the training set and synthetic images of\n",
      "new subjects not in the training set. By using recent advances in GAN training,\n",
      "we show that the synthetic images generated by our model are photo-realistic,\n",
      "and that training with augmented datasets can indeed increase the accuracy of\n",
      "face recognition models as compared with models trained with real images alone. \n",
      "\n",
      "\n",
      "The extension of image generation to video generation turns out to be a very\n",
      "difficult task, since the temporal dimension of videos introduces an extra\n",
      "challenge during the generation process. Besides, due to the limitation of\n",
      "memory and training stability, the generation becomes increasingly challenging\n",
      "with the increase of the resolution/duration of videos. In this work, we\n",
      "exploit the idea of progressive growing of Generative Adversarial Networks\n",
      "(GANs) for higher resolution video generation. In particular, we begin to\n",
      "produce video samples of low-resolution and short-duration, and then\n",
      "progressively increase both resolution and duration alone (or jointly) by\n",
      "adding new spatiotemporal convolutional layers to the current networks.\n",
      "Starting from the learning on a very raw-level spatial appearance and temporal\n",
      "movement of the video distribution, the proposed progressive method learns\n",
      "spatiotemporal information incrementally to generate higher resolution videos.\n",
      "Furthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to\n",
      "improve the distribution learning on the video data of high-dimension and\n",
      "mixed-spatiotemporal distribution. SWGAN loss replaces the distance between\n",
      "joint distributions by that of one-dimensional marginal distributions, making\n",
      "the loss easier to compute. We evaluate the proposed model on our collected\n",
      "face video dataset of 10,900 videos to generate photorealistic face videos of\n",
      "256x256x32 resolution. In addition, our model also reaches a record inception\n",
      "score of 14.57 in unsupervised action recognition dataset UCF-101. \n",
      "\n",
      "\n",
      "Although Generative Adversarial Network (GAN) can be used to generate the\n",
      "realistic image, improper use of these technologies brings hidden concerns. For\n",
      "example, GAN can be used to generate a tampered video for specific people and\n",
      "inappropriate events, creating images that are detrimental to a particular\n",
      "person, and may even affect that personal safety. In this paper, we will\n",
      "develop a deep forgery discriminator (DeepFD) to efficiently and effectively\n",
      "detect the computer-generated images. Directly learning a binary classifier is\n",
      "relatively tricky since it is hard to find the common discriminative features\n",
      "for judging the fake images generated from different GANs. To address this\n",
      "shortcoming, we adopt contrastive loss in seeking the typical features of the\n",
      "synthesized images generated by different GANs and follow by concatenating a\n",
      "classifier to detect such computer-generated images. Experimental results\n",
      "demonstrate that the proposed DeepFD successfully detected 94.7% fake images\n",
      "generated by several state-of-the-art GANs. \n",
      "\n",
      "\n",
      "In this paper, we propose a method, called GridFace, to reduce facial\n",
      "geometric variations and improve the recognition performance. Our method\n",
      "rectifies the face by local homography transformations, which are estimated by\n",
      "a face rectification network. To encourage the image generation with canonical\n",
      "views, we apply a regularization based on the natural face distribution. We\n",
      "learn the rectification network and recognition network in an end-to-end\n",
      "manner. Extensive experiments show our method greatly reduces geometric\n",
      "variations, and gains significant improvements in unconstrained face\n",
      "recognition scenarios. \n",
      "\n",
      "\n",
      "We propose a method for learning landmark detectors for visual objects (such\n",
      "as the eyes and the nose in a face) without any manual supervision. We cast\n",
      "this as the problem of generating images that combine the appearance of the\n",
      "object as seen in a first example image with the geometry of the object as seen\n",
      "in a second example image, where the two examples differ by a viewpoint change\n",
      "and/or an object deformation. In order to factorize appearance and geometry, we\n",
      "introduce a tight bottleneck in the geometry-extraction process that selects\n",
      "and distils geometry-related features. Compared to standard image generation\n",
      "problems, which often use generative adversarial networks, our generation task\n",
      "is conditioned on both appearance and geometry and thus is significantly less\n",
      "ambiguous, to the point that adopting a simple perceptual loss formulation is\n",
      "sufficient. We demonstrate that our approach can learn object landmarks from\n",
      "synthetic image deformations or videos, all without manual supervision, while\n",
      "outperforming state-of-the-art unsupervised landmark detectors. We further show\n",
      "that our method is applicable to a large variety of datasets - faces, people,\n",
      "3D objects, and digits - without any modifications. \n",
      "\n",
      "\n",
      "Conditional generative adversarial networks (cGAN) have led to large\n",
      "improvements in the task of conditional image generation, which lies at the\n",
      "heart of computer vision. The major focus so far has been on performance\n",
      "improvement, while there has been little effort in making cGAN more robust to\n",
      "noise. The regression (of the generator) might lead to arbitrarily large errors\n",
      "in the output, which makes cGAN unreliable for real-world applications. In this\n",
      "work, we introduce a novel conditional GAN model, called RoCGAN, which\n",
      "leverages structure in the target space of the model to address the issue. Our\n",
      "model augments the generator with an unsupervised pathway, which promotes the\n",
      "outputs of the generator to span the target manifold even in the presence of\n",
      "intense noise. We prove that RoCGAN share similar theoretical properties as GAN\n",
      "and experimentally verify that our model outperforms existing state-of-the-art\n",
      "cGAN architectures by a large margin in a variety of domains including images\n",
      "from natural scenes and faces. \n",
      "\n",
      "\n",
      "A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery.\n",
      "For the face stimuli generated by a pre-trained active appearance model (AAM),\n",
      "the responses of neurons in the areas of the primate brain that are responsible\n",
      "for face recognition exhibit strong linear relationship with the shape\n",
      "variables and appearance variables of the AAM that generates the face stimuli.\n",
      "In this paper, we show that this behavior can be replicated by a deep\n",
      "generative model called the generator network, which assumes that the observed\n",
      "signals are generated by latent random variables via a top-down convolutional\n",
      "neural network. Specifically, we learn the generator network from the face\n",
      "images generated by a pre-trained AAM model using variational auto-encoder, and\n",
      "we show that the inferred latent variables of the learned generator network\n",
      "have strong linear relationship with the shape and appearance variables of the\n",
      "AAM model that generates the face images. Unlike the AAM model that has an\n",
      "explicit shape model where the shape variables generate the control points or\n",
      "landmarks, the generator network has no such shape model and shape variables.\n",
      "Yet the generator network can learn the shape knowledge in the sense that some\n",
      "of the latent variables of the learned generator network capture the shape\n",
      "variations in the face images generated by AAM. \n",
      "\n",
      "\n",
      "In this paper, we study the problem of multi-domain image generation, the\n",
      "goal of which is to generate pairs of corresponding images from different\n",
      "domains. With the recent development in generative models, image generation has\n",
      "achieved great progress and has been applied to various computer vision tasks.\n",
      "However, multi-domain image generation may not achieve the desired performance\n",
      "due to the difficulty of learning the correspondence of different domain\n",
      "images, especially when the information of paired samples is not given. To\n",
      "tackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is\n",
      "capable of learning to generate corresponding images in the absence of paired\n",
      "training data. RegCGAN is based on the conditional GAN, and we introduce two\n",
      "regularizers to guide the model to learn the corresponding semantics of\n",
      "different domains. We evaluate the proposed model on several tasks for which\n",
      "paired training data is not given, including the generation of edges and\n",
      "photos, the generation of faces with different attributes, etc. The\n",
      "experimental results show that our model can successfully generate\n",
      "corresponding images for all these tasks, while outperforms the baseline\n",
      "methods. We also introduce an approach of applying RegCGAN to unsupervised\n",
      "domain adaptation. \n",
      "\n",
      "\n",
      "The task of face attribute manipulation has found increasing applications,\n",
      "but still remains challenging with the requirement of editing the attributes of\n",
      "a face image while preserving its unique details. In this paper, we choose to\n",
      "combine the Variational AutoEncoder (VAE) and Generative Adversarial Network\n",
      "(GAN) for photorealistic image generation. We propose an effective method to\n",
      "modify a modest amount of pixels in the feature maps of an encoder, changing\n",
      "the attribute strength continuously without hindering global information. Our\n",
      "training objectives of VAE and GAN are reinforced by the supervision of face\n",
      "recognition loss and cycle consistency loss for faithful preservation of face\n",
      "details. Moreover, we generate facial masks to enforce background consistency,\n",
      "which allows our training to focus on manipulating the foreground face rather\n",
      "than background. Experimental results demonstrate our method, called\n",
      "Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with\n",
      "changing attributes and outperforms prior methods in detail preservation. \n",
      "\n",
      "\n",
      "This paper studies the problem of blind face restoration from an\n",
      "unconstrained blurry, noisy, low-resolution, or compressed image (i.e.,\n",
      "degraded observation). For better recovery of fine facial details, we modify\n",
      "the problem setting by taking both the degraded observation and a high-quality\n",
      "guided image of the same identity as input to our guided face restoration\n",
      "network (GFRNet). However, the degraded observation and guided image generally\n",
      "are different in pose, illumination and expression, thereby making plain CNNs\n",
      "(e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle\n",
      "this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a\n",
      "reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow\n",
      "field for warping the guided image to correct pose and expression (i.e., warped\n",
      "guidance), while the RecNet takes the degraded observation and warped guidance\n",
      "as input to produce the restoration result. Due to that the ground-truth flow\n",
      "field is unavailable, landmark loss together with total variation\n",
      "regularization are incorporated to guide the learning of WarpNet. Furthermore,\n",
      "to make the model applicable to blind restoration, our GFRNet is trained on the\n",
      "synthetic data with versatile settings on blur kernel, noise level,\n",
      "downsampling scale factor, and JPEG quality factor. Experiments show that our\n",
      "GFRNet not only performs favorably against the state-of-the-art image and face\n",
      "restoration methods, but also generates visually photo-realistic results on\n",
      "real degraded facial images. \n",
      "\n",
      "\n",
      "We propose a novel end-to-end semi-supervised adversarial framework to\n",
      "generate photorealistic face images of new identities with wide ranges of\n",
      "expressions, poses, and illuminations conditioned by a 3D morphable model.\n",
      "Previous adversarial style-transfer methods either supervise their networks\n",
      "with large volume of paired data or use unpaired data with a highly\n",
      "under-constrained two-way generative framework in an unsupervised fashion. We\n",
      "introduce pairwise adversarial supervision to constrain two-way domain\n",
      "adaptation by a small number of paired real and synthetic images for training\n",
      "along with the large volume of unpaired data. Extensive qualitative and\n",
      "quantitative experiments are performed to validate our idea. Generated face\n",
      "images of new identities contain pose, lighting and expression diversity and\n",
      "qualitative results show that they are highly constraint by the synthetic input\n",
      "image while adding photorealism and retaining identity information. We combine\n",
      "face images generated by the proposed method with the real data set to train\n",
      "face recognition algorithms. We evaluated the model on two challenging data\n",
      "sets: LFW and IJB-A. We observe that the generated images from our framework\n",
      "consistently improves over the performance of deep face recognition network\n",
      "trained with Oxford VGG Face dataset and achieves comparable results to the\n",
      "state-of-the-art. \n",
      "\n",
      "\n",
      "Smile detection from unconstrained facial images is a specialized and\n",
      "challenging problem. As one of the most informative expressions, smiles convey\n",
      "basic underlying emotions, such as happiness and satisfaction, which lead to\n",
      "multiple applications, e.g., human behavior analysis and interactive\n",
      "controlling. Compared to the size of databases for face recognition, far less\n",
      "labeled data is available for training smile detection systems. To leverage the\n",
      "large amount of labeled data from face recognition datasets and to alleviate\n",
      "overfitting on smile detection, an efficient transfer learning-based smile\n",
      "detection approach is proposed in this paper. Unlike previous works which use\n",
      "either hand-engineered features or train deep convolutional networks from\n",
      "scratch, a well-trained deep face recognition model is explored and fine-tuned\n",
      "for smile detection in the wild. Three different models are built as a result\n",
      "of fine-tuning the face recognition model with different inputs, including\n",
      "aligned, unaligned and grayscale images generated from the GENKI-4K dataset.\n",
      "Experiments show that the proposed approach achieves improved state-of-the-art\n",
      "performance. Robustness of the model to noise and blur artifacts is also\n",
      "evaluated in this paper. \n",
      "\n",
      "\n",
      "This paper presents HeNet, a hierarchical ensemble neural network, applied to\n",
      "classify hardware-generated control flow traces for malware detection. Deep\n",
      "learning-based malware detection has so far focused on analyzing executable\n",
      "files and runtime API calls. Static code analysis approaches face challenges\n",
      "due to obfuscated code and adversarial perturbations. Behavioral data collected\n",
      "during execution is more difficult to obfuscate but recent research has shown\n",
      "successful attacks against API call based malware classifiers. We investigate\n",
      "control flow based characterization of a program execution to build robust deep\n",
      "learning malware classifiers. HeNet consists of a low-level behavior model and\n",
      "a top-level ensemble model. The low-level model is a per-application behavior\n",
      "model, trained via transfer learning on a time-series of images generated from\n",
      "control flow trace of an execution. We use Intel$^\\circledR$ Processor Trace\n",
      "enabled processor for low overhead execution tracing and design a lightweight\n",
      "image conversion and segmentation of the control flow trace. The top-level\n",
      "ensemble model aggregates the behavior classification of all the trace segments\n",
      "and detects an attack. The use of hardware trace adds portability to our system\n",
      "and the use of deep learning eliminates the manual effort of feature\n",
      "engineering. We evaluate HeNet against real-world exploitations of PDF readers.\n",
      "HeNet achieves 100\\% accuracy and 0\\% false positive on test set, and higher\n",
      "classification accuracy compared to classical machine learning algorithms. \n",
      "\n",
      "\n",
      "Person Re-identification (re-id) faces two major challenges: the lack of\n",
      "cross-view paired training data and learning discriminative identity-sensitive\n",
      "and view-invariant features in the presence of large pose variations. In this\n",
      "work, we address both problems by proposing a novel deep person image\n",
      "generation model for synthesizing realistic person images conditional on the\n",
      "pose. The model is based on a generative adversarial network (GAN) designed\n",
      "specifically for pose normalization in re-id, thus termed pose-normalization\n",
      "GAN (PN-GAN). With the synthesized images, we can learn a new type of deep\n",
      "re-id feature free of the influence of pose variations. We show that this\n",
      "feature is strong on its own and complementary to features learned with the\n",
      "original images. Importantly, under the transfer learning setting, we show that\n",
      "our model generalizes well to any new re-id dataset without the need for\n",
      "collecting any training data for model fine-tuning. The model thus has the\n",
      "potential to make re-id model truly scalable. \n",
      "\n",
      "\n",
      "It is unknown what kind of biases modern in the wild face datasets have\n",
      "because of their lack of annotation. A direct consequence of this is that total\n",
      "recognition rates alone only provide limited insight about the generalization\n",
      "ability of a Deep Convolutional Neural Networks (DCNNs). We propose to\n",
      "empirically study the effect of different types of dataset biases on the\n",
      "generalization ability of DCNNs. Using synthetically generated face images, we\n",
      "study the face recognition rate as a function of interpretable parameters such\n",
      "as face pose and light. The proposed method allows valuable details about the\n",
      "generalization performance of different DCNN architectures to be observed and\n",
      "compared. In our experiments, we find that: 1) Indeed, dataset bias has a\n",
      "significant influence on the generalization performance of DCNNs. 2) DCNNs can\n",
      "generalize surprisingly well to unseen illumination conditions and large\n",
      "sampling gaps in the pose variation. 3) Using the presented methodology we\n",
      "reveal that the VGG-16 architecture outperforms the AlexNet architecture at\n",
      "face recognition tasks because it can much better generalize to unseen face\n",
      "poses, although it has significantly more parameters. 4) We uncover a main\n",
      "limitation of current DCNN architectures, which is the difficulty to generalize\n",
      "when different identities to not share the same pose variation. 5) We\n",
      "demonstrate that our findings on synthetic data also apply when learning from\n",
      "real-world data. Our face image generator is publicly available to enable the\n",
      "community to benchmark other DCNN architectures. \n",
      "\n",
      "\n",
      "Deep generative models learned through adversarial training have become\n",
      "increasingly popular for their ability to generate naturalistic image textures.\n",
      "However, aside from their texture, the visual appearance of objects is\n",
      "significantly influenced by their shape geometry; information which is not\n",
      "taken into account by existing generative models. This paper introduces the\n",
      "Geometry-Aware Generative Adversarial Networks (GAGAN) for incorporating\n",
      "geometric information into the image generation process. Specifically, in GAGAN\n",
      "the generator samples latent variables from the probability space of a\n",
      "statistical shape model. By mapping the output of the generator to a canonical\n",
      "coordinate frame through a differentiable geometric transformation, we enforce\n",
      "the geometry of the objects and add an implicit connection from the prior to\n",
      "the generated object. Experimental results on face generation indicate that the\n",
      "GAGAN can generate realistic images of faces with arbitrary facial attributes\n",
      "such as facial expression, pose, and morphology, that are of better quality\n",
      "than current GAN-based methods. Our method can be used to augment any existing\n",
      "GAN architecture and improve the quality of the images generated. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks are proved to be efficient on various kinds\n",
      "of image generation tasks. However, it is still a challenge if we want to\n",
      "generate images precisely. Many researchers focus on how to generate images\n",
      "with one attribute. But image generation under multiple attributes is still a\n",
      "tough work. In this paper, we try to generate a variety of face images under\n",
      "multiple constraints using a pipeline process. The Pip-GAN (Pipeline Generative\n",
      "Adversarial Network) we present employs a pipeline network structure which can\n",
      "generate a complex facial image step by step using a neutral face image. We\n",
      "applied our method on two face image databases and demonstrate its ability to\n",
      "generate convincing novel images of unseen identities under multiple conditions\n",
      "previously. \n",
      "\n",
      "\n",
      "Recent research has demonstrated the ability to estimate gaze on mobile\n",
      "devices by performing inference on the image from the phone's front-facing\n",
      "camera, and without requiring specialized hardware. While this offers wide\n",
      "potential applications such as in human-computer interaction, medical diagnosis\n",
      "and accessibility (e.g., hands free gaze as input for patients with motor\n",
      "disorders), current methods are limited as they rely on collecting data from\n",
      "real users, which is a tedious and expensive process that is hard to scale\n",
      "across devices. There have been some attempts to synthesize eye region data\n",
      "using 3D models that can simulate various head poses and camera settings,\n",
      "however these lack in realism.\n",
      "  In this paper, we improve upon a recently suggested method, and propose a\n",
      "generative adversarial framework to generate a large dataset of high resolution\n",
      "colorful images with high diversity (e.g., in subjects, head pose, camera\n",
      "settings) and realism, while simultaneously preserving the accuracy of gaze\n",
      "labels. The proposed approach operates on extended regions of the eye, and even\n",
      "completes missing parts of the image. Using this rich synthesized dataset, and\n",
      "without using any additional training data from real users, we demonstrate\n",
      "improvements over state-of-the-art for estimating 2D gaze position on mobile\n",
      "devices. We further demonstrate cross-device generalization of model\n",
      "performance, as well as improved robustness to diverse head pose, blur and\n",
      "distance. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence\n",
      "in a high-resolution setting with a computational constrain of GPU memory\n",
      "capacity has been beset with difficulty due to the known lack of convergence\n",
      "rate stability. In order to boost network convergence of DCGAN (Deep\n",
      "Convolutional Generative Adversarial Networks) [Radford et al. 2016] and\n",
      "achieve good-looking high-resolution results we propose a new layered network,\n",
      "HDCGAN, that incorporates current state-of-the-art techniques for this effect.\n",
      "Glasses, a mechanism to arbitrarily improve the final GAN generated results by\n",
      "enlarging the input size by a telescope {\\zeta} is also presented. A novel\n",
      "bias-free dataset, Curt\\'o & Zarza, containing human faces from different\n",
      "ethnical groups in a wide variety of illumination conditions and image\n",
      "resolutions is introduced. Curt\\'o is enhanced with HDCGAN synthetic images,\n",
      "thus being the first GAN augmented dataset of faces. We conduct extensive\n",
      "experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and\n",
      "Curt\\'o. HDCGAN is the current state-of-the-art in synthetic image generation\n",
      "on CelebA achieving a MS-SSIM of 0.1978 and a FR\\'ECHET Inception Distance of\n",
      "8.44. \n",
      "\n",
      "\n",
      "Generating high fidelity identity-preserving faces with different facial\n",
      "attributes has a wide range of applications. Although a number of generative\n",
      "models have been developed to tackle this problem, there is still much room for\n",
      "further improvement.In paticular, the current solutions usually ignore the\n",
      "perceptual information of images, which we argue that it benefits the output of\n",
      "a high-quality image while preserving the identity information, especially in\n",
      "facial attributes learning area.To this end, we propose to train GAN\n",
      "iteratively via regularizing the min-max process with an integrated loss, which\n",
      "includes not only the per-pixel loss but also the perceptual loss. In contrast\n",
      "to the existing methods only deal with either image generation or\n",
      "transformation, our proposed iterative architecture can achieve both of them.\n",
      "Experiments on the multi-label facial dataset CelebA demonstrate that the\n",
      "proposed model has excellent performance on recognizing multiple attributes,\n",
      "generating a high-quality image, and transforming image with controllable\n",
      "attributes. \n",
      "\n",
      "\n",
      "Object Transfiguration replaces an object in an image with another object\n",
      "from a second image. For example it can perform tasks like \"putting exactly\n",
      "those eyeglasses from image A on the nose of the person in image B\". Usage of\n",
      "exemplar images allows more precise specification of desired modifications and\n",
      "improves the diversity of conditional image generation. However, previous\n",
      "methods that rely on feature space operations, require paired data and/or\n",
      "appearance models for training or disentangling objects from background. In\n",
      "this work, we propose a model that can learn object transfiguration from two\n",
      "unpaired sets of images: one set containing images that \"have\" that kind of\n",
      "object, and the other set being the opposite, with the mild constraint that the\n",
      "objects be located approximately at the same place. For example, the training\n",
      "data can be one set of reference face images that have eyeglasses, and another\n",
      "set of images that have not, both of which spatially aligned by face landmarks.\n",
      "Despite the weak 0/1 labels, our model can learn an \"eyeglasses\" subspace that\n",
      "contain multiple representatives of different types of glasses. Consequently,\n",
      "we can perform fine-grained control of generated images, like swapping the\n",
      "glasses in two images by swapping the projected components in the \"eyeglasses\"\n",
      "subspace, to create novel images of people wearing eyeglasses.\n",
      "  Overall, our deterministic generative model learns disentangled attribute\n",
      "subspaces from weakly labeled data by adversarial training. Experiments on\n",
      "CelebA and Multi-PIE datasets validate the effectiveness of the proposed model\n",
      "on real world data, in generating images with specified eyeglasses, smiling,\n",
      "hair styles, and lighting conditions etc. The code is available online. \n",
      "\n",
      "\n",
      "We present variational generative adversarial networks, a general learning\n",
      "framework that combines a variational auto-encoder with a generative\n",
      "adversarial network, for synthesizing images in fine-grained categories, such\n",
      "as faces of a specific person or objects in a category. Our approach models an\n",
      "image as a composition of label and latent attributes in a probabilistic model.\n",
      "By varying the fine-grained category label fed into the resulting generative\n",
      "model, we can generate images in a specific category with randomly drawn values\n",
      "on a latent attribute vector. Our approach has two novel aspects. First, we\n",
      "adopt a cross entropy loss for the discriminative and classifier network, but a\n",
      "mean discrepancy objective for the generative network. This kind of asymmetric\n",
      "loss function makes the GAN training more stable. Second, we adopt an encoder\n",
      "network to learn the relationship between the latent space and the real image\n",
      "space, and use pairwise feature matching to keep the structure of generated\n",
      "images. We experiment with natural images of faces, flowers, and birds, and\n",
      "demonstrate that the proposed models are capable of generating realistic and\n",
      "diverse samples with fine-grained category labels. We further show that our\n",
      "models can be applied to other tasks, such as image inpainting,\n",
      "super-resolution, and data augmentation for training better face recognition\n",
      "models. \n",
      "\n",
      "\n",
      "3D shape models are naturally parameterized using vertices and faces, \\ie,\n",
      "composed of polygons forming a surface. However, current 3D learning paradigms\n",
      "for predictive and generative tasks using convolutional neural networks focus\n",
      "on a voxelized representation of the object. Lifting convolution operators from\n",
      "the traditional 2D to 3D results in high computational overhead with little\n",
      "additional benefit as most of the geometry information is contained on the\n",
      "surface boundary. Here we study the problem of directly generating the 3D shape\n",
      "surface of rigid and non-rigid shapes using deep convolutional neural networks.\n",
      "We develop a procedure to create consistent `geometry images' representing the\n",
      "shape surface of a category of 3D objects. We then use this consistent\n",
      "representation for category-specific shape surface generation from a parametric\n",
      "representation or an image by developing novel extensions of deep residual\n",
      "networks for the task of geometry image generation. Our experiments indicate\n",
      "that our network learns a meaningful representation of shape surfaces allowing\n",
      "it to interpolate between shape orientations and poses, invent new shape\n",
      "surfaces and reconstruct 3D shape surfaces from previously unseen images. \n",
      "\n",
      "\n",
      "Recently there has been an enormous interest in generative models for images\n",
      "in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and\n",
      "Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular\n",
      "models. While VAEs tend to produce excellent reconstructions but blurry\n",
      "samples, GANs generate sharp but slightly distorted images. In this paper we\n",
      "propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold:\n",
      "(i) To generated new images conditioned on visual descriptions, and (ii) modify\n",
      "the image, by fixing the latent representation of image and varying the visual\n",
      "description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA\n",
      "and a modified version of MNIST datasets and demonstrate the ability of our\n",
      "model to generate new images as well as to modify a given image by changing\n",
      "attributes. \n",
      "\n",
      "\n",
      "We use CNNs to build a system that both classifies images of faces based on a\n",
      "variety of different facial attributes and generates new faces given a set of\n",
      "desired facial characteristics. After introducing the problem and providing\n",
      "context in the first section, we discuss recent work related to image\n",
      "generation in Section 2. In Section 3, we describe the methods used to\n",
      "fine-tune our CNN and generate new images using a novel approach inspired by a\n",
      "Gaussian mixture model. In Section 4, we discuss our working dataset and\n",
      "describe our preprocessing steps and handling of facial attributes. Finally, in\n",
      "Sections 5, 6 and 7, we explain our experiments and results and conclude in the\n",
      "following section. Our classification system has 82\\% test accuracy.\n",
      "Furthermore, our generation pipeline successfully creates well-formed faces. \n",
      "\n",
      "\n",
      "Distinguishing subtle differences in attributes is valuable, yet learning to\n",
      "make visual comparisons remains non-trivial. Not only is the number of possible\n",
      "comparisons quadratic in the number of training images, but also access to\n",
      "images adequately spanning the space of fine-grained visual differences is\n",
      "limited. We propose to overcome the sparsity of supervision problem via\n",
      "synthetically generated images. Building on a state-of-the-art image generation\n",
      "engine, we sample pairs of training images exhibiting slight modifications of\n",
      "individual attributes. Augmenting real training image pairs with these\n",
      "examples, we then train attribute ranking models to predict the relative\n",
      "strength of an attribute in novel pairs of real images. Our results on datasets\n",
      "of faces and fashion images show the great promise of bootstrapping imperfect\n",
      "image generators to counteract sample sparsity for learning to rank. \n",
      "\n",
      "\n",
      "We study the problem of transferring a sample in one domain to an analog\n",
      "sample in another domain. Given two related domains, S and T, we would like to\n",
      "learn a generative function G that maps an input sample from S to the domain T,\n",
      "such that the output of a given function f, which accepts inputs in either\n",
      "domains, would remain unchanged. Other than the function f, the training data\n",
      "is unsupervised and consist of a set of samples from each domain. The Domain\n",
      "Transfer Network (DTN) we present employs a compound loss function that\n",
      "includes a multiclass GAN loss, an f-constancy component, and a regularizing\n",
      "component that encourages G to map samples from T to themselves. We apply our\n",
      "method to visual domains including digits and face images and demonstrate its\n",
      "ability to generate convincing novel images of previously unseen entities,\n",
      "while preserving their identity. \n",
      "\n",
      "\n",
      "We present an overview and first results of the Stratospheric Observatory For\n",
      "Infrared Astronomy Massive (SOMA) Star Formation Survey, which is using the\n",
      "FORCAST instrument to image massive protostars from\n",
      "$\\sim10$--$40\\:\\rm{\\mu}\\rm{m}$. These wavelengths trace thermal emission from\n",
      "warm dust, which in Core Accretion models mainly emerges from the inner regions\n",
      "of protostellar outflow cavities. Dust in dense core envelopes also imprints\n",
      "characteristic extinction patterns at these wavelengths, causing intensity\n",
      "peaks to shift along the outflow axis and profiles to become more symmetric at\n",
      "longer wavelengths. We present observational results for the first eight\n",
      "protostars in the survey, i.e., multiwavelength images, including some\n",
      "ancillary ground-based MIR observations and archival {\\it{Spitzer}} and\n",
      "{\\it{Herschel}} data. These images generally show extended MIR/FIR emission\n",
      "along directions consistent with those of known outflows and with shorter\n",
      "wavelength peak flux positions displaced from the protostar along the\n",
      "blueshifted, near-facing sides, thus confirming qualitative predictions of Core\n",
      "Accretion models. We then compile spectral energy distributions and use these\n",
      "to derive protostellar properties by fitting theoretical radiative transfer\n",
      "models. Zhang and Tan models, based on the Turbulent Core Model of McKee and\n",
      "Tan, imply the sources have protostellar masses $m_*\\sim10$--50$\\:M_\\odot$\n",
      "accreting at $\\sim10^{-4}$--$10^{-3}\\:M_\\odot\\:{\\rm{yr}}^{-1}$ inside cores of\n",
      "initial masses $M_c\\sim30$--500$\\:M_\\odot$ embedded in clumps with mass surface\n",
      "densities $\\Sigma_{\\rm{cl}}\\sim0.1$--3$\\:{\\rm{g\\:cm}^{-2}}$. Fitting Robitaille\n",
      "et al. models typically leads to slightly higher protostellar masses, but with\n",
      "disk accretion rates $\\sim100\\times$ smaller. We discuss reasons for these\n",
      "differences and overall implications of these first survey results for massive\n",
      "star formation theories. \n",
      "\n",
      "\n",
      "In this work, we consider the task of generating highly-realistic images of a\n",
      "given face with a redirected gaze. We treat this problem as a specific instance\n",
      "of conditional image generation and suggest a new deep architecture that can\n",
      "handle this task very well as revealed by numerical comparison with prior art\n",
      "and a user study. Our deep architecture performs coarse-to-fine warping with an\n",
      "additional intensity correction of individual pixels. All these operations are\n",
      "performed in a feed-forward manner, and the parameters associated with\n",
      "different operations are learned jointly in the end-to-end fashion. After\n",
      "learning, the resulting neural network can synthesize images with manipulated\n",
      "gaze, while the redirection angle can be selected arbitrarily from a certain\n",
      "range and provided as an input to the network. \n",
      "\n",
      "\n",
      "Image generation remains a fundamental problem in artificial intelligence in\n",
      "general and deep learning in specific. The generative adversarial network (GAN)\n",
      "was successful in generating high quality samples of natural images. We propose\n",
      "a model called composite generative adversarial network, that reveals the\n",
      "complex structure of images with multiple generators in which each generator\n",
      "generates some part of the image. Those parts are combined by alpha blending\n",
      "process to create a new single image. It can generate, for example, background\n",
      "and face sequentially with two generators, after training on face dataset.\n",
      "Training was done in an unsupervised way without any labels about what each\n",
      "generator should generate. We found possibilities of learning the structure by\n",
      "using this generative model empirically. \n",
      "\n",
      "\n",
      "This work explores conditional image generation with a new image density\n",
      "model based on the PixelCNN architecture. The model can be conditioned on any\n",
      "vector, including descriptive labels or tags, or latent embeddings created by\n",
      "other networks. When conditioned on class labels from the ImageNet database,\n",
      "the model is able to generate diverse, realistic scenes representing distinct\n",
      "animals, objects, landscapes and structures. When conditioned on an embedding\n",
      "produced by a convolutional network given a single image of an unseen face, it\n",
      "generates a variety of new portraits of the same person with different facial\n",
      "expressions, poses and lighting conditions. We also show that conditional\n",
      "PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\n",
      "the gated convolutional layers in the proposed model improve the log-likelihood\n",
      "of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\n",
      "with greatly reduced computational cost. \n",
      "\n",
      "\n",
      "For applications such as airport border control, biometric technologies that\n",
      "can process many capture subjects quickly, efficiently, with weak supervision,\n",
      "and with minimal discomfort are desirable. Facial recognition is particularly\n",
      "appealing because it is minimally invasive yet offers relatively good\n",
      "recognition performance. Unfortunately, the combination of weak supervision and\n",
      "minimal invasiveness makes even highly accurate facial recognition systems\n",
      "susceptible to spoofing via presentation attacks. Thus, there is great demand\n",
      "for an effective and low cost system capable of rejecting such attacks.To this\n",
      "end we introduce PARAPH -- a novel hardware extension that exploits different\n",
      "measurements of light polarization to yield an image space in which\n",
      "presentation media are readily discernible from Bona Fide facial\n",
      "characteristics. The PARAPH system is inexpensive with an added cost of less\n",
      "than 10 US dollars. The system makes two polarization measurements in rapid\n",
      "succession, allowing them to be approximately pixel-aligned, with a frame rate\n",
      "limited by the camera, not the system. There are no moving parts above the\n",
      "molecular level, due to the efficient use of twisted nematic liquid crystals.\n",
      "We present evaluation images using three presentation attack media next to an\n",
      "actual face -- high quality photos on glossy and matte paper and a video of the\n",
      "face on an LCD. In each case, the actual face in the image generated by PARAPH\n",
      "is structurally discernible from the presentations, which appear either as\n",
      "noise (print attacks) or saturated images (replay attacks). \n",
      "\n",
      "\n",
      "This paper investigates a novel problem of generating images from visual\n",
      "attributes. We model the image as a composite of foreground and background and\n",
      "develop a layered generative model with disentangled latent variables that can\n",
      "be learned end-to-end using a variational auto-encoder. We experiment with\n",
      "natural images of faces and birds and demonstrate that the proposed models are\n",
      "capable of generating realistic and diverse samples with disentangled latent\n",
      "representations. We use a general energy minimization algorithm for posterior\n",
      "inference of latent variables given novel images. Therefore, the learned\n",
      "generative models show excellent quantitative and visual results in the tasks\n",
      "of attribute-conditioned image reconstruction and completion. \n",
      "\n",
      "\n",
      "Learning the distribution of images in order to generate new samples is a\n",
      "challenging task due to the high dimensionality of the data and the highly\n",
      "non-linear relations that are involved. Nevertheless, some promising results\n",
      "have been reported in the literature recently,building on deep network\n",
      "architectures. In this work, we zoom in on a specific type of image generation:\n",
      "given an image and knowing the category of objects it belongs to (e.g. faces),\n",
      "our goal is to generate a similar and plausible image, but with some altered\n",
      "attributes. This is particularly challenging, as the model needs to learn to\n",
      "disentangle the effect of each attribute and to apply a desired attribute\n",
      "change to a given input image, while keeping the other attributes and overall\n",
      "object appearance intact. To this end, we learn a convolutional network, where\n",
      "the desired attribute information is encoded then merged with the encoded image\n",
      "at feature map level. We show promising results, both qualitatively as well as\n",
      "quantitatively, in the context of a retrieval experiment, on two face datasets\n",
      "(MultiPie and CAS-PEAL-R1). \n",
      "\n",
      "\n",
      "Video forgery attack threatens the surveillance system by replacing the video\n",
      "captures with unrealistic synthesis, which can be powered by the latest augment\n",
      "reality and virtual reality technologies. From the machine perception aspect,\n",
      "visual objects often have RF signatures that are naturally synchronized with\n",
      "them during recording. In contrast to video captures, the RF signatures are\n",
      "more difficult to attack given their concealed and ubiquitous nature. In this\n",
      "work, we investigate multimodal video forgery attack detection methods using\n",
      "both vision and wireless modalities. Since wireless signal-based human\n",
      "perception is environmentally sensitive, we propose a self-supervised training\n",
      "strategy to enable the system to work without external annotation and thus can\n",
      "adapt to different environments. Our method achieves a perfect human detection\n",
      "accuracy and a high forgery attack detection accuracy of 94.38% which is\n",
      "comparable with supervised methods. \n",
      "\n",
      "\n",
      "Videos are prone to tampering attacks that alter the meaning and deceive the\n",
      "audience. Previous video forgery detection schemes find tiny clues to locate\n",
      "the tampered areas. However, attackers can successfully evade supervision by\n",
      "destroying such clues using video compression or blurring. This paper proposes\n",
      "a video watermarking network for tampering localization. We jointly train a\n",
      "3D-UNet-based watermark embedding network and a decoder that predicts the\n",
      "tampering mask. The perturbation made by watermark embedding is close to\n",
      "imperceptible. Considering that there is no off-the-shelf differentiable video\n",
      "codec simulator, we propose to mimic video compression by ensembling simulation\n",
      "results of other typical attacks, e.g., JPEG compression and blurring, as an\n",
      "approximation. Experimental results demonstrate that our method generates\n",
      "watermarked videos with good imperceptibility and robustly and accurately\n",
      "locates tampered areas within the attacked version. \n",
      "\n",
      "\n",
      "In recent years, with the rapid development of face editing and generation,\n",
      "more and more fake videos are circulating on social media, which has caused\n",
      "extreme public concerns. Existing face forgery detection methods based on\n",
      "frequency domain find that the GAN forged images have obvious grid-like visual\n",
      "artifacts in the frequency spectrum compared to the real images. But for\n",
      "synthesized videos, these methods only confine to single frame and pay little\n",
      "attention to the most discriminative part and temporal frequency clue among\n",
      "different frames. To take full advantage of the rich information in video\n",
      "sequences, this paper performs video forgery detection on both spatial and\n",
      "temporal frequency domains and proposes a Discrete Cosine Transform-based\n",
      "Forgery Clue Augmentation Network (FCAN-DCT) to achieve a more comprehensive\n",
      "spatial-temporal feature representation. FCAN-DCT consists of a backbone\n",
      "network and two branches: Compact Feature Extraction (CFE) module and Frequency\n",
      "Temporal Attention (FTA) module. We conduct thorough experimental assessments\n",
      "on two visible light (VIS) based datasets WildDeepfake and Celeb-DF (v2), and\n",
      "our self-built video forgery dataset DeepfakeNIR, which is the first video\n",
      "forgery dataset on near-infrared modality. The experimental results demonstrate\n",
      "the effectiveness of our method on detecting forgery videos in both VIS and NIR\n",
      "scenarios. \n",
      "\n",
      "\n",
      "The cybersecurity breaches expose surveillance video streams to forgery\n",
      "attacks, under which authentic streams are falsified to hide unauthorized\n",
      "activities. Traditional video forensics approaches can localize forgery traces\n",
      "using spatial-temporal analysis on relatively long video clips, while falling\n",
      "short in real-time forgery detection. The recent work correlates time-series\n",
      "camera and wireless signals to detect looped videos but cannot realize\n",
      "fine-grained forgery localization. To overcome these limitations, we propose\n",
      "Secure-Pose, which exploits the pervasive coexistence of surveillance and Wi-Fi\n",
      "infrastructures to defend against video forgery attacks in a real-time and\n",
      "fine-grained manner. We observe that coexisting camera and Wi-Fi signals convey\n",
      "common human semantic information and forgery attacks on video streams will\n",
      "decouple such information correspondence. Particularly, retrievable human pose\n",
      "features are first extracted from concurrent video and Wi-Fi channel state\n",
      "information (CSI) streams. Then, a lightweight detection network is developed\n",
      "to accurately discover forgery attacks and an efficient localization algorithm\n",
      "is devised to seamlessly track forgery traces in video streams. We implement\n",
      "Secure-Pose using one Logitech camera and two Intel 5300 NICs and evaluate it\n",
      "in different environments. Secure-Pose achieves a high detection accuracy of\n",
      "98.7% and localizes abnormal objects under playback and tampering attacks. \n",
      "\n",
      "\n",
      "The rapid progress of photorealistic synthesis techniques has reached at a\n",
      "critical point where the boundary between real and manipulated images starts to\n",
      "blur. Thus, benchmarking and advancing digital forgery analysis have become a\n",
      "pressing issue. However, existing face forgery datasets either have limited\n",
      "diversity or only support coarse-grained analysis. To counter this emerging\n",
      "threat, we construct the ForgeryNet dataset, an extremely large face forgery\n",
      "dataset with unified annotations in image- and video-level data across four\n",
      "tasks: 1) Image Forgery Classification, including two-way (real / fake),\n",
      "three-way (real / fake with identity-replaced forgery approaches / fake with\n",
      "identity-remained forgery approaches), and n-way (real and 15 respective\n",
      "forgery approaches) classification. 2) Spatial Forgery Localization, which\n",
      "segments the manipulated area of fake images compared to their corresponding\n",
      "source real images. 3) Video Forgery Classification, which re-defines the\n",
      "video-level forgery classification with manipulated frames in random positions.\n",
      "This task is important because attackers in real world are free to manipulate\n",
      "any target frame. and 4) Temporal Forgery Localization, to localize the\n",
      "temporal segments which are manipulated. ForgeryNet is by far the largest\n",
      "publicly available deep face forgery dataset in terms of data-scale (2.9\n",
      "million images, 221,247 videos), manipulations (7 image-level approaches, 8\n",
      "video-level approaches), perturbations (36 independent and more mixed\n",
      "perturbations) and annotations (6.3 million classification labels, 2.9 million\n",
      "manipulated area annotations and 221,247 temporal forgery segment labels). We\n",
      "perform extensive benchmarking and studies of existing face forensics methods\n",
      "and obtain several valuable observations. \n",
      "\n",
      "\n",
      "The cybersecurity breaches render surveillance systems vulnerable to video\n",
      "forgery attacks, under which authentic live video streams are tampered to\n",
      "conceal illegal human activities under surveillance cameras. Traditional video\n",
      "forensics approaches can detect and localize forgery traces in each video frame\n",
      "using computationally-expensive spatial-temporal analysis, while falling short\n",
      "in real-time verification of live video feeds. The recent work correlates\n",
      "time-series camera and wireless signals to recognize replayed surveillance\n",
      "videos using event-level timing information but it cannot realize fine-grained\n",
      "forgery detection and localization on each frame. To fill this gap, this paper\n",
      "proposes Secure-Pose, a novel cross-modal forgery detection and localization\n",
      "system for live surveillance videos using WiFi signals near the camera spot. We\n",
      "observe that coexisting camera and WiFi signals convey common human semantic\n",
      "information and the presence of forgery attacks on video frames will decouple\n",
      "such information correspondence. Secure-Pose extracts effective human pose\n",
      "features from synchronized multi-modal signals and detects and localizes\n",
      "forgery traces under both inter-frame and intra-frame attacks in each frame. We\n",
      "implement Secure-Pose using a commercial camera and two Intel 5300 NICs and\n",
      "evaluate it in real-world environments. Secure-Pose achieves a high detection\n",
      "accuracy of 95.1% and can effectively localize tampered objects under different\n",
      "forgery attacks. \n",
      "\n",
      "\n",
      "Forgery operations on video contents are nowadays within the reach of anyone,\n",
      "thanks to the availability of powerful and user-friendly editing software.\n",
      "Integrity verification and authentication of videos represent a major interest\n",
      "in both journalism (e.g., fake news debunking) and legal environments dealing\n",
      "with digital evidence (e.g., a court of law). While several strategies and\n",
      "different forensics traces have been proposed in recent years, latest solutions\n",
      "aim at increasing the accuracy by combining multiple detectors and features.\n",
      "This paper presents a video forgery localization framework that verifies the\n",
      "self-consistency of coding traces between and within video frames, by fusing\n",
      "the information derived from a set of independent feature descriptors. The\n",
      "feature extraction step is carried out by means of an explainable convolutional\n",
      "neural network architecture, specifically designed to look for and classify\n",
      "coding artifacts. The overall framework was validated in two typical forgery\n",
      "scenarios: temporal and spatial splicing. Experimental results show an\n",
      "improvement to the state-of-the-art on temporal splicing localization and also\n",
      "promising performance in the newly tackled case of spatial splicing, on both\n",
      "synthetic and real-world videos. \n",
      "\n",
      "\n",
      "There are concerns that new approaches to the synthesis of high quality face\n",
      "videos may be misused to manipulate videos with malicious intent. The research\n",
      "community therefore developed methods for the detection of modified footage and\n",
      "assembled benchmark datasets for this task. In this paper, we examine how the\n",
      "performance of forgery detectors depends on the presence of artefacts that the\n",
      "human eye can see. We introduce a new benchmark dataset for face video forgery\n",
      "detection, of unprecedented quality. It allows us to demonstrate that existing\n",
      "detection techniques have difficulties detecting fakes that reliably fool the\n",
      "human eye. We thus introduce a new family of detectors that examine\n",
      "combinations of spatial and temporal features and outperform existing\n",
      "approaches both in terms of detection accuracy and generalization. \n",
      "\n",
      "\n",
      "Rapid progress in deep learning is continuously making it easier and cheaper\n",
      "to generate video forgeries. Hence, it becomes very important to have a\n",
      "reliable way of detecting these forgeries. This paper describes such an\n",
      "approach for various tampering scenarios. The problem is modelled as a\n",
      "per-frame binary classification task. We propose to use transfer learning from\n",
      "face recognition task to improve tampering detection on many different facial\n",
      "manipulation scenarios. Furthermore, in low resolution settings, where single\n",
      "frame detection performs poorly, we try to make use of neighboring frames for\n",
      "middle frame classification. We evaluate both approaches on the public\n",
      "FaceForensics benchmark, achieving state of the art accuracy. \n",
      "\n",
      "\n",
      "Videos can be manipulated by duplicating a sequence of consecutive frames\n",
      "with the goal of concealing or imitating a specific content in the same video.\n",
      "In this paper, we propose a novel coarse-to-fine framework based on deep\n",
      "Convolutional Neural Networks to automatically detect and localize such frame\n",
      "duplication. First, an I3D network finds coarse-level matches between candidate\n",
      "duplicated frame sequences and the corresponding selected original frame\n",
      "sequences. Then a Siamese network based on ResNet architecture identifies\n",
      "fine-level correspondences between an individual duplicated frame and the\n",
      "corresponding selected frame. We also propose a robust statistical approach to\n",
      "compute a video-level score indicating the likelihood of manipulation or\n",
      "forgery. Additionally, for providing manipulation localization information we\n",
      "develop an inconsistency detector based on the I3D network to distinguish the\n",
      "duplicated frames from the selected original frames. Quantified evaluation on\n",
      "two challenging video forgery datasets clearly demonstrates that this approach\n",
      "performs significantly better than four recent state-of-the-art methods. \n",
      "\n",
      "\n",
      "Video forgery detection is becoming an important issue in recent years,\n",
      "because modern editing software provide powerful and easy-to-use tools to\n",
      "manipulate videos. In this paper we propose to perform detection by means of\n",
      "deep learning, with an architecture based on autoencoders and recurrent neural\n",
      "networks. A training phase on a few pristine frames allows the autoencoder to\n",
      "learn an intrinsic model of the source. Then, forged material is singled out as\n",
      "anomalous, as it does not fit the learned model, and is encoded with a large\n",
      "reconstruction error. Recursive networks, implemented with the long short-term\n",
      "memory model, are used to exploit temporal dependencies. Preliminary results on\n",
      "forged videos show the potential of this approach. \n",
      "\n",
      "\n",
      "The Digital Forgeries though not visibly identifiable to human perception it\n",
      "may alter or meddle with underlying natural statistics of digital content.\n",
      "Tampering involves fiddling with video content in order to cause damage or make\n",
      "unauthorized alteration/modification. Tampering detection in video is\n",
      "cumbersome compared to image when considering the properties of the video.\n",
      "Tampering impacts need to be studied and the applied technique/method is used\n",
      "to establish the factual information for legal course in judiciary. In this\n",
      "paper we give an overview of the prior literature and challenges involved in\n",
      "video forgery detection where passive approach is found. \n",
      "\n",
      "\n",
      "Realistic fake videos are a potential tool for spreading harmful\n",
      "misinformation given our increasing online presence and information intake.\n",
      "This paper presents a multimodal learning-based method for detection of real\n",
      "and fake videos. The method combines information from three modalities - audio,\n",
      "video, and physiology. We investigate two strategies for combining the video\n",
      "and physiology modalities, either by augmenting the video with information from\n",
      "the physiology or by novelly learning the fusion of those two modalities with a\n",
      "proposed Graph Convolutional Network architecture. Both strategies for\n",
      "combining the two modalities rely on a novel method for generation of visual\n",
      "representations of physiological signals. The detection of real and fake videos\n",
      "is then based on the dissimilarity between the audio and modified video\n",
      "modalities. The proposed method is evaluated on two benchmark datasets and the\n",
      "results show significant increase in detection performance compared to previous\n",
      "methods. \n",
      "\n",
      "\n",
      "Business Collaboration Platforms like Microsoft Teams and Slack enable\n",
      "teamwork by supporting text chatting and third-party resource integration. A\n",
      "user can access online file storage, make video calls, and manage a code\n",
      "repository, all from within the platform, thus making them a hub for sensitive\n",
      "communication and resources. The key enabler for these productivity features is\n",
      "a third-party application model. We contribute an experimental security\n",
      "analysis of this model and the third-party apps. Performing this analysis is\n",
      "challenging because commercial platforms and their apps are closed-source\n",
      "systems. Our analysis methodology is to systematically investigate different\n",
      "types of interactions possible between apps and users. We discover that the\n",
      "access control model in these systems violates two fundamental security\n",
      "principles: least privilege and complete mediation. These violations enable a\n",
      "malicious app to exploit the confidentiality and integrity of user messages and\n",
      "third-party resources connected to the platform. We construct proof-of-concept\n",
      "attacks that can: (1) eavesdrop on user messages without having permission to\n",
      "read those messages; (2) launch fake video calls; (3) automatically merge code\n",
      "into repositories without user approval or involvement. Finally, we provide an\n",
      "analysis of countermeasures that systems like Slack and Microsoft Teams can\n",
      "adopt today. \n",
      "\n",
      "\n",
      "Demonstrating the veracity of videos is a longstanding problem that has\n",
      "recently become more urgent and acute. It is extremely hard to accurately\n",
      "detect manipulated videos using content analysis, especially in the face of\n",
      "subtle, yet effective, manipulations, such as frame rate changes or skin tone\n",
      "adjustments. One prominent alternative to content analysis is to securely embed\n",
      "provenance information into videos. However, prior approaches have poor\n",
      "performance and/or granularity that is too coarse. To this end, we construct\n",
      "Vronicle -- a video provenance system that offers fine-grained provenance\n",
      "information and substantially better performance. It allows a video consumer to\n",
      "authenticate the camera that originated the video and the exact sequence of\n",
      "video filters that were subsequently applied to it. Vronicle exploits the\n",
      "increasing popularity and availability of Trusted Execution Environments (TEEs)\n",
      "on many types of computing platforms.\n",
      "  One contribution of Vronicle is the design of provenance information that\n",
      "allows the consumer to verify various aspects of the video, thereby defeating\n",
      "numerous fake-video creation methods. Vronicle's adversarial model allows for a\n",
      "powerful adversary that can manipulate the video (e.g., in transit) and the\n",
      "software state outside the TEE. Another contribution is the use of\n",
      "fixed-function Intel SGX enclaves to post-process videos. This design\n",
      "facilitates verification of provenance information.\n",
      "  We present a prototype implementation of Vronicle (to be open sourced), which\n",
      "relies on current technologies, making it readily deployable. Our evaluation\n",
      "demonstrates that Vronicle's performance is well-suited for offline use-cases. \n",
      "\n",
      "\n",
      "Although current deep learning-based face forgery detectors achieve\n",
      "impressive performance in constrained scenarios, they are vulnerable to samples\n",
      "created by unseen manipulation methods. Some recent works show improvements in\n",
      "generalisation but rely on cues that are easily corrupted by common\n",
      "post-processing operations such as compression. In this paper, we propose\n",
      "LipForensics, a detection approach capable of both generalising to novel\n",
      "manipulations and withstanding various distortions. LipForensics targets\n",
      "high-level semantic irregularities in mouth movements, which are common in many\n",
      "generated videos. It consists in first pretraining a spatio-temporal network to\n",
      "perform visual speech recognition (lipreading), thus learning rich internal\n",
      "representations related to natural mouth motion. A temporal network is\n",
      "subsequently finetuned on fixed mouth embeddings of real and forged data in\n",
      "order to detect fake videos based on mouth movements without overfitting to\n",
      "low-level, manipulation-specific artefacts. Extensive experiments show that\n",
      "this simple approach significantly surpasses the state-of-the-art in terms of\n",
      "generalisation to unseen manipulations and robustness to perturbations, as well\n",
      "as shed light on the factors responsible for its performance. Code is available\n",
      "on GitHub. \n",
      "\n",
      "\n",
      "The growth of misinformation technology necessitates the need to identify\n",
      "fake videos. One approach to preventing the consumption of these fake videos is\n",
      "provenance which allows the user to authenticate media content to its original\n",
      "source. This research designs and investigates the use of provenance indicators\n",
      "to help users identify fake videos. We first interview users regarding their\n",
      "experiences with different misinformation modes (text, image, video) to guide\n",
      "the design of indicators within users' existing perspectives. Then, we conduct\n",
      "a participatory design study to develop and design fake video indicators.\n",
      "Finally, we evaluate participant-designed indicators via both expert\n",
      "evaluations and quantitative surveys with a large group of end-users. Our\n",
      "results provide concrete design guidelines for the emerging issue of fake\n",
      "videos. Our findings also raise concerns regarding users' tendency to\n",
      "overgeneralize from misinformation warning messages, suggesting the need for\n",
      "further research on warning design in the ongoing fight against misinformation. \n",
      "\n",
      "\n",
      "Face anti-spoofing is crucial for the security of face recognition system, by\n",
      "avoiding invaded with presentation attack. Previous works have shown the\n",
      "effectiveness of using depth and temporal supervision for this task. However,\n",
      "depth supervision is often considered only in a single frame, and temporal\n",
      "supervision is explored by utilizing certain signals which is not robust to the\n",
      "change of scenes. In this work, motivated by two stream ConvNets, we propose a\n",
      "novel two stream FreqSaptialTemporalNet for face anti-spoofing which\n",
      "simultaneously takes advantage of frequent, spatial and temporal information.\n",
      "Compared with existing methods which mine spoofing cues in multi-frame RGB\n",
      "image, we make multi-frame spectrum image as one input stream for the\n",
      "discriminative deep neural network, encouraging the primary difference between\n",
      "live and fake video to be automatically unearthed. Extensive experiments show\n",
      "promising improvement results using the proposed architecture. Meanwhile, we\n",
      "proposed a concise method to obtain a large amount of spoofing training data by\n",
      "utilizing a frequent augmentation pipeline, which contributes detail\n",
      "visualization between live and fake images as well as data insufficiency issue\n",
      "when training large networks. \n",
      "\n",
      "\n",
      "We present our on-going effort of constructing a large-scale benchmark for\n",
      "face forgery detection. The first version of this benchmark,\n",
      "DeeperForensics-1.0, represents the largest face forgery detection dataset by\n",
      "far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times\n",
      "larger than existing datasets of the same kind. Extensive real-world\n",
      "perturbations are applied to obtain a more challenging benchmark of larger\n",
      "scale and higher diversity. All source videos in DeeperForensics-1.0 are\n",
      "carefully collected, and fake videos are generated by a newly proposed\n",
      "end-to-end face swapping framework. The quality of generated videos outperforms\n",
      "those in existing datasets, validated by user studies. The benchmark features a\n",
      "hidden test set, which contains manipulated videos achieving high deceptive\n",
      "scores in human evaluations. We further contribute a comprehensive study that\n",
      "evaluates five representative detection baselines and make a thorough analysis\n",
      "of different settings. \n",
      "\n",
      "\n",
      "Recent years have seen fast development in synthesizing realistic human faces\n",
      "using AI technologies. Such fake faces can be weaponized to cause negative\n",
      "personal and social impact. In this work, we develop technologies to defend\n",
      "individuals from becoming victims of recent AI synthesized fake videos by\n",
      "sabotaging would-be training data. This is achieved by disrupting deep neural\n",
      "network (DNN) based face detection method with specially designed imperceptible\n",
      "adversarial perturbations to reduce the quality of the detected faces. We\n",
      "describe attacking schemes under white-box, gray-box and black-box settings,\n",
      "each with decreasing information about the DNN based face detectors. We\n",
      "empirically show the effectiveness of our methods in disrupting\n",
      "state-of-the-art DNN based face detectors on several datasets. \n",
      "\n",
      "\n",
      "YouTube is the leading social media platform for sharing videos. As a result,\n",
      "it is plagued with misleading content that includes staged videos presented as\n",
      "real footages from an incident, videos with misrepresented context and videos\n",
      "where audio/video content is morphed. We tackle the problem of detecting such\n",
      "misleading videos as a supervised classification task. We develop UCNet - a\n",
      "deep network to detect fake videos and perform our experiments on two datasets\n",
      "- VAVD created by us and publicly available FVC [8]. We achieve a macro\n",
      "averaged F-score of 0.82 while training and testing on a 70:30 split of FVC,\n",
      "while the baseline model scores 0.36. We find that the proposed model\n",
      "generalizes well when trained on one dataset and tested on the other. \n",
      "\n",
      "\n",
      "The presence of a corresponding talking face has been shown to significantly\n",
      "improve speech intelligibility in noisy conditions and for hearing impaired\n",
      "population. In this paper, we present a system that can generate landmark\n",
      "points of a talking face from an acoustic speech in real time. The system uses\n",
      "a long short-term memory (LSTM) network and is trained on frontal videos of 27\n",
      "different speakers with automatically extracted face landmarks. After training,\n",
      "it can produce talking face landmarks from the acoustic speech of unseen\n",
      "speakers and utterances. The training phase contains three key steps. We first\n",
      "transform landmarks of the first video frame to pin the two eye points into two\n",
      "predefined locations and apply the same transformation on all of the following\n",
      "video frames. We then remove the identity information by transforming the\n",
      "landmarks into a mean face shape across the entire training dataset. Finally,\n",
      "we train an LSTM network that takes the first- and second-order temporal\n",
      "differences of the log-mel spectrogram as input to predict face landmarks in\n",
      "each frame. We evaluate our system using the mean-squared error (MSE) loss of\n",
      "landmarks of lips between predicted and ground-truth landmarks as well as their\n",
      "first- and second-order temporal differences. We further evaluate our system by\n",
      "conducting subjective tests, where the subjects try to distinguish the real and\n",
      "fake videos of talking face landmarks. Both tests show promising results. \n",
      "\n",
      "\n",
      "With recent advances in computer vision and graphics, it is now possible to\n",
      "generate videos with extremely realistic synthetic faces, even in real time.\n",
      "Countless applications are possible, some of which raise a legitimate alarm,\n",
      "calling for reliable detectors of fake videos. In fact, distinguishing between\n",
      "original and manipulated video can be a challenge for humans and computers\n",
      "alike, especially when the videos are compressed or have low resolution, as it\n",
      "often happens on social networks. Research on the detection of face\n",
      "manipulations has been seriously hampered by the lack of adequate datasets. To\n",
      "this end, we introduce a novel face manipulation dataset of about half a\n",
      "million edited images (from over 1000 videos). The manipulations have been\n",
      "generated with a state-of-the-art face editing approach. It exceeds all\n",
      "existing video manipulation datasets by at least an order of magnitude. Using\n",
      "our new dataset, we introduce benchmarks for classical image forensic tasks,\n",
      "including classification and segmentation, considering videos compressed at\n",
      "various quality levels. In addition, we introduce a benchmark evaluation for\n",
      "creating indistinguishable forgeries with known ground truth; for instance with\n",
      "generative refinement models. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for abst in result_df[\"abstract\"]:\n",
    "    print(abst,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6e4af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An optical microscopic examination of thinly cut stained tissue on glass\n",
      "slides prepared from a FFPE tissue blocks is the gold standard for tissue\n",
      "diagnostics. In addition, the diagnostic abilities and expertise of any\n",
      "pathologist is dependent on their direct experience with common as well as\n",
      "rarer variant morphologies. Recently, deep learning approaches have been used\n",
      "to successfully show a high level of accuracy for such tasks. However,\n",
      "obtaining expert-level annotated images is an expensive and time-consuming task\n",
      "and artificially synthesized histological images can prove greatly beneficial.\n",
      "Here, we present an approach to not only generate histological images that\n",
      "reproduce the diagnostic morphologic features of common disease but also\n",
      "provide a user ability to generate new and rare morphologies. Our approach\n",
      "involves developing a generative adversarial network model that synthesizes\n",
      "pathology images constrained by class labels. We investigated the ability of\n",
      "this framework in synthesizing realistic prostate and colon tissue images and\n",
      "assessed the utility of these images in augmenting diagnostic ability of\n",
      "machine learning methods as well as their usability by a panel of experienced\n",
      "anatomic pathologists. Synthetic data generated by our framework performed\n",
      "similar to real data in training a deep learning model for diagnosis.\n",
      "Pathologists were not able to distinguish between real and synthetic images and\n",
      "showed a similar level of inter-observer agreement for prostate cancer grading.\n",
      "We extended the approach to significantly more complex images from colon\n",
      "biopsies and showed that the complex microenvironment in such tissues can also\n",
      "be reproduced. Finally, we present the ability for a user to generate deepfake\n",
      "histological images via a simple markup of sematic labels. \n",
      "\n",
      "\n",
      "We present a novel conditional Generative Adversarial Network (cGAN)\n",
      "architecture that is capable of generating 3D Computed Tomography scans in\n",
      "voxels from noisy and/or pixelated approximations and with the potential to\n",
      "generate full synthetic 3D scan volumes. We believe conditional cGAN to be a\n",
      "tractable approach to generate 3D CT volumes, even though the problem of\n",
      "generating full resolution deep fakes is presently impractical due to GPU\n",
      "memory limitations. We present results for autoencoder, denoising, and\n",
      "depixelating tasks which are trained and tested on two novel COVID19 CT\n",
      "datasets. Our evaluation metrics, Peak Signal to Noise ratio (PSNR) range from\n",
      "12.53 - 46.46 dB, and the Structural Similarity index ( SSIM) range from 0.89\n",
      "to 1. \n",
      "\n",
      "\n",
      "Video represents the majority of internet traffic today, driving a continual\n",
      "race between the generation of higher quality content, transmission of larger\n",
      "file sizes, and the development of network infrastructure. In addition, the\n",
      "recent COVID-19 pandemic fueled a surge in the use of video conferencing tools.\n",
      "Since videos take up considerable bandwidth (~100 Kbps to a few Mbps), improved\n",
      "video compression can have a substantial impact on network performance for live\n",
      "and pre-recorded content, providing broader access to multimedia content\n",
      "worldwide. We present a novel video compression pipeline, called Txt2Vid, which\n",
      "dramatically reduces data transmission rates by compressing webcam videos\n",
      "(\"talking-head videos\") to a text transcript. The text is transmitted and\n",
      "decoded into a realistic reconstruction of the original video using recent\n",
      "advances in deep learning based voice cloning and lip syncing models. Our\n",
      "generative pipeline achieves two to three orders of magnitude reduction in the\n",
      "bitrate as compared to the standard audio-video codecs (encoders-decoders),\n",
      "while maintaining equivalent Quality-of-Experience based on a subjective\n",
      "evaluation by users (n = 242) in an online study. The Txt2Vid framework opens\n",
      "up the potential for creating novel applications such as enabling audio-video\n",
      "communication during poor internet connectivity, or in remote terrains with\n",
      "limited bandwidth. The code for this work is available at\n",
      "https://github.com/tpulkit/txt2vid.git. \n",
      "\n",
      "\n",
      "Synthetic data generated by generative models can enhance the performance and\n",
      "capabilities of data-hungry deep learning models in medical imaging. However,\n",
      "there is (1) limited availability of (synthetic) datasets and (2) generative\n",
      "models are complex to train, which hinders their adoption in research and\n",
      "clinical applications. To reduce this entry barrier, we propose medigan, a\n",
      "one-stop shop for pretrained generative models implemented as an open-source\n",
      "framework-agnostic Python library. medigan allows researchers and developers to\n",
      "create, increase, and domain-adapt their training data in just a few lines of\n",
      "code. Guided by design decisions based on gathered end-user requirements, we\n",
      "implement medigan based on modular components for generative model (i)\n",
      "execution, (ii) visualisation, (iii) search & ranking, and (iv) contribution.\n",
      "The library's scalability and design is demonstrated by its growing number of\n",
      "integrated and readily-usable pretrained generative models consisting of 21\n",
      "models utilising 9 different Generative Adversarial Network architectures\n",
      "trained on 11 datasets from 4 domains, namely, mammography, endoscopy, x-ray,\n",
      "and MRI. Furthermore, 3 applications of medigan are analysed in this work,\n",
      "which include (a) enabling community-wide sharing of restricted data, (b)\n",
      "investigating generative model evaluation metrics, and (c) improving clinical\n",
      "downstream tasks. In (b), extending on common medical image synthesis\n",
      "assessment and reporting standards, we show Fr\\'echet Inception Distance\n",
      "variability based on image normalisation and radiology-specific feature\n",
      "extraction. \n",
      "\n",
      "\n",
      "MRI and CT are most widely used medical imaging modalities. It is often\n",
      "necessary to acquire multi-modality images for diagnosis and treatment such as\n",
      "radiotherapy planning. However, multi-modality imaging is not only costly but\n",
      "also introduces misalignment between MRI and CT images. To address this\n",
      "challenge, computational conversion is a viable approach between MRI and CT\n",
      "images, especially from MRI to CT images. In this paper, we propose to use an\n",
      "emerging deep learning framework called diffusion and score-matching models in\n",
      "this context. Specifically, we adapt denoising diffusion probabilistic and\n",
      "score-matching models, use four different sampling strategies, and compare\n",
      "their performance metrics with that using a convolutional neural network and a\n",
      "generative adversarial network model. Our results show that the diffusion and\n",
      "score-matching models generate better synthetic CT images than the CNN and GAN\n",
      "models. Furthermore, we investigate the uncertainties associated with the\n",
      "diffusion and score-matching networks using the Monte-Carlo method, and improve\n",
      "the results by averaging their Monte-Carlo outputs. Our study suggests that\n",
      "diffusion and score-matching models are powerful to generate high quality\n",
      "images conditioned on an image obtained using a complementary imaging modality,\n",
      "analytically rigorous with clear explainability, and highly competitive with\n",
      "CNNs and GANs for image synthesis. \n",
      "\n",
      "\n",
      "Multiple Sclerosis (MS) is a chronic progressive neurological disease\n",
      "characterized by the development of lesions in the white matter of the brain.\n",
      "T2-fluid-attenuated inversion recovery (FLAIR) brain magnetic resonance imaging\n",
      "(MRI) provides superior visualization and characterization of MS lesions,\n",
      "relative to other MRI modalities. Longitudinal brain FLAIR MRI in MS, involving\n",
      "repetitively imaging a patient over time, provides helpful information for\n",
      "clinicians towards monitoring disease progression. Predicting future whole\n",
      "brain MRI examinations with variable time lag has only been attempted in\n",
      "limited applications, such as healthy aging and structural degeneration in\n",
      "Alzheimer's Disease. In this article, we present novel modifications to deep\n",
      "learning architectures for MS FLAIR image synthesis, in order to support\n",
      "prediction of longitudinal images in a flexible continuous way. This is\n",
      "achieved with learned transposed convolutions, which support modelling time as\n",
      "a spatially distributed array with variable temporal properties at different\n",
      "spatial locations. Thus, this approach can theoretically model\n",
      "spatially-specific time-dependent brain development, supporting the modelling\n",
      "of more rapid growth at appropriate physical locations, such as the site of an\n",
      "MS brain lesion. This approach also supports the clinician user to define how\n",
      "far into the future a predicted examination should target. Accurate prediction\n",
      "of future rounds of imaging can inform clinicians of potentially poor patient\n",
      "outcomes, which may be able to contribute to earlier treatment and better\n",
      "prognoses. Four distinct deep learning architectures have been developed. The\n",
      "ISBI2015 longitudinal MS dataset was used to validate and compare our proposed\n",
      "approaches. Results demonstrate that a modified ACGAN achieves the best\n",
      "performance and reduces variability in model accuracy. \n",
      "\n",
      "\n",
      "We propose a method for synthesizing cardiac MR images with plausible heart\n",
      "shapes and realistic appearances for the purpose of generating labeled data for\n",
      "deep-learning (DL) training. It breaks down the image synthesis into label\n",
      "deformation and label-to-image translation tasks. The former is achieved via\n",
      "latent space interpolation in a VAE model, while the latter is accomplished via\n",
      "a conditional GAN model. We devise an approach for label manipulation in the\n",
      "latent space of the trained VAE model, namely pathology synthesis, aiming to\n",
      "synthesize a series of pseudo-pathological synthetic subjects with\n",
      "characteristics of a desired heart disease. Furthermore, we propose to model\n",
      "the relationship between 2D slices in the latent space of the VAE via\n",
      "estimating the correlation coefficient matrix between the latent vectors and\n",
      "utilizing it to correlate elements of randomly drawn samples before decoding to\n",
      "image space. This simple yet effective approach results in generating 3D\n",
      "consistent subjects from 2D slice-by-slice generations. Such an approach could\n",
      "provide a solution to diversify and enrich the available database of cardiac MR\n",
      "images and to pave the way for the development of generalizable DL-based image\n",
      "analysis algorithms. The code will be available at\n",
      "https://github.com/sinaamirrajab/CardiacPathologySynthesis. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) are state of the art for image\n",
      "synthesis. Here, we present dapi2ck, a novel GAN-based approach to synthesize\n",
      "cytokeratin (CK) staining from immunofluorescent (IF) DAPI staining of nuclei\n",
      "in non-small cell lung cancer (NSCLC) images. We use the synthetic CK to\n",
      "segment epithelial regions, which, compared to expert annotations, yield\n",
      "equally good results as segmentation on stained CK. Considering the limited\n",
      "number of markers in a multiplexed IF (mIF) panel, our approach allows to\n",
      "replace CK by another marker addressing the complexity of the tumor\n",
      "micro-environment (TME) to facilitate patient selection for immunotherapies. In\n",
      "contrast to stained CK, dapi2ck does not suffer from issues like unspecific CK\n",
      "staining or loss of tumoral CK expression. \n",
      "\n",
      "\n",
      "Lung nodule detection in chest X-ray (CXR) images is common to early\n",
      "screening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis\n",
      "(CAD) systems can support radiologists for nodule screening in CXR. However, it\n",
      "requires large-scale and diverse medical data with high-quality annotations to\n",
      "train such robust and accurate CADs. To alleviate the limited availability of\n",
      "such datasets, lung nodule synthesis methods are proposed for the sake of data\n",
      "augmentation. Nevertheless, previous methods lack the ability to generate\n",
      "nodules that are realistic with the size attribute desired by the detector. To\n",
      "address this issue, we introduce a novel lung nodule synthesis framework in\n",
      "this paper, which decomposes nodule attributes into three main aspects\n",
      "including shape, size, and texture, respectively. A GAN-based Shape Generator\n",
      "firstly models nodule shapes by generating diverse shape masks. The following\n",
      "Size Modulation then enables quantitative control on the diameters of the\n",
      "generated nodule shapes in pixel-level granularity. A coarse-to-fine gated\n",
      "convolutional Texture Generator finally synthesizes visually plausible nodule\n",
      "textures conditioned on the modulated shape masks. Moreover, we propose to\n",
      "synthesize nodule CXR images by controlling the disentangled nodule attributes\n",
      "for data augmentation, in order to better compensate for the nodules that are\n",
      "easily missed in the detection task. Our experiments demonstrate the enhanced\n",
      "image quality, diversity, and controllability of the proposed lung nodule\n",
      "synthesis framework. We also validate the effectiveness of our data\n",
      "augmentation on greatly improving nodule detection performance. \n",
      "\n",
      "\n",
      "Imputation of missing images via source-to-target modality translation can\n",
      "facilitate downstream tasks in medical imaging. A pervasive approach for\n",
      "synthesizing target images involves one-shot mapping through generative\n",
      "adversarial networks (GAN). Yet, GAN models that implicitly characterize the\n",
      "image distribution can suffer from limited sample fidelity and diversity. Here,\n",
      "we propose a novel method based on adversarial diffusion modeling, SynDiff, for\n",
      "improved reliability in medical image synthesis. To capture a direct correlate\n",
      "of the image distribution, SynDiff leverages a conditional diffusion process to\n",
      "progressively map noise and source images onto the target image. For fast and\n",
      "accurate image sampling during inference, large diffusion steps are coupled\n",
      "with adversarial projections in the reverse diffusion direction. To enable\n",
      "training on unpaired datasets, a cycle-consistent architecture is devised with\n",
      "two coupled diffusion processes to synthesize the target given source and the\n",
      "source given target. Extensive assessments are reported on the utility of\n",
      "SynDiff against competing GAN and diffusion models in multi-contrast MRI and\n",
      "MRI-CT translation. Our demonstrations indicate that SynDiff offers superior\n",
      "performance against competing baselines both qualitatively and quantitatively. \n",
      "\n",
      "\n",
      "Medical image synthesis has attracted increasing attention because it could\n",
      "generate missing image data, improving diagnosis and benefits many downstream\n",
      "tasks. However, so far the developed synthesis model is not adaptive to unseen\n",
      "data distribution that presents domain shift, limiting its applicability in\n",
      "clinical routine. This work focuses on exploring domain adaptation (DA) of 3D\n",
      "image-to-image synthesis models. First, we highlight the technical difference\n",
      "in DA between classification, segmentation and synthesis models. Second, we\n",
      "present a novel efficient adaptation approach based on 2D variational\n",
      "autoencoder which approximates 3D distributions. Third, we present empirical\n",
      "studies on the effect of the amount of adaptation data and the key\n",
      "hyper-parameters. Our results show that the proposed approach can significantly\n",
      "improve the synthesis accuracy on unseen domains in a 3D setting. The code is\n",
      "publicly available at\n",
      "https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis \n",
      "\n",
      "\n",
      "The destitution of image data and corresponding expert annotations limit the\n",
      "training capacities of AI diagnostic models and potentially inhibit their\n",
      "performance. To address such a problem of data and label scarcity, generative\n",
      "models have been developed to augment the training datasets. Previously\n",
      "proposed generative models usually require manually adjusted annotations (e.g.,\n",
      "segmentation masks) or need pre-labeling. However, studies have found that\n",
      "these pre-labeling based methods can induce hallucinating artifacts, which\n",
      "might mislead the downstream clinical tasks, while manual adjustment could be\n",
      "onerous and subjective. To avoid manual adjustment and pre-labeling, we propose\n",
      "a novel controllable and simultaneous synthesizer (dubbed CS$^2$) in this study\n",
      "to generate both realistic images and corresponding annotations at the same\n",
      "time. Our CS$^2$ model is trained and validated using high resolution CT (HRCT)\n",
      "data collected from COVID-19 patients to realize an efficient infections\n",
      "segmentation with minimal human intervention. Our contributions include 1) a\n",
      "conditional image synthesis network that receives both style information from\n",
      "reference CT images and structural information from unsupervised segmentation\n",
      "masks, and 2) a corresponding segmentation mask synthesis network to\n",
      "automatically segment these synthesized images simultaneously. Our experimental\n",
      "studies on HRCT scans collected from COVID-19 patients demonstrate that our\n",
      "CS$^2$ model can lead to realistic synthesized datasets and promising\n",
      "segmentation results of COVID infections compared to the state-of-the-art\n",
      "nnUNet trained and fine-tuned in a fully supervised manner. \n",
      "\n",
      "\n",
      "Here we present a structural similarity index measure (SSIM) guided\n",
      "conditional Generative Adversarial Network (cGAN) that generatively performs\n",
      "image-to-image (i2i) synthesis to generate photo-accurate protein channels in\n",
      "multiplexed spatial proteomics images. This approach can be utilized to\n",
      "accurately generate missing spatial proteomics channels that were not included\n",
      "during experimental data collection either at the bench or the clinic.\n",
      "Experimental spatial proteomic data from the Human BioMolecular Atlas Program\n",
      "(HuBMAP) was used to generate spatial representations of missing proteins\n",
      "through a U-Net based image synthesis pipeline. HuBMAP channels were\n",
      "hierarchically clustered by the (SSIM) as a heuristic to obtain the minimal set\n",
      "needed to recapitulate the underlying biology represented by the spatial\n",
      "landscape of proteins. We subsequently prove that our SSIM based architecture\n",
      "allows for scaling of generative image synthesis to slides with up to 100\n",
      "channels, which is better than current state of the art algorithms which are\n",
      "limited to data with 11 channels. We validate these claims by generating a new\n",
      "experimental spatial proteomics data set from human lung adenocarcinoma tissue\n",
      "sections and show that a model trained on HuBMAP can accurately synthesize\n",
      "channels from our new data set. The ability to recapitulate experimental data\n",
      "from sparsely stained multiplexed histological slides containing spatial\n",
      "proteomic will have tremendous impact on medical diagnostics and drug\n",
      "development, and also raises important questions on the medical ethics of\n",
      "utilizing data produced by generative image synthesis in the clinical setting.\n",
      "The algorithm that we present in this paper will allow researchers and\n",
      "clinicians to save time and costs in proteomics based histological staining\n",
      "while also increasing the amount of data that they can generate through their\n",
      "experiments. \n",
      "\n",
      "\n",
      "Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical\n",
      "practice as each contrast provides complementary information. However, the\n",
      "availability of each contrast may vary amongst patients in reality. This poses\n",
      "challenges to both radiologists and automated image analysis algorithms. A\n",
      "general approach for tackling this problem is missing data imputation, which\n",
      "aims to synthesize the missing contrasts from existing ones. While several\n",
      "convolutional neural network (CNN) based algorithms have been proposed, they\n",
      "suffer from the fundamental limitations of CNN models, such as requirement for\n",
      "fixed numbers of input and output channels, inability to capture long-range\n",
      "dependencies, and lack of interpretability. In this paper, we formulate missing\n",
      "data imputation as a sequence-to-sequence learning problem and propose a\n",
      "multi-contrast multi-scale Transformer (MMT), which can take any subset of\n",
      "input contrasts and synthesize those that are missing. MMT consists of a\n",
      "multi-scale Transformer encoder that builds hierarchical representations of\n",
      "inputs combined with a multi-scale Transformer decoder that generates the\n",
      "outputs in a coarse-to-fine fashion. Thanks to the proposed multi-contrast Swin\n",
      "Transformer blocks, it can efficiently capture intra- and inter-contrast\n",
      "dependencies for accurate image synthesis. Moreover, MMT is inherently\n",
      "interpretable. It allows us to understand the importance of each input contrast\n",
      "in different regions by analyzing the in-built attention maps of Transformer\n",
      "blocks in the decoder. Extensive experiments on two large-scale multi-contrast\n",
      "MRI datasets demonstrate that MMT outperforms the state-of-the-art methods\n",
      "quantitatively and qualitatively. \n",
      "\n",
      "\n",
      "In recent years, generative adversarial networks (GANs) have gained\n",
      "tremendous popularity for potential applications in medical imaging, such as\n",
      "medical image synthesis, restoration, reconstruction, translation, as well as\n",
      "objective image quality assessment. Despite the impressive progress in\n",
      "generating high-resolution, perceptually realistic images, it is not clear if\n",
      "modern GANs reliably learn the statistics that are meaningful to a downstream\n",
      "medical imaging application. In this work, the ability of a state-of-the-art\n",
      "GAN to learn the statistics of canonical stochastic image models (SIMs) that\n",
      "are relevant to objective assessment of image quality is investigated. It is\n",
      "shown that although the employed GAN successfully learned several basic first-\n",
      "and second-order statistics of the specific medical SIMs under consideration\n",
      "and generated images with high perceptual quality, it failed to correctly learn\n",
      "several per-image statistics pertinent to the these SIMs, highlighting the\n",
      "urgent need to assess medical image GANs in terms of objective measures of\n",
      "image quality. \n",
      "\n",
      "\n",
      "Ultrasound (US) imaging is widely used for anatomical structure inspection in\n",
      "clinical diagnosis. The training of new sonographers and deep learning based\n",
      "algorithms for US image analysis usually requires a large amount of data.\n",
      "However, obtaining and labeling large-scale US imaging data are not easy tasks,\n",
      "especially for diseases with low incidence. Realistic US image synthesis can\n",
      "alleviate this problem to a great extent. In this paper, we propose a\n",
      "generative adversarial network (GAN) based image synthesis framework. Our main\n",
      "contributions include: 1) we present the first work that can synthesize\n",
      "realistic B-mode US images with high-resolution and customized texture editing\n",
      "features; 2) to enhance structural details of generated images, we propose to\n",
      "introduce auxiliary sketch guidance into a conditional GAN. We superpose the\n",
      "edge sketch onto the object mask and use the composite mask as the network\n",
      "input; 3) to generate high-resolution US images, we adopt a progressive\n",
      "training strategy to gradually generate high-resolution images from\n",
      "low-resolution images. In addition, a feature loss is proposed to minimize the\n",
      "difference of high-level features between the generated and real images, which\n",
      "further improves the quality of generated images; 4) the proposed US image\n",
      "synthesis method is quite universal and can also be generalized to the US\n",
      "images of other anatomical structures besides the three ones tested in our\n",
      "study (lung, hip joint, and ovary); 5) extensive experiments on three large US\n",
      "image datasets are conducted to validate our method. Ablation studies,\n",
      "customized texture editing, user studies, and segmentation tests demonstrate\n",
      "promising results of our method in synthesizing realistic US images. \n",
      "\n",
      "\n",
      "Modern generative models, such as generative adversarial networks (GANs),\n",
      "hold tremendous promise for several areas of medical imaging, such as\n",
      "unconditional medical image synthesis, image restoration, reconstruction and\n",
      "translation, and optimization of imaging systems. However, procedures for\n",
      "establishing stochastic image models (SIMs) using GANs remain generic and do\n",
      "not address specific issues relevant to medical imaging. In this work,\n",
      "canonical SIMs that simulate realistic vessels in angiography images are\n",
      "employed to evaluate procedures for establishing SIMs using GANs. The GAN-based\n",
      "SIM is compared to the canonical SIM based on its ability to reproduce those\n",
      "statistics that are meaningful to the particular medically realistic SIM\n",
      "considered. It is shown that evaluating GANs using classical metrics and\n",
      "medically relevant metrics may lead to different conclusions about the fidelity\n",
      "of the trained GANs. This work highlights the need for the development of\n",
      "objective metrics for evaluating GANs. \n",
      "\n",
      "\n",
      "The microstructure is significant for exploring the physical properties of\n",
      "hardened cement paste. In general, the microstructures of hardened cement paste\n",
      "are obtained by microscopy. As a popular method, scanning electron microscopy\n",
      "(SEM) can acquire high-quality 2D images but fails to obtain 3D\n",
      "microstructures.Although several methods, such as microtomography (Micro-CT)\n",
      "and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM), can acquire 3D\n",
      "microstructures, these fail to obtain high-quality 3D images or consume\n",
      "considerable cost. To address these issues, a method based on solid texture\n",
      "synthesis is proposed, synthesizing high-quality 3D microstructural image of\n",
      "hardened cement paste. This method includes 2D backscattered electron (BSE)\n",
      "image acquisition and 3D microstructure synthesis phases. In the approach, the\n",
      "synthesis model is based on solid texture synthesis, capturing microstructure\n",
      "information of the acquired 2D BSE image and generating high-quality 3D\n",
      "microstructures. In experiments, the method is verified on actual 3D Micro-CT\n",
      "images and 2D BSE images. Finally, qualitative experiments demonstrate that the\n",
      "3D microstructures generated by our method have similar visual characteristics\n",
      "to the given 2D example. Furthermore, quantitative experiments prove that the\n",
      "synthetic 3D results are consistent with the actual instance in terms of\n",
      "porosity, particle size distribution, and grey scale co-occurrence matrix. \n",
      "\n",
      "\n",
      "The removal of non-brain signal from magnetic resonance imaging (MRI) data,\n",
      "known as skull-stripping, is an integral component of many neuroimage analysis\n",
      "streams. Despite their abundance, popular classical skull-stripping methods are\n",
      "usually tailored to images with specific acquisition properties, namely\n",
      "near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are\n",
      "prevalent in research settings. As a result, existing tools tend to adapt\n",
      "poorly to other image types, such as stacks of thick slices acquired with fast\n",
      "spin-echo (FSE) MRI that are common in the clinic. While learning-based\n",
      "approaches for brain extraction have gained traction in recent years, these\n",
      "methods face a similar burden, as they are only effective for image types seen\n",
      "during the training procedure. To achieve robust skull-stripping across a\n",
      "landscape of imaging protocols, we introduce SynthStrip, a rapid,\n",
      "learning-based brain-extraction tool. By leveraging anatomical segmentations to\n",
      "generate an entirely synthetic training dataset with anatomies, intensity\n",
      "distributions, and artifacts that far exceed the realistic range of medical\n",
      "images, SynthStrip learns to successfully generalize to a variety of real\n",
      "acquired brain images, removing the need for training data with target\n",
      "contrasts. We demonstrate the efficacy of SynthStrip for a diverse set of image\n",
      "acquisitions and resolutions across subject populations, ranging from newborn\n",
      "to adult. We show substantial improvements in accuracy over popular\n",
      "skull-stripping baselines -- all with a single trained model. Our method and\n",
      "labeled evaluation data are available at https://w3id.org/synthstrip. \n",
      "\n",
      "\n",
      "Magnetic Resonance Imaging (MRI) typically recruits multiple sequences\n",
      "(defined here as \"modalities\"). As each modality is designed to offer different\n",
      "anatomical and functional clinical information, there are evident disparities\n",
      "in the imaging content across modalities. Inter- and intra-modality affine and\n",
      "non-rigid image registration is an essential medical image analysis process in\n",
      "clinical imaging, as for example before imaging biomarkers need to be derived\n",
      "and clinically evaluated across different MRI modalities, time phases and\n",
      "slices. Although commonly needed in real clinical scenarios, affine and\n",
      "non-rigid image registration is not extensively investigated using a single\n",
      "unsupervised model architecture. In our work, we present an un-supervised deep\n",
      "learning registration methodology which can accurately model affine and\n",
      "non-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a\n",
      "fundamental inter-modality registration property that is not considered in deep\n",
      "learning registration algorithms. To address inverse-consistency, our\n",
      "methodology performs bi-directional cross-modality image synthesis to learn\n",
      "modality-invariant latent rep-resentations, while involves two factorised\n",
      "transformation networks and an inverse-consistency loss to learn\n",
      "topology-preserving anatomical transformations. Overall, our model (named\n",
      "\"FIRE\") shows improved performances against the reference standard baseline\n",
      "method on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI\n",
      "data experiments. \n",
      "\n",
      "\n",
      "MRI entails a great amount of cost, time and effort for the generation of all\n",
      "the modalities that are recommended for efficient diagnosis and treatment\n",
      "planning. Recent advancements in deep learning research show that generative\n",
      "models have achieved substantial improvement in the aspects of style transfer\n",
      "and image synthesis. In this work, we formulate generating the missing MR\n",
      "modality from existing MR modalities as an imputation problem using style\n",
      "transfer. With a multiple-to-one mapping, we model a network that accommodates\n",
      "domain specific styles in generating the target image. We analyse the style\n",
      "diversity both within and across MR modalities. Our model is tested on the\n",
      "BraTS'18 dataset and the results obtained are observed to be on par with the\n",
      "state-of-the-art in terms of visual metrics, SSIM and PSNR. After being\n",
      "evaluated by two expert radiologists, we show that our model is efficient,\n",
      "extendable, and suitable for clinical applications. \n",
      "\n",
      "\n",
      "The existence of completely aligned and paired multi-modal neuroimaging data\n",
      "has proved its effectiveness in diagnosis of brain diseases. However,\n",
      "collecting the full set of well-aligned and paired data is impractical or even\n",
      "luxurious, since the practical difficulties may include high cost, long time\n",
      "acquisition, image corruption, and privacy issues. A realistic solution is to\n",
      "explore either an unsupervised learning or a semi-supervised learning to\n",
      "synthesize the absent neuroimaging data. In this paper, we tend to approach\n",
      "multi-modality brain image synthesis task from different perspectives, which\n",
      "include the level of supervision, the range of modality synthesis, and the\n",
      "synthesis-based downstream tasks. Particularly, we provide in-depth analysis on\n",
      "how cross-modality brain image synthesis can improve the performance of\n",
      "different downstream tasks. Finally, we evaluate the challenges and provide\n",
      "several open directions for this community. All resources are available at\n",
      "https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis \n",
      "\n",
      "\n",
      "The existence of completely aligned and paired multi-modal neuroimaging data\n",
      "has proved its effectiveness in the diagnosis of brain diseases. However,\n",
      "collecting the full set of well-aligned and paired data is impractical, since\n",
      "the practical difficulties may include high cost, long time acquisition, image\n",
      "corruption, and privacy issues. Previously, the misaligned unpaired\n",
      "neuroimaging data (termed as MUD) are generally treated as noisy label.\n",
      "However, such a noisy label-based method fail to accomplish well when\n",
      "misaligned data occurs distortions severely. For example, the angle of rotation\n",
      "is different. In this paper, we propose a novel federated self-supervised\n",
      "learning (FedMed) for brain image synthesis. An affine transform loss (ATL) was\n",
      "formulated to make use of severely distorted images without violating privacy\n",
      "legislation for the hospital. We then introduce a new data augmentation\n",
      "procedure for self-supervised training and fed it into three auxiliary heads,\n",
      "namely auxiliary rotation, auxiliary translation and auxiliary scaling heads.\n",
      "The proposed method demonstrates the advanced performance in both the quality\n",
      "of our synthesized results under a severely misaligned and unpaired data\n",
      "setting, and better stability than other GAN-based algorithms. The proposed\n",
      "method also reduces the demand for deformable registration while encouraging to\n",
      "leverage the misaligned and unpaired data. Experimental results verify the\n",
      "outstanding performance of our learning paradigm compared to other\n",
      "state-of-the-art approaches. \n",
      "\n",
      "\n",
      "Data-driven paradigms using machine learning are becoming ubiquitous in image\n",
      "processing and communications. In particular, image-to-image (I2I) translation\n",
      "is a generic and widely used approach to image processing problems, such as\n",
      "image synthesis, style transfer, and image restoration. At the same time,\n",
      "neural image compression has emerged as a data-driven alternative to\n",
      "traditional coding approaches in visual communications. In this paper, we study\n",
      "the combination of these two paradigms into a joint I2I compression and\n",
      "translation framework, focusing on multi-domain image synthesis. We first\n",
      "propose distributed I2I translation by integrating quantization and entropy\n",
      "coding into an I2I translation framework (i.e. I2Icodec). In practice, the\n",
      "image compression functionality (i.e. autoencoding) is also desirable,\n",
      "requiring to deploy alongside I2Icodec a regular image codec. Thus, we further\n",
      "propose a unified framework that allows both translation and autoencoding\n",
      "capabilities in a single codec. Adaptive residual blocks conditioned on the\n",
      "translation/compression mode provide flexible adaptation to the desired\n",
      "functionality. The experiments show promising results in both I2I translation\n",
      "and image compression using a single model. \n",
      "\n",
      "\n",
      "Generative models have been applied in the medical imaging domain for various\n",
      "image recognition and synthesis tasks. However, a more controllable and\n",
      "interpretable image synthesis model is still lacking yet necessary for\n",
      "important applications such as assisting in medical training. In this work, we\n",
      "leverage the efficient self-attention and contrastive learning modules and\n",
      "build upon state-of-the-art generative adversarial networks (GANs) to achieve\n",
      "an attribute-aware image synthesis model, termed AttributeGAN, which can\n",
      "generate high-quality histopathology images based on multi-attribute inputs. In\n",
      "comparison to existing single-attribute conditional generative models, our\n",
      "proposed model better reflects input attributes and enables smoother\n",
      "interpolation among attribute values. We conduct experiments on a\n",
      "histopathology dataset containing stained H&E images of urothelial carcinoma\n",
      "and demonstrate the effectiveness of our proposed model via comprehensive\n",
      "quantitative and qualitative comparisons with state-of-the-art models as well\n",
      "as different variants of our model. Code is available at\n",
      "https://github.com/karenyyy/MICCAI2021AttributeGAN. \n",
      "\n",
      "\n",
      "Existing deep learning-based approaches for histopathology image analysis\n",
      "require large annotated training sets to achieve good performance; but\n",
      "annotating histopathology images is slow and resource-intensive. Conditional\n",
      "generative adversarial networks have been applied to generate synthetic\n",
      "histopathology images to alleviate this issue, but current approaches fail to\n",
      "generate clear contours for overlapped and touching nuclei. In this study, We\n",
      "propose a sharpness loss regularized generative adversarial network to\n",
      "synthesize realistic histopathology images. The proposed network uses\n",
      "normalized nucleus distance map rather than the binary mask to encode nuclei\n",
      "contour information. The proposed sharpness loss enhances the contrast of\n",
      "nuclei contour pixels. The proposed method is evaluated using four image\n",
      "quality metrics and segmentation results on two public datasets. Both\n",
      "quantitative and qualitative results demonstrate that the proposed approach can\n",
      "generate realistic histopathology images with clear nuclei contours. \n",
      "\n",
      "\n",
      "In this paper, we propose a method to add constraints that are\n",
      "un-formulatable in generative adversarial networks (GAN)-based arbitrary size\n",
      "RAW Bayer image generation. It is shown theoretically that by using the\n",
      "transformed data in GAN training, it is able to improve the learning of the\n",
      "original data distribution, owing to the invariant of Jensen-Shannon (JS)\n",
      "divergence between two distributions under invertible and differentiable\n",
      "transformation. Benefiting from the proposed method, RAW Bayer pattern images\n",
      "can be generated by configuring the transformation as demosaicing. It is shown\n",
      "that by adding another transformation, the proposed method is able to\n",
      "synthesize high-quality RAW Bayer images with arbitrary size. Experimental\n",
      "results show that images generated by the proposed method outperform the\n",
      "existing methods in the Fr\\'echet inception distance (FID) score, peak signal\n",
      "to noise ratio (PSNR), and mean structural similarity (MSSIM), and the training\n",
      "process is more stable. To the best knowledge of the authors, there is no\n",
      "open-source, large-scale image dataset in the RAW Bayer domain, which is\n",
      "crucial for research works aiming to explore the image signal processing (ISP)\n",
      "pipeline design for computer vision tasks. Converting the existing commonly\n",
      "used color image datasets to their corresponding RAW Bayer versions, the\n",
      "proposed method can be a promising solution to the RAW image dataset problem.\n",
      "We also show in the experiments that, by training object detection frameworks\n",
      "using the synthesized RAW Bayer images, they can be used in an end-to-end\n",
      "manner (from RAW images to vision tasks) with negligible performance\n",
      "degradation. \n",
      "\n",
      "\n",
      "Accurate and automated super-resolution image synthesis is highly desired\n",
      "since it has the great potential to circumvent the need for acquiring high-cost\n",
      "medical scans and a time-consuming preprocessing pipeline of neuroimaging data.\n",
      "However, existing deep learning frameworks are solely designed to predict\n",
      "high-resolution (HR) image from a low-resolution (LR) one, which limits their\n",
      "generalization ability to brain graphs (i.e., connectomes). A small body of\n",
      "works has focused on superresolving brain graphs where the goal is to predict a\n",
      "HR graph from a single LR graph. Although promising, existing works mainly\n",
      "focus on superresolving graphs belonging to the same domain (e.g., functional),\n",
      "overlooking the domain fracture existing between multimodal brain data\n",
      "distributions (e.g., morphological and structural). To this aim, we propose a\n",
      "novel inter-domain adaptation framework namely, Learn to SuperResolve Brain\n",
      "Graphs with Knowledge Distillation Network (L2S-KDnet), which adopts a\n",
      "teacher-student paradigm to superresolve brain graphs. Our teacher network is a\n",
      "graph encoder-decoder that firstly learns the LR brain graph embeddings, and\n",
      "secondly learns how to align the resulting latent representations to the HR\n",
      "ground truth data distribution using an adversarial regularization. Ultimately,\n",
      "it decodes the HR graphs from the aligned embeddings. Next, our student network\n",
      "learns the knowledge of the aligned brain graphs as well as the topological\n",
      "structure of the predicted HR graphs transferred from the teacher. We further\n",
      "leverage the decoder of the teacher to optimize the student network. L2S-KDnet\n",
      "presents the first TS architecture tailored for brain graph super-resolution\n",
      "synthesis that is based on inter-domain alignment. Our experimental results\n",
      "demonstrate substantial performance gains over benchmark methods. \n",
      "\n",
      "\n",
      "Segmentation of Prostate Cancer (PCa) tissues from Gleason graded\n",
      "histopathology images is vital for accurate diagnosis. Although deep learning\n",
      "(DL) based segmentation methods achieve state-of-the-art accuracy, they rely on\n",
      "large datasets with manual annotations. We propose a method to synthesize for\n",
      "PCa histopathology images by learning the geometrical relationship between\n",
      "different disease labels using self-supervised learning. We use a weakly\n",
      "supervised segmentation approach that uses Gleason score to segment the\n",
      "diseased regions and the resulting segmentation map is used to train a Shape\n",
      "Restoration Network (ShaRe-Net) to predict missing mask segments in a\n",
      "self-supervised manner. Using DenseUNet as the backbone generator architecture\n",
      "we incorporate latent variable sampling to inject diversity in the image\n",
      "generation process and thus improve robustness. Experiments on multiple\n",
      "histopathology datasets demonstrate the superiority of our method over\n",
      "competing image synthesis methods for segmentation tasks. Ablation studies show\n",
      "the benefits of integrating geometry and diversity in generating high-quality\n",
      "images, and our self-supervised approach with limited class-labeled data\n",
      "achieves similar performance as fully supervised learning. \n",
      "\n",
      "\n",
      "Multiplex immunofluorescence (MxIF) is an emerging imaging technique that\n",
      "produces the high sensitivity and specificity of single-cell mapping. With a\n",
      "tenet of 'seeing is believing', MxIF enables iterative staining and imaging\n",
      "extensive antibodies, which provides comprehensive biomarkers to segment and\n",
      "group different cells on a single tissue section. However, considerable\n",
      "depletion of the scarce tissue is inevitable from extensive rounds of staining\n",
      "and bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging\n",
      "can globally fail for particular rounds ('missing stain''). In this work, we\n",
      "focus on the 'missing stain' issue. It would be appealing to develop digital\n",
      "image synthesis approaches to restore missing stain images without losing more\n",
      "tissue physically. Herein, we aim to develop image synthesis approaches for\n",
      "eleven MxIF structural molecular markers (i.e., epithelial and stromal) on real\n",
      "samples. We propose a novel multi-channel high-resolution image synthesis\n",
      "approach, called pixN2N-HD, to tackle possible missing stain scenarios via a\n",
      "high-resolution generative adversarial network (GAN). Our contribution is\n",
      "three-fold: (1) a single deep network framework is proposed to tackle missing\n",
      "stain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four\n",
      "years of computational time to 20 hours when covering all possible missing\n",
      "stains scenarios, with up to five missing stains (e.g., '(N-1)-to-1',\n",
      "'(N-2)-to-2'); and (3) this work is the first comprehensive experimental study\n",
      "of investigating cross-stain synthesis in MxIF. Our results elucidate a\n",
      "promising direction of advancing MxIF imaging with deep image synthesis. \n",
      "\n",
      "\n",
      "As an effective way to integrate the information contained in multiple\n",
      "medical images under different modalities, medical image synthesis and fusion\n",
      "have emerged in various clinical applications such as disease diagnosis and\n",
      "treatment planning. In this paper, an invertible and variable augmented network\n",
      "(iVAN) is proposed for medical image synthesis and fusion. In iVAN, the channel\n",
      "number of the network input and output is the same through variable\n",
      "augmentation technology, and data relevance is enhanced, which is conducive to\n",
      "the generation of characterization information. Meanwhile, the invertible\n",
      "network is used to achieve the bidirectional inference processes. Due to the\n",
      "invertible and variable augmentation schemes, iVAN can not only be applied to\n",
      "the mappings of multi-input to one-output and multi-input to multi-output, but\n",
      "also be applied to one-input to multi-output. Experimental results demonstrated\n",
      "that the proposed method can obtain competitive or superior performance in\n",
      "comparison to representative medical image synthesis and fusion methods. \n",
      "\n",
      "\n",
      "Medical images, especially volumetric images, are of high resolution and\n",
      "often exceed the capacity of standard desktop GPUs. As a result, most deep\n",
      "learning-based medical image analysis tasks require the input images to be\n",
      "downsampled, often substantially, before these can be fed to a neural network.\n",
      "However, downsampling can lead to a loss of image quality, which is undesirable\n",
      "especially in reconstruction tasks, where the fine geometric details need to be\n",
      "preserved. In this paper, we propose that high-resolution images can be\n",
      "reconstructed in a coarse-to-fine fashion, where a deep learning algorithm is\n",
      "only responsible for generating a coarse representation of the image, which\n",
      "consumes moderate GPU memory. For producing the high-resolution outcome, we\n",
      "propose two novel methods: learned voxel rearrangement of the coarse output and\n",
      "hierarchical image synthesis. Compared to the coarse output, the\n",
      "high-resolution counterpart allows for smooth surface triangulation, which can\n",
      "be 3D-printed in the highest possible quality. Experiments of this paper are\n",
      "carried out on the dataset of AutoImplant 2021\n",
      "(https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial\n",
      "implant design. The dataset contains high-resolution skulls that can be viewed\n",
      "as 2D manifolds embedded in a 3D space. Codes associated with this study can be\n",
      "accessed at https://github.com/Jianningli/voxel_rearrangement. \n",
      "\n",
      "\n",
      "Brain age estimation based on magnetic resonance imaging (MRI) is an active\n",
      "research area in early diagnosis of some neurodegenerative diseases (e.g.\n",
      "Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain\n",
      "underdevelopment for the young group. Deep learning methods have achieved the\n",
      "state-of-the-art performance in many medical image analysis tasks, including\n",
      "brain age estimation. However, the performance and generalisability of the deep\n",
      "learning model are highly dependent on the quantity and quality of the training\n",
      "data set. Both collecting and annotating brain MRI data are extremely\n",
      "time-consuming. In this paper, to overcome the data scarcity problem, we\n",
      "propose a generative adversarial network (GAN) based image synthesis method.\n",
      "Different from the existing GAN-based methods, we integrate a task-guided\n",
      "branch (a regression model for age estimation) to the end of the generator in\n",
      "GAN. By adding a task-guided loss to the conventional GAN loss, the learned\n",
      "low-dimensional latent space and the synthesised images are more task-specific.\n",
      "It helps to boost the performance of the down-stream task by combining the\n",
      "synthesised images and real images for model training. The proposed method was\n",
      "evaluated on a public brain MRI data set for age estimation. Our proposed\n",
      "method outperformed (statistically significant) a deep convolutional neural\n",
      "network based regression model and the GAN-based image synthesis method without\n",
      "the task-guided branch. More importantly, it enables the identification of\n",
      "age-related brain regions in the image space. The code is available on GitHub\n",
      "(https://github.com/ruizhe-l/tgb-gan). \n",
      "\n",
      "\n",
      "With the success of deep learning-based methods applied in medical image\n",
      "analysis, convolutional neural networks (CNNs) have been investigated for\n",
      "classifying liver disease from ultrasound (US) data. However, the scarcity of\n",
      "available large-scale labeled US data has hindered the success of CNNs for\n",
      "classifying liver disease from US data. In this work, we propose a novel\n",
      "generative adversarial network (GAN) architecture for realistic diseased and\n",
      "healthy liver US image synthesis. We adopt the concept of stacking to\n",
      "synthesize realistic liver US data. Quantitative and qualitative evaluation is\n",
      "performed on 550 in-vivo B-mode liver US images collected from 55 subjects. We\n",
      "also show that the synthesized images, together with real in vivo data, can be\n",
      "used to significantly improve the performance of traditional CNN architectures\n",
      "for Nonalcoholic fatty liver disease (NAFLD) classification. \n",
      "\n",
      "\n",
      "Image synthesis via Generative Adversarial Networks (GANs) of\n",
      "three-dimensional (3D) medical images has great potential that can be extended\n",
      "to many medical applications, such as, image enhancement and disease\n",
      "progression modeling. However, current GAN technologies for 3D medical image\n",
      "synthesis need to be significantly improved to be readily adapted to real-world\n",
      "medical problems. In this paper, we extend the state-of-the-art StyleGAN2\n",
      "model, which natively works with two-dimensional images, to enable 3D image\n",
      "synthesis. In addition to the image synthesis, we investigate the\n",
      "controllability and interpretability of the 3D-StyleGAN via style vectors\n",
      "inherited form the original StyleGAN2 that are highly suitable for medical\n",
      "applications: (i) the latent space projection and reconstruction of unseen real\n",
      "images, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and\n",
      "feasibility with ~12,000 three-dimensional full brain MR T1 images, although it\n",
      "can be applied to any 3D volumetric images. Furthermore, we explore different\n",
      "configurations of hyperparameters to investigate potential improvement of the\n",
      "image synthesis with larger networks. The codes and pre-trained networks are\n",
      "available online: https://github.com/sh4174/3DStyleGAN. \n",
      "\n",
      "\n",
      "This paper strives to generate a synthetic computed tomography (CT) image\n",
      "from a magnetic resonance (MR) image. The synthetic CT image is valuable for\n",
      "radiotherapy planning when only an MR image is available. Recent approaches\n",
      "have made large strides in solving this challenging synthesis problem with\n",
      "convolutional neural networks that learn a mapping from MR inputs to CT\n",
      "outputs. In this paper, we find that all existing approaches share a common\n",
      "limitation: reconstruction breaks down in and around the high-frequency parts\n",
      "of CT images. To address this common limitation, we introduce\n",
      "frequency-supervised deep networks to explicitly enhance high-frequency\n",
      "MR-to-CT image reconstruction. We propose a frequency decomposition layer that\n",
      "learns to decompose predicted CT outputs into low- and high-frequency\n",
      "components, and we introduce a refinement module to improve high-frequency\n",
      "reconstruction through high-frequency adversarial learning. Experimental\n",
      "results on a new dataset with 45 pairs of 3D MR-CT brain images show the\n",
      "effectiveness and potential of the proposed approach. Code is available at\n",
      "\\url{https://github.com/shizenglin/Frequency-Supervised-MR-to-CT-Image-Synthesis}. \n",
      "\n",
      "\n",
      "Positron Emission Tomography (PET) is an important tool for studying\n",
      "Alzheimer's disease (AD). PET scans can be used as diagnostics tools, and to\n",
      "provide molecular characterization of patients with cognitive disorders.\n",
      "However, multiple tracers are needed to measure glucose metabolism (18F-FDG),\n",
      "synaptic vesicle protein (11C-UCB-J), and $\\beta$-amyloid (11C-PiB).\n",
      "Administering multiple tracers to patient will lead to high radiation dose and\n",
      "cost. In addition, access to PET scans using new or less-available tracers with\n",
      "sophisticated production methods and short half-life isotopes may be very\n",
      "limited. Thus, it is desirable to develop an efficient multi-tracer PET\n",
      "synthesis model that can generate multi-tracer PET from single-tracer PET.\n",
      "Previous works on medical image synthesis focus on one-to-one fixed domain\n",
      "translations, and cannot simultaneously learn the feature from multi-tracer\n",
      "domains. Given 3 or more tracers, relying on previous methods will also create\n",
      "a heavy burden on the number of models to be trained. To tackle these issues,\n",
      "we propose a 3D unified anatomy-aware cyclic adversarial network (UCAN) for\n",
      "translating multi-tracer PET volumes with one unified generative model, where\n",
      "MR with anatomical information is incorporated. Evaluations on a multi-tracer\n",
      "PET dataset demonstrate the feasibility that our UCAN can generate high-quality\n",
      "multi-tracer PET volumes, with NMSE less than 15% for all PET tracers. \n",
      "\n",
      "\n",
      "Generative adversarial models with convolutional neural network (CNN)\n",
      "backbones have recently been established as state-of-the-art in numerous\n",
      "medical image synthesis tasks. However, CNNs are designed to perform local\n",
      "processing with compact filters, and this inductive bias compromises learning\n",
      "of contextual features. Here, we propose a novel generative adversarial\n",
      "approach for medical image synthesis, ResViT, that leverages the contextual\n",
      "sensitivity of vision transformers along with the precision of convolution\n",
      "operators and realism of adversarial learning.} ResViT's generator employs a\n",
      "central bottleneck comprising novel aggregated residual transformer (ART)\n",
      "blocks that synergistically combine residual convolutional and transformer\n",
      "modules. Residual connections in ART blocks promote diversity in captured\n",
      "representations, while a channel compression module distills task-relevant\n",
      "information. A weight sharing strategy is introduced among ART blocks to\n",
      "mitigate computational burden. A unified implementation is introduced to avoid\n",
      "the need to rebuild separate synthesis models for varying source-target\n",
      "modality configurations. Comprehensive demonstrations are performed for\n",
      "synthesizing missing sequences in multi-contrast MRI, and CT images from MRI.\n",
      "Our results indicate superiority of ResViT against competing CNN- and\n",
      "transformer-based methods in terms of qualitative observations and quantitative\n",
      "metrics. \n",
      "\n",
      "\n",
      "While medical image segmentation is an important task for computer aided\n",
      "diagnosis, the high expertise requirement for pixelwise manual annotations\n",
      "makes it a challenging and time consuming task. Since conventional data\n",
      "augmentations do not fully represent the underlying distribution of the\n",
      "training set, the trained models have varying performance when tested on images\n",
      "captured from different sources. Most prior work on image synthesis for data\n",
      "augmentation ignore the interleaved geometric relationship between different\n",
      "anatomical labels. We propose improvements over previous GAN-based medical\n",
      "image synthesis methods by learning the relationship between different\n",
      "anatomical labels. We use a weakly supervised segmentation method to obtain\n",
      "pixel level semantic label map of images which is used learn the intrinsic\n",
      "relationship of geometry and shape across semantic labels. Latent space\n",
      "variable sampling results in diverse generated images from a base image and\n",
      "improves robustness. We use the synthetic images from our method to train\n",
      "networks for segmenting COVID-19 infected areas from lung CT images. The\n",
      "proposed method outperforms state-of-the-art segmentation methods on a public\n",
      "dataset. Ablation studies also demonstrate benefits of integrating geometry and\n",
      "diversity. \n",
      "\n",
      "\n",
      "Recently, Conditional Generative Adversarial Network (Conditional GAN) have\n",
      "shown very promising performance in several image-to-image translation\n",
      "applications. However, the uses of these conditional GANs are quite limited to\n",
      "low-resolution images, such as 256X256.The Pix2Pix-HD is a recent attempt to\n",
      "utilize the conditional GAN for high-resolution image synthesis. In this paper,\n",
      "we propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for\n",
      "high-resolution image-to-image translation up to 2048X1024 resolution. The\n",
      "proposed model is trained by allowing the flow of gradients from\n",
      "multiple-discriminators to a single generator at multiple scales. The proposed\n",
      "MSG U-Net architecture leads to photo-realistic high-resolution image-to-image\n",
      "translation. Moreover, the proposed model is computationally efficient as\n",
      "com-pared to the Pix2Pix-HD with an improvement in the inference time nearly by\n",
      "2.5 times. We provide the code of MSG U-Net model at\n",
      "https://github.com/laxmaniron/MSG-U-Net. \n",
      "\n",
      "\n",
      "In medical image synthesis, model training could be challenging due to the\n",
      "inconsistencies between images of different modalities even with the same\n",
      "patient, typically caused by internal status/tissue changes as different\n",
      "modalities are usually obtained at a different time. This paper proposes a\n",
      "novel deep learning method, Structure-aware Generative Adversarial Network\n",
      "(SA-GAN), that preserves the shapes and locations of in-consistent structures\n",
      "when generating medical images. SA-GAN is employed to generate synthetic\n",
      "computed tomography (synCT) images from magnetic resonance imaging (MRI) with\n",
      "two parallel streams: the global stream translates the input from the MRI to\n",
      "the CT domain while the local stream automatically segments the inconsistent\n",
      "organs, maintains their locations and shapes in MRI, and translates the organ\n",
      "intensities to CT. Through extensive experiments on a pelvic dataset, we\n",
      "demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\n",
      "and organ segmentation and supports MR-only treatment planning in disease sites\n",
      "with internal organ status changes. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have become increasingly powerful,\n",
      "generating mind-blowing photorealistic images that mimic the content of\n",
      "datasets they were trained to replicate. One recurrent theme in medical imaging\n",
      "is whether GANs can also be effective at generating workable medical data as\n",
      "they are for generating realistic RGB images. In this paper, we perform a\n",
      "multi-GAN and multi-application study to gauge the benefits of GANs in medical\n",
      "imaging. We tested various GAN architectures from basic DCGAN to more\n",
      "sophisticated style-based GANs on three medical imaging modalities and organs\n",
      "namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\n",
      "well-known and widely utilized datasets from which their FID score were\n",
      "computed to measure the visual acuity of their generated images. We further\n",
      "tested their usefulness by measuring the segmentation accuracy of a U-Net\n",
      "trained on these generated images.\n",
      "  Results reveal that GANs are far from being equal as some are ill-suited for\n",
      "medical imaging applications while others are much better off. The\n",
      "top-performing GANs are capable of generating realistic-looking medical images\n",
      "by FID standards that can fool trained experts in a visual Turing test and\n",
      "comply to some metrics. However, segmentation results suggests that no GAN is\n",
      "capable of reproducing the full richness of a medical datasets. \n",
      "\n",
      "\n",
      "Purpose: Different Magnetic resonance imaging (MRI) modalities of the same\n",
      "anatomical structure are required to present different pathological information\n",
      "from the physical level for diagnostic needs. However, it is often difficult to\n",
      "obtain full-sequence MRI images of patients owing to limitations such as time\n",
      "consumption and high cost. The purpose of this work is to develop an algorithm\n",
      "for target MRI sequences prediction with high accuracy, and provide more\n",
      "information for clinical diagnosis. Methods: We propose a deep learning based\n",
      "multi-modal computing model for MRI synthesis with feature disentanglement\n",
      "strategy. To take full advantage of the complementary information provided by\n",
      "different modalities, multi-modal MRI sequences are utilized as input. Notably,\n",
      "the proposed approach decomposes each input modality into modality-invariant\n",
      "space with shared information and modality-specific space with specific\n",
      "information, so that features are extracted separately to effectively process\n",
      "the input data. Subsequently, both of them are fused through the adaptive\n",
      "instance normalization (AdaIN) layer in the decoder. In addition, to address\n",
      "the lack of specific information of the target modality in the test phase, a\n",
      "local adaptive fusion (LAF) module is adopted to generate a modality-like\n",
      "pseudo-target with specific information similar to the ground truth. Results:\n",
      "To evaluate the synthesis performance, we verify our method on the BRATS2015\n",
      "dataset of 164 subjects. The experimental results demonstrate our approach\n",
      "significantly outperforms the benchmark method and other state-of-the-art\n",
      "medical image synthesis methods in both quantitative and qualitative measures.\n",
      "Compared with the pix2pixGANs method, the PSNR improves from 23.68 to 24.8.\n",
      "Conclusion: The proposed method could be effective in prediction of target MRI\n",
      "sequences, and useful for clinical diagnosis and treatment. \n",
      "\n",
      "\n",
      "Anomaly detection in visual data refers to the problem of differentiating\n",
      "abnormal appearances from normal cases. Supervised approaches have been\n",
      "successfully applied to different domains, but require an abundance of labeled\n",
      "data. Due to the nature of how anomalies occur and their underlying generating\n",
      "processes, it is hard to characterize and label them. Recent advances in deep\n",
      "generative-based models have sparked interest in applying such methods for\n",
      "unsupervised anomaly detection and have shown promising results in medical and\n",
      "industrial inspection domains. In this work we evaluate a crucial part of the\n",
      "unsupervised visual anomaly detection pipeline, that is needed for normal\n",
      "appearance modeling, as well as the ability to reconstruct closest looking\n",
      "normal and tumor samples. We adapt and evaluate different high-resolution\n",
      "state-of-the-art generative models from the face synthesis domain and\n",
      "demonstrate their superiority over currently used approaches on a challenging\n",
      "domain of digital pathology. Multifold improvement in image synthesis is\n",
      "demonstrated in terms of the quality and resolution of the generated images,\n",
      "validated also against the supervised model. \n",
      "\n",
      "\n",
      "In Fluorescein Angiography (FA), an exogenous dye is injected in the\n",
      "bloodstream to image the vascular structure of the retina. The injected dye can\n",
      "cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\n",
      "death. In contrast, color fundus imaging is a non-invasive technique used for\n",
      "photographing the retina but does not have sufficient fidelity for capturing\n",
      "its vascular structure. The only non-invasive method for capturing retinal\n",
      "vasculature is optical coherence tomography-angiography (OCTA). However, OCTA\n",
      "equipment is quite expensive, and stable imaging is limited to small areas on\n",
      "the retina. In this paper, we propose a novel conditional generative\n",
      "adversarial network (GAN) capable of simultaneously synthesizing FA images from\n",
      "fundus photographs while predicting retinal degeneration. The proposed system\n",
      "has the benefit of addressing the problem of imaging retinal vasculature in a\n",
      "non-invasive manner as well as predicting the existence of retinal\n",
      "abnormalities. We use a semi-supervised approach to train our GAN using\n",
      "multiple weighted losses on different modalities of data. Our experiments\n",
      "validate that the proposed architecture exceeds recent state-of-the-art\n",
      "generative networks for fundus-to-angiography synthesis. Moreover, our vision\n",
      "transformer-based discriminators generalize quite well on out-of-distribution\n",
      "data sets for retinal disease prediction. \n",
      "\n",
      "\n",
      "Photoacoustic tomography (PAT) has the potential to recover morphological and\n",
      "functional tissue properties such as blood oxygenation with high spatial\n",
      "resolution and in an interventional setting. However, decades of research\n",
      "invested in solving the inverse problem of recovering clinically relevant\n",
      "tissue properties from spectral measurements have failed to produce solutions\n",
      "that can quantify tissue parameters robustly in a clinical setting. Previous\n",
      "attempts to address the limitations of model-based approaches with machine\n",
      "learning were hampered by the absence of labeled reference data needed for\n",
      "supervised algorithm training. While this bottleneck has been tackled by\n",
      "simulating training data, the domain gap between real and simulated images\n",
      "remains a huge unsolved challenge. As a first step to address this bottleneck,\n",
      "we propose a novel approach to PAT data simulation, which we refer to as\n",
      "\"learning to simulate\". Our approach involves subdividing the challenge of\n",
      "generating plausible simulations into two disjoint problems: (1) Probabilistic\n",
      "generation of realistic tissue morphology, represented by semantic segmentation\n",
      "maps and (2) pixel-wise assignment of corresponding optical and acoustic\n",
      "properties. In the present work, we focus on the first challenge. Specifically,\n",
      "we leverage the concept of Generative Adversarial Networks (GANs) trained on\n",
      "semantically annotated medical imaging data to generate plausible tissue\n",
      "geometries. According to an initial in silico feasibility study our approach is\n",
      "well-suited for contributing to realistic PAT image synthesis and could thus\n",
      "become a fundamental step for deep learning-based quantitative PAT. \n",
      "\n",
      "\n",
      "A Magnetic Resonance Imaging (MRI) exam typically consists of the acquisition\n",
      "of multiple MR pulse sequences, which are required for a reliable diagnosis.\n",
      "Each sequence can be parameterized through multiple acquisition parameters\n",
      "affecting MR image contrast, signal-to-noise ratio, resolution, or scan time.\n",
      "With the rise of generative deep learning models, approaches for the synthesis\n",
      "of MR images are developed to either synthesize additional MR contrasts,\n",
      "generate synthetic data, or augment existing data for AI training. However,\n",
      "current generative approaches for the synthesis of MR images are only trained\n",
      "on images with a specific set of acquisition parameter values, limiting the\n",
      "clinical value of these methods as various sets of acquisition parameter\n",
      "settings are used in clinical practice. Therefore, we trained a generative\n",
      "adversarial network (GAN) to generate synthetic MR knee images conditioned on\n",
      "various acquisition parameters (repetition time, echo time, image orientation).\n",
      "This approach enables us to synthesize MR images with adjustable image\n",
      "contrast. In a visual Turing test, two experts mislabeled 40.5% of real and\n",
      "synthetic MR images, demonstrating that the image quality of the generated\n",
      "synthetic and real MR images is comparable. This work can support radiologists\n",
      "and technologists during the parameterization of MR sequences by previewing the\n",
      "yielded MR contrast, can serve as a valuable tool for radiology training, and\n",
      "can be used for customized data generation to support AI training. \n",
      "\n",
      "\n",
      "Medical image segmentation is routinely performed to isolate regions of\n",
      "interest, such as organs and lesions. Currently, deep learning is the state of\n",
      "the art for automatic segmentation, but is usually limited by the need for\n",
      "supervised training with large datasets that have been manually segmented by\n",
      "trained clinicians. The goal of semi-superised and unsupervised image\n",
      "segmentation is to greatly reduce, or even eliminate, the need for training\n",
      "data and therefore to minimze the burden on clinicians when training\n",
      "segmentation models. To this end we introduce a novel network architecture for\n",
      "capable of unsupervised and semi-supervised image segmentation called\n",
      "TricycleGAN. This approach uses three generative models to learn translations\n",
      "between medical images and segmentation maps using edge maps as an intermediate\n",
      "step. Distinct from other approaches based on generative networks, TricycleGAN\n",
      "relies on shape priors rather than colour and texture priors. As such, it is\n",
      "particularly well-suited for several domains of medical imaging, such as\n",
      "ultrasound imaging, where commonly used visual cues may be absent. We present\n",
      "experiments with TricycleGAN on a clinical dataset of kidney ultrasound images\n",
      "and the benchmark ISIC 2018 skin lesion dataset. \n",
      "\n",
      "\n",
      "The use of fundus images for the early screening of eye diseases is of great\n",
      "clinical importance. Due to its powerful performance, deep learning is becoming\n",
      "more and more popular in related applications, such as lesion segmentation,\n",
      "biomarkers segmentation, disease diagnosis and image synthesis. Therefore, it\n",
      "is very necessary to summarize the recent developments in deep learning for\n",
      "fundus images with a review paper. In this review, we introduce 143 application\n",
      "papers with a carefully designed hierarchy. Moreover, 33 publicly available\n",
      "datasets are presented. Summaries and analyses are provided for each task.\n",
      "Finally, limitations common to all tasks are revealed and possible solutions\n",
      "are given. We will also release and regularly update the state-of-the-art\n",
      "results and newly-released datasets at https://github.com/nkicsl/Fundus Review\n",
      "to adapt to the rapid development of this field. \n",
      "\n",
      "\n",
      "We propose a novel framework for controllable pathological image synthesis\n",
      "for data augmentation. Inspired by CycleGAN, we perform cycle-consistent\n",
      "image-to-image translation between two domains: healthy and pathological.\n",
      "Guided by a semantic mask, an adversarially trained generator synthesizes\n",
      "pathology on a healthy image in the specified location. We demonstrate our\n",
      "approach on an institutional dataset of cerebral microbleeds in traumatic brain\n",
      "injury patients. We utilize synthetic images generated with our method for data\n",
      "augmentation in cerebral microbleeds detection. Enriching the training dataset\n",
      "with synthetic images exhibits the potential to increase detection performance\n",
      "for cerebral microbleeds in traumatic brain injury patients. \n",
      "\n",
      "\n",
      "Tagged magnetic resonance imaging (MRI) is a widely used imaging technique\n",
      "for measuring tissue deformation in moving organs. Due to tagged MRI's\n",
      "intrinsic low anatomical resolution, another matching set of cine MRI with\n",
      "higher resolution is sometimes acquired in the same scanning session to\n",
      "facilitate tissue segmentation, thus adding extra time and cost. To mitigate\n",
      "this, in this work, we propose a novel dual-cycle constrained bijective VAE-GAN\n",
      "approach to carry out tagged-to-cine MR image synthesis. Our method is based on\n",
      "a variational autoencoder backbone with cycle reconstruction constrained\n",
      "adversarial training to yield accurate and realistic cine MR images given\n",
      "tagged MR images. Our framework has been trained, validated, and tested using\n",
      "1,768, 416, and 1,560 subject-independent paired slices of tagged and cine MRI\n",
      "from twenty healthy subjects, respectively, demonstrating superior performance\n",
      "over the comparison methods. Our method can potentially be used to reduce the\n",
      "extra acquisition time and cost, while maintaining the same workflow for\n",
      "further motion analyses. \n",
      "\n",
      "\n",
      "While Positron emission tomography (PET) imaging has been widely used in\n",
      "diagnosis of number of diseases, it has costly acquisition process which\n",
      "involves radiation exposure to patients. However, magnetic resonance imaging\n",
      "(MRI) is a safer imaging modality that does not involve patient's exposure to\n",
      "radiation. Therefore, a need exists for an efficient and automated PET image\n",
      "generation from MRI data. In this paper, we propose a new frequency-aware\n",
      "attention U-net for generating synthetic PET images. Specifically, we\n",
      "incorporate attention mechanism into different U-net layers responsible for\n",
      "estimating low/high frequency scales of the image. Our frequency-aware\n",
      "attention Unet computes the attention scores for feature maps in low/high\n",
      "frequency layers and use it to help the model focus more on the most important\n",
      "regions, leading to more realistic output images. Experimental results on 30\n",
      "subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset\n",
      "demonstrate good performance of the proposed model in PET image synthesis that\n",
      "achieved superior performance, both qualitative and quantitative, over current\n",
      "state-of-the-arts. \n",
      "\n",
      "\n",
      "Mainstream deep models for three-dimensional MRI synthesis are either\n",
      "cross-sectional or volumetric depending on the input. Cross-sectional models\n",
      "can decrease the model complexity, but they may lead to discontinuity\n",
      "artifacts. On the other hand, volumetric models can alleviate the discontinuity\n",
      "artifacts, but they might suffer from loss of spatial resolution due to\n",
      "increased model complexity coupled with scarce training data. To mitigate the\n",
      "limitations of both approaches, we propose a novel model that progressively\n",
      "recovers the target volume via simpler synthesis tasks across individual\n",
      "orientations. \n",
      "\n",
      "\n",
      "Image synthesis from corrupted contrasts increases the diversity of\n",
      "diagnostic information available for many neurological diseases. Recently the\n",
      "image-to-image translation has experienced significant levels of interest\n",
      "within medical research, beginning with the successful use of the Generative\n",
      "Adversarial Network (GAN) to the introduction of cyclic constraint extended to\n",
      "multiple domains. However, in current approaches, there is no guarantee that\n",
      "the mapping between the two image domains would be unique or one-to-one. In\n",
      "this paper, we introduce a novel approach to unpaired image-to-image\n",
      "translation based on the invertible architecture. The invertible property of\n",
      "the flow-based architecture assures a cycle-consistency of image-to-image\n",
      "translation without additional loss functions. We utilize the temporal\n",
      "information between consecutive slices to provide more constraints to the\n",
      "optimization for transforming one domain to another in unpaired volumetric\n",
      "medical images. To capture temporal structures in the medical images, we\n",
      "explore the displacement between the consecutive slices using a deformation\n",
      "field. In our approach, the deformation field is used as a guidance to keep the\n",
      "translated slides realistic and consistent across the translation. The\n",
      "experimental results have shown that the synthesized images using our proposed\n",
      "approach are able to archive a competitive performance in terms of mean squared\n",
      "error, peak signal-to-noise ratio, and structural similarity index when\n",
      "compared with the existing deep learning-based methods on three standard\n",
      "datasets, i.e. HCP, MRBrainS13, and Brats2019. \n",
      "\n",
      "\n",
      "Microscopic images from multiple modalities can produce plentiful\n",
      "experimental information. In practice, biological or physical constraints under\n",
      "a given observation period may prevent researchers from acquiring enough\n",
      "microscopic scanning. Recent studies demonstrate that image synthesis is one of\n",
      "the popular approaches to release such constraints. Nonetheless, most existing\n",
      "synthesis approaches only translate images from the source domain to the target\n",
      "domain without solid geometric associations. To embrace this challenge, we\n",
      "propose an innovative model architecture, BANIS, to synthesize diversified\n",
      "microscopic images from multi-source domains with distinct geometric features.\n",
      "The experimental outcomes indicate that BANIS successfully synthesizes\n",
      "favorable image pairs on C. elegans microscopy embryonic images. To the best of\n",
      "our knowledge, BANIS is the first application to synthesize microscopic images\n",
      "that associate distinct spatial geometric features from multi-source domains. \n",
      "\n",
      "\n",
      "Automatic analysis of spatio-temporal microscopy images is inevitable for\n",
      "state-of-the-art research in the life sciences. Recent developments in deep\n",
      "learning provide powerful tools for automatic analyses of such image data, but\n",
      "heavily depend on the amount and quality of provided training data to perform\n",
      "well. To this end, we developed a new method for realistic generation of\n",
      "synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei.\n",
      "The method combines spatiotemporal statistical shape models of different cell\n",
      "cycle stages with a conditional GAN to generate time series of cell populations\n",
      "and provides instance-level control of cell cycle stage and the fluorescence\n",
      "intensity of generated cells. We show the effect of the GAN conditioning and\n",
      "create a set of synthetic images that can be readily used for training and\n",
      "benchmarking of cell segmentation and tracking approaches. \n",
      "\n",
      "\n",
      "Motivated by the lack of publicly available datasets of chest radiographs of\n",
      "positive patients with Coronavirus disease 2019 (COVID-19), we build the\n",
      "first-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high\n",
      "fidelity using an unsupervised domain adaptation approach by leveraging class\n",
      "conditioning and adversarial training. Our contributions are twofold. First, we\n",
      "show considerable performance improvements on COVID-19 detection using various\n",
      "deep learning architectures when employing synthetic images as additional\n",
      "training set. Second, we show how our image synthesis method can serve as a\n",
      "data anonymization tool by achieving comparable detection performance when\n",
      "trained only on synthetic data. In addition, the proposed data generation\n",
      "framework offers a viable solution to the COVID-19 detection in particular, and\n",
      "to medical image classification tasks in general. Our publicly available\n",
      "benchmark dataset consists of 21,295 synthetic COVID-19 chest X-ray images. The\n",
      "insights gleaned from this dataset can be used for preventive actions in the\n",
      "fight against the COVID-19 pandemic. \n",
      "\n",
      "\n",
      "Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the\n",
      "developing brain but is not suitable for second-trimester anomaly screening,\n",
      "for which ultrasound (US) is employed. Although expert sonographers are adept\n",
      "at reading US images, MR images which closely resemble anatomical images are\n",
      "much easier for non-experts to interpret. Thus in this paper we propose to\n",
      "generate MR-like images directly from clinical US images. In medical image\n",
      "analysis such a capability is potentially useful as well, for instance for\n",
      "automatic US-MRI registration and fusion. The proposed model is end-to-end\n",
      "trainable and self-supervised without any external annotations. Specifically,\n",
      "based on an assumption that the US and MRI data share a similar anatomical\n",
      "latent space, we first utilise a network to extract the shared latent features,\n",
      "which are then used for MRI synthesis. Since paired data is unavailable for our\n",
      "study (and rare in practice), pixel-level constraints are infeasible to apply.\n",
      "We instead propose to enforce the distributions to be statistically\n",
      "indistinguishable, by adversarial learning in both the image domain and feature\n",
      "space. To regularise the anatomical structures between US and MRI during\n",
      "synthesis, we further propose an adversarial structural constraint. A new\n",
      "cross-modal attention technique is proposed to utilise non-local spatial\n",
      "information, by encouraging multi-modal knowledge fusion and propagation. We\n",
      "extend the approach to consider the case where 3D auxiliary information (e.g.,\n",
      "3D neighbours and a 3D location index) from volumetric data is also available,\n",
      "and show that this improves image synthesis. The proposed approach is evaluated\n",
      "quantitatively and qualitatively with comparison to real fetal MR images and\n",
      "other approaches to synthesis, demonstrating its feasibility of synthesising\n",
      "realistic MR images. \n",
      "\n",
      "\n",
      "Deep learning motivated by convolutional neural networks has been highly\n",
      "successful in a range of medical imaging problems like image classification,\n",
      "image segmentation, image synthesis etc. However for validation and\n",
      "interpretability, not only do we need the predictions made by the model but\n",
      "also how confident it is while making those predictions. This is important in\n",
      "safety critical applications for the people to accept it. In this work, we used\n",
      "an encoder decoder architecture based on variational inference techniques for\n",
      "segmenting brain tumour images. We evaluate our work on the publicly available\n",
      "BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\n",
      "Union (IOU) as the evaluation metrics. Our model is able to segment brain\n",
      "tumours while taking into account both aleatoric uncertainty and epistemic\n",
      "uncertainty in a principled bayesian manner. \n",
      "\n",
      "\n",
      "The success of supervised lesion segmentation algorithms using Computed\n",
      "Tomography (CT) exams depends significantly on the quantity and variability of\n",
      "samples available for training. While annotating such data constitutes a\n",
      "challenge itself, the variability of lesions in the dataset also depends on the\n",
      "prevalence of different types of lesions. This phenomenon adds an inherent bias\n",
      "to lesion segmentation algorithms that can be diminished, among different\n",
      "possibilities, using aggressive data augmentation methods. In this paper, we\n",
      "present a method for implanting realistic lesions in CT slices to provide a\n",
      "rich and controllable set of training samples and ultimately improving semantic\n",
      "segmentation network performances for delineating lesions in CT exams. Our\n",
      "results show that implanting synthetic lesions not only improves (up to around\n",
      "12\\%) the segmentation performance considering different architectures but also\n",
      "that this improvement is consistent among different image synthesis networks.\n",
      "We conclude that increasing the variability of lesions synthetically in terms\n",
      "of size, density, shape, and position seems to improve the performance of\n",
      "segmentation models for liver lesion segmentation in CT slices. \n",
      "\n",
      "\n",
      "Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has\n",
      "spread rapidly since December 2019. Real-time reverse transcription polymerase\n",
      "chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play\n",
      "an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits\n",
      "of quick reporting, a low cost, and high sensitivity for the detection of\n",
      "pulmonary infection. Recently, deep-learning-based computer vision methods have\n",
      "demonstrated great promise for use in medical imaging applications, including\n",
      "X-rays, magnetic resonance imaging, and CT imaging. However, training a\n",
      "deep-learning model requires large volumes of data, and medical staff faces a\n",
      "high risk when collecting COVID-19 CT data due to the high infectivity of the\n",
      "disease. Another issue is the lack of experts available for data labeling. In\n",
      "order to meet the data requirements for COVID-19 CT imaging, we propose a CT\n",
      "image synthesis approach based on a conditional generative adversarial network\n",
      "that can effectively generate high-quality and realistic COVID-19 CT images for\n",
      "use in deep-learning-based medical imaging tasks. Experimental results show\n",
      "that the proposed method outperforms other state-of-the-art image synthesis\n",
      "methods with the generated COVID-19 CT images and indicates promising for\n",
      "various machine learning applications including semantic segmentation and\n",
      "classification. \n",
      "\n",
      "\n",
      "Generative adversarial networks (GANs) have provided promising data\n",
      "enrichment solutions by synthesizing high-fidelity images. However, generating\n",
      "large sets of labeled images with new anatomical variations remains unexplored.\n",
      "We propose a novel method for synthesizing cardiac magnetic resonance (CMR)\n",
      "images on a population of virtual subjects with a large anatomical variation,\n",
      "introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human\n",
      "phantom. We investigate two conditional image synthesis approaches grounded on\n",
      "a semantically-consistent mask-guided image generation technique: 4-class and\n",
      "8-class XCAT-GANs. The 4-class technique relies on only the annotations of the\n",
      "heart; while the 8-class technique employs a predicted multi-tissue label map\n",
      "of the heart-surrounding organs and provides better guidance for our\n",
      "conditional image synthesis. For both techniques, we train our conditional\n",
      "XCAT-GAN with real images paired with corresponding labels and subsequently at\n",
      "the inference time, we substitute the labels with the XCAT derived ones.\n",
      "Therefore, the trained network accurately transfers the tissue-specific\n",
      "textures to the new label maps. By creating 33 virtual subjects of synthetic\n",
      "CMR images at the end-diastolic and end-systolic phases, we evaluate the\n",
      "usefulness of such data in the downstream cardiac cavity segmentation task\n",
      "under different augmentation strategies. Results demonstrate that even with\n",
      "only 20% of real images (40 volumes) seen during training, segmentation\n",
      "performance is retained with the addition of synthetic CMR images. Moreover,\n",
      "the improvement in utilizing synthetic images for augmenting the real data is\n",
      "evident through the reduction of Hausdorff distance up to 28% and an increase\n",
      "in the Dice score up to 5%, indicating a higher similarity to the ground truth\n",
      "in all dimensions. \n",
      "\n",
      "\n",
      "Ischemic stroke lesion segmentation from Computed Tomography Perfusion (CTP)\n",
      "images is important for accurate diagnosis of stroke in acute care units.\n",
      "However, it is challenged by low image contrast and resolution of the perfusion\n",
      "parameter maps, in addition to the complex appearance of the lesion. To deal\n",
      "with this problem, we propose a novel framework based on synthesized pseudo\n",
      "Diffusion-Weighted Imaging (DWI) from perfusion parameter maps to obtain better\n",
      "image quality for more accurate segmentation. Our framework consists of three\n",
      "components based on Convolutional Neural Networks (CNNs) and is trained\n",
      "end-to-end. First, a feature extractor is used to obtain both a low-level and\n",
      "high-level compact representation of the raw spatiotemporal Computed Tomography\n",
      "Angiography (CTA) images. Second, a pseudo DWI generator takes as input the\n",
      "concatenation of CTP perfusion parameter maps and our extracted features to\n",
      "obtain the synthesized pseudo DWI. To achieve better synthesis quality, we\n",
      "propose a hybrid loss function that pays more attention to lesion regions and\n",
      "encourages high-level contextual consistency. Finally, we segment the lesion\n",
      "region from the synthesized pseudo DWI, where the segmentation network is based\n",
      "on switchable normalization and channel calibration for better performance.\n",
      "Experimental results showed that our framework achieved the top performance on\n",
      "ISLES 2018 challenge and: 1) our method using synthesized pseudo DWI\n",
      "outperformed methods segmenting the lesion from perfusion parameter maps\n",
      "directly; 2) the feature extractor exploiting additional spatiotemporal CTA\n",
      "images led to better synthesized pseudo DWI quality and higher segmentation\n",
      "accuracy; and 3) the proposed loss functions and network structure improved the\n",
      "pseudo DWI synthesis and lesion segmentation performance. \n",
      "\n",
      "\n",
      "Recently, high dynamic range (HDR) image reconstruction based on the multiple\n",
      "exposure stack from a given single exposure utilizes a deep learning framework\n",
      "to generate high-quality HDR images. These conventional networks focus on the\n",
      "exposure transfer task to reconstruct the multi-exposure stack. Therefore, they\n",
      "often fail to fuse the multi-exposure stack into a perceptually pleasant HDR\n",
      "image as the inversion artifacts occur. We tackle the problem in stack\n",
      "reconstruction-based methods by proposing a novel framework with a fully\n",
      "differentiable high dynamic range imaging (HDRI) process. By explicitly using\n",
      "the loss, which compares the network's output with the ground truth HDR image,\n",
      "our framework enables a neural network that generates the multiple exposure\n",
      "stack for HDRI to train stably. In other words, our differentiable HDR\n",
      "synthesis layer helps the deep neural network to train to create multi-exposure\n",
      "stacks while reflecting the precise correlations between multi-exposure images\n",
      "in the HDRI process. In addition, our network uses the image decomposition and\n",
      "the recursive process to facilitate the exposure transfer task and to\n",
      "adaptively respond to recursion frequency. The experimental results show that\n",
      "the proposed network outperforms the state-of-the-art quantitative and\n",
      "qualitative results in terms of both the exposure transfer tasks and the whole\n",
      "HDRI process. \n",
      "\n",
      "\n",
      "Magnetic Resonance (MR) Imaging and Computed Tomography (CT) are the primary\n",
      "diagnostic imaging modalities quite frequently used for surgical planning and\n",
      "analysis. A general problem with medical imaging is that the acquisition\n",
      "process is quite expensive and time-consuming. Deep learning techniques like\n",
      "generative adversarial networks (GANs) can help us to leverage the possibility\n",
      "of an image to image translation between multiple imaging modalities, which in\n",
      "turn helps in saving time and cost. These techniques will help to conduct\n",
      "surgical planning under CT with the feedback of MRI information. While previous\n",
      "studies have shown paired and unpaired image synthesis from MR to CT, image\n",
      "synthesis from CT to MR still remains a challenge, since it involves the\n",
      "addition of extra tissue information. In this manuscript, we have implemented\n",
      "two different variations of Generative Adversarial Networks exploiting the\n",
      "cycling consistency and structural similarity between both CT and MR image\n",
      "modalities on a pelvis dataset, thus facilitating a bidirectional exchange of\n",
      "content and style between these image modalities. The proposed GANs translate\n",
      "the input medical images by different mechanisms, and hence generated images\n",
      "not only appears realistic but also performs well across various comparison\n",
      "metrics, and these images have also been cross verified with a radiologist. The\n",
      "radiologist verification has shown that slight variations in generated MR and\n",
      "CT images may not be exactly the same as their true counterpart but it can be\n",
      "used for medical purposes. \n",
      "\n",
      "\n",
      "As a powerful technique in medical imaging, image synthesis is widely used in\n",
      "applications such as denoising, super resolution and modality transformation\n",
      "etc. Recently, the revival of deep neural networks made immense progress in the\n",
      "field of medical imaging. Although many deep leaning based models have been\n",
      "proposed to improve the image synthesis accuracy, the evaluation of the model\n",
      "uncertainty, which is highly important for medical applications, has been a\n",
      "missing part. In this work, we propose to use Bayesian conditional generative\n",
      "adversarial network (GAN) with concrete dropout to improve image synthesis\n",
      "accuracy. Meanwhile, an uncertainty calibration approach is involved in the\n",
      "whole pipeline to make the uncertainty generated by Bayesian network\n",
      "interpretable. The method is validated with the T1w to T2w MR image translation\n",
      "with a brain tumor dataset of 102 subjects. Compared with the conventional\n",
      "Bayesian neural network with Monte Carlo dropout, results of the proposed\n",
      "method reach a significant lower RMSE with a p-value of 0.0186. Improvement of\n",
      "the calibration of the generated uncertainty by the uncertainty recalibration\n",
      "method is also illustrated. \n",
      "\n",
      "\n",
      "Exploiting learning algorithms under scarce data regimes is a limitation and\n",
      "a reality of the medical imaging field. In an attempt to mitigate the problem,\n",
      "we propose a data augmentation protocol based on generative adversarial\n",
      "networks. We condition the networks at a pixel-level (segmentation mask) and at\n",
      "a global-level information (acquisition environment or lesion type). Such\n",
      "conditioning provides immediate access to the image-label pairs while\n",
      "controlling global class specific appearance of the synthesized images. To\n",
      "stimulate synthesis of the features relevant for the segmentation task, an\n",
      "additional passive player in a form of segmentor is introduced into the\n",
      "adversarial game. We validate the approach on two medical datasets: BraTS,\n",
      "ISIC. By controlling the class distribution through injection of synthetic\n",
      "images into the training set we achieve control over the accuracy levels of the\n",
      "datasets' classes. \n",
      "\n",
      "\n",
      "Skin lesion datasets consist predominantly of normal samples with only a\n",
      "small percentage of abnormal ones, giving rise to the class imbalance problem.\n",
      "Also, skin lesion images are largely similar in overall appearance owing to the\n",
      "low inter-class variability. In this paper, we propose a two-stage framework\n",
      "for automatic classification of skin lesion images using adversarial training\n",
      "and transfer learning toward melanoma detection. In the first stage, we\n",
      "leverage the inter-class variation of the data distribution for the task of\n",
      "conditional image synthesis by learning the inter-class mapping and\n",
      "synthesizing under-represented class samples from the over-represented ones\n",
      "using unpaired image-to-image translation. In the second stage, we train a deep\n",
      "convolutional neural network for skin lesion classification using the original\n",
      "training set combined with the newly synthesized under-represented class\n",
      "samples. The training of this classifier is carried out by minimizing the focal\n",
      "loss function, which assists the model in learning from hard examples, while\n",
      "down-weighting the easy ones. Experiments conducted on a dermatology image\n",
      "benchmark demonstrate the superiority of our proposed approach over several\n",
      "standard baseline methods, achieving significant performance improvements.\n",
      "Interestingly, we show through feature visualization and analysis that our\n",
      "method leads to context based lesion assessment that can reach an expert\n",
      "dermatologist level. \n",
      "\n",
      "\n",
      "Ultrasound (US) is widely accepted in clinic for anatomical structure\n",
      "inspection. However, lacking in resources to practice US scan, novices often\n",
      "struggle to learn the operation skills. Also, in the deep learning era,\n",
      "automated US image analysis is limited by the lack of annotated samples.\n",
      "Efficiently synthesizing realistic, editable and high resolution US images can\n",
      "solve the problems. The task is challenging and previous methods can only\n",
      "partially complete it. In this paper, we devise a new framework for US image\n",
      "synthesis. Particularly, we firstly adopt a sketch generative adversarial\n",
      "networks (Sgan) to introduce background sketch upon object mask in a\n",
      "conditioned generative adversarial network. With enriched sketch cues, Sgan can\n",
      "generate realistic US images with editable and fine-grained structure details.\n",
      "Although effective, Sgan is hard to generate high resolution US images. To\n",
      "achieve this, we further implant the Sgan into a progressive growing scheme\n",
      "(PGSgan). By smoothly growing both generator and discriminator, PGSgan can\n",
      "gradually synthesize US images from low to high resolution. By synthesizing\n",
      "ovary and follicle US images, our extensive perceptual evaluation, user study\n",
      "and segmentation results prove the promising efficacy and efficiency of the\n",
      "proposed PGSgan. \n",
      "\n",
      "\n",
      "Medical image segmentation is an important task for computer aided diagnosis.\n",
      "Pixelwise manual annotations of large datasets require high expertise and is\n",
      "time consuming. Conventional data augmentations have limited benefit by not\n",
      "fully representing the underlying distribution of the training set, thus\n",
      "affecting model robustness when tested on images captured from different\n",
      "sources. Prior work leverages synthetic images for data augmentation ignoring\n",
      "the interleaved geometric relationship between different anatomical labels. We\n",
      "propose improvements over previous GAN-based medical image synthesis methods by\n",
      "jointly encoding the intrinsic relationship of geometry and shape. Latent space\n",
      "variable sampling results in diverse generated images from a base image and\n",
      "improves robustness. Given those augmented images generated by our method, we\n",
      "train the segmentation network to enhance the segmentation performance of\n",
      "retinal optical coherence tomography (OCT) images. The proposed method\n",
      "outperforms state-of-the-art segmentation methods on the public RETOUCH dataset\n",
      "having images captured from different acquisition procedures. Ablation studies\n",
      "and visual analysis also demonstrate benefits of integrating geometry and\n",
      "diversity. \n",
      "\n",
      "\n",
      "We propose a hybrid controllable image generation method to synthesize\n",
      "anatomically meaningful 3D+t labeled Cardiac Magnetic Resonance (CMR) images.\n",
      "Our hybrid method takes the mechanistic 4D eXtended CArdiac Torso (XCAT) heart\n",
      "model as the anatomical ground truth and synthesizes CMR images via a\n",
      "data-driven Generative Adversarial Network (GAN). We employ the\n",
      "state-of-the-art SPatially Adaptive De-normalization (SPADE) technique for\n",
      "conditional image synthesis to preserve the semantic spatial information of\n",
      "ground truth anatomy. Using the parameterized motion model of the XCAT heart,\n",
      "we generate labels for 25 time frames of the heart for one cardiac cycle at 18\n",
      "locations for the short axis view. Subsequently, realistic images are generated\n",
      "from these labels, with modality-specific features that are learned from real\n",
      "CMR image data. We demonstrate that style transfer from another cardiac image\n",
      "can be accomplished by using a style encoder network. Due to the flexibility of\n",
      "XCAT in creating new heart models, this approach can result in a realistic\n",
      "virtual population to address different challenges the medical image analysis\n",
      "research community is facing such as expensive data collection. Our proposed\n",
      "method has a great potential to synthesize 4D controllable CMR images with\n",
      "annotations and adaptable styles to be used in various supervised multi-site,\n",
      "multi-vendor applications in medical image analysis. \n",
      "\n",
      "\n",
      "Generative Adversarial Networks (GANs) have proven successful for\n",
      "unsupervised image generation. Several works have extended GANs to image\n",
      "inpainting by conditioning the generation with parts of the image to be\n",
      "reconstructed. Despite their success, these methods have limitations in\n",
      "settings where only a small subset of the image pixels is known beforehand. In\n",
      "this paper we investigate the effectiveness of conditioning GANs when very few\n",
      "pixel values are provided. We propose a modelling framework which results in\n",
      "adding an explicit cost term to the GAN objective function to enforce\n",
      "pixel-wise conditioning. We investigate the influence of this regularization\n",
      "term on the quality of the generated images and the fulfillment of the given\n",
      "pixel constraints. Using the recent PacGAN technique, we ensure that we keep\n",
      "diversity in the generated samples. Conducted experiments on FashionMNIST show\n",
      "that the regularization term effectively controls the trade-off between quality\n",
      "of the generated images and the conditioning. Experimental evaluation on the\n",
      "CIFAR-10 and CelebA datasets evidences that our method achieves accurate\n",
      "results both visually and quantitatively in term of Fr\\'echet Inception\n",
      "Distance, while still enforcing the pixel conditioning. We also evaluate our\n",
      "method on a texture image generation task using fully-convolutional networks.\n",
      "As a final contribution, we apply the method to a classical geological\n",
      "simulation application. \n",
      "\n",
      "\n",
      "Automatic segmentation of anatomical landmarks from ultrasound (US) plays an\n",
      "important role in the management of preterm neonates with a very low birth\n",
      "weight due to the increased risk of developing intraventricular hemorrhage\n",
      "(IVH) or other complications. One major problem in developing an automatic\n",
      "segmentation method for this task is the limited availability of annotated\n",
      "data. To tackle this issue, we propose a novel image synthesis method using\n",
      "multi-scale self attention generator to synthesize US images from various\n",
      "segmentation masks. We show that our method can synthesize high-quality US\n",
      "images for every manipulated segmentation label with qualitative and\n",
      "quantitative improvements over the recent state-of-the-art synthesis methods.\n",
      "Furthermore, for the segmentation task, we propose a novel method, called\n",
      "Confidence-guided Brain Anatomy Segmentation (CBAS) network, where segmentation\n",
      "and corresponding confidence maps are estimated at different scales. In\n",
      "addition, we introduce a technique which guides CBAS to learn the weights based\n",
      "on the confidence measure about the estimate. Extensive experiments demonstrate\n",
      "that the proposed method for both synthesis and segmentation tasks achieve\n",
      "significant improvements over the recent state-of-the-art methods. In\n",
      "particular, we show that the new synthesis framework can be used to generate\n",
      "realistic US images which can be used to improve the performance of a\n",
      "segmentation algorithm. \n",
      "\n",
      "\n",
      "Medical image synthesis has gained a great focus recently, especially after\n",
      "the introduction of Generative Adversarial Networks (GANs). GANs have been used\n",
      "widely to provide anatomically-plausible and diverse samples for augmentation\n",
      "and other applications, including segmentation and super resolution. In our\n",
      "previous work, Deep Convolutional GANs were used to generate synthetic\n",
      "mammogram lesions, masses mainly, that could enhance the classification\n",
      "performance in imbalanced datasets. In this new work, a deeper investigation\n",
      "was carried out to explore other aspects of the generated images evaluation,\n",
      "i.e., realism, feature space distribution, and observers studies. t-Stochastic\n",
      "Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and\n",
      "fake images to enable 2D visualisations. Additionally, two expert radiologists\n",
      "performed a realism-evaluation study. Visualisations showed that the generated\n",
      "images have a similar feature distribution of the real ones, avoiding outliers.\n",
      "Moreover, Receiver Operating Characteristic (ROC) curve showed that the\n",
      "radiologists could not, in many cases, distinguish between synthetic and real\n",
      "lesions, giving 48% and 61% accuracies in a balanced sample set. \n",
      "\n",
      "\n",
      "Contrast enhancement and noise removal are coupled problems for low-light\n",
      "image enhancement. The existing Retinex based methods do not take the coupling\n",
      "relation into consideration, resulting in under or over-smoothing of the\n",
      "enhanced images. To address this issue, this paper presents a novel progressive\n",
      "Retinex framework, in which illumination and noise of low-light image are\n",
      "perceived in a mutually reinforced manner, leading to noise reduction low-light\n",
      "enhancement results. Specifically, two fully pointwise convolutional neural\n",
      "networks are devised to model the statistical regularities of ambient light and\n",
      "image noise respectively, and to leverage them as constraints to facilitate the\n",
      "mutual learning process. The proposed method not only suppresses the\n",
      "interference caused by the ambiguity between tiny textures and image noises,\n",
      "but also greatly improves the computational efficiency. Moreover, to solve the\n",
      "problem of insufficient training data, we propose an image synthesis strategy\n",
      "based on camera imaging model, which generates color images corrupted by\n",
      "illumination-dependent noises. Experimental results on both synthetic and real\n",
      "low-light images demonstrate the superiority of our proposed approaches against\n",
      "the State-Of-The-Art (SOTA) low-light enhancement methods. \n",
      "\n",
      "\n",
      "Microstructures of a material form the bridge linking processing conditions -\n",
      "which can be controlled, to the material property - which is the primary\n",
      "interest in engineering applications. Thus a critical task in material design\n",
      "is establishing the processing-structure relationship, which requires domain\n",
      "expertise and techniques that can model the high-dimensional material\n",
      "microstructure. This work proposes a deep learning based approach that models\n",
      "the processing-structure relationship as a conditional image synthesis problem.\n",
      "In particular, we develop an auxiliary classifier Wasserstein GAN with gradient\n",
      "penalty (ACWGAN-GP) to synthesize microstructures under a given processing\n",
      "condition. This approach is free of feature engineering, requires modest domain\n",
      "knowledge and is applicable to a wide range of material systems. We demonstrate\n",
      "this approach using the ultra high carbon steel (UHCS) database, where each\n",
      "microstructure is annotated with a label describing the cooling method it was\n",
      "subjected to. Our results show that ACWGAN-GP can synthesize high-quality\n",
      "multiphase microstructures for a given cooling method. \n",
      "\n",
      "\n",
      "Multi-contrast MRI protocols increase the level of morphological information\n",
      "available for diagnosis. Yet, the number and quality of contrasts is limited in\n",
      "practice by various factors including scan time and patient motion. Synthesis\n",
      "of missing or corrupted contrasts can alleviate this limitation to improve\n",
      "clinical utility. Common approaches for multi-contrast MRI involve either\n",
      "one-to-one and many-to-one synthesis methods. One-to-one methods take as input\n",
      "a single source contrast, and they learn a latent representation sensitive to\n",
      "unique features of the source. Meanwhile, many-to-one methods receive multiple\n",
      "distinct sources, and they learn a shared latent representation more sensitive\n",
      "to common features across sources. For enhanced image synthesis, here we\n",
      "propose a multi-stream approach that aggregates information across multiple\n",
      "source images via a mixture of multiple one-to-one streams and a joint\n",
      "many-to-one stream. The shared feature maps generated in the many-to-one stream\n",
      "and the complementary feature maps generated in the one-to-one streams are\n",
      "combined with a fusion block. The location of the fusion block is adaptively\n",
      "modified to maximize task-specific performance. Qualitative and quantitative\n",
      "assessments on T1-, T2-, PD-weighted and FLAIR images clearly demonstrate the\n",
      "superior performance of the proposed method compared to previous\n",
      "state-of-the-art one-to-one and many-to-one methods. \n",
      "\n",
      "\n",
      "Deep learning approaches based on convolutional neural networks (CNNs) have\n",
      "been successful in solving a number of problems in medical imaging, including\n",
      "image segmentation. In recent years, it has been shown that CNNs are vulnerable\n",
      "to attacks in which the input image is perturbed by relatively small amounts of\n",
      "noise so that the CNN is no longer able to perform a segmentation of the\n",
      "perturbed image with sufficient accuracy. Therefore, exploring methods on how\n",
      "to attack CNN-based models as well as how to defend models against attacks have\n",
      "become a popular topic as this also provides insights into the performance and\n",
      "generalization abilities of CNNs. However, most of the existing work assumes\n",
      "unrealistic attack models, i.e. the resulting attacks were specified in\n",
      "advance. In this paper, we propose a novel approach for generating adversarial\n",
      "examples to attack CNN-based segmentation models for medical images. Our\n",
      "approach has three key features: 1) The generated adversarial examples exhibit\n",
      "anatomical variations (in form of deformations) as well as appearance\n",
      "perturbations; 2) The adversarial examples attack segmentation models so that\n",
      "the Dice scores decrease by a pre-specified amount; 3) The attack is not\n",
      "required to be specified beforehand. We have evaluated our approach on\n",
      "CNN-based approaches for the multi-organ segmentation problem in 2D CT images.\n",
      "We show that the proposed approach can be used to attack different CNN-based\n",
      "segmentation models. \n",
      "\n",
      "\n",
      "Synthesis of high resolution images using Generative Adversarial Networks\n",
      "(GANs) is challenging, which usually requires numbers of high-end graphic cards\n",
      "with large memory and long time of training. In this paper, we propose a\n",
      "two-stage framework to accelerate the training process of synthesizing high\n",
      "resolution images. High resolution images are first transformed to small codes\n",
      "via the trained encoder and decoder networks. The code in latent space is times\n",
      "smaller than the original high resolution images. Then, we train a code\n",
      "generation network to learn the distribution of the latent codes. In this way,\n",
      "the generator only learns to generate small latent codes instead of large\n",
      "images. Finally, we decode the generated latent codes to image space via the\n",
      "decoder networks so as to output the synthesized high resolution images.\n",
      "Experimental results show that the proposed method accelerates the training\n",
      "process significantly and increases the quality of the generated samples. The\n",
      "proposed acceleration framework makes it possible to generate high resolution\n",
      "images using less training time with limited hardware resource. After using the\n",
      "proposed acceleration method, it takes only 3 days to train a 1024 *1024 image\n",
      "generator on Celeba-HQ dataset using just one NVIDIA P100 graphic card. \n",
      "\n",
      "\n",
      "Positron emission tomography (PET) imaging is an imaging modality for\n",
      "diagnosing a number of neurological diseases. In contrast to Magnetic Resonance\n",
      "Imaging (MRI), PET is costly and involves injecting a radioactive substance\n",
      "into the patient. Motivated by developments in modality transfer in vision, we\n",
      "study the generation of certain types of PET images from MRI data. We derive\n",
      "new flow-based generative models which we show perform well in this small\n",
      "sample size regime (much smaller than dataset sizes available in standard\n",
      "vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks\n",
      "and a relation network that maps the latent spaces to each other. We discuss\n",
      "how given the prior distribution, learning the conditional distribution of PET\n",
      "given the MRI image reduces to obtaining the conditional distribution between\n",
      "the two latent codes w.r.t. the two image types. We also extend our framework\n",
      "to leverage 'side' information (or attributes) when available. By controlling\n",
      "the PET generation through 'conditioning' on age, our model is also able to\n",
      "capture brain FDG-PET (hypometabolism) changes, as a function of age. We\n",
      "present experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI)\n",
      "dataset with 826 subjects, and obtain good performance in PET image synthesis,\n",
      "qualitatively and quantitatively better than recent works. \n",
      "\n",
      "\n",
      "We propose InSituNet, a deep learning based surrogate model to support\n",
      "parameter space exploration for ensemble simulations that are visualized in\n",
      "situ. In situ visualization, generating visualizations at simulation time, is\n",
      "becoming prevalent in handling large-scale simulations because of the I/O and\n",
      "storage constraints. However, in situ visualization approaches limit the\n",
      "flexibility of post-hoc exploration because the raw simulation data are no\n",
      "longer available. Although multiple image-based approaches have been proposed\n",
      "to mitigate this limitation, those approaches lack the ability to explore the\n",
      "simulation parameters. Our approach allows flexible exploration of parameter\n",
      "space for large-scale ensemble simulations by taking advantage of the recent\n",
      "advances in deep learning. Specifically, we design InSituNet as a convolutional\n",
      "regression model to learn the mapping from the simulation and visualization\n",
      "parameters to the visualization results. With the trained model, users can\n",
      "generate new images for different simulation parameters under various\n",
      "visualization settings, which enables in-depth analysis of the underlying\n",
      "ensemble simulations. We demonstrate the effectiveness of InSituNet in\n",
      "combustion, cosmology, and ocean simulations through quantitative and\n",
      "qualitative evaluations. \n",
      "\n",
      "\n",
      "Deep learning (DL) has shown great potential in medical image enhancement\n",
      "problems, such as super-resolution or image synthesis. However, to date, little\n",
      "consideration has been given to uncertainty quantification over the output\n",
      "image. Here we introduce methods to characterise different components of\n",
      "uncertainty in such problems and demonstrate the ideas using diffusion MRI\n",
      "super-resolution. Specifically, we propose to account for $intrinsic$\n",
      "uncertainty through a heteroscedastic noise model and for $parameter$\n",
      "uncertainty through approximate Bayesian inference, and integrate the two to\n",
      "quantify $predictive$ uncertainty over the output image. Moreover, we introduce\n",
      "a method to propagate the predictive uncertainty on a multi-channelled image to\n",
      "derived scalar parameters, and separately quantify the effects of intrinsic and\n",
      "parameter uncertainty therein. The methods are evaluated for super-resolution\n",
      "of two different signal representations of diffusion MR images---DTIs and Mean\n",
      "Apparent Propagator MRI---and their derived quantities such as MD and FA, on\n",
      "multiple datasets of both healthy and pathological human brains. Results\n",
      "highlight three key benefits of uncertainty modelling for improving the safety\n",
      "of DL-based image enhancement systems. Firstly, incorporating uncertainty\n",
      "improves the predictive performance even when test data departs from training\n",
      "data. Secondly, the predictive uncertainty highly correlates with errors, and\n",
      "is therefore capable of detecting predictive \"failures\". Results demonstrate\n",
      "that such an uncertainty measure enables subject-specific and voxel-wise risk\n",
      "assessment of the output images. Thirdly, we show that the method for\n",
      "decomposing predictive uncertainty into its independent sources provides\n",
      "high-level \"explanations\" for the performance by quantifying how much\n",
      "uncertainty arises from the inherent difficulty of the task or the limited\n",
      "training examples. \n",
      "\n",
      "\n",
      "Severe color casts, low contrast and blurriness of underwater images caused\n",
      "by light absorption and scattering result in a difficult task for exploring\n",
      "underwater environments. Different from most of previous underwater image\n",
      "enhancement methods that compute light attenuation along object-camera path\n",
      "through hazy image formation model, we propose a novel jointly wavelength\n",
      "compensation and dehazing network (JWCDN) that takes into account the\n",
      "wavelength attenuation along surface-object path and the scattering along\n",
      "object-camera path simultaneously. By embedding a simplified underwater\n",
      "formation model into generative adversarial network, we can jointly estimates\n",
      "the transmission map, wavelength attenuation and background light via different\n",
      "network modules, and uses the simplified underwater image formation model to\n",
      "recover degraded underwater images. Especially, a multi-scale densely connected\n",
      "encoder-decoder network is proposed to leverage features from multiple layers\n",
      "for estimating the transmission map. To further improve the recovered image, we\n",
      "use an edge preserving network module to enhance the detail of the recovered\n",
      "image. Moreover, to train the proposed network, we propose a novel underwater\n",
      "image synthesis method that generates underwater images with inherent optical\n",
      "properties of different water types. The synthesis method can simulate the\n",
      "color, contrast and blurriness appearance of real-world underwater environments\n",
      "simultaneously. Extensive experiments on synthetic and real-world underwater\n",
      "images demonstrate that the proposed method yields comparable or better results\n",
      "on both subjective and objective assessments, compared with several\n",
      "state-of-the-art methods. \n",
      "\n",
      "\n",
      "Medical imaging plays a critical role in various clinical applications.\n",
      "However, due to multiple considerations such as cost and risk, the acquisition\n",
      "of certain image modalities could be limited. To address this issue, many\n",
      "cross-modality medical image synthesis methods have been proposed. However, the\n",
      "current methods cannot well model the hard-to-synthesis regions (e.g., tumor or\n",
      "lesion regions). To address this issue, we propose a simple but effective\n",
      "strategy, that is, we propose a dual-discriminator (dual-D) adversarial\n",
      "learning system, in which, a global-D is used to make an overall evaluation for\n",
      "the synthetic image, and a local-D is proposed to densely evaluate the local\n",
      "regions of the synthetic image. More importantly, we build an adversarial\n",
      "attention mechanism which targets at better modeling hard-to-synthesize regions\n",
      "(e.g., tumor or lesion regions) based on the local-D. Experimental results show\n",
      "the robustness and accuracy of our method in synthesizing fine-grained target\n",
      "images from the corresponding source images. In particular, we evaluate our\n",
      "method on two datasets, i.e., to address the tasks of generating T2 MRI from T1\n",
      "MRI for the brain tumor images and generating MRI from CT. Our method\n",
      "outperforms the state-of-the-art methods under comparison in all datasets and\n",
      "tasks. And the proposed difficult-region-aware attention mechanism is also\n",
      "proved to be able to help generate more realistic images, especially for the\n",
      "hard-to-synthesize regions. \n",
      "\n",
      "\n",
      "One of the major obstacles in automatic polyp detection during colonoscopy is\n",
      "the lack of labeled polyp training images. In this paper, we propose a\n",
      "framework of conditional adversarial networks to increase the number of\n",
      "training samples by generating synthetic polyp images. Using a normal binary\n",
      "form of polyp mask which represents only the polyp position as an input\n",
      "conditioned image, realistic polyp image generation is a difficult task in a\n",
      "generative adversarial networks approach. We propose an edge filtering-based\n",
      "combined input conditioned image to train our proposed networks. This enables\n",
      "realistic polyp image generations while maintaining the original structures of\n",
      "the colonoscopy image frames. More importantly, our proposed framework\n",
      "generates synthetic polyp images from normal colonoscopy images which have the\n",
      "advantage of being relatively easy to obtain. The network architecture is based\n",
      "on the use of multiple dilated convolutions in each encoding part of our\n",
      "generator network to consider large receptive fields and avoid many\n",
      "contractions of a feature map size. An image resizing with convolution for\n",
      "upsampling in the decoding layers is considered to prevent artifacts on\n",
      "generated images. We show that the generated polyp images are not only\n",
      "qualitatively realistic but also help to improve polyp detection performance. \n",
      "\n",
      "\n",
      "Phantoms for surgical training are able to mimic cutting and suturing\n",
      "properties and patient-individual shape of organs, but lack a realistic visual\n",
      "appearance that captures the heterogeneity of surgical scenes. In order to\n",
      "overcome this in endoscopic approaches, hyperrealistic concepts have been\n",
      "proposed to be used in an augmented reality-setting, which are based on deep\n",
      "image-to-image transformation methods. Such concepts are able to generate\n",
      "realistic representations of phantoms learned from real intraoperative\n",
      "endoscopic sequences. Conditioned on frames from the surgical training process,\n",
      "the learned models are able to generate impressive results by transforming\n",
      "unrealistic parts of the image (e.g.\\ the uniform phantom texture is replaced\n",
      "by the more heterogeneous texture of the tissue). Image-to-image synthesis\n",
      "usually learns a mapping $G:X~\\to~Y$ such that the distribution of images from\n",
      "$G(X)$ is indistinguishable from the distribution $Y$. However, it does not\n",
      "necessarily force the generated images to be consistent and without artifacts.\n",
      "In the endoscopic image domain this can affect depth cues and stereo\n",
      "consistency of a stereo image pair, which ultimately impairs surgical vision.\n",
      "We propose a cross-domain conditional generative adversarial network approach\n",
      "(GAN) that aims to generate more consistent stereo pairs. The results show\n",
      "substantial improvements in depth perception and realism evaluated by 3 domain\n",
      "experts and 3 medical students on a 3D monitor over the baseline method. In 84\n",
      "of 90 instances our proposed method was preferred or rated equal to the\n",
      "baseline. \n",
      "\n",
      "\n",
      "Skin lesion segmentation is a vital task in skin cancer diagnosis and further\n",
      "treatment. Although deep learning based approaches have significantly improved\n",
      "the segmentation accuracy, these algorithms are still reliant on having a large\n",
      "enough dataset in order to achieve adequate results. Inspired by the immense\n",
      "success of generative adversarial networks (GANs), we propose a GAN-based\n",
      "augmentation of the original dataset in order to improve the segmentation\n",
      "performance. In particular, we use the segmentation masks available in the\n",
      "training dataset to train the Mask2Lesion model, and use the model to generate\n",
      "new lesion images given any arbitrary mask, which are then used to augment the\n",
      "original training dataset. We test Mask2Lesion augmentation on the ISBI ISIC\n",
      "2017 Skin Lesion Segmentation Challenge dataset and achieve an improvement of\n",
      "5.17% in the mean Dice score as compared to a model trained with only classical\n",
      "data augmentation techniques. \n",
      "\n",
      "\n",
      "Thanks to the recent success of generative adversarial network (GAN) for\n",
      "image synthesis, there are many exciting GAN approaches that successfully\n",
      "synthesize MR image contrast from other images with different contrasts. These\n",
      "approaches are potentially important for image imputation problems, where\n",
      "complete set of data is often difficult to obtain and image synthesis is one of\n",
      "the key solutions for handling the missing data problem. Unfortunately, the\n",
      "lack of the scalability of the existing GAN-based image translation approaches\n",
      "poses a fundamental challenge to understand the nature of the MR contrast\n",
      "imputation problem: which contrast does matter? Here, we present a systematic\n",
      "approach using Collaborative Generative Adversarial Networks (CollaGAN), which\n",
      "enable the learning of the joint image manifold of multiple MR contrasts to\n",
      "investigate which contrasts are essential. Our experimental results showed that\n",
      "the exogenous contrast from contrast agents is not replaceable, but other\n",
      "endogenous contrast such as T1, T2, etc can be synthesized from other contrast.\n",
      "These findings may give important guidance to the acquisition protocol design\n",
      "for MR in real clinical environment. \n",
      "\n",
      "\n",
      "We present SR3, an approach to image Super-Resolution via Repeated\n",
      "Refinement. SR3 adapts denoising diffusion probabilistic models to conditional\n",
      "image generation and performs super-resolution through a stochastic denoising\n",
      "process. Inference starts with pure Gaussian noise and iteratively refines the\n",
      "noisy output using a U-Net model trained on denoising at various noise levels.\n",
      "SR3 exhibits strong performance on super-resolution tasks at different\n",
      "magnification factors, on faces and natural images. We conduct human evaluation\n",
      "on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\n",
      "GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\n",
      "outputs, while GANs do not exceed a fool rate of 34%. We further show the\n",
      "effectiveness of SR3 in cascaded image generation, where generative models are\n",
      "chained with super-resolution models, yielding a competitive FID score of 11.3\n",
      "on ImageNet. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for abst in result_df[result_df.primary_category==\"eess.IV\"][\"abstract\"]:\n",
    "    print(abst,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e2bff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5d138",
   "metadata": {},
   "source": [
    "- \"deepfake\"\n",
    "- '\"deep fake\"'\n",
    "- '\"fake video\"'\n",
    "- '\"video forgery\"'\n",
    "- '\"image generation\" \"face\"'\n",
    "\n",
    "?? \"video editing\"\n",
    "?? \"image inpainting\"\n",
    "\n",
    "ARXIV time bias!!!!!\n",
    "\n",
    "\n",
    "Arxiv: image synthesis kulcsszó CS-re szűrve jó lehet!\n",
    "\n",
    "\" használata harmadolja a találatokat\n",
    "\n",
    "\n",
    "https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=image+synthesis&terms-0-field=all&classification-computer_science=y&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e862d0",
   "metadata": {},
   "source": [
    "#!pip install pmdarima\n",
    "from pmdarima.arima import auto_arima\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "publish_df = pd.to_datetime(publish_dates).to_frame().resample(\"MS\").count()\n",
    "\n",
    "#publish_df = publish_df[publish_df.index.year >= 2016]\n",
    "\n",
    "\n",
    "model = auto_arima(publish_df, start_p=0, start_q=0)\n",
    "#model.fit(df)\n",
    "\n",
    "def forecast_to_df(model, steps=10):\n",
    "    forecast, conf_int = model.predict(n_periods=steps, return_conf_int=True)\n",
    "    pred_df=pd.DataFrame()\n",
    "    pred_df[\"lower\"] = conf_int[:,0]\n",
    "    pred_df[\"upper\"] = conf_int[:,1]\n",
    "    pred_df[\"pred\"] = forecast.values\n",
    "    return pred_df\n",
    "\n",
    "STEPS = 60\n",
    "\n",
    "pred_df = forecast_to_df(model, steps=STEPS)\n",
    "\n",
    "print(model)\n",
    "print(publish_df.index[-1])\n",
    "\n",
    "pred_df.set_index(pd.date_range(start=publish_df.index[-1],periods=STEPS, freq=\"MS\"), inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(22,7))\n",
    "ax.plot(publish_df[publish_df.index.year >= 2016],label='Monthly publication frequencies')\n",
    "ax.plot(pred_df, label=['Lower dound',\"Upper bound\",'Projection mean'])\n",
    "ax.fill_between(x=pred_df.index,y1=pred_df['lower'],y2=pred_df['upper'],alpha=0.3)\n",
    "ax.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
